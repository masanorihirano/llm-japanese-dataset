[
  {
    "index": "F15000",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、x2の二乗、x2x3、などと続いていく。",
    "output": "There would be terms like x1 squared, x1x2, x1x3, you know, x1x4 up to x1x100 and then you have x2 squared, x2x3 and so on."
  },
  {
    "index": "F15001",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして二次の項だけを含めたとしても、つまり、これらの2つの積の項、つまりx1掛けるx1などで、そしてnが100の場合だと、結局は約5000程のフィーチャーとなる。",
    "output": "And if you include just the second order terms, that is, the terms that are a product of, you know, two of these terms, x1 times x1 and so on, then, for the case of n equals 100, you end up with about five thousand features."
  },
  {
    "index": "F15002",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして漸近的に、二次のフィーチャーの数はオーダーnの二乗で成長する。ここでnはもとのフィーチャーの数。",
    "output": "And, asymptotically, the number of quadratic features grows roughly as order n squared, where n is the number of the original features, like x1 through x100 that we had."
  },
  {
    "index": "F15003",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たとえばx1からx100まであった訳だが、その場合は実際nの二乗割る2に近かった。",
    "output": "And its actually closer to n squared over two."
  },
  {
    "index": "F15004",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、すべての2次式フィーチャーを含めるのはよいアイデアではなさそうだ。",
    "output": "So including all the quadratic features doesn't seem like it's maybe a good idea, because that is a lot of features and you might up overfitting the training set, and it can also be computationally expensive, you know, to be working with that many features."
  },
  {
    "index": "F15005",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また、そのようなたくさんのフィーチャーを扱うと、計算コストが高くなりうる。",
    "output": "One thing you could do is include only a subset of these, so if you include only the features x1 squared, x2 squared, x3 squared, up to maybe x100 squared, then the number of features is much smaller."
  },
  {
    "index": "F15006",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つできることはこれらのサブセットのみを入れることだ。",
    "output": "Here you have only 100 such quadratic features, but this is not enough features and certainly won't let you fit the data set like that on the upper left."
  },
  {
    "index": "F15007",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとフィーチャーの数はかなり少なくなる。そのような2次式フィーチャーが100個だけあるとして、これは不十分なフィーチャーであり左上のようなデータセットにはまずフィットさせられないだろう。",
    "output": "In fact, if you include only these quadratic features together with the original x1, and so on, up to x100 features, then you can actually fit very interesting hypotheses."
  },
  {
    "index": "F15008",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実のところ、これらの2次式フィーチャーと共に元々のx1等々からx100までを入れるだけでも、かなり興味深い仮説をフィットさせることは出来る。",
    "output": "So, you can fit things like, you know, access a line of the ellipses like these, but you certainly cannot fit a more complex data set like that shown here."
  },
  {
    "index": "F15009",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば楕円の仮説、例えばこんなのとかにはフィットさせる事が出来る。だが、ここに示したようなより複雑なデータにはフィットさせられない。",
    "output": "So 5000 features seems like a lot, if you were to include the cubic, or third order known of each others, the x1, x2, x3."
  },
  {
    "index": "F15010",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "5000のフィーチャーと聞くと凄いたくさんに感じるだろうが、三乗の項を含めるとつまり三次の多項式を含めると、x1x2x3とか、x1の二乗掛けるx2、x10x11x17などなど。",
    "output": "You know, x1 squared, x2, x10 and x11, x17 and so on."
  },
  {
    "index": "F15011",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんなフィーチャーがたくさんになるのは想像出来るだろう。",
    "output": "You can imagine there are gonna be a lot of these features."
  },
  {
    "index": "F15012",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから元々のフィーチャーセットが大きいときこれらの高次の多項式フィーチャーを含めることは、実に劇的にフィーチャー空間を膨張させる。nが大きいときに非線形の分類器を作るのに、追加のフィーチャーを用意することはよい方法ではなさそうだ。",
    "output": "In fact, they are going to be order and cube such features and if any is 100 you can compute that, you end up with on the order of about 170,000 such cubic features and so including these higher auto-polynomial features when your original feature set end is large this really dramatically blows up your feature space and this doesn't seem like a good way to come up with additional features with which to build none many classifiers when n is large."
  },
  {
    "index": "F15013",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多くの機械学習問題にとって、nはかなり大きいだろう。",
    "output": "For many machine learning problems, n will be pretty large."
  },
  {
    "index": "F15014",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コンピュータビジョンの問題について考える。",
    "output": "Let's consider the problem of computer vision."
  },
  {
    "index": "F15015",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "機械学習を使って分類器を学習させたいとする。画像を調べ、それが車かどうかを判定するものだ。",
    "output": "And suppose you want to use machine learning to train a classifier to examine an image and tell us whether or not the image is a car."
  },
  {
    "index": "F15016",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多くの人は、コンピュータビジョンがどうして難しいのかと疑問に思う。",
    "output": "Many people wonder why computer vision could be difficult."
  },
  {
    "index": "F15017",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばあなたや私がこの写真を見ると、これが何であるかはとても明白だ。",
    "output": "I mean when you and I look at this picture it is so obvious what this is."
  },
  {
    "index": "F15018",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうして学習アルゴリズムがこの写真が何であるかを判定し損ねるだろうかと思うだろう。",
    "output": "You wonder how is it that a learning algorithm could possibly fail to know what this picture is."
  },
  {
    "index": "F15019",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コンピュータビジョンが難しい理由を理解するために、画像の小さな部分を拡大してみよう。この小さな赤い矩形の領域だ。",
    "output": "To understand why computer vision is hard let's zoom into a small part of the image like that area where the little red rectangle is."
  },
  {
    "index": "F15020",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたや私が車を見るとき、コンピュータは実はこのように見ている。",
    "output": "It turns out that where you and I see a car, the computer sees that."
  },
  {
    "index": "F15021",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コンピュータが見ているのはピクセル輝度値のマトリックスあるいはグリッドで、画像の各ピクセルの明るさを表している。",
    "output": "What it sees is this matrix, or this grid, of pixel intensity values that tells us the brightness of each pixel in the image."
  },
  {
    "index": "F15022",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりコンピュータビジョン問題はこのようなピクセル輝度値のマトリックスを見て、数値が車のドアハンドルを表しているとわかることだ。",
    "output": "So the computer vision problem is to look at this matrix of pixel intensity values, and tell us that these numbers represent the door handle of a car."
  },
  {
    "index": "F15023",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、我々が機械学習を使って車の検出器を作るときに行うのは、分類トレーニングセットを考えること、例えばそれは、車の分類例をいくつか含み、そして車ではない物の例をいくつか含む。そしてそのトレーニングセットを学習アルゴリズムに与え分類器を学習させる。",
    "output": "Concretely, when we use machine learning to build a car detector, what we do is we come up with a label training set, with, let's say, a few label examples of cars and a few label examples of things that are not cars, then we give our training set to the learning algorithm trained a classifier and then, you know, we may test it and show the new image and ask, \"What is this new thing?\"."
  },
  {
    "index": "F15024",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "」と尋ね、上手くいけば検出器はそれが車であると認識する。",
    "output": "And hopefully it will recognize that that is a car."
  },
  {
    "index": "F15025",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうして非線形の仮説が必要かを理解するために、我々が学習アルゴリズムに与えるであろう車の画像や車でない画像をいくつか見てみよう。",
    "output": "To understand why we need nonlinear hypotheses, let's take a look at some of the images of cars and maybe non-cars that we might feed to our learning algorithm."
  },
  {
    "index": "F15026",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "画像の中から一対のピクセルを選んで、ピクセル1の座標、ピクセル2の座標とする。そしてこの車を特定の位置にプロットする。",
    "output": "Let's pick a couple of pixel locations in our images, so that's pixel one location and pixel two location, and let's plot this car, you know, at the location, at a certain point, depending on the intensities of pixel one and pixel two."
  },
  {
    "index": "F15027",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他の画像についてもいくつかやってみよう。",
    "output": "And let's do this with a few other images."
  },
  {
    "index": "F15028",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "車の別の例をとると、同じ2つのピクセル座標を見て、この画像はピクセル1が異なる輝度を持ち、ピクセル2も別の輝度を持っている。",
    "output": "So let's take a different example of the car and you know, look at the same two pixel locations and that image has a different intensity for pixel one and a different intensity for pixel two."
  },
  {
    "index": "F15029",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのため結局図上の異なる位置に置かれる。",
    "output": "So, it ends up at a different location on the figure."
  },
  {
    "index": "F15030",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に陰性の例もいくつか同様にプロットしてみよう。",
    "output": "And then let's plot some negative examples as well."
  },
  {
    "index": "F15031",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "プラスは車を表しマイナスは車でない物を表す。",
    "output": "That's a non-car, that's a non-car ."
  },
  {
    "index": "F15032",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最終的にわかるのは車と車でない物が空間内の異なる領域に分布しているということで、我々に必要なのは2つのクラスを分離しようとする何らかの非線形の仮説だ。",
    "output": "And if we do this for more and more examples using the pluses to denote cars and minuses to denote non-cars, what we'll find is that the cars and non-cars end up lying in different regions of the space, and what we need therefore is some sort of non-linear hypotheses to try to separate out the two classes."
  },
  {
    "index": "F15033",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フィーチャー空間の次元は何だろうか?",
    "output": "What is the dimension of the feature space?"
  },
  {
    "index": "F15034",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "仮にたった50x50のピクセル画像を使うとしよう。",
    "output": "Suppose we were to use just 50 by 50 pixel images."
  },
  {
    "index": "F15035",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "かなり小さな画像、一辺が50ピクセルしかない画像を想定する。",
    "output": "Now that suppose our images were pretty small ones, just 50 pixels on the side."
  },
  {
    "index": "F15036",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると2500ピクセルあることになるので、フィーチャーサイズの次元はn=2500、ここでフィーチャーベクトルxはすべてのピクセル検査のリストだ。これはピクセル1の明るさ、ピクセル2の明るさ、等々から最後のピクセルの明るさまでのことで、典型的なコンピュータ表現では各々は、グレイスケール値の場合例えば0から255までの値となるだろう。",
    "output": "Then we would have 2500 pixels, and so the dimension of our feature size will be N equals 2500 where our feature vector x is a list of all the pixel testings, you know, the pixel brightness of pixel one, the brightness of pixel two, and so on down to the pixel brightness of the last pixel where, you know, in a typical computer representation, each of these may be values between say 0 to 255 if it gives us the grayscale value."
  },
  {
    "index": "F15037",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "RGB画像を使い赤、緑、青を分ける場合は、n=7500となるだろう。",
    "output": "If we were using RGB images with separate red, green and blue values, we would have n equals 7500."
  },
  {
    "index": "F15038",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしすべての2次式フィーチャーを取り込んで、非線形の仮説を学習させようとしたらつまり、XiかけるXjの形のすべての項を取り込み、それが2500ピクセルあったら結局合計で300万ピクセルになる。",
    "output": "So, if we were to try to learn a nonlinear hypothesis by including all the quadratic features, that is all the terms of the form, you know, Xi times Xj, while with the 2500 pixels we would end up with a total of three million features."
  },
  {
    "index": "F15039",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1つのトレーニング例当たりに、これら300万フィーチャーのすべてを見つけて、表現する計算は非常に高くつくだろう。",
    "output": "And that's just too large to be reasonable; the computation would be very expensive to find and to represent all of these three million features per training example."
  },
  {
    "index": "F15040",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、単純なロジスティック回帰に2次や3次のフィーチャーを加えたもの-これはnが大きい時に非線形の仮説を学習させるのにはまったく向いていない。フィーチャーが多くなりすぎるからだ。",
    "output": "So, simple logistic regression together with adding in maybe the quadratic or the cubic features - that's just not a good way to learn complex nonlinear hypotheses when n is large because you just end up with too many features."
  },
  {
    "index": "F15041",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以降のいくつかのビデオでは、ニューラルネットワークについて教えよう。これは複雑な仮説、複雑な非線形の仮説を学習させるのにずっとよい方法だということがわかる。",
    "output": "In the next few videos, I would like to tell you about Neural Networks, which turns out to be a much better way to learn complex hypotheses, complex nonlinear hypotheses even when your input feature space, even when n is large."
  },
  {
    "index": "F15042",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてついでにニューラルネットワークの歴史的に重要な応用についてのおもしろい動画をいくつかお見せしよう。あなたも後ほど見るこれらの動画をおもしろいと思ってくれるといいが。",
    "output": "And along the way I'll also get to show you a couple of fun videos of historically important applications of Neural networks as well that I hope those videos that we'll see later will be fun for you to watch as well."
  },
  {
    "index": "F15043",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークは極めて古いアルゴリズムで、もともとそれは脳を模倣する機械を得る事を目標としていた。",
    "output": "Neural Networks are a pretty old algorithm that was originally motivated by the goal of having machines that can mimic the brain."
  },
  {
    "index": "F15044",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このクラスではもちろん、ニューラルネットワークを教えるのは異なる種類の機械学習の問題で、それがとても役立つからで、論理的な動機づけによる、という訳では無い。",
    "output": "Now in this class, of course I'm teaching Neural Networks to you because they work really well for different machine learning problems and not, certainly not because they're logically motivated."
  },
  {
    "index": "F15045",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこのビデオではニューラルネットワークのある程度の背景知識を教えたい、それらが何をしてくれるか、の感覚を養うために。",
    "output": "In this video, I'd like to give you some of the background on Neural Networks. So that we can get a sense of what we can expect them to do."
  },
  {
    "index": "F15046",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんにち的な機械学習野問題に適用する、という意味でも、あなたがたの中に将来の大きなAIの夢を見る人々がいつの日か真の知的な機械を作る為にも。",
    "output": "Both in the sense of applying them to modern day machinery problems, as well as for those of you that might be interested in maybe the big AI dream of someday building truly intelligent machines."
  },
  {
    "index": "F15047",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークがそれにどのように関係するのかも。",
    "output": "Also, how Neural Networks might pertain to that."
  },
  {
    "index": "F15048",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークの起源は脳を模倣しようとしたアルゴリズムにあり、それは理解も出来る。学習システムを作ろう、と思ったら何故この、恐らくもっとも驚くべき学習する機械である所の、この脳を模倣しないというのか?",
    "output": "The origins of Neural Networks was as algorithms that try to mimic the brain and those a sense that if we want to build learning systems while why not mimic perhaps the most amazing learning machine we know about, which is perhaps the brain."
  },
  {
    "index": "F15049",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いや、する(反語)ニューラルネットワークは1980年代から1990年代にかけて、とても広く使われていた。だが様々な理由で90年代の後半には人気は下火となっていた。",
    "output": "Neural Networks came to be very widely used throughout the 1980's and 1990's and for various reasons as popularity diminished in the late 90's."
  },
  {
    "index": "F15050",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがより最近になって、ニューラルネットワークは大復活を遂げた。",
    "output": "But more recently, Neural Networks have had a major recent resurgence."
  },
  {
    "index": "F15051",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この復活劇の理由の一つにはニューラルネットワークは計算量的になかなか高く付くアルゴリズムだというのがある。だから近年になってようやく大規模なニューラルネットワークを走らせるのに十分なくらいコンピュータが速くなった。",
    "output": "One of the reasons for this resurgence is that Neural Networks are computationally some what more expensive algorithm and so, it was only, you know, maybe somewhat more recently that computers became fast enough to really run large scale Neural Networks and because of that as well as a few other technical reasons which we'll talk about later, modern Neural Networks today are the state of the art technique for many applications."
  },
  {
    "index": "F15052",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では脳を模倣しよう、と考えると、人間の脳というのはとてもたくさんの素晴らしい事が出来る、でしょ?",
    "output": "So, when you think about mimicking the brain while one of the human brain does tell me same things, right?"
  },
  {
    "index": "F15053",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "脳は画像処理を学習する事も出来るし、聞き取りも学習出来るし、感触の処理も学習出来る。",
    "output": "The brain can learn to see process images than to hear, learn to process our sense of touch."
  },
  {
    "index": "F15054",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "数学を学ぶ事も出来るし、計算学ぶ事も出来る。脳はそれらたくさんの素晴らしい事が出来る。",
    "output": "We can, you know, learn to do math, learn to do calculus, and the brain does so many different and amazing things."
  },
  {
    "index": "F15055",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんなに素晴らしいのだから、その脳を模倣しようとすればたくさんの異なるソフトウェアを書かなきゃいけなさそうに見える、これら脳が教えてくれる、様々な魅力的で驚くべき物達を模倣する為に。",
    "output": "It seems like if you want to mimic the brain it seems like you have to write lots of different pieces of software to mimic all of these different fascinating, amazing things that the brain tell us, but does is this fascinating hypothesis that the way the brain does all of these different things is not worth like a thousand different programs, but instead, the way the brain does it is worth just a single learning algorithm."
  },
  {
    "index": "F15056",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは単なる仮説だが、この証拠とも取れる事を幾つか共有したい。",
    "output": "This is just a hypothesis but let me share with you some of the evidence for this."
  },
  {
    "index": "F15057",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "脳のこの部分、この小さな赤い部分は聴覚皮質だ。そして今あなたが私の声を理解している方法はあなたの耳が音声信号を拾い上げて、その音声信号をあなたの聴覚皮質へと送り、そしてその聴覚皮質こそが私の言葉を理解してくれる訳だ。",
    "output": "This part of the brain, that little red part of the brain, is your auditory cortex and the way you're understanding my voice now is your ear is taking the sound signal and routing the sound signal to your auditory cortex and that's what's allowing you to understand my words."
  },
  {
    "index": "F15058",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "神経学者は以下のような興味深い実験を行なってきた、それは耳から聴覚皮質への線をカットして、それをつなぎなおすーーこの場合は動物の脳だが、目から視神経へのシグナルを聴覚皮質に送られるように。",
    "output": "Neuroscientists have done the following fascinating experiments where you cut the wire from the ears to the auditory cortex and you re-wire, in this case an animal's brain, so that the signal from the eyes to the optic nerve eventually gets routed to the auditory cortex."
  },
  {
    "index": "F15059",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをやってみると、聴覚皮質は見る事を学習する事が分かった!",
    "output": "If you do this it turns out, the auditory cortex will learn to see."
  },
  {
    "index": "F15060",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは、見る、という言葉から想起される全ての事について言える。",
    "output": "And this is in every single sense of the word see as we know it."
  },
  {
    "index": "F15061",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれを動物に行うと、その動物は視覚による区分のタスクが行え、つまりそれらの動物は画像を見て、それらの画像に基づいて適切な決定を行う事が出来る。そしてそれらは、ここの脳の組織で行うのだ。",
    "output": "So, if you do this to the animals, the animals can perform visual discrimination task and as they can look at images and make appropriate decisions based on the images and they're doing it with that piece of brain tissue."
  },
  {
    "index": "F15062",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここにもう一つ別の例がある。",
    "output": "Here's another example."
  },
  {
    "index": "F15063",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "脳のこの赤い部分は体性感覚皮質だ。",
    "output": "That red piece of brain tissue is your somatosensory cortex."
  },
  {
    "index": "F15064",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同じような再結合処理を行うと体性感覚皮質は見る事を学習する。",
    "output": "If you do a similar re-wiring process then the somatosensory cortex will learn to see. Because of this and other similar experiments, these are called neuro-rewiring experiments."
  },
  {
    "index": "F15065",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これやその他の似たような実験のため、、、これらは神経再接続実験と呼ばれるが、こんな意味で、物理的に同じ脳の組織が視覚や音や触覚を処理出来るのなら、学習アルゴリズムも一つで、視覚や音や触覚を処理出来ているのかもしれない。",
    "output": "There's this sense that if the same piece of physical brain tissue can process sight or sound or touch then maybe there is one learning algorithm that can process sight or sound or touch."
  },
  {
    "index": "F15066",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして何千もの異なるプログラムを実装する代わりにまたは何千もの異なるアルゴリズムを使う代わりに脳が行なっているような、何千もの素晴らしい事をするのに必要なのは、ひょっとしたら脳の学習アルゴリズムが行なっている事の、何らかの近似を見出して、それを実装する事かもしれない。",
    "output": "And instead of needing to implement a thousand different programs or a thousand different algorithms to do, you know, the thousand wonderful things that the brain does, maybe what we need to do is figure out some approximation or to whatever the brain's learning algorithm is and implement that and that the brain learned by itself how to process these different types of data."
  },
  {
    "index": "F15067",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその脳が実際にこれら様々な種類のデータを処理する方法を学習する方法は、驚く程大部分は、どうやらどんな種類のセンサーも脳のほとんどどこにでもつなぐ事が出来るようなのだ。そしてどうやら、そうすると、脳はそれをどう扱うかを学習するらしい。",
    "output": "To a surprisingly large extent, it seems as if we can plug in almost any sensor to almost any part of the brain and so, within the reason, the brain will learn to deal with it."
  },
  {
    "index": "F15068",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここにさらなる例を幾つか用意した。",
    "output": "Here are a few more examples."
  },
  {
    "index": "F15069",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "上部左は、舌で物を見る事を学習する例だ。",
    "output": "On the upper left is an example of learning to see with your tongue."
  },
  {
    "index": "F15070",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがどうなってるかというと、これはBrainPortという、FDAが行なっているシステムで盲目の人が見えるようになるのを手伝うという物。それがどうなってるかというと、おでこにグレースケールのカメラを付けて前に向けて、それが低解像度のグレースケールの像であなたの前にある物を映す。",
    "output": "The way it works is--this is actually a system called BrainPort undergoing FDA trials now to help blind people see--but the way it works is, you strap a grayscale camera to your forehead, facing forward, that takes the low resolution grayscale image of what's in front of you and you then run a wire to an array of electrodes that you place on your tongue so that each pixel gets mapped to a location on your tongue where maybe a high voltage corresponds to a dark pixel and a low voltage corresponds to a bright pixel and, even as it does today, with this sort of system you and I will be able to learn to see, you know, in tens of minutes with our tongues."
  },
  {
    "index": "F15071",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは二番目の例でエコー位置、または人力ソナーだ。",
    "output": "Here's a second example of human echo location or human sonar."
  },
  {
    "index": "F15072",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを行うには2つ方法がある。",
    "output": "So there are two ways you can do this."
  },
  {
    "index": "F15073",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "指を鳴らすか舌を打つかだ。",
    "output": "You can either snap your fingers, or click your tongue."
  },
  {
    "index": "F15074",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが盲目の人がこんにちこれを実際に学校で練習して、環境から音が跳ね返るパターンの解釈を学習している、それがソナーだ。",
    "output": "But there are blind people today that are actually being trained in schools to do this and learn to interpret the pattern of sounds bouncing off your environment - that's sonar."
  },
  {
    "index": "F15075",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "youtubeを検索してみると、実際の動画があるよ。",
    "output": "So, if after you search on YouTube, there are actually videos of this amazing kid who tragically because of cancer had his eyeballs removed, so this is a kid with no eyeballs."
  },
  {
    "index": "F15076",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "悲運にも目玉にガンを患った為目玉を失った子供が、ーーつまりこの子供は目玉が無いのだーーだが指を鳴らして、歩き回って何にもぶつからないで済んでる。",
    "output": "But by snapping his fingers, he can walk around and never hit anything."
  },
  {
    "index": "F15077",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スケートボードにも乗れて、バスケットボールのフープにシュートも出来る。これが目玉の無い子供なのだ。",
    "output": "He can shoot a basketball into a hoop and this is a kid with no eyeballs."
  },
  {
    "index": "F15078",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "三番目の例はHapticBeltで、腰の回りにストラップをつけて、ブサーをたくさんつける、そして一番北のブザーがいつも鳴るようにする。",
    "output": "Third example is the Haptic Belt where if you have a strap around your waist, ring up buzzers and always have the northmost one buzzing."
  },
  {
    "index": "F15079",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると人間に、鳥などが北がどちらかを感じるような方向感覚を身につける事が出来る。",
    "output": "You can give a human a direction sense similar to maybe how birds can, you know, sense where north is."
  },
  {
    "index": "F15080",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてちょっと気味が悪い例としては、カエルに三番目の目をつなげると、カエルはその目をどう使うかを学習する、という物。",
    "output": "And, some of the bizarre example, but if you plug a third eye into a frog, the frog will learn to use that eye as well."
  },
  {
    "index": "F15081",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上は、極めて驚くべきほどじゃないか。ほとんどどんなセンサーを脳につなげても、脳はそのデータから、そのデータをどう扱うかを勝手に学んでいく様が、いかに広い範囲で成立しているかという事は。",
    "output": "So, it's pretty amazing to what extent is as if you can plug in almost any sensor to the brain and the brain's learning algorithm will just figure out how to learn from that data and deal with that data."
  },
  {
    "index": "F15082",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして脳の学習アルゴリズムがどんな物かを見つけ出し、それを実装する、またはその近似でもコンピュータで実装出来れば、それはたぶん、それはAI、人工知能、その夢みる所の、真に知能を持った機械を作るという野望への、とても大きな一歩となりうる。",
    "output": "And there's a sense that if we can figure out what the brain's learning algorithm is, and, you know, implement it or implement some approximation to that algorithm on a computer, maybe that would be our best shot at, you know, making real progress towards the AI, the artificial intelligence dream of someday building truly intelligent machines."
  },
  {
    "index": "F15083",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今回はもちろん、ニューラルネットワークを教えるのはこの遥か彼方のAIの夢への扉を開くから、という訳では無い。とは言うものの、私は個人的にはその夢が私の研究人生で追求している物の一つだが。",
    "output": "Now, of course, I'm not teaching Neural Networks, you know, just because they might give us a window into this far-off AI dream, even though I'm personally, that's one of the things that I personally work on in my research life."
  },
  {
    "index": "F15084",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこのクラスで私がニューラルネットワークを教える主な理由はそれは実際にとても有効なステートオブジアートな、こんにちの機械学習への応用となっているからだ。",
    "output": "But the main reason I'm teaching Neural Networks in this class is because it's actually a very effective state of the art technique for modern day machine learning applications."
  },
  {
    "index": "F15085",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから今後の一連のビデオで、ニューラルネットワークの技術的詳細に入っていく。あなたがこんにち的な機械学習の応用が出来るようになって、問題に対してうまく機能させられるようになる為に。",
    "output": "So, in the next few videos, we'll start diving into the technical details of Neural Networks so that you can apply them to modern-day machine learning applications and get them to work well on problems."
  },
  {
    "index": "F15086",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが私にとっては、それがとってもエキサイトな理由は、たぶんそれらが将来には、人間のように学習するアルゴリズムとはどんなものか、を想像させる扉を開いてくれる事なのかもしれないけどね。",
    "output": "But for me, you know, one of the reasons the excite me is that maybe they give us this window into what we might do if we're also thinking of what algorithms might someday be able to learn in a manner similar to humankind."
  },
  {
    "index": "F15087",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、ニューラルネットワークをどう表現するかについて話す。",
    "output": "In this video, I want to start telling you about how we represent neural networks."
  },
  {
    "index": "F15088",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、我らの仮説をどう表現するか、または、ニューラルネットワークを使う時はどうモデルを表現するか、という事。",
    "output": "In other words, how we represent our hypothesis or how we represent our model when using neural networks."
  },
  {
    "index": "F15089",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークは脳内のニューロン、またはニューロンのネットワークをシミュレートする事で発展した。",
    "output": "Neural networks were developed as simulating neurons or networks of neurons in the brain."
  },
  {
    "index": "F15090",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから仮説の表現を説明するために、脳の中のニューロンを一つ取り出すとどんな感じか見てみよう。",
    "output": "So, to explain the hypothesis representation let's start by looking at what a single neuron in the brain looks like."
  },
  {
    "index": "F15091",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたや私の脳は、こんなニューロンがたくさんごちゃごちゃ詰まっている。ニューロンとは脳の細胞で特徴的なのは2つある。",
    "output": "Your brain and mine is jam packed full of neurons like these and neurons are cells in the brain."
  },
  {
    "index": "F15092",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ目はニューロンには細胞体があり、ーこんな感じのーで、さらに、ニューロンは幾つかの入力のワイヤがある事で、これらはdendriteと呼ばれていて、それは入力のワイヤと考えられる。",
    "output": "The neuron has a cell body, like so, and moreover, the neuron has a number of input wires, and these are called the dendrites."
  },
  {
    "index": "F15093",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらが他の場所からの入力を受け取る。",
    "output": "You think of them as input wires, and these receive inputs from other locations."
  },
  {
    "index": "F15094",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてニューロンは出力ワイヤも持っていて、それはaxonと呼ばれる。そしてこの出力ワイヤは他のニューロンにシグナルを送るのに、言い換えると他のニューロンにメッセージを送るのに使われる。",
    "output": "And a neuron also has an output wire called an Axon, and this output wire is what it uses to send signals to other neurons, so to send messages to other neurons."
  },
  {
    "index": "F15095",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり一番単純なレベルでは、ニューロンとは何かというと、たくさんの入力を入力ワイヤから受け取りなんらかの計算を行い、その出力をaxonを通して脳内の他のニューロンに送る計算ユニットと考えることが出来る。",
    "output": "So, at a simplistic level what a neuron is, is a computational unit that gets a number of inputs through it input wires and does some computation and then it says outputs via its axon to other nodes or to other neurons in the brain."
  },
  {
    "index": "F15096",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはニューロンのグループのイラストだ。",
    "output": "Here's a illustration of a group of neurons."
  },
  {
    "index": "F15097",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューロンがお互いにコミュニケートする方法は僅かな電気信号によってだ。それらはスパイクとも呼ばれている。",
    "output": "The way that neurons communicate with each other is with little pulses of electricity, they are also called spikes but that just means pulses of electricity."
  },
  {
    "index": "F15098",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だけどそれらは単なる小さな電気を意味するに過ぎない。ここに一つニューロンがあり、それがする事といえば、もしメッセージを送りたい時はaxonを通して、別のニューロンにわずかな電気のパルスを送る。",
    "output": "So here is one neuron and what it does is if it wants a send a message what it does is sends a little pulse of electricity."
  },
  {
    "index": "F15099",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれがaxonだ。この出力ワイヤがあって、それがこうして、二番目のニューロンの入力ワイヤ、またの名をdendriteにつながっている。",
    "output": "Varis axon to some different neuron and here, this axon that is this open wire, connects to the dendrites of this second neuron over here, which then accepts this incoming message that some computation."
  },
  {
    "index": "F15100",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそこからこの信号に入力のメッセージとして受け取って、なんらかの計算を行い、またさらに他のニューロンへ出力メッセージをaxonを通して送るかもしれない。",
    "output": "And they, in turn, decide to send out this message on this axon to other neurons, and this is the process by which all human thought happens."
  },
  {
    "index": "F15101",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、全ての人類が考えた時に起こるプロセスで、これらのニューロンが計算をして、メッセージを他のニューロンに送る。与えられた入力に対する結果として。",
    "output": "It's these Neurons doing computations and passing messages to other neurons as a result of what other inputs they've got."
  },
  {
    "index": "F15102",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、これはまた、我らの感覚や筋肉が機能する方法でもある。",
    "output": "And, by the way, this is how our senses and our muscles work as well."
  },
  {
    "index": "F15103",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし筋肉の一つを動かそうとすれば、それが実現されるのは、ニューロンが筋肉に電気のパルスを送り、それが筋肉を収縮させたり、目の場合、何らかのセンサー、例えば目とかの場合、脳にメッセージを送るには電気のパルスを脳にあるニューロンに送るという手段を通してだ。",
    "output": "If you want to move one of your muscles the way that where else in your neuron may send this electricity to your muscle and that causes your muscles to contract and your eyes, some senses like your eye must send a message to your brain while it does it senses hosts electricity entity to a neuron in your brain like so."
  },
  {
    "index": "F15104",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークにおいては、いや我らがコンピュータで実装する人工的なニューラルネットワークにおいては、と言うべきか、その場合、ニューロンがやってる事のとてもシンプルなモデルを使う事になる。ニューロンを単なるロジスティックの単位としてモデル化する。",
    "output": "In a neuro network, or rather, in an artificial neuron network that we've implemented on the computer, we're going to use a very simple model of what a neuron does we're going to model a neuron as just a logistic unit."
  },
  {
    "index": "F15105",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから黄色で円をこんな感じで描いたら、これはニューロンの本体みたいな役割をしている、と考えてくれ。そしてそこに、いくつかの入力をdendritesまたの名を入力ワイヤを通して食わす。",
    "output": "So, when I draw a yellow circle like that, you should think of that as a playing a role analysis, who's maybe the body of a neuron, and we then feed the neuron a few inputs who's various dendrites or input wiles."
  },
  {
    "index": "F15106",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとニューロンはなんらかの計算行い、この出力ワイヤからなんらかの値を出力する。生物的なニューロンならそれはaxonに相当する。",
    "output": "And output some value on this output wire, or in the biological neuron, this is an axon."
  },
  {
    "index": "F15107",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこんなダイアグラムを描いた時はいつでも、これの意味する所は、h(x)の計算であり、それは1足すeのマイナスシータ転置x分の1で、xとシータはいつも通り、パラメータベクトルとかを表している。",
    "output": "And whenever I draw a diagram like this, what this means is that this represents a computation of h of x equals one over one plus e to the negative theta transpose x, where as usual, x and theta are our parameter vectors, like so."
  },
  {
    "index": "F15108",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれはとても単純化した、ちょっとあまりにも単純化しすぎた感じのニューロンが行なっている事のモデルでいくつかの入力、x1、x2、x3を受け取りそんな風に計算された何らか値を出力する、という。",
    "output": "So this is a very simple, maybe a vastly oversimplified model, of the computations that the neuron does, where it gets a number of inputs, x1, x2, x3 and it outputs some value computed like so."
  },
  {
    "index": "F15109",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークを書く時は普通は入力のノードだけをx1、x2、x3と描くのだが、たまに、そちらの方が便利な時に限り追加のノード、x0を描く事もある。",
    "output": "When I draw a neural network, usually I draw only the input nodes x1, x2, x3."
  },
  {
    "index": "F15110",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがx0は1と決まっているので、このノードは描いたり描かなかったりする。",
    "output": "Sometimes when it's useful to do so, I'll draw an extra node for x0."
  },
  {
    "index": "F15111",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは単に、その例にとって記しておいた方が便利かどうかで決めてる。最後に、もう一つだけ用語を導入しておく。",
    "output": "This x0 now that's sometimes called the bias unit or the bias neuron, but because x0 is already equal to 1, sometimes, I draw this, sometimes I won't just depending on whatever is more notationally convenient for that example."
  },
  {
    "index": "F15112",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークについて話してる時は、このニューロン、人工的なニューロンのsigmoid関数またはロジスティック関数を、アクティベーション関数と呼ぶ事がある。",
    "output": "Finally, one last bit of terminology when we talk about neural networks, sometimes we'll say that this is a neuron or an artificial neuron with a Sigmoid or logistic activation function. So this activation function in the neural network terminology."
  },
  {
    "index": "F15113",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このアクティベーション関数というのはニューラルネットワークの用語でこれはこの非線形のg(z)イコール1足すeの-z分の1の、もう一つの呼び名に過ぎない。",
    "output": "This is just another term for that function for that non-linearity g(z) = 1 over 1+e to the -z."
  },
  {
    "index": "F15114",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方でここまではシータをモデルのパラメータと呼んできたし、今後もだいたいはその用語を使い続けるが、ニューラルネットワークではニューラルネットワークの文献では人々はたまにモデルのウェイトと呼んでいるのを見かけるかもしれない。",
    "output": "And whereas so far I've been calling theta the parameters of the model, I'll mostly continue to use that terminology."
  },
  {
    "index": "F15115",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このウェイトというのはこのモデルのパラメータと完全に同じ意味だ。",
    "output": "Here, it's a copy to the parameters, but in neural networks, in the neural network literature sometimes you might hear people talk about weights of a model and weights just means exactly the same thing as parameters of a model."
  },
  {
    "index": "F15116",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このクラスのビデオではだいたいパラメータという用語を使うが、たまに他の人がウェイトって用語を使ってるのを聞くことがあるかもしれない。",
    "output": "But I'll mostly continue to use the terminology parameters in these videos, but sometimes, you might hear others use the weights terminology."
  },
  {
    "index": "F15117",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このちっぽけなダイアグラムは単体のニューロンを表している。",
    "output": "So, this little diagram represents a single neuron."
  },
  {
    "index": "F15118",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークとは単にこれらのニューロンが幾つか集まったグループの事だ。",
    "output": "What a neural network is, is just a group of this different neurons strong together."
  },
  {
    "index": "F15119",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、ここに我らの入力単位、x1、x2、x3があり、繰り返しになるがたまにこの追加のx0のノードを描いたり描かなかったりする。",
    "output": "Completely, here we have input units x1, x2, x3 and once again, sometimes you can draw this extra note x0 and Sometimes not, just flow that in here."
  },
  {
    "index": "F15120",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここに、我らは3つのニューロンを持っている。",
    "output": "And here we have three neurons which have written 81, 82, 83. I'll talk about those indices later."
  },
  {
    "index": "F15121",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして繰り返すが、もし必要ならこのa0という追加のバイアスユニットをここに足す事もある。",
    "output": "And once again we can if we want add in just a0 and add the mixture bias unit there."
  },
  {
    "index": "F15122",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、この三番目のノードが最後のレイヤーにある。そしてこの三番目のノードが仮説であるh(x)の計算結果を出力する。",
    "output": "And then finally we have this third node and the final layer, and there's this third node that outputs the value that the hypothesis h(x) computes."
  },
  {
    "index": "F15123",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この最初のレイヤーは入力レイヤーとも呼ばれる。何故ならこれが我らのフィーチャーであるx1、x2、x3をインプットする所だから。",
    "output": "To introduce a bit more terminology, in a neural network, the first layer, this is also called the input layer because this is where we Input our features, x1, x2, x3."
  },
  {
    "index": "F15124",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後のレイヤーは出力レイヤーとも呼ばれる。何故ならこのレイヤーがここにあるニューロンこそが、仮説による最終的な計算結果を出力するから。",
    "output": "The final layer is also called the output layer because that layer has a neuron, this one over here, that outputs the final value computed by a hypothesis."
  },
  {
    "index": "F15125",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして2つの間にあるレイヤーを隠れたレイヤー(hiddenlayer)と呼ぶ。",
    "output": "And then, layer 2 in between, this is called the hidden layer."
  },
  {
    "index": "F15126",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "隠れたレイヤーという用語はそんな良い用語とは思わないが、直感的には教師あり学習では、入力と正解の出力は見れる訳だが隠れたレイヤーはトレーニングセットでは観測出来ない値だ。",
    "output": "The term hidden layer isn't a great terminology, but this ideation is that, you know, you supervised early, where you get to see the inputs and get to see the correct outputs, where there's a hidden layer of values you don't get to observe in the training setup."
  },
  {
    "index": "F15127",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからxでもyでも無い物は、隠れたレイヤーと呼んでいる。",
    "output": "It's not x, and it's not y, and so we call those hidden."
  },
  {
    "index": "F15128",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてのちほど、一つよりも多い隠れたレイヤーの例を見ていく。だがこの例では入力レイヤーであるレイヤー1が一つに、一つの隠れたレイヤーのレイヤー2に、出力レイヤーのレイヤー3がある。",
    "output": "And they try to see neural nets with more than one hidden layer but in this example, we have one input layer, Layer 1, one hidden layer, Layer 2, and one output layer, Layer 3."
  },
  {
    "index": "F15129",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが基本的には入力レイヤーでなく、そして出力レイヤーでも無い物はなんでも隠れたレイヤーと呼ぶ。",
    "output": "But basically, anything that isn't an input layer and isn't an output layer is called a hidden layer."
  },
  {
    "index": "F15130",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、私はこのニューラルネットワークが何をするのかをとっても分かりやすくしたい。",
    "output": "So I want to be really clear about what this neural network is doing."
  },
  {
    "index": "F15131",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これに埋め込まれた計算過程を順番に見ていこう。このダイアグラムで表現されている物の。",
    "output": "Let's step through the computational steps that are and body represented by this diagram."
  },
  {
    "index": "F15132",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークで表現されている特定の計算を説明する為に、もうちょっと記法を追加しておく。",
    "output": "To explain these specific computations represented by a neural network, here's a little bit more notation."
  },
  {
    "index": "F15133",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "上付き添字のjと下付き添字のiをレイヤーjにあるニューロンiまたはユニットiのアクティベーションを示すのに使う。",
    "output": "I'm going to use a superscript j subscript i to denote the activation of neuron i or of unit i in layer j."
  },
  {
    "index": "F15134",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、これは上付き添字2の下付き添字1でこれは二番目のレイヤーのつまり隠れたレイヤーの最初のユニットだ。",
    "output": "So completely this gave superscript to sub group one, that's the activation of the first unit in layer two, in our hidden layer."
  },
  {
    "index": "F15135",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてアクティベーションという言葉で、特定のニューロンからの計算結果の値、つまり出力の値を指す。",
    "output": "And by activation I just mean the value that's computed by and as output by a specific."
  },
  {
    "index": "F15136",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらに、我らのニューラルネットワークはこれらの行列、シータの上付き添字jでパラメータ化される。ここでシータjは一つのレイヤーから、、、例えば最初のレイヤーから二番目のレイヤーへと、とか、二番目のレイヤーから三番目のレイヤーへと、などをマッピングする関数を制御するウェイトとなる行列だ。",
    "output": "In addition, new network is parametrize by these matrixes, theta super script j Where theta j is going to be a matrix of weights controlling the function mapping form one layer, maybe the first layer to the second layer, or from the second layer to the third layer."
  },
  {
    "index": "F15137",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これがこのダイアグラムで表現される計算だ。",
    "output": "So here are the computations that are represented by this diagram."
  },
  {
    "index": "F15138",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここにある、最初の隠れたユニットは以下のように計算された値だ:a(2)1イコールsigmoid関数、またはsigmoidアクティベーション関数またはロジスティックアクティベーション関数と呼ばれるが、それがこんな形の入力の線形の組み合わせに適用される。",
    "output": "This first hidden unit here has it's value computed as follows, there's a is a21 is equal to the sigma function of the sigma activation function, also called the logistics activation function, apply to this sort of linear combination of these inputs."
  },
  {
    "index": "F15139",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの二番目の隠れたユニットはこのsigmoid関数で計算される、このアクティベーションの値だ。",
    "output": "And then this second hidden unit has this activation value computer as sigmoid of this."
  },
  {
    "index": "F15140",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以下同様に、この三番目の隠れたユニットはこの式で計算される。",
    "output": "And similarly for this third hidden unit is computed by that formula."
  },
  {
    "index": "F15141",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりシータ1の次元はそれはパラメータの行列で3つの入力ユニットと3つの隠れユニットを合わせた物だから、つまりシータ1は3、、、シータ1はつまり3x4次元の行列だ。",
    "output": "So here we have 3 theta 1 which is matrix of parameters governing our mapping from our three different units, our hidden units. Theta 1 is going to be a 3."
  },
  {
    "index": "F15142",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的にはネットワークがレイヤーjにsのjだけのユニットを、j+1番目のレイヤーにsのj+1個のユニットがあるとすると、行列であるところのシータjはレイヤーjからレイヤーj+1のマッピングの関数を決定する訳だが、その次元は、sのj+1掛けるsのj足す1となる。",
    "output": "And more generally, if a network has SJU units in there j and sj + 1 units and sj + 1 then the matrix theta j which governs the function mapping from there sj + 1."
  },
  {
    "index": "F15143",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがsjに1を足す、という事。オーケー?",
    "output": "That will have to mention sj +1 by sj + 1 I'll just be clear about this notation right."
  },
  {
    "index": "F15144",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりsの下付き添字j+1に足す事の、、、じゃなかった、掛けるだ。",
    "output": "This is Subscript j + 1 and that's s subscript j, and then this whole thing, plus 1, this whole thing (sj + 1), okay?"
  },
  {
    "index": "F15145",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "えーと、sのj+1に、掛ける事のsのjに足すことの1、最後の足す1は添字じゃないよ。",
    "output": "So that's s subscript j + 1 by, So that's s subscript j + 1 by sj + 1 where this plus one is not part of the subscript."
  },
  {
    "index": "F15146",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはイコール、、、ところでそれは、a(3)の1と書けて、それはイコール、これとなる。",
    "output": "Finally, there's a loss of this final and after that we have one more unit which computer h of x and that's equal can also be written as a(3)1 and that's equal to this."
  },
  {
    "index": "F15147",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして気づいたかもしれないが私は上付き添字で2とここでは書いた。その理由は、シータの上付き添字の2はパラメータの行列、またの名をウェイトの行列でそれは隠れユニット、それはレイヤー2のユニットの事だが、それとレイヤー3のユニットをそれは出力ユニットだが、それをマップする関数を制御する。",
    "output": "And you notice that I've written this with a superscript two here, because theta of superscript two is the matrix of parameters, or the matrix of weights that controls the function that maps from the hidden units, that is the layer two units to the one layer three unit, that is the output unit."
  },
  {
    "index": "F15148",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まとめると、我らがやった事はここにあるような絵がどう人工的なニューラルネットワークを定義するかを見た。それは入力値のxを願わくばあるyの予測値にマップする関数を定義する。",
    "output": "To summarize, what we've done is shown how a picture like this over here defines an artificial neural network which defines a function h that maps with x's input values to hopefully to some space that provisions y."
  },
  {
    "index": "F15149",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの仮説はパラメータでパラメトライズされていて、そのパラメータは大文字のシータで示している。こうする事でシータを変える事で異なる仮説が得られる訳だ。",
    "output": "And these hypothesis are parameterized by parameters denoting with a capital theta so that, as we vary theta, we get different hypothesis and we get different functions."
  },
  {
    "index": "F15150",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりxからyへとマップする、別の関数が得られるって事だ。",
    "output": "Mapping say from x to y."
  },
  {
    "index": "F15151",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、これは仮説をニューラルネットワークで表す方法の数学による定義を与えてくれる。",
    "output": "So this gives us a mathematical definition of how to represent the hypothesis in the neural network."
  },
  {
    "index": "F15152",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以後の一連のビデオではこれらの仮説の表現が何をするかの直感を伝えたいと思う。それと同時に、例を幾つか見て、それらをどう効率的に計算するかもお話する。",
    "output": "In the next few videos what I would like to do is give you more intuition about what these hypothesis representations do, as well as go through a few examples and talk about how to compute them efficiently."
  },
  {
    "index": "F15153",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオとこの次のビデオで具体的な詳細例をやっていく事で、ニューラルネットワークがどのように入力の複雑な非線形の関数を計算出来るのかを見ていき、これを通して何故ニューラルネットワークが複雑で非線形な仮説を学習するのに用いる事が出来るか、その心を伝えたいと思う。",
    "output": "In this and the next video I want to work through a detailed example showing how a neural network can compute a complex non linear function of the input."
  },
  {
    "index": "F15154",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二値の値のフィーチャー、x1とx2があるとしよう。",
    "output": "Consider the following problem where we have features X1 and X2 that are binary values."
  },
  {
    "index": "F15155",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "値は0か1のどちらか。",
    "output": "So, either 0 or 1."
  },
  {
    "index": "F15156",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりx1とx2は2つの可能な値のどちらかしかとれない。",
    "output": "So, X1 and X2 can each take on only one of two possible values."
  },
  {
    "index": "F15157",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例では、2つの陽性の手本と2つの陰性の手本だけを描いたが、これを、より複雑な学習問題を単純化した物と考える事が出来て、その複雑な問題では、たくさんの陽性の手本が右上と左下にあり、そしてたくさんの陰性の手本が丸で示されていて、やりたい事は非線型の陽性と陰性の手本を分離する為の決定境界を学習させたい、という物。",
    "output": "In this example, I've drawn only two positive examples and two negative examples. That you can think of this as a simplified version of a more complex learning problem where we may have a bunch of positive examples in the upper right and lower left and a bunch of negative examples denoted by the circles."
  },
  {
    "index": "F15158",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、どのようにニューラルネットワークがこれを行う事が出来るだろうか?そこで右の例をそのまま使うのでは無く、この左側の、より簡単に精査出来る例を使っていく。",
    "output": "And what we'd like to do is learn a non-linear division of boundary that may need to separate the positive and negative examples."
  },
  {
    "index": "F15159",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、これが実際になんなのかというと、ターゲットのラベルyを計算する物で、それはイコールx1XORx2だ。",
    "output": "So, how can a neural network do this and rather than using the example and the variable to use this maybe easier to examine example on the left. Concretely what this is, is really computing the type of label y equals x 1 x or x 2."
  },
  {
    "index": "F15160",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体例としては実はXNORを使う方がちょっとだけ良い事が判明している。",
    "output": "It turns out that these specific examples in the works out a little bit better if we use the XNOR example instead."
  },
  {
    "index": "F15161",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら二つはもちろん等しい。",
    "output": "These two are the same of course."
  },
  {
    "index": "F15162",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてどちらかだけがtrueの時はy=0となる。",
    "output": "This means not x1 or x2 and so, we're going to have positive examples of either both are true or both are false and what have as y equals 1, y equals 1."
  },
  {
    "index": "F15163",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして我らはこの種のトレーニングセットにフィットするようなニューラルネットワークが得られるのか?という事を見出したい。",
    "output": "And we're going to have y equals 0 if only one of them is true and we're going to figure out if we can get a neural network to fit to this sort of training set."
  },
  {
    "index": "F15164",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "xnorの例に適合するネットワークを構築する為に、もう少し簡単な例であるAND関数に適合するネットワークを見る事から始めて行こう。",
    "output": "In order to build up to a network that fits the XNOR example we're going to start with a slightly simpler one and show a network that fits the AND function."
  },
  {
    "index": "F15165",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれは0か1だ。そしてターゲットのラベルyはイコールx1ANDx2としよう。",
    "output": "Concretely, let's say we have input x1 and x2 that are again binaries so, it's either 0 or 1 and let's say our target labels y = x1 AND x2."
  },
  {
    "index": "F15166",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは論理積のANDだ。",
    "output": "This is a logical AND."
  },
  {
    "index": "F15167",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、一ユニットのネットワークで、このAND関数を計算する物が得られるだろうか?",
    "output": "So, can we get a one-unit network to compute this logical AND function?"
  },
  {
    "index": "F15168",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを行う為に、バイアスユニットも描いておく、この+1のユニットを。",
    "output": "In order to do so, I'm going to actually draw in the bias unit as well the plus one unit."
  },
  {
    "index": "F15169",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いま、単に適当な値をこのネットワークのウェイト、あるいはパラメータに割り振ってみよう。",
    "output": "Now let me just assign some values to the weights or parameters of this network."
  },
  {
    "index": "F15170",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この図にパラメータを書きこんでいく。",
    "output": "I'm gonna write down the parameters on this diagram here, -30 here."
  },
  {
    "index": "F15171",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このx0は+1で、このユニットになる。そしてパラメータの値+20は、x1に掛ける物で、そして+20はx2に掛けるパラメータだ。",
    "output": "And what this mean is just that I'm assigning a value of -30 to the value associated with X0 this +1 going into this unit and a parameter value of +20 that multiplies to X1 a value of +20 for the parameter that multiplies into x 2."
  },
  {
    "index": "F15172",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、これは仮説hのxがイコールg(-30+20x1+20x2)だと言っている訳だ。",
    "output": "So, concretely it's the same that the hypothesis h(x)=g(-30+20 X1 plus 20 X2."
  },
  {
    "index": "F15173",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、こういう風にニューラルネットワークの図の上にこれらのウェイト、パラメータを、書きこむコンベンションもある、という事だ。",
    "output": "So, sometimes it's just convenient to draw these weights. Draw these parameters up here in the diagram within and of course this- 30."
  },
  {
    "index": "F15174",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれはシータ1の1,2。だがそうするよりも、これらのパラメータがネットワークのエッジに関連づけられている、と考える方が分かりやすい。",
    "output": "This is theta 1 of 1 1 and that's theta 1 of 1 2 but it's just easier to think about it as associating these parameters with the edges of the network."
  },
  {
    "index": "F15175",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこの小さな一つのニューロンのネットワークが何を計算しているか、見ていこう。",
    "output": "Let's look at what this little single neuron network will compute."
  },
  {
    "index": "F15176",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "思い出せるように、sigmoidのアクティベーション関数であるgのzはこんな物だった。",
    "output": "Just to remind you the sigmoid activation function g(z) looks like this."
  },
  {
    "index": "F15177",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは0から始まり、スムースに上昇していき、0.5で交わり、そして1に漸近していく。目印になりそうな点を置いておくと、横軸の値、zがイコール4.6だと、sigmoid関数はイコール0.99となる。",
    "output": "It starts from 0 rises smoothly crosses 0.5 and then it asymptotic as 1 and to give you some landmarks, if the horizontal axis value z is equal to 4.6 then the sigmoid function is equal to 0.99."
  },
  {
    "index": "F15178",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはとても1に近い。そしてある種対称的な位置として-4.6の時は、sigmoid関数はイコール0.01となる、これはとても0に近い。",
    "output": "This is very close to 1 and kind of symmetrically, if it's -4.6 then the sigmoid function there is 0.01 which is very close to 0."
  },
  {
    "index": "F15179",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "x1とx2の、4つの可能な入力値の組み合わせを見ていこう、そしてそれぞれの場合に仮説が何を出力するかを見ていこう。",
    "output": "Let's look at the four possible input values for x1 and x2 and look at what the hypotheses will output in that case."
  },
  {
    "index": "F15180",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "x1とx2がどちらも0なら、これを見ると、x1とx2がどちらもイコール0なら、その場合は仮説はgの-30の点となる。",
    "output": "If x1 and x2 are both equal to 0."
  },
  {
    "index": "F15181",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはとても0に近い。もしx1=0でx2=1なら、この式は評価するとgとなる、つまりsigmoid関数が-10に適用される。",
    "output": "If you look at this, if x1 x2 are both equal to 0 then the hypothesis of g of -30."
  },
  {
    "index": "F15182",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれもまた、このプロット上では左の遥か彼方なので、これもまたとても0に近い。",
    "output": "So, this is a very far to the left of this diagram so it will be very close to 0."
  },
  {
    "index": "F15183",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これもまたgの-10となる。",
    "output": "If x 1 equals 0 and x equals 1, then this formula here evaluates the g that is the sigma function applied to -10, and again that's you know to the far left of this plot and so, that's again very close to 0."
  },
  {
    "index": "F15184",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりx1がイコール1で、x2が0だと、これは-30+20で、-10。",
    "output": "This is also g of minus 10 that is f x 1 is equal to 1 and x 2 0, this minus 30 plus 20 which is minus 10 and finally if x 1 equals 1 x 2 equals 1 then you have g of minus 30 plus 20 plus 20."
  },
  {
    "index": "F15185",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、x1=1でx2=1の時は、その時はgの-30+20+20で、つまりそれはgの+10となる、それはとても1に近い値となる。",
    "output": "So, that's g of positive 10 which is there for very close to 1."
  },
  {
    "index": "F15186",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの列を見てみると、これは論理積のAND関数だ。",
    "output": "And if you look in this column this is exactly the logical and function."
  },
  {
    "index": "F15187",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これはhのxは、だいたいx1ANDx2を計算している。",
    "output": "So, this is computing h of x is approximately x 1 and x 2."
  },
  {
    "index": "F15188",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、それはx1とx2の両方が1の時にだけ、1を出力する。",
    "output": "In other words it outputs one If and only if x2, x1 and x2, are both equal to 1."
  },
  {
    "index": "F15189",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで簡単な真理値表を書いてみると、我らのニューラルネットワークが計算してる論理関数が何か、が分かりやすい。",
    "output": "So, by writing out our little truth table like this we manage to figure what's the logical function that our neural network computes."
  },
  {
    "index": "F15190",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここのネットワークはOR関数を計算している。",
    "output": "This network showed here computes the OR function."
  },
  {
    "index": "F15191",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どう機能するか見てみよう。仮説を書き下してみればそれが計算する事はgの-10+20x1+20x2だ。",
    "output": "If you are write out the hypothesis that this confusing g of -10 + 20 x 1 + 20 x 2 and so you fill in these values."
  },
  {
    "index": "F15192",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、もしこれらの値を埋めれば、gの-10はだいたい0となり、gの10はだいたい1、という感じになる。",
    "output": "You find that's g of minus 10 which is approximately 0."
  },
  {
    "index": "F15193",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらはだいたい1、そしてだいたい1、そしてこれらの数は本質的には論理和のOR関数となっている。",
    "output": "g of 10 which is approximately 1 and so on and these are approximately 1 and approximately 1 and these numbers are essentially the logical OR function."
  },
  {
    "index": "F15194",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、いまやニューラルネットワークの一つのニューロンが、論理関数である所のANDとかORを計算するのにどう使えるのか、分かったんじゃないかなぁ。",
    "output": "So, hopefully with this you now understand how single neurons in a neural network can be used to compute logical functions like AND and OR and so on."
  },
  {
    "index": "F15195",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、引き続きこれらの例を構築していき、さらにより複雑な例を見ていく。",
    "output": "In the next video we'll continue building on these examples and work through a more complex example."
  },
  {
    "index": "F15196",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこではニューラルネットワークがどのように、ユニットの複数のレイヤーで、もっと複雑な関数を計算する事が出来るのかを見ていく、xor関数とかxnor関数のような。",
    "output": "We'll get to show you how a neural network now with multiple layers of units can be used to compute more complex functions like the XOR function or the XNOR function."
  },
  {
    "index": "F15197",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオではニューラルネットワークがどのように複雑な非線形の仮説を計算するかを見ていく。",
    "output": "In this video I'd like to keep working through our example to show how a Neural Network can compute complex non linear hypothesis."
  },
  {
    "index": "F15198",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではニューラルネットワークがどのように関数x1ANDx2とか、関数x1ORx2を計算するかを見てきた、ここでx1とx2は二値の値をとる。つまり、取りうる可能な値は0か1だけ。",
    "output": "In the last video we saw how a Neural Network can be used to compute the functions x1 AND x2, and the function x1 OR x2 when x1 and x2 are binary, that is when they take on values 0,1."
  },
  {
    "index": "F15199",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また、我らは否定を計算するネットワークも持ちうる。つまりNOTx1関数。",
    "output": "We can also have a network to compute negation, that is to compute the function not x1."
  },
  {
    "index": "F15200",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このネットワークを書き出してみよう。",
    "output": "Let me just write down the ways associated with this network."
  },
  {
    "index": "F15201",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たった一つだけの入力フィーチャーx1がこの場合にはあり、そしてバイアスユニットとして+1がある。",
    "output": "We have only one input feature x1 in this case and the bias unit +1."
  },
  {
    "index": "F15202",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれに、ウェイト+10と-20を付与すると、仮説はこれを計算する事になる。hのxは、イコール、sigmoid関数の10-20*x1となる。",
    "output": "And if I associate this with the weights plus 10 and -20, then my hypothesis is computing this h(x) equals sigmoid (10- 20 x1)."
  },
  {
    "index": "F15203",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、x1が0の時には仮説はgの10-20*0を計算する事になり、これはgの10となる。",
    "output": "So when x1 is equal to 0, my hypothesis would be computing g(10- 20 x 0) is just 10."
  },
  {
    "index": "F15204",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてxがイコール1の時はgの-10となりこれはだいたい0となる。",
    "output": "And so that's approximately 1, and when x is equal to 1, this will be g(-10) which is approximately equal to 0."
  },
  {
    "index": "F15205",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの値が何なのかを眺めてみると、これはようするにNOTx1関数だ。",
    "output": "And if you look at what these values are, that's essentially the not x1 function."
  },
  {
    "index": "F15206",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり否定を行うには基本的には負の大きなウェイトを否定したい変数の前におけば良い。",
    "output": "Cells include negations, the general idea is to put that large negative weight in front of the variable you want to negate."
  },
  {
    "index": "F15207",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合は-20を、x1に掛けてる。これがx1を否定するやり方の基本的な考え方。",
    "output": "Minus 20 multiplied by x1 and that's the general idea of how you end up negating x1."
  },
  {
    "index": "F15208",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、例えば以下のような関数を計算したい時に自力でやり方を見つけ出せるだろう:(NOTx1)AND(NOTx2)解答の一部分としては、たぶん負の大きなウェイトをx1とx2に置く事になるだろう、そしてそれを出力が一つだけのニューラルネットワークを得る為に食わせる。",
    "output": "And so in an example that I hope that you can figure out yourself."
  },
  {
    "index": "F15209",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いいかい?つまりこの大きな論理関数、(NOTx1)AND(NOTx2)がイコール1となるのはx1=x2=0の時で、その時のみ1となる。",
    "output": "If you want to compute a function like this NOT x1 AND NOT x2, part of that will probably be putting large negative weights in front of x1 and x2, but it should be feasible."
  },
  {
    "index": "F15210",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これは論理関数で、これはNOTx1、つまりX1は0でなくてはいけない、という意味で、さらにNOTx2。",
    "output": "So you get a neural network with just one output unit to compute this as well. All right, so this logical function, NOT x1 AND NOT x2, is going to be equal to 1 if and only if x1 equals x2 equals 0."
  },
  {
    "index": "F15211",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはx2もまた0でなくてはならない、という意味だ。",
    "output": "All right since this is a logical function, this says NOT x1 means x1 must be 0 and NOT x2, that means x2 must be equal to 0 as well."
  },
  {
    "index": "F15212",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこの論理関数が1となるのは、x1とx2の両方がイコール0の時だけだ。以上で、この論理関数を計算する小さなニューラルネットワークをどうやって構築すれば良いかも分かるだろう。",
    "output": "So this logical function is equal to 1 if and only if both x1 and x2 are equal to 0 and hopefully you should be able to figure out how to make a small neural network to compute this logical function as well."
  },
  {
    "index": "F15213",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、今度はこの三つの部品を一つに組み合わせて、つまりx1ANDx2を計算するネットワークと(NOTx1)AND(NOTx2)を計算するネットワークと、最後はx1ORx2を計算する為のネットワーク。",
    "output": "Now, taking the three pieces that we have put together as the network for computing x1 AND x2, and the network computing for computing NOT x1 AND NOT x2."
  },
  {
    "index": "F15214",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らはこれら三つの要素を組み合わせる事により、このx1XNORx2関数が計算出来るはずだ。",
    "output": "And one last network computing for computing x1 OR x2, we should be able to put these three pieces together to compute this x1 XNOR x2 function."
  },
  {
    "index": "F15215",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで再掲しておくと、これがx1とx2とすると、我らがこれから計算しようとしているこの関数は陰性の手本がこことここに、そして陽性の手本がこことここにあるような物だった。",
    "output": "And just to remind you if this is x1, x2, this function that we want to compute would have negative examples here and here, and we'd have positive examples there and there."
  },
  {
    "index": "F15216",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これから、明らかに陽性と陰性の手本を分離するには非線型の決定境界が要る。",
    "output": "And so clearly this will need a non linear decision boundary in order to separate the positive and negative examples."
  },
  {
    "index": "F15217",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ネットワークを書き下してみよう。",
    "output": "Let's draw the network."
  },
  {
    "index": "F15218",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "入力として、+1,x1,x2を取り、そして最初の隠れユニットをここに作る。",
    "output": "I'm going to take my input +1, x1, x2 and create my first hidden unit here."
  },
  {
    "index": "F15219",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをa(2)1と呼ぶ事にする、何故ならこれは最初の隠れユニットだから。",
    "output": "I'm gonna call this a 21 cuz that's my first hidden unit."
  },
  {
    "index": "F15220",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてウェイトを赤のネットワークから、つまりx1ANDx2のネットワークからコピーする。",
    "output": "And I'm gonna copy the weight over from the red network, the x1 and x2."
  },
  {
    "index": "F15221",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり-30,20,20。",
    "output": "As well so then -30, 20, 20."
  },
  {
    "index": "F15222",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、二番目の隠れユニットを作ろう。それをa(2)2と呼ぶ事にする。",
    "output": "Next let me create a second hidden unit which I'm going to call a 2 2."
  },
  {
    "index": "F15223",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはレイヤー2の二番目の隠れユニットだ。",
    "output": "That is the second hidden unit of layer two."
  },
  {
    "index": "F15224",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで真理値表から値をひっぱってこよう。",
    "output": "And so, let's pull some of the truth table values."
  },
  {
    "index": "F15225",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "赤いネットワークはx1ANDx2を計算している事を知っている。つまりこれは、だいたい0,0,0,1となる、x1とx2の値に応じて。",
    "output": "For the red network, we know that was computing the x1 and x2, and so this will be approximately 0 0 0 1, depending on the values of x1 and x2, and for a 2 2, the cyan network."
  },
  {
    "index": "F15226",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてa(2)2については、シアン色のネットワークで、これは(NOTx1)AND(NOTx2)関数だと知っているから、出力は4つのx1とx2の入力に対して1,0,0,0となる。",
    "output": "The function NOT x1 AND NOT x2, that outputs 1 0 0 0, for the 4 values of x1 and x2."
  },
  {
    "index": "F15227",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、出力ノードを作る。",
    "output": "Finally, I'm going to create my output node, my output unit that is a 3 1."
  },
  {
    "index": "F15228",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはh(x)の出力で、ここにはORのネットワークからコピーして、ここにはバイアスユニットの+1が必要だ。",
    "output": "This is one more output h(x) and I'm going to copy over the old network for that."
  },
  {
    "index": "F15229",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからそれを書きこむ、緑のネットワークからウェイトをコピーする。",
    "output": "And I'm going to need a +1 bias unit here, so you draw that in, And I'm going to copy over the weights from the green networks."
  },
  {
    "index": "F15230",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、-10,20,20で、これがOR関数を実装する事を前に見ている。",
    "output": "So that's -10, 20, 20 and we know earlier that this computes the OR function."
  },
  {
    "index": "F15231",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では真理値表を見ていこう。",
    "output": "So let's fill in the truth table entries."
  },
  {
    "index": "F15232",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初のエントリでは、0OR1だから1となる。次は0OR0だから、0となり、0OR0は0。",
    "output": "So the first entry is 0 OR 1 which can be 1 that makes 0 OR 0 which is 0, 0 OR 0 which is 0, 1 OR 0 and that falls to 1."
  },
  {
    "index": "F15233",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1OR0は1で、つまり、hのxがイコール1となるのは、x1とx2がどちらも0の時か、またはx1とx2がどちらも1の時だけだ。具体的には、hのxはこれら二つの位置の時にはぴったり1を出力し、それ以外の場合は0を出力する。",
    "output": "And thus h(x) is equal to 1 when either both x1 and x2 are zero or when x1 and x2 are both 1 and concretely h(x) outputs 1 exactly at these two locations and then outputs 0 otherwise."
  },
  {
    "index": "F15234",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "かくして、このニューラルネットワークでもって、それは入力レイヤーに1つの隠れレイヤーと1つの出力レイヤーがあるような物だが、これでXNOR関数のような非線型の決定境界を計算出来る、このような関数を。",
    "output": "And thus will this neural network, which has a input layer, one hidden layer, and one output layer, we end up with a nonlinear decision boundary that computes this XNOR function."
  },
  {
    "index": "F15235",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてより一般的には直感的には、入力レイヤーには単に生の入力を置いて、そして隠れレイヤーがあり、そこでは、ここに示したもうちょっとだけ複雑な入力の関数を計算している。",
    "output": "And the more general intuition is that in the input layer, we just have our four inputs."
  },
  {
    "index": "F15236",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらはさらにちょっとだけ複雑な関数で、さらにもう一つレイヤーを追加する事で、さらに複雑な非線型の関数も計算する事が出来る。",
    "output": "Then we have a hidden layer, which computed some slightly more complex functions of the inputs that its shown here this is slightly more complex functions. And then by adding yet another layer we end up with an even more complex non linear function."
  },
  {
    "index": "F15237",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、ニューラルネットワークを用いると、極めて複雑な関数を計算しうるかの直感的な説明だ。",
    "output": "And this is a sort of intuition about why neural networks can compute pretty complicated functions."
  },
  {
    "index": "F15238",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "複数のレイヤーがある時に、二番目のレイヤーは比較的単純な入力の関数でも、三番目のレイヤーを足す事でさらに複雑な関数を計算する為にネットワークを作りあげる事が出来、そしてその次のレイヤーで、さらに複雑な関数を計算する事も出来る。",
    "output": "That when you have multiple layers you have relatively simple function of the inputs of the second layer. But the third layer I can build on that to complete even more complex functions, and then the layer after that can compute even more complex functions."
  },
  {
    "index": "F15239",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオのまとめとして、楽しい例を紹介したい、これはニューラルネットワークを適用した例で、より深い所のレイヤーがより複雑な計算をしていく、という直感を捉えた物となっている。",
    "output": "To wrap up this video, I want to show you a fun example of an application of a the Neural Network that captures this intuition of the deeper layers computing more complex features."
  },
  {
    "index": "F15240",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私がお見せするビデオは私の良き友人、YonKhunからもらった物だ。",
    "output": "I want to show you a video of that customer a good friend of mine Yann LeCunj."
  },
  {
    "index": "F15241",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "YonはNewYorkUniversity、NYUの教授だ。彼はニューラルネットワークの初期の頃の研究のパイオニアであり、今ではこの分野での、ある種伝説になった男で、彼のアイデアは今や世界中のあらゆる所であらゆる製品、アプリケーションに使われている。",
    "output": "Yann is a professor at New York University, NYU and he was one of the early pioneers of Neural Network reasearch and is sort of a legend in the field now and his ideas are used in all sorts of products and applications throughout the world now."
  },
  {
    "index": "F15242",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、私があなたにお見せするのは、彼の初期の頃の仕事の一つ、手書き認識の為にニューラルネットワークを用いる、という物だ。手書きの数字認識の為に。",
    "output": "So I wanna show you a video from some of his early work in which he was using a neural network to recognize handwriting, to do handwritten digit recognition."
  },
  {
    "index": "F15243",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このクラスの最初の方で、このクラスの最初で、最初期のニューラルネットワークの成功した使い道はそれをzipコード(郵便番号)を読むのに用いるという物だった、と言った。郵便を送る助けとなるように、郵便番号を読む為に。",
    "output": "You might remember early in this class, at the start of this class I said that one of the earliest successes of neural networks was trying to use it to read zip codes to help USPS Laws and read postal codes."
  },
  {
    "index": "F15244",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これがその試みの一種だ。つまりこれこそがあの問題を解決しようとするアルゴリズムの一種だ。",
    "output": "So this is one of the attempts, this is one of the algorithms used to try to address that problem."
  },
  {
    "index": "F15245",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ビデオでは、みえているこの、ここの領域、これがネットワークに入力する手書きの文字がみえている領域だ。",
    "output": "In the video that I'll show you this area here is the input area that shows a canvasing character shown to the network."
  },
  {
    "index": "F15246",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このここの列はネットワークの最初の隠れレイヤーによる計算結果のフィーチャーを可視化した物を表示している、つまり最初の隠れレイヤだ。",
    "output": "This column here shows a visualization of the features computed by sort of the first hidden layer of the network. So that the first hidden layer of the network and so the first hidden layer, this visualization shows different features."
  },
  {
    "index": "F15247",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この可視化は別々のフィーチャーを表示している、検出された様々なエッジや線を。",
    "output": "Different edges and lines and so on detected."
  },
  {
    "index": "F15248",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは次の隠れレイヤーを可視化した物だ。",
    "output": "This is a visualization of the next hidden layer."
  },
  {
    "index": "F15249",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より深い位置の隠れレイヤーはどう解釈したらいいか難しい。そしてこれが、次の隠れレイヤーが計算している物を可視化した物だ。",
    "output": "It's kinda harder to see, harder to understand the deeper, hidden layers, and that's a visualization of why the next hidden layer is confusing."
  },
  {
    "index": "F15250",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の隠れレイヤーよりもずっと分かりにくい。だが、最後に、これらの学習されたフィーチャーを全て出力レイヤーに食わせて、ここに最終的な答えを表示する。",
    "output": "You probably have a hard time seeing what's going on much beyond the first hidden layer, but then finally, all of these learned features get fed to the upper layer."
  },
  {
    "index": "F15251",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最終的な、ニューラルネットワークが見た手書き文字が何なのかの予測を。",
    "output": "And shown over here is the final answer, it's the final predictive value for what handwritten digit the neural network thinks it is being shown."
  },
  {
    "index": "F15252",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、ビデオを見てみよう。",
    "output": "So let's take a look at the video."
  },
  {
    "index": "F15253",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、お楽しみいただけただろうか。このビデオでニューラルネットワークが学習出来る極めて複雑な関数がどんな感じかの感覚をつかめただろうか。",
    "output": "So I hope you enjoyed the video and that this hopefully gave you some intuition about the source of pretty complicated functions neural networks can learn."
  },
  {
    "index": "F15254",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでは、この入力画像を取り、この生のピクセルを入力に取り、そして最初のレイヤーの終わりまでに何らかのフィーチャーの集合を計算し、次のレイヤーの終わりまでにさらに複雑なフィーチャーを計算し、さらに複雑なフィーチャー、これらのフィーチャーは最後の、本質的にはロジスティック回帰の分類器の入力として使われ、その分類器が、ネットワークが見た数字がなんなのかを正確に予測するのに使われる。",
    "output": "And these features can then be used by essentially the final layer of the logistic classifiers to make accurate predictions without the numbers that the network sees."
  },
  {
    "index": "F15255",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、ニューラルネットワークを用いてマルチクラスの分類問題を行う方法を話す。そこでは、1つより多くのカテゴリがあって、それらのどれかを区別したい。",
    "output": "In this video, I want to tell you about how to use neural networks to do multiclass classification where we may have more than one category that we're trying to distinguish amongst."
  },
  {
    "index": "F15256",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオの最後の所で、手書き数字認識の問題を見た、あれは実の所、マルチクラスの分類問題だ。何故なら10個の可能なカテゴリがあり、そのどれかを認識したい、という問題だったから、0から9までの。",
    "output": "In the last part of the last video, where we had the handwritten digit recognition problem, that was actually a multiclass classification problem because there were ten possible categories for recognizing the digits from 0 through 9 and so, if you want us to fill you in on the details of how to do that."
  },
  {
    "index": "F15257",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークでマルチクラスの分類を行う方法は、本質的には1vsALL法の拡張だ。",
    "output": "The way we do multiclass classification in a neural network is essentially an extension of the one versus all method."
  },
  {
    "index": "F15258",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コンピュータビジョンの例を考える、今回は始めにやった車だけを認識する、という例の代わりに、4つのカテゴリの物体を認識したいとする。画像が与えられた時にそれが歩行者か、車か、バイクか、トラックかを決めたい。",
    "output": "So, let's say that we have a computer vision example, where instead of just trying to recognize cars as in the original example that I started off with, but let's say that we're trying to recognize, you know, four categories of objects and given an image we want to decide if it is a pedestrian, a car, a motorcycle or a truck."
  },
  {
    "index": "F15259",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合には、我らがやる事としては、4つの出力ユニットのあるニューラルネットワークを構築する、という事が考えられる、我らのニューラルネットワークが4つの数のベクトルを出力するように。",
    "output": "If that's the case, what we would do is we would build a neural network with four output units so that our neural network now outputs a vector of four numbers."
  },
  {
    "index": "F15260",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、今回は出力が実際に4つの数のベクトルである必要がある。",
    "output": "So, the output now is actually needing to be a vector of four numbers and what we're going to try to do is get the first output unit to classify: is the image a pedestrian, yes or no."
  },
  {
    "index": "F15261",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてやろうとしている事は、最初の出力ユニットに画像が歩行者かどうかをyesかnoで分類させ、二番目のユニットに画像が車かどうかをyesかnoで分類させ、このユニットには画像がバイクかどうかをyesまたはnoで分類させ、そしてこれに、画像がトラックかどうかをyesかnoで分類させる。",
    "output": "The second unit to classify: is the image a car, yes or no."
  },
  {
    "index": "F15262",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "かくして、歩行者の画像の時には、理想的にはネットワークに1,0,0,0を出力して欲しい。車の時には0,1,0,0を出力して欲しい。",
    "output": "This unit to classify: is the image a motorcycle, yes or no, and this would classify: is the image a truck, yes or no."
  },
  {
    "index": "F15263",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがバイクの時には、0,0,1,0が得られる、というか出力して欲しい、などなど。",
    "output": "And thus, when the image is of a pedestrian, we would ideally want the network to output 1, 0, 0, 0, when it is a car we want it to output 0, 1, 0, 0, when this is a motorcycle, we get it to or rather, we want it to output 0, 0, 1, 0 and so on."
  },
  {
    "index": "F15264",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、ロジスティック回帰の時にやった1vsALL法にすぎない。",
    "output": "So this is just like the \"one versus all\" method that we talked about when we were describing logistic regression, and here we have essentially four logistic regression classifiers, each of which is trying to recognize one of the four classes that we want to distinguish amongst."
  },
  {
    "index": "F15265",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここでは、ようするに4つのロジスティック回帰の分類器があり、各分類器はおのおの、見分けたい4つのクラスの一つを認識しようと試みる。",
    "output": "So, rearranging the slide of it, here's our neural network with four output units and those are what we want h of x to be when we have the different images, and the way we're going to represent the training set in these settings is as follows."
  },
  {
    "index": "F15266",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、スライドをちょっと並べ替えて、これは4つの出力ユニットを持ったニューラルネットワークで、そしてこれらは、それぞれの画像の時にhのxに期待する物だ。このような前提でトレーニングセットを表す方法は、以下のようになる。",
    "output": "So, when we have a training set with different images of pedestrians, cars, motorcycles and trucks, what we're going to do in this example is that whereas previously we had written out the labels as y being an integer from 1, 2, 3 or 4."
  },
  {
    "index": "F15267",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり一つのトレーニング手本としては、xi,yiのペアとなり、xiは4つの物体のどれかの画像で、そしてyiはこれらのベクトルのどれか一つ。",
    "output": "And so one training example will be one pair Xi colon Yi where Xi is an image with, you know one of the four objects and Yi will be one of these vectors."
  },
  {
    "index": "F15268",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして望むらくは、我らのニューラルネットワークが以下のような、何らかの値を出力する方法を見つける事だ。",
    "output": "And hopefully, we can find a way to get our Neural Networks to output some value."
  },
  {
    "index": "F15269",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは、hのxがだいたいyとなるような物、ここでhのxとyiは両方とも、我らの例では、4つのクラスがある時には4つの次元となる。",
    "output": "So, the h of x is approximately y and both h of x and Yi, both of these are going to be in our example, four dimensional vectors when we have four classes."
  },
  {
    "index": "F15270",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がマルチクラスの分類問題をどうニューラルネットワークにやらせるか、だ。",
    "output": "So, that's how you get neural network to do multiclass classification."
  },
  {
    "index": "F15271",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この話で我らの仮説をニューラルネットワークでどう表現するか、をまとめられたと思う。",
    "output": "This wraps up our discussion on how to represent Neural Networks that is on our hypotheses representation."
  },
  {
    "index": "F15272",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "続く一連のビデオでは、どうトレーニングセットを取るか、そしてどうやってニューラルネットワークの仮説のパラメータを自動的に学習するかを議論したい。",
    "output": "In the next set of videos, let's start to talk about how take a training set and how to automatically learn the parameters of the neural network."
  },
  {
    "index": "F15273",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークはこんにち使える中でもっとも強力な学習アルゴリズムの一つだ。",
    "output": "Neural networks are one of the most powerful learning algorithms that we have today."
  },
  {
    "index": "F15274",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオとその後の一連のビデオで所与のトレーニングセットに対してニューラルネットワークのパラメータをフィッティングする為の学習アルゴリズムの話をしていく。",
    "output": "In this and in the next few videos, I'd like to start talking about a learning algorithm for fitting the parameters of a neural network given a training set."
  },
  {
    "index": "F15275",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ほとんどの学習アルゴリズムの議論と同様、ニューラルネットワークのパラメータのフィッティングでもコスト関数から始める事にする。",
    "output": "As with the discussion of most of our learning algorithms, we're going to begin by talking about the cost function for fitting the parameters of the network."
  },
  {
    "index": "F15276",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークの分類問題への応用に私はフォーカスしたいと思う。",
    "output": "I'm going to focus on the application of neural networks to classification problems."
  },
  {
    "index": "F15277",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "左に描いてあるようなネットワークがあったとする。",
    "output": "So suppose we have a network like that shown on the left."
  },
  {
    "index": "F15278",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこんな感じのトレーニングセット、xiとyiのペアがm個あるようなのがあったとする。",
    "output": "And suppose we have a training set like this is x I, y I pairs of M training example."
  },
  {
    "index": "F15279",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "大文字のLをネットワークに存在するレイヤーの総数を表すのに使う。",
    "output": "I'm going to use upper case L to denote the total number of layers in this network."
  },
  {
    "index": "F15280",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば左に見えるネットワークの場合、大文字のLはイコール4だ。",
    "output": "So for the network shown on the left we would have capital L equals 4."
  },
  {
    "index": "F15281",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてsの下付き添字lでユニットの総数を表す事にする。",
    "output": "I'm going to use S subscript L to denote the number of units, that is the number of neurons."
  },
  {
    "index": "F15282",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはネットワークのレイヤーlに存在するニューロンの総数のうち、バイアスユニットは含めない数だ。",
    "output": "Not counting the bias unit in their L of the network."
  },
  {
    "index": "F15283",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、s1と言えば、入力レイヤーの事でイコール3ユニット。s2はこの例だと5ユニット。",
    "output": "So for example, we would have a S one, which is equal there, equals S three unit, S two in my example is five units."
  },
  {
    "index": "F15284",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして出力レイヤのs4はそれはsLとも等しい。というのは大文字のLは4だから。",
    "output": "And the output layer S four, which is also equal to S L because capital L is equal to four."
  },
  {
    "index": "F15285",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例における出力レイヤは4ユニットとなっている。",
    "output": "The output layer in my example under that has four units."
  },
  {
    "index": "F15286",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "2つの種類の分類問題を扱っていく事になる。",
    "output": "We're going to consider two types of classification problems."
  },
  {
    "index": "F15287",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ目はバイナリ分類、それはyの取りうる値は0か1のどちらかだけの場合。",
    "output": "The first is Binary classification, where the labels y are either 0 or 1."
  },
  {
    "index": "F15288",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合は出力ユニットは一つだけとなる。",
    "output": "In this case, we will have 1 output unit, so this Neural Network unit on top has 4 output units, but if we had binary classification we would have only one output unit that computes h(x)."
  },
  {
    "index": "F15289",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この上にあるニューラルネットワークの場合は4つの出力ユニットがあるが、バイナリ分類なら、出力ユニットは一つだけで、それはhのxを計算する。",
    "output": "And the output of the neural network would be h(x) is going to be a real number."
  },
  {
    "index": "F15290",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそのニューラルネットワークの出力は、つまりhのxは実数となる。",
    "output": "And in this case the number of output units, S L, where L is again the index of the final layer."
  },
  {
    "index": "F15291",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのケースでは、出力ユニットの総数sLはところでLは最後のレイヤのインデックスなのでというのはLはそのネットワークのレイヤーの総数だからだが、すると出力レイヤに存在するユニットの総数は、1に等しい。",
    "output": "Cuz that's the number of layers we have in the network so the number of units we have in the output layer is going to be equal to 1."
  },
  {
    "index": "F15292",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合、あとでノーテーションを簡素化する為、K=1とも書く事にする。つまりKもまた、出力レイヤのユニットの総数を表す訳だ。",
    "output": "In this case to simplify notation later, I'm also going to set K=1 so you can think of K as also denoting the number of units in the output layer."
  },
  {
    "index": "F15293",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "2つ目のタイプの分類問題はマルチクラスの分類問題だ。それは、K個の異なる分類がある問題。",
    "output": "The second type of classification problem we'll consider will be multi-class classification problem where we may have K distinct classes."
  },
  {
    "index": "F15294",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは4つのクラスがある場合だった。このタイプでは、大文字のK個だけの出力ユニットがあり、我らの仮説はK次元のベクトルを出力することになる。",
    "output": "So our early example had this representation for y if we have 4 classes, and in this case we will have capital K output units and our hypothesis or output vectors that are K dimensional."
  },
  {
    "index": "F15295",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして出力ユニットの総数はKと等しくなり、このKは通常は、3以上なのがこのタイプとなる。",
    "output": "And the number of output units will be equal to K."
  },
  {
    "index": "F15296",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら、もし2クラスしか無い時はonevsall法を使う必要が無いからだ。",
    "output": "And usually we would have K greater than or equal to 3 in this case, because if we had two causes, then we don't need to use the one verses all method."
  },
  {
    "index": "F15297",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "onevsall法を使わなくてはいけないのはKが3以上の時だけなので2つしかクラスが無い時は一つの出力ユニットしか必要としない。",
    "output": "We use the one verses all method only if we have K greater than or equals V classes, so having only two classes we will need to use only one upper unit."
  },
  {
    "index": "F15298",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではここで、ニューラルネットワークのコスト関数を定義しよう。",
    "output": "Now let's define the cost function for our neural network."
  },
  {
    "index": "F15299",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークで使うコスト関数は、ロジスティック回帰で使った物を一般化した物だ。",
    "output": "The cost function we use for the neural network is going to be a generalization of the one that we use for logistic regression."
  },
  {
    "index": "F15300",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰ではコスト関数Jのシータを最小化したのだった。そのJのシータとは-1/mのこのコスト関数にこの追加の正規化の項を足した物だった。",
    "output": "For logistic regression we used to minimize the cost function J(theta) that was minus 1/m of this cost function and then plus this extra regularization term here, where this was a sum from J=1 through n, because we did not regularize the bias term theta0."
  },
  {
    "index": "F15301",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならバイアス項であるシータ0は正規化しないからだった。ニューラルネットワーク向けにはコスト関数はこれを一般化したものとなる。",
    "output": "For a neural network, our cost function is going to be a generalization of this."
  },
  {
    "index": "F15302",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでは基本的には単に一つのロジスティック回帰の出力ユニットしか無いのではなく、その代わりにK個のユニットがある訳だ。",
    "output": "Where instead of having basically just one, which is the compression output unit, we may instead have K of them."
  },
  {
    "index": "F15303",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークはRのKのベクトルを出力する。ここでKは1となる事もある、その時はバイナリ分類問題という事。",
    "output": "Our new network now outputs vectors in R K where R might be equal to 1 if we have a binary classification problem."
  },
  {
    "index": "F15304",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "h(x)の添字iという記法でi番目の出力を示す。",
    "output": "I'm going to use this notation h(x) subscript i to denote the ith output."
  },
  {
    "index": "F15305",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、h(x)はK次元ベクトルなのでこの添字iは単にニューラルネットワークからの出力のベクトルからi番目を選び取るだけだ。",
    "output": "That is, h(x) is a k-dimensional vector and so this subscript i just selects out the ith element of the vector that is output by my neural network."
  },
  {
    "index": "F15306",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コスト関数のJのシータは今や以下のようになる。",
    "output": "My cost function J(theta) is now going to be the following."
  },
  {
    "index": "F15307",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "-1/mにロジスティック回帰の時と似たような項の和となっている。",
    "output": "Is - 1 over M of a sum of a similar term to what we have for logistic regression, except that we have the sum from K equals 1 through K."
  },
  {
    "index": "F15308",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、この和がkが1からKまでというのがあるのが違いか。",
    "output": "This summation is basically a sum over my K output."
  },
  {
    "index": "F15309",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その和は基本的にはK個の出力ユニットに対する和だ。",
    "output": "So if I have four output units, that is if the final layer of my neural network has four output units, then this is a sum from k equals one through four of basically the logistic regression algorithm's cost function but summing that cost function over each of my four output units in turn."
  },
  {
    "index": "F15310",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし出力ユニットが4つあるとすると、それはつまりニューラルネットワークの最後のレイヤーが4つの出力ユニットを持つという事だが、するとその時この和はこれはkが1から4までの基本的にはロジスティック回帰のアルゴリズムのコスト関数なんだが、しかしそのコスト関数を4つの出力ユニット分を一つずつ足していく所が違う。",
    "output": "And so you notice in particular that this applies to Yk Hk, because we're basically taking the K upper units, and comparing that to the value of Yk which is that one of those vectors saying what cost it should be. And finally, the second term here is the regularization term, similar to what we had for the logistic regression."
  },
  {
    "index": "F15311",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして気づいたかもしれないが、これは特に、yのkとhのkに適用される。",
    "output": "This summation term looks really complicated, but all it's doing is it's summing over these terms theta j i l for all values of i j and l."
  },
  {
    "index": "F15312",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら、基本的には我らはk番目の出力ユニットをとり、それをyのkと比較しているから。",
    "output": "Except that we don't sum over the terms corresponding to these bias values like we have for logistic progression."
  },
  {
    "index": "F15313",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも実際にやってるのはこれらの項、シータのijlの和を全てのijlの値に渡って取るだけだ。",
    "output": "Completely, we don't sum over the terms responding to where i is equal to 0."
  },
  {
    "index": "F15314",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはというと、ニューロンの活性化を計算してるときこういう感じの項があるΘ(セータ)i0プラスセータのi1x1プラス、、、などなど。",
    "output": "So that is because when we're computing the activation of a neuron, we have terms like these."
  },
  {
    "index": "F15315",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ここにある0に対応する値はx0とかa0の係数となっている、つまり、これはバイアスユニットの係数って事。",
    "output": "And so the values with a zero there, that corresponds to something that multiplies into an x0 or an a0."
  },
  {
    "index": "F15316",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてロジスティック回帰から類推出来るように、正規化の項ではそれらの項は足し合わせない。何故ならそれらは正規化したくないから。",
    "output": "And so this is kinda like a bias unit and by analogy to what we were doing for logistic progression, we won't sum over those terms in our regularization term because we don't want to regularize them and string their values as zero."
  },
  {
    "index": "F15317",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこれは単なる一つの慣習に過ぎず、別にiを0からslまで足し合わせたとしても、大差なく、だいたいうまく行く。",
    "output": "But this is just one possible convention, and even if you were to sum over i equals 0 up to Sl, it would work about the same and doesn't make a big difference."
  },
  {
    "index": "F15318",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこっちの慣習、つまりバイアス項は正規化しない流派の方がちょっとだけ普及してると思う。",
    "output": "But maybe this convention of not regularizing the bias term is just slightly more common."
  },
  {
    "index": "F15319",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がネットワークをフィッティングするのに使用するコスト関数です。",
    "output": "So that's the cost function we're going to use for our neural network."
  },
  {
    "index": "F15320",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、コスト関数を最適化するアルゴリズムに入っていきます。",
    "output": "In the next video we'll start to talk about an algorithm for trying to optimize the cost function."
  },
  {
    "index": "F15321",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではニューラルネットワークのコスト関数について話した。",
    "output": "In the previous video, we talked about a cost function for the neural network."
  },
  {
    "index": "F15322",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、コスト関数の最小化を試みるアルゴリズムの話に入っていこう。",
    "output": "In this video, let's start to talk about an algorithm, for trying to minimize the cost function."
  },
  {
    "index": "F15323",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、バックプロパゲーション(後に伝播)アルゴリズムについて話す。",
    "output": "In particular, we'll talk about the back propagation algorithm."
  },
  {
    "index": "F15324",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは前回のビデオで書き下したコスト関数だ。",
    "output": "Here's the cost function that we wrote down in the previous video."
  },
  {
    "index": "F15325",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今回やりたいのはJのシータを最小化するようなシータを探したい。",
    "output": "What we'd like to do is try to find parameters theta to try to minimize j of theta."
  },
  {
    "index": "F15326",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最急降下法なりより進んだアルゴリズムなりを使うためにはこの入力であるパラメータ、シータをとり、Jのシータとこれらの偏微分の項を計算するコードを書く必要がある。",
    "output": "In order to use either gradient descent or one of the advance optimization algorithms."
  },
  {
    "index": "F15327",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらが、計算しなくてはならない偏微分の項だ。",
    "output": "What we need to do therefore is to write code that takes this input the parameters theta and computes j of theta and these partial derivative terms."
  },
  {
    "index": "F15328",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コスト関数であるJのシータを計算するには、この上の式をただ使うだけで良い。",
    "output": "Remember, that the parameters in the the neural network of these things, theta superscript l subscript ij, that's the real number and so, these are the partial derivative terms we need to compute."
  },
  {
    "index": "F15329",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこのビデオの残りのほとんどでフォーカスしたいのは、どうやってこれらの偏微分の項を計算出来るか、という方。",
    "output": "In order to compute the cost function j of theta, we just use this formula up here and so, what I want to do for the most of this video is focus on talking about how we can compute these partial derivative terms."
  },
  {
    "index": "F15330",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つしかトレーニング手本が無い場合から始めよう。トレーニングセットの全体がたった一つの手本から構成されている場合を思い浮かべてくれ。",
    "output": "Let's start by talking about the case of when we have only one training example, so imagine, if you will that our entire training set comprises only one training example which is a pair xy."
  },
  {
    "index": "F15331",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "x1とかy1とは書かずにただこう書く事にする。",
    "output": "I'm not going to write x1y1 just write this."
  },
  {
    "index": "F15332",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つのトレーニングの手本をx、yと書いて、この一つのトレーニング手本に対してどんな計算を行うのか順番に見ていこう。",
    "output": "Write a one training example as xy and let's tap through the sequence of calculations we would do with this one training example."
  },
  {
    "index": "F15333",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初にやる事はフォワードプロパゲーション(前方に伝播)を与えられた入力に対して仮説が実際に何を出力するかを計算する為、適用する。",
    "output": "The first thing we do is we apply forward propagation in order to compute whether a hypotheses actually outputs given the input."
  },
  {
    "index": "F15334",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、a(1)と呼ばれている物はこの最初のレイヤーのアクティベーションの値、つまり入力の値だったのを思い出そう。",
    "output": "Concretely, the called the a(1) is the activation values of this first layer that was the input there."
  },
  {
    "index": "F15335",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからそれをxに設定し、次にz(2)=シータ(1)a(1)を計算して、そこからa(2)イコールg、つまりsigmoid関数をz(2)に適用する。この結果が最初の中間レイヤーのアクティベーションとなる。",
    "output": "So, I'm going to set that to x and then we're going to compute z(2) equals theta(1) a(1) and a(2) equals g, the sigmoid activation function applied to z(2) and this would give us our activations for the first middle layer."
  },
  {
    "index": "F15336",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりネットワークのレイヤー2に対応する。また、これらのバイアス項も足す。",
    "output": "That is for layer two of the network and we also add those bias terms."
  },
  {
    "index": "F15337",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこから伝播させる為に、もう2ステップさらに適用してa(3)と、、、a(4)を計算する。これはh(x)の出力でもある。",
    "output": "Next we apply 2 more steps of this four and propagation to compute a(3) and a(4) which is also the upwards of a hypotheses h of x."
  },
  {
    "index": "F15338",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がフォワードプロパゲーションのベクトル化された実装だ。以上でまた、ニューラルネットワーク内の全てのアクティベーション値も計算出来る。",
    "output": "So this is our vectorized implementation of forward propagation and it allows us to compute the activation values for all of the neurons in our neural network."
  },
  {
    "index": "F15339",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、微分を計算する為にバックプロパゲーション(後方に伝播)と呼ばれるアルゴリズムを使っていく。",
    "output": "Next, in order to compute the derivatives, we're going to use an algorithm called back propagation."
  },
  {
    "index": "F15340",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "デルタには上付き添字のlに下付き添字のjがつく。それはある意味でレイヤーlにあるノードjの誤差を表す物だ。",
    "output": "The intuition of the back propagation algorithm is that for each note we're going to compute the term delta superscript l subscript j that's going to somehow represent the error of note j in the layer l."
  },
  {
    "index": "F15341",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もういちど思い出すとaの添字のl、下の添え字のjはレイヤーlにあるj番目のアクティベーションだった。そして、このデルタの項はある意味で、そのノードのアクティベーションの誤差を捕捉する、と考えられる。",
    "output": "So, recall that a superscript l subscript j that does the activation of the j of unit in layer l and so, this delta term is in some sense going to capture our error in the activation of that neural duo."
  },
  {
    "index": "F15342",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、そのノードのアクティベーションの期待される値からどのくらいずれているかだ。",
    "output": "So, how we might wish the activation of that note is slightly different."
  },
  {
    "index": "F15343",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に見てみると、右のニューラルネットワークは4つのレイヤーを持ってる。",
    "output": "Concretely, taking the example neural network that we have on the right which has four layers."
  },
  {
    "index": "F15344",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、大文字のLは4。",
    "output": "And so capital L is equal to 4."
  },
  {
    "index": "F15345",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それぞれの出力のユニットに対し、いまこのデルタ項を計算しようとしている。",
    "output": "For each output unit, we're going to compute this delta term."
  },
  {
    "index": "F15346",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、この4つ目のレイヤーの、このjのユニットのデルタは以下に等しい、このユニットのアクティベーションの値から引くことの、この、ぼくたちのトレーニングセットの中の、実際に観測された値。",
    "output": "So, delta for the j of unit in the fourth layer is equal to just the activation of that unit minus what was the actual value of 0 in our training example."
  },
  {
    "index": "F15347",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、ここのこの項は、h(x)の下添字jとも書ける。でしょ?",
    "output": "So, this term here can also be written h of x subscript j, right."
  },
  {
    "index": "F15348",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのデルタ項は単に我らの仮説の出力した値と、トレーニングセットでのyの値との差分でしか無い。ここでyの下添字jはトレーニング手本のベクトルの値yのj番目の要素って意味だった。",
    "output": "So this delta term is just the difference between when a hypotheses output and what was the value of y in our training set whereas y subscript j is the j of element of the vector value y in our labeled training set."
  },
  {
    "index": "F15349",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、もしデルタ、a、yをベクトルだとしてもこれはやはり成立し、これはベクトル化された実装となる。それは単にデルタ4にa4-yをセットする。",
    "output": "And by the way, if you think of delta a and y as vectors then you can also take those and come up with a vectorized implementation of it, which is just delta 4 gets set as a4 minus y."
  },
  {
    "index": "F15350",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、デルタ4、a4、yはそれぞれベクトルでその次元はネットワークの出力ユニットの数に等しい。",
    "output": "Where here, each of these delta 4 a4 and y, each of these is a vector whose dimension is equal to the number of output units in our network."
  },
  {
    "index": "F15351",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これで我らは今やネットワークの誤差項であるところのデルタ4を計算した。",
    "output": "So we've now computed the era term's delta 4 for our network."
  },
  {
    "index": "F15352",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にやる事はネットワーク内のより手前のレイヤーのデルタ項を計算する事だ。",
    "output": "What we do next is compute the delta terms for the earlier layers in our network."
  },
  {
    "index": "F15353",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがデルタ3を計算する為の式だ。デルタ3イコールシータ3の転置掛けるデルタ4。",
    "output": "Here's a formula for computing delta 3 is delta 3 is equal to theta 3 transpose times delta 4."
  },
  {
    "index": "F15354",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのドット掛け算は、MATLABなんかにある、要素毎の積。",
    "output": "And this dot times, this is the element y's multiplication operation that we know from MATLAB."
  },
  {
    "index": "F15355",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "g'(z3)は、これもベクトル。そしてこのドット掛け算はこれら2つのベクトルを要素ごとに掛け合わせた物。",
    "output": "So delta 3 transpose delta 4, that's a vector; g prime z3 that's also a vector and so dot times is in element y's multiplication between these two vectors."
  },
  {
    "index": "F15356",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この項、g'(z3)は正式にはアクティベーション関数のgを入力値がz3の所で微分した値。",
    "output": "This term g prime of z3, that formally is actually the derivative of the activation function g evaluated at the input values given by z3."
  },
  {
    "index": "F15357",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし解析学を知ってるなら自分自身で実行してみて私が得たのと同じ答えになる事を確かめられるはずだが、現実的にはようするにどういう意味か、答えを教えちゃおう。",
    "output": "If you know calculus, you can try to work it out yourself and see that you can simplify it to the same answer that I get. But I'll just tell you pragmatically what that means."
  },
  {
    "index": "F15358",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実際にこのg'を計算する為にやるべき事はこれらの微分項はa3ドット掛ける1-a3で、a3はアクティベーションのベクトルだ。",
    "output": "What you do to compute this g prime, these derivative terms is just a3 dot times1 minus A3 where A3 is the vector of activations."
  },
  {
    "index": "F15359",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この1はベクトルで全要素が1を意味し、このa3もそのレイヤーのアクティベーションの値のベクトルだ。",
    "output": "1 is the vector of ones and A3 is again the activation the vector of activation values for that layer."
  },
  {
    "index": "F15360",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に似たような式をデルタ2にも適用する。それも似たような公式で計算出来る。",
    "output": "Next you apply a similar formula to compute delta 2 where again that can be computed using a similar formula."
  },
  {
    "index": "F15361",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "解析学を知ってればこの式が数学的にアクティベーション関数であるところのgの微分と等しい事を示せるだろう。それがまさにg'の事だった。",
    "output": "Only now it is a2 like so and I then prove it here but you can actually, it's possible to prove it if you know calculus that this expression is equal to mathematically, the derivative of the g function of the activation function, which I'm denoting by g prime."
  },
  {
    "index": "F15362",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だって最初のレイヤーは入力レイヤに対応してるのだから、それって単にトレーニングセットで実際に観察される値なのでそれに関連した誤差も何も無い。",
    "output": "And finally, that's it and there is no delta1 term, because the first layer corresponds to the input layer and that's just the feature we observed in our training sets, so that doesn't have any error associated with that."
  },
  {
    "index": "F15363",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ようするに、その値はまったく変更したいなんて思ってない訳だ。",
    "output": "It's not like, you know, we don't really want to try to change those values."
  },
  {
    "index": "F15364",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからデルタ項はこの例だとレイヤー2、3、4にしか無いって訳だ。",
    "output": "And so we have delta terms only for layers 2, 3 and for this example."
  },
  {
    "index": "F15365",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バックプロパゲーションという名前はデルタ項を出力レイヤから計算しはじめてレイヤーを遡っていき隠れレイヤのデルタ項を計算していき、その次にさらにもう一歩戻ってデルタ2を計算して、、、という事からついた名前。つまりある意味で誤差を出力レイヤーからレイヤー3に、そこからさらに前にと伝播(プロパゲート)させていくから、バックプロパゲーションという名前な訳。",
    "output": "The name back propagation comes from the fact that we start by computing the delta term for the output layer and then we go back a layer and compute the delta terms for the third hidden layer and then we go back another step to compute delta 2 and so, we're sort of back propagating the errors from the output layer to layer 3 to their to hence the name back complication."
  },
  {
    "index": "F15366",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、微分はめちゃくちゃ大変だが凄い時間かかるが、単にこれらを一つ一つ計算していけばとても平凡なやり方でかなり面倒だけど数学的に示せるんだがーー本当に示せるんだけど、、、ほんとほんと。",
    "output": "Finally, the derivation is surprisingly complicated, surprisingly involved but if you just do this few steps steps of computation it is possible to prove viral frankly some what complicated mathematical proof."
  },
  {
    "index": "F15367",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし正規化の項を無視すれば我らが欲しい偏微分の項は正確にアクティベーションとこれらのデルタ項で与えられる。",
    "output": "It's possible to prove that if you ignore authorization then the partial derivative terms you want are exactly given by the activations and these delta terms."
  },
  {
    "index": "F15368",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはラムダ無視してる、言い換えると正規化項を。ラムダが0の場合って事だ。",
    "output": "This is ignoring lambda or alternatively the regularization term lambda will equal to 0."
  },
  {
    "index": "F15369",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この細かい事、正規化項については後で修正するがバックプロパゲーションを実行してこれらのデルタ項を計算する事で全てのパラメータについてこれらの偏微分の項を手早く計算出来る。",
    "output": "We'll fix this detail later about the regularization term, but so by performing back propagation and computing these delta terms, you can, you know, pretty quickly compute these partial derivative terms for all of your parameters."
  },
  {
    "index": "F15370",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たくさんの事が出てきたね。",
    "output": "So this is a lot of detail."
  },
  {
    "index": "F15371",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらを全部合わせてパラメータに関する微分の計算をどう実装するのか議論しよう。",
    "output": "Let's take everything and put it all together to talk about how to implement back propagation to compute derivatives with respect to your parameters."
  },
  {
    "index": "F15372",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "しかもトレーニングセットがたくさんあるケースを、一つしか無いケースではなく。こんな風にやる。",
    "output": "And for the case of when we have a large training set, not just a training set of one example, here's what we do."
  },
  {
    "index": "F15373",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "m個のトレーニング手本があるとする。",
    "output": "Suppose we have a training set of m examples like that shown here."
  },
  {
    "index": "F15374",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初にやる事はこれらのデルタl下付き添字ijを、、、ところでこの三角の記号、これは実際はギリシャ文字のアルファベットで大文字のデルタだ。",
    "output": "The first thing we're going to do is we're going to set these delta l subscript i j. So this triangular symbol?"
  },
  {
    "index": "F15375",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前のスライドにあったデルタは小文字のデルタ。",
    "output": "The symbol we had on the previous slide was the lower case delta."
  },
  {
    "index": "F15376",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この三角形は大文字のデルタ。",
    "output": "So the triangle is capital delta."
  },
  {
    "index": "F15377",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最終的には、この大文字のデルタlijは偏微分の項、、、Jのシータの、シータlijに関する偏微分の項を計算するのに使う事になる。",
    "output": "Eventually, this capital delta l i j will be used to compute the partial derivative term, partial derivative respect to theta l i j of J of theta."
  },
  {
    "index": "F15378",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すぐに見る事となるがこれらのデルタはこれらの偏微分を計算する為にちょっとずつ値を足していく為のアキュームレーターとして使う事になる。",
    "output": "So as we'll see in a second, these deltas are going to be used as accumulators that will slowly add things in order to compute these partial derivatives."
  },
  {
    "index": "F15379",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にトレーニングセットをforループで回す。",
    "output": "Next, we're going to loop through our training set."
  },
  {
    "index": "F15380",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、iが1からmまでのfor文、つまりi番目のイテレーションの時はトレーニング手本のxiとyiに関する計算をしてるという事。では、最初にやることは、a1、つまり入力レイヤーのアクティベーションに対しxiをセットする。",
    "output": "So, we'll say for i equals 1 through m and so for the i iteration, we're going to working with the training example xi, yi."
  },
  {
    "index": "F15381",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはi番目のトレーニング手本の入力を表すから。そして次に、フォワードプロパゲーションを適用してレイヤー2、レイヤー3と最後のレイヤーである所のレイヤーLまでを計算していく。",
    "output": "So the first thing we're going to do is set a1 which is the activations of the input layer, set that to be equal to xi is the inputs for our i training example, and then we're going to perform forward propagation to compute the activations for layer two, layer three and so on up to the final layer, layer capital L."
  },
  {
    "index": "F15382",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、yiとラベルづけされた現在見ているトレーニング手本の出力を用いて、ここの出力の誤差を計算する。つまりデルタLは仮説の出力結果引くことのターゲットにしてる観測値。",
    "output": "Next, we're going to use the output label yi from this specific example we're looking at to compute the error term for delta L for the output there."
  },
  {
    "index": "F15383",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてバックプロパゲーションのアルゴリズムを用いて、デルタL-1、デルタL-2、、、とデルタ2までを計算する。ここでもデルタ1は求めない。",
    "output": "And then we're going to use the back propagation algorithm to compute delta L minus 1, delta L minus 2, and so on down to delta 2 and once again there is now delta 1 because we don't associate an error term with the input layer."
  },
  {
    "index": "F15384",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら入力レイヤに対応した誤差というのは想定しないから。そして最後に大文字のΔ(デルタ)の項を使う前の行で書いた偏微分の項を蓄積していく為に。",
    "output": "And finally, we're going to use these capital delta terms to accumulate these partial derivative terms that we wrote down on the previous line."
  },
  {
    "index": "F15385",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところでこの式を眺めて見ると、これもベクトル化出来そうだ。",
    "output": "And by the way, if you look at this expression, it's possible to vectorize this too."
  },
  {
    "index": "F15386",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的にはデルタijを添字のijがインデックスの行列とみなすと、デルタlは行列としてこう書き直せる。",
    "output": "Concretely, if you think of delta ij as a matrix, indexed by subscript ij."
  },
  {
    "index": "F15387",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "デルタlはデルタl足すことの小文字のデルタl+1にalの転置を掛けた物で更新する、と。",
    "output": "Then, if delta L is a matrix we can rewrite this as delta L, gets updated as delta L plus lower case delta L plus one times aL transpose."
  },
  {
    "index": "F15388",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がベクトル化したこれの実装でこれは自動的に全てのi,jに対して値を更新してくれる。",
    "output": "So that's a vectorized implementation of this that automatically does an update for all values of i and j."
  },
  {
    "index": "F15389",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、forループの中身を実行した後でforループの外に出るが、そこで以下を計算する。",
    "output": "Finally, after executing the body of the four-loop we then go outside the four-loop and we compute the following."
  },
  {
    "index": "F15390",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "大文字のDを二つの場合、jが0の時とjが0以外の時に分けて以下のように計算していく。",
    "output": "We compute capital D as follows and we have two separate cases for j equals zero and j not equals zero."
  },
  {
    "index": "F15391",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "jが0の時とはバイアス項に対応するのでだからこのケースでは追加の正規化項が無いのだ。",
    "output": "The case of j equals zero corresponds to the bias term so when j equals zero that's why we're missing is an extra regularization term."
  },
  {
    "index": "F15392",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に正式な証明は極めて複雑だが、頑張れば示せる事としてはこれらDの項をひとたび計算してしまえば、コスト関数のパラメータでの偏微分にぴったりと一致する、という事。だからそれらを最急降下法にでもより高度な最適化アルゴリズムにでも使う事が出来る。",
    "output": "Finally, while the formal proof is pretty complicated what you can show is that once you've computed these D terms, that is exactly the partial derivative of the cost function with respect to each of your perimeters and so you can use those in either gradient descent or in one of the advanced authorization algorithms."
  },
  {
    "index": "F15393",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが、バックプロパゲーションアルゴリズムだ。それとニューラルネットワークのコスト関数の偏微分係数を計算する方法。",
    "output": "So that's the back propagation algorithm and how you compute derivatives of your cost function for a neural network."
  },
  {
    "index": "F15394",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがまるですごいたくさんのディティールとステップが数珠つなぎになった代物に見えることはよくわかってるんだけど。",
    "output": "I know this looks like this was a lot of details and this was a lot of steps strung together."
  },
  {
    "index": "F15395",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも、プログラミングの宿題と、あとのビデオでまた、まとめ直すのですべてのピースのアルゴリズムを全部まとめて。きみがもしバックプロパゲーションを用いてニューラルネットワークのコスト関数のパラメータによる偏微分係数を実装したくなったときに何しなくてはいけないかがはっきり分かるようにね。",
    "output": "But both in the programming assignments write out and later in this video, we'll give you a summary of this so we can have all the pieces of the algorithm together so that you know exactly what you need to implement if you want to implement back propagation to compute the derivatives of your neural network's cost function with respect to those parameters."
  },
  {
    "index": "F15396",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回の動画では、バックプロパゲーションについて話した。",
    "output": "In the previous video, we talked about the backpropagation algorithm."
  },
  {
    "index": "F15397",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多くの人にとっては最初に見たら第一印象は「うげぇ。",
    "output": "To a lot of people seeing it for the first time, their first impression is often that wow this is a really complicated algorithm, and there are all these different steps, and I'm not sure how they fit together."
  },
  {
    "index": "F15398",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして「私、実はそれらがどう組み合わせて使うのか良く分かってないんだぁ、、、」とか、さらに「これらの込み入ったステップはなんだかブラックボックスみたいだ!",
    "output": "And it's kinda this black box of all these complicated steps."
  },
  {
    "index": "F15399",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたがバックプロパゲーションを、もしそんな風に感じていたとしても、実は問題ありません。",
    "output": "In case that's how you're feeling about backpropagation, that's actually okay."
  },
  {
    "index": "F15400",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バックプロパゲーションは残念な事に線形回帰やロジスティック回帰に比べると数学的にクリーンという訳でも数学的にシンプルなアルゴリズムという訳でもありません。",
    "output": "Backpropagation maybe unfortunately is a less mathematically clean, or less mathematically simple algorithm, compared to linear regression or logistic regression."
  },
  {
    "index": "F15401",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも実の所、私も長年、ひょっとしたら今日ですら、バックプロパゲーションが何やってるのかいまいち直感的に理解出来てないなぁ、と思う事はあるけれど、使う分には問題なくとてもしっかりと使えてきました。",
    "output": "And I've actually used backpropagation, you know, pretty successfully for many years."
  },
  {
    "index": "F15402",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうすれば自分で動かす事は出来るようになるよ。そしてこのビデオでやりたい事はバックプロパゲーションの手順を、もうちょっとだけ機械的に見てみたい。",
    "output": "And even today I still don't sometimes feel like I have a very good sense of just what it's doing, or intuition about what back propagation is doing."
  },
  {
    "index": "F15403",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうする事で、もうちょっと感覚的にバックプロパゲーションを機械的に行う手順はどんな感じかを伝えたい。そうする事でこれは少なくとも良さそうなアルゴリズムだな、と思ってもらえたら幸い。",
    "output": "If, for those of you that are doing the programming exercises, that will at least mechanically step you through the different steps of how to implement back prop."
  },
  {
    "index": "F15404",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "と思った君に、ちょっとだけ魔法をかけてあげよう。実際の所、それでも問題無いです!",
    "output": "So you'll be able to get it to work for yourself."
  },
  {
    "index": "F15405",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしそう感じてたとしても、私はバックプロパゲーションを長年使ってきたよ。",
    "output": "And what I want to do in this video is look a little bit more at the mechanical steps of backpropagation, and try to give you a little more intuition about what the mechanical steps the back prop is doing to hopefully convince you that, you know, it's at least a reasonable algorithm."
  },
  {
    "index": "F15406",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たまに理解の難しい事のあるアルゴリズムだけどね。",
    "output": "In case even after this video in case back propagation still seems very black box and kind of like a, too many complicated steps and a little bit magical to you, that's actually okay."
  },
  {
    "index": "F15407",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも願わくばこの動画がバックプロパゲーションを理解する助けにならんことを!",
    "output": "And Even though I've used back prop for many years, sometimes this is a difficult algorithm to understand, but hopefully this video will help a little bit."
  },
  {
    "index": "F15408",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まずフォワードプロパゲーションが何をしてるのか別の角度から詳しく見ていこう。",
    "output": "In order to better understand backpropagation, let's take another closer look at what forward propagation is doing."
  },
  {
    "index": "F15409",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バイアスユニットを数えない。そして2つの隠れユニットがこのレイヤーにありさらに2つの隠れユニットがその次のレイヤーにある。",
    "output": "Here's a neural network with two input units that is not counting the bias unit, and two hidden units in this layer, and two hidden units in the next layer."
  },
  {
    "index": "F15410",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に出力ユニット。",
    "output": "And then, finally, one output unit."
  },
  {
    "index": "F15411",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらのカウント、2、2、2は、このトップのバイアスユニットを数えていない。",
    "output": "Again, these counts two, two, two, are not counting these bias units on top."
  },
  {
    "index": "F15412",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フォーワードプロパゲーションをわかりやすく説明するためにこのネットワークをちょっと違う風に描いてみよう。",
    "output": "In order to illustrate forward propagation, I'm going to draw this network a little bit differently."
  },
  {
    "index": "F15413",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、このニューラルネットのノードを大きな楕円で描くことで、その中にテキストを書けるようにする。",
    "output": "And in particular I'm going to draw this neuro-network with the nodes drawn as these very fat ellipsis, so that I can write text in them."
  },
  {
    "index": "F15414",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フォワードプロパゲーションを実行する時はある手本に対してそれを手本x(i)、y(i)としよう、そしてこのx(i)こそが入力レイヤーに食わせる物だ。",
    "output": "And it'll be this x i that we feed into the input layer. So this maybe x i 2 and x i 2 are the values we set the input layer to."
  },
  {
    "index": "F15415",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、このx(i)の1とx(i)の2の2つの値が入力レイヤーにセットする値で、そしてその値を最初の隠れレイヤーへとフォワードプロパゲートするには、z(2)の1とz(2)の2を計算し、ところで、これらは入力ユニットからの入力の重み付け和だ。",
    "output": "And when we forward propagated to the first hidden layer here, what we do is compute z (2) 1 and z (2) 2. So these are the weighted sum of inputs of the input units."
  },
  {
    "index": "F15416",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、ロジスティック関数のsigmoid関数を適用するーーーsigmoidアクティベーション関数をzの値に適用すると、これらのアクティベーションの値が得られる。",
    "output": "And then we apply the sigmoid of the logistic function, and the sigmoid activation function applied to the z value. Here's are the activation values."
  },
  {
    "index": "F15417",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり以上で、a(2)の1とa(2)の2が得られる。",
    "output": "So that gives us a (2) 1 and a (2) 2."
  },
  {
    "index": "F15418",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にそれをまたフォワードプロパゲートして、ここのz(3)の1にsigmoidのロジスティック関数、アクティベーション関数を適用し、そしてa(3)の1を得る。",
    "output": "Apply the sigmoid of the logistic function, the activation function to that to get a (3) 1."
  },
  {
    "index": "F15419",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に、z(4)の1を得て、そこにアクティベーション関数を適用してa(4)の1を得るまで続ける。",
    "output": "And similarly, like so until we get z (4) 1. Apply the activation function."
  },
  {
    "index": "F15420",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これこそが、ネットワークの最終的な出力の値だ。",
    "output": "This gives us a (4)1, which is the final output value of the neural network."
  },
  {
    "index": "F15421",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ちょっとスペース開ける為にこの矢印を消す。",
    "output": "Let's erase this arrow to give myself some more space."
  },
  {
    "index": "F15422",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの計算が実際に何をやってるのか見てみよう。",
    "output": "And if you look at what this computation really is doing, focusing on this hidden unit, let's say."
  },
  {
    "index": "F15423",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この隠れユニットにフォーカスしてみると、このウェイト、マゼンダで示したがウェイト、シータ(2)10だとしよう。",
    "output": "We have to add this weight. Shown in magenta there is my weight theta (2) 1 0, the indexing is not important."
  },
  {
    "index": "F15424",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、こんな風に、ここは赤でシータ(2)11、そしてここのウェイトは緑、、、というよりはシアンで描いたのがシータ(2)12。",
    "output": "And this way here, which I'm highlighting in red, that is theta (2) 1 1 and this weight here, which I'm drawing in cyan, is theta (2) 1 2."
  },
  {
    "index": "F15425",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がz(3)1をz(3)1を計算する方法。このウェイトにこの値を掛ける。",
    "output": "So the way we compute this value, z(3)1 is, z(3)1 is as equal to this magenta weight times this value."
  },
  {
    "index": "F15426",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりシータ(2)10掛ける1、足すことの赤のウェイト掛けるこの値、つまりシータ(2)11掛けるa(2)1。",
    "output": "So that's theta (2) 10 x 1. And then plus this red weight times this value, so that's theta(2) 11 times a(2)1."
  },
  {
    "index": "F15427",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後にこのシアン、掛けるこの値。つまり、足すことのシータ(2)12掛けるa(2)1。",
    "output": "And finally this cyan weight times this value, which is therefore plus theta(2)12 times a(2)1."
  },
  {
    "index": "F15428",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのビデオの後半で見ると明らかになるが、バックプロパゲーションでやる事もとてもこれと似たプロセスだったりする。",
    "output": "And it turns out that as we'll see later in this video, what backpropagation is doing is doing a process very similar to this."
  },
  {
    "index": "F15429",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "違いは計算の流れがネットワークの左から右へと流れる代わりに、そこでは計算はネットワークの左から右へと流れる。",
    "output": "Except that instead of the computations flowing from the left to the right of this network, the computations since their flow from the right to the left of the network."
  },
  {
    "index": "F15430",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれととても似た計算を用いて、二枚のスライドでちゃんと説明する。",
    "output": "And using a very similar computation as this."
  },
  {
    "index": "F15431",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バックプロパゲーションが何やってるのかより良く理解する為に、コスト関数を見てみよう。",
    "output": "To better understand what backpropagation is doing, let's look at the cost function."
  },
  {
    "index": "F15432",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つしか出力ユニットが無い時のコスト関数を。",
    "output": "It's just the cost function that we had for when we have only one output unit."
  },
  {
    "index": "F15433",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし一つよりも多くの出力ユニットがある時は単に出力ユニットのインデックスについて足し合わせてやれば良い。",
    "output": "If we have more than one output unit, we just have a summation you know over the output units indexed by k there."
  },
  {
    "index": "F15434",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし出力ユニットが一つしかなければこれがコスト関数。",
    "output": "If you have only one output unit then this is a cost function."
  },
  {
    "index": "F15435",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フォワードプロパゲーションとバックプロパゲーションを一度に一つの手本データずつに対して行う。",
    "output": "And we do forward propagation and backpropagation on one example at a time."
  },
  {
    "index": "F15436",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから一つの手本、x(i)、y(i)にフォーカスしよう。そして出力ユニットは一つのケースについてフォーカスしよう。",
    "output": "So let's just focus on the single example, x (i) y (i) and focus on the case of having one output unit."
  },
  {
    "index": "F15437",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりここのy(i)は単なる実数。",
    "output": "So y (i) here is just a real number."
  },
  {
    "index": "F15438",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらに正規化は無視しよう。つまりラムダは0。",
    "output": "And let's ignore regularization, so lambda equals 0."
  },
  {
    "index": "F15439",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうすればこの最後の正規化の項は消えるからね。",
    "output": "And this final term, that regularization term, goes away."
  },
  {
    "index": "F15440",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、この和の中を見てみれば、i番目のトレーニング手本に対応するコストの項はつまりx(i)とy(i)に対応したコストはこの式で与えられる。",
    "output": "Now if you look inside the summation, you find that the cost term associated with the training example, that is the cost associated with the training example x(i), y(i)."
  },
  {
    "index": "F15441",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがi番目のトレーニング手本に対応したコストという事になる。",
    "output": "That's going to be given by this expression. So, the cost to live off examplie i is written as follows."
  },
  {
    "index": "F15442",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのコスト関数がやってるのは誤差の二乗に似た感じの役割。",
    "output": "And what this cost function does is it plays a role similar to the squared arrow."
  },
  {
    "index": "F15443",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこの複雑な式を見る代わりにi番目に対応したコストをニューラルネットワークの出力と実際の値との差分の二乗みたいなもんだと考えても良い。",
    "output": "So, rather than looking at this complicated expression, if you want you can think of cost of i being approximately the square difference between what the neural network outputs, versus what is the actual value."
  },
  {
    "index": "F15444",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰の時と同様に実際にはこちらのlogを使ったちょびっと複雑なコスト関数の方がいいんだけど、感覚的に理解する、という点では誤差の二乗のコスト関数なんだと考えて差し支えない。",
    "output": "Just as in logistic repression, we actually prefer to use the slightly more complicated cost function using the log. But for the purpose of intuition, feel free to think of the cost function as being the sort of the squared error cost function."
  },
  {
    "index": "F15445",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのコストのiはどのくらいネットワークがうまく手本iを予測しているかを測ってる。",
    "output": "And so this cost(i) measures how well is the network doing on correctly predicting example i."
  },
  {
    "index": "F15446",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どのくらい出力が実際の観測値、y(i)と近いかを。",
    "output": "How close is the output to the actual observed label y(i)?"
  },
  {
    "index": "F15447",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではバックプロパゲーションが何をやってるかを見ていこう。",
    "output": "Now let's look at what backpropagation is doing."
  },
  {
    "index": "F15448",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "有用な直感的な説明としては、バックプロパゲーションはこれらのデルタ上付き添字l下付き添字jの項を計算している、というのがある。",
    "output": "One useful intuition is that backpropagation is computing these delta superscript l subscript j terms."
  },
  {
    "index": "F15449",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらをアクティベーションの値の「誤差」と考える事が出来る、l番目のレイヤーのj番目のユニットのアクティベーション。",
    "output": "And we can think of these as the quote error of the activation value that we got for unit j in the layer, in the lth layer."
  },
  {
    "index": "F15450",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より正式には解析学が得意な人向けだろうけど、より正式には、デルタ項が実際になんなのかというと、これだ!",
    "output": "More formally, for, and this is maybe only for those of you who are familiar with calculus."
  },
  {
    "index": "F15451",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらはz(l)jによる偏微分係数。このzは入力の重み付き和を計算したもので、これらによるコスト関数の偏微分となる。",
    "output": "More formally, what the delta terms actually are is this, they're the partial derivative with respect to z,l,j, that is this weighted sum of inputs that were confusing these z terms."
  },
  {
    "index": "F15452",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的にはコスト関数はラベルyとこのh(x)の、ネットワークの出力した値の関数。",
    "output": "So concretely, the cost function is a function of the label y and of the value, this h of x output value neural network."
  },
  {
    "index": "F15453",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしこのネットワークの中に入ってそれらのz(l)jの値をちょっとずらしたら、これらの値がニューラルネットに影響を与えて最終的にはコスト関数も変わる。",
    "output": "And if we could go inside the neural network and just change those z l j values a little bit, then that will affect these values that the neural network is outputting. And that will end up changing the cost function."
  },
  {
    "index": "F15454",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もう一度言っておくとこれはほんと解析得意な人向けの話。",
    "output": "And again really, this is only for those of you who are expert in Calculus."
  },
  {
    "index": "F15455",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "偏微分に慣れ親しんでて、快適に使える人向け。これらのデルタの項は我らの計算している、これらの中間項によるコスト関数の偏微分となっている。",
    "output": "If you're comfortable with partial derivatives, what these delta terms are is they turn out to be the partial derivative of the cost function, with respect to these intermediate terms that were confusing."
  },
  {
    "index": "F15456",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれらは、これらの中間の値の計算に影響を与えて、ニューラルネットワークの最終出力、h(x)に影響を与えて、結果として全体のコストに影響を与える為にどれだけニューラルネットワークのウェイトを変更すれば良いかの指標となっている。",
    "output": "And so they're a measure of how much would we like to change the neural network's weights, in order to affect these intermediate values of the computation. So as to affect the final output of the neural network h(x) and therefore affect the overall cost."
  },
  {
    "index": "F15457",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この最後の部分の偏微分の直感がいまいちピンと来て無くても、心配ご無用。",
    "output": "In case this lost part of this partial derivative intuition, in case that doesn't make sense."
  },
  {
    "index": "F15458",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここから後は偏微分なんて計算出来なくてもやっていけるから。",
    "output": "Don't worry about the rest of this, we can do without really talking about partial derivatives."
  },
  {
    "index": "F15459",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも、バックプロパゲーションが何をやっているのかもうちょっと詳しく見てみよう。",
    "output": "But let's look in more detail about what backpropagation is doing."
  },
  {
    "index": "F15460",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "出力レイヤについては、最初にセットされるデルタ項だが、それはデルタ(4)1をy(i)、、、もしこのトレーニング手本のiについてフォワードプロパゲーションを行い、さらにバックプロパゲーションを行うとすると、y(i)-a(4)1となる。",
    "output": "For the output layer, the first set's this delta term, delta (4) 1, as y (i) if we're doing forward propagation and back propagation on this training example i. That says y(i) minus a(4)1."
  },
  {
    "index": "F15461",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、それは本当に誤差だ。",
    "output": "So this is really the error, right?"
  },
  {
    "index": "F15462",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実際の値であるyから、引くことの予言された値なのだから。こんな風にデルタ(4)1を計算する。",
    "output": "It's the difference between the actual value of y minus what was the value predicted, and so we're gonna compute delta(4)1 like so."
  },
  {
    "index": "F15463",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、これらの値を後ろ(バック)へと伝播(プロパゲート)させていく。",
    "output": "Next we're gonna do, propagate these values backwards."
  },
  {
    "index": "F15464",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すぐに説明する。で、一つ前のデルタ項を計算する事になる。",
    "output": "I'll explain this in a second, and end up computing the delta terms for the previous layer."
  },
  {
    "index": "F15465",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "結局、デルタ(3)1とデルタ(3)2になる。",
    "output": "We're gonna end up with delta(3)1. Delta(3)2."
  },
  {
    "index": "F15466",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその後に、これをさらに後ろへと伝播させていき、デルタ(2)1とデルタ(2)2を計算する事になる。",
    "output": "And then we're gonna propagate this further backward, and end up computing delta(2)1 and delta(2)2."
  },
  {
    "index": "F15467",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでくると、バックプロパゲーションの計算はかなりフォワードプロパゲーションのアルゴリズムを実行するのに似通ってくる。",
    "output": "Now the backpropagation calculation is a lot like running the forward propagation algorithm, but doing it backwards."
  },
  {
    "index": "F15468",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それの意味する所はこうだ。",
    "output": "So here's what I mean."
  },
  {
    "index": "F15469",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうやってこのデルタ(2)2まで来たのか見てみよう。",
    "output": "Let's look at how we end up with this value of delta(2)2. So we have delta(2)2."
  },
  {
    "index": "F15470",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フォワードプロパゲーションみたいに幾つかのウェイトにラベルをつけよう。",
    "output": "And similar to forward propagation, let me label a couple of the weights."
  },
  {
    "index": "F15471",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このウェイトはシアンの色で、シータ(2)の12で、そしてこの下のウェイトは、赤で書こう、これを以後、シータ(2)の22と呼ぼう。",
    "output": "So this weight, which I'm going to draw in cyan. Let's say that weight is theta(2)1 2, and this one down here when we highlight this in red."
  },
  {
    "index": "F15472",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、デルタ(2)の2がどう計算されるかこのノードはどう計算されるのか?見てみよう。",
    "output": "That is going to be let's say theta(2) of 2 2."
  },
  {
    "index": "F15473",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "計算する為にやる事はこの値を取ってこのウェイトを掛ける、そしてそれを足し合わせる事のこの値掛けるそのウェイト。",
    "output": "So if we look at how delta(2)2, is computed, how it's computed with this note."
  },
  {
    "index": "F15474",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれは本当にこれらのデルタの値の重み付き和だ。",
    "output": "It turns out that what we're going to do, is gonna take this value and multiply it by this weight, and add it to this value multiplied by that weight."
  },
  {
    "index": "F15475",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "対応するエッジの重みで重みづけする。具体的に書き下してみよう。",
    "output": "So it's really a weighted sum of these delta values, weighted by the corresponding edge strength."
  },
  {
    "index": "F15476",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このデルタ(2)2はイコール、シータ(2)の12ーーこれはマゼンダ色で描いた重みーー掛ける事のデルタ(3)1、足すことの次は赤の奴。",
    "output": "So completely, let me fill this in, this delta(2)2 is going to be equal to, Theta(2)1 2 is that magenta lay times delta(3)1. Plus, and the thing I had in red, that's theta (2)2 times delta (3)2."
  },
  {
    "index": "F15477",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり本当に、文字通り、この赤のウェイト掛けるこの値、足すことのこのマゼンダのウェイト掛けることのこの値。",
    "output": "So it's really literally this red wave times this value, plus this magenta weight times this value."
  },
  {
    "index": "F15478",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがこの値、デルタを計算する方法。もう一つ見てみよう。",
    "output": "And that's how we wind up with that value of delta."
  },
  {
    "index": "F15479",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この値を見てみる。",
    "output": "And just as another example, let's look at this value."
  },
  {
    "index": "F15480",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この値はどうやったら得られるか?",
    "output": "How do we get that value?"
  },
  {
    "index": "F15481",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは似た感じで、このウェイトをとりあえず緑で表しておくと、このウェイトをシータ(3)の12とすると、デルタ(3)2はその場合、イコール緑のウェイト、シータ(3)12掛けるデルタ(4)1だ。",
    "output": "Then we have that delta (3) 2 is going to be equal to that green weight, theta (3) 12 times delta (4) 1."
  },
  {
    "index": "F15482",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、ここまでの所、デルタの値を隠れユニットにだけ書いてきた。そしてバイアスのユニットには書いて来なかった。",
    "output": "And by the way, so far I've been writing the delta values only for the hidden units, but excluding the bias units."
  },
  {
    "index": "F15483",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バックプロパゲーションのアルゴリズムをどう定義するか、またはどう実装するかによって、これらのバイアスユニットに対するデルタの値を計算する事になったりする。",
    "output": "Depending on how you define the backpropagation algorithm, or depending on how you implement it, you know, you may end up implementing something that computes delta values for these bias units as well."
  },
  {
    "index": "F15484",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその値を変更する方法は存在しない。",
    "output": "The bias units always output the value of plus one, and they are just what they are, and there's no way for us to change the value."
  },
  {
    "index": "F15485",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからバックプロパゲーションの実装方法によるが、私の普段の実装方法だと、これらのデルタの値は計算してる。",
    "output": "And so, depending on your implementation of back prop, the way I usually implement it."
  },
  {
    "index": "F15486",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも、ただその結果を捨てていて、使っていない。",
    "output": "I do end up computing these delta values, but we just discard them, we don't use them."
  },
  {
    "index": "F15487",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならそれらは結局のところ微分を計算するのに必要な計算に含まれていないから。",
    "output": "Because they don't end up being part of the calculation needed to compute a derivative."
  },
  {
    "index": "F15488",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以上でバックプロパゲーションが何をしているか、感覚的な理解がちょっとでも与えられたら幸い。",
    "output": "So hopefully that gives you a little better intuition about what back propegation is doing."
  },
  {
    "index": "F15489",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこれらを見てもなお、全部魔法のようであまりにもブラックボックスに感じられた場合はあとのビデオ、「PuttingItTogether」(ビデオのタイトル)の中で、バックプロパゲーションが何をしているかについて、さらなる直感を提供したいと思っている。",
    "output": "In case of all of this still seems sort of magical, sort of black box, in a later video, in the putting it together video, I'll try to get a little bit more intuition about what backpropagation is doing."
  },
  {
    "index": "F15490",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは、可視化したり、それが本当の所何をやっているのかを理解するのは難しいアルゴリズムだ。",
    "output": "But unfortunately this is a difficult algorithm to try to visualize and understand what it is really doing."
  },
  {
    "index": "F15491",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私の推測するところによると、たくさんの人がとてもうまくこのアルゴリズムを使えていて、実装してみると、とても効率的な学習アルゴリズムだ、たとえ中がどうなってるのか、正確に何が起こるかを可視化するのが難しいにせよ。",
    "output": "And if you implement the algorithm you can have a very effective learning algorithm. Even though the inner workings of exactly how it works can be harder to visualize."
  },
  {
    "index": "F15492",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではコスト関数の微分を計算する為にバックプロパゲーションをどう使うかを議論した。",
    "output": "In the previous video, we talked about how to use back propagation to compute the derivatives of your cost function."
  },
  {
    "index": "F15493",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、行列からベクトルへのパラメータのアンロールという、細かい実装の話を簡単に行う、アドバンスドな最適化ルーチンを使うのに必要となるから。",
    "output": "In this video, I want to quickly tell you about one implementational detail of unrolling your parameters from matrices into vectors, which we need in order to use the advanced optimization routines."
  },
  {
    "index": "F15494",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的にいこう、パラメータシータを受け取ってコスト関数とその微分を返す関数を実装したとしよう。",
    "output": "Concretely, let's say you've implemented a cost function that takes this input, you know, parameters theta and returns the cost function and returns derivatives."
  },
  {
    "index": "F15495",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、これをfminuncのようなアドバンスドな最適化アルゴリズムに渡す事が出来る、ところで、このfminuncは唯一の選択肢という訳では無い。",
    "output": "Then you can pass this to an advanced authorization algorithm by fminunc and fminunc isn't the only one by the way."
  },
  {
    "index": "F15496",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもそれらは全てコスト関数のポインタとシータの初期値を受け取る。",
    "output": "But what all of them do is take those input pointedly the cost function, and some initial value of theta."
  },
  {
    "index": "F15497",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらのルーチンはシータとシータの初期値をどちらもパラメータベクトルと想定する、RのnとかRのn+1とか。",
    "output": "And both, and these routines assume that theta and the initial value of theta, that these are parameter vectors, maybe Rn or Rn plus 1."
  },
  {
    "index": "F15498",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらはベクトルだが、コスト関数の実装が二番目の返値として返す微分項もRのnなりRのn+1なりを仮定する。",
    "output": "But these are vectors and it also assumes that, you know, your cost function will return as a second return value this gradient which is also Rn and Rn plus 1."
  },
  {
    "index": "F15499",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれもベクトルだ。",
    "output": "So also a vector."
  },
  {
    "index": "F15500",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰で使ってる時はこれで問題無かったのだが、今やニューラルネットワークなので、パラメータはもうベクトルでは無くなってしまった。今やパラメータはこれらの行列で四段のニューラルネットワークだとするとパラメータ行列シータ1、シータ2,シータ3を持ちOctaveではこれらは行列Theta1、Theta2、Theta3と表されるだろう。",
    "output": "This worked fine when we were using logistic progression but now that we're using a neural network our parameters are no longer vectors, but instead they are these matrices where for a full neural network we would have parameter matrices theta 1, theta 2, theta 3 that we might represent in Octave as these matrices theta 1, theta 2, theta 3."
  },
  {
    "index": "F15501",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様にこれらのgradientの項として返されると期待しているのは、前回のビデオでこれらのgradient項をどう計算するかを扱ったが、それらは大文字のD1、D2、D3で、それはOctaveでは行列D1、D2、D3として表される。",
    "output": "Well, in the previous video we showed how to compute these gradient matrices, which was capital D1, capital D2, capital D3, which we might represent an octave as matrices D1, D2, D3."
  },
  {
    "index": "F15502",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、これらの行列をとってどうベクトルにアンロールするかをお話する。",
    "output": "In this video I want to quickly tell you about the idea of how to take these matrices and unroll them into vectors."
  },
  {
    "index": "F15503",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここのシータとして渡すのに適切なフォーマットにしたりここのgradientから取り出す為に。",
    "output": "So that they end up being in a format suitable for passing into as theta here off for getting out for a gradient there."
  },
  {
    "index": "F15504",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に、入力レイヤとして10個のユニットがあり、隠れレイヤとして10ユニット、そして出力レイヤとしてユニット一つとしよう。そしてs1はレイヤ1のユニット数、s2はレイヤ2のユニット数、そしてs3はレイヤ3のユニット数とする。",
    "output": "Concretely, let's say we have a neural network with one input layer with ten units, hidden layer with ten units and one output layer with just one unit, so s1 is the number of units in layer one and s2 is the number of units in layer two, and s3 is a number of units in layer three."
  },
  {
    "index": "F15505",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合、行列シータの次元とDの次元はこれらの式で与えられる。",
    "output": "In this case, the dimension of your matrices theta and D are going to be given by these expressions."
  },
  {
    "index": "F15506",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、シータ1は10x11行列、などとなる。",
    "output": "For example, theta one is going to a 10 by 11 matrix and so on."
  },
  {
    "index": "F15507",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれらの行列をベクトルと変換したければ、変換したければ、可能な手としてはTheta1、Theta2、Theta3に対してこんなコードを書くと、3つのシータ行列から全ての要素を取り出して、つまりシータ1の全要素、シータ2の全要素、シータ3の全要素を取り出して、それらをアンロール(展開)して、それら全要素を一つの長いベクトルに突っ込む。",
    "output": "vectors. What you can do is take your theta 1, theta 2, theta 3, and write this piece of code and this will take all the elements of your three theta matrices and take all the elements of theta one, all the elements of theta 2, all the elements of theta 3, and unroll them and put all the elements into a big long vector."
  },
  {
    "index": "F15508",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがthetaVecとなる。同様に二番目のコマンドはDの行列全てを大きな長いベクトル、DVecにアンロールする。",
    "output": "Which is thetaVec and similarly the second command would take all of your D matrices and unroll them into a big long vector and call them DVec."
  },
  {
    "index": "F15509",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後にもしベクトルの表現から行列の表現に戻したくなったら、例えばシータ1を取り戻したいと思ったとすると、やるべき事はまずthetaVecから最初の110個の要素を取り出す。",
    "output": "And finally if you want to go back from the vector representations to the matrix representations. What you do to get back to theta one say is take thetaVec and pull out the first 110 elements."
  },
  {
    "index": "F15510",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりシータ1は110個の要素があるという事、何故ならそれは10x11の行列だから。だから最初の110個の要素を取り出しそしてそこで、単に変形してtheta1に戻す事が出来る。",
    "output": "So theta 1 has 110 elements because it's a 10 by 11 matrix so that pulls out the first 110 elements and then you can use the reshape command to reshape those back into theta 1."
  },
  {
    "index": "F15511",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様にシータ2を取り戻すには、次の110要素を取り出しreshapeすれば良い。",
    "output": "And similarly, to get back theta 2 you pull out the next 110 elements and reshape it."
  },
  {
    "index": "F15512",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてシータ3は、最後の11要素を取り出し、reshapeを実行してシータ3が取り戻せる。",
    "output": "And for theta 3, you pull out the final eleven elements and run reshape to get back the theta 3."
  },
  {
    "index": "F15513",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例の為にシータ1を、onesの10x11にセットしよう、つまり全ての要素が1の行列になる。",
    "output": "So for this example let's set theta 1 equal to be ones of 10 by 11, so it's a matrix of all ones."
  },
  {
    "index": "F15514",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "見やすいように、シータ2は2掛けるonesの10x11と、さらにシータ3は3掛けるonesの1x11としよう。",
    "output": "And just to make this easier seen, let's set that to be 2 times ones, 10 by 11 and let's set theta 3 equals 3 times 1's of 1 by 11."
  },
  {
    "index": "F15515",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり以上で3つの異なる行列シータ1、シータ2、シータ3が出来た。",
    "output": "So this is 3 separate matrices: theta 1, theta 2, theta 3."
  },
  {
    "index": "F15516",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら全てをベクトルに突っ込みたい。",
    "output": "We want to put all of these as a vector."
  },
  {
    "index": "F15517",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "thetaVecは、イコールtheta1;theta2;theta3だ。",
    "output": "ThetaVec equals theta 1; theta 2 theta 3."
  },
  {
    "index": "F15518",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "真ん中にあるのはコロンだ。これでthetaVecはとても長いベクトルとなる。",
    "output": "Right, that's a colon in the middle and like so and now thetavec is going to be a very long vector."
  },
  {
    "index": "F15519",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "231要素のベクトルだ。",
    "output": "That's 231 elements."
  },
  {
    "index": "F15520",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを表示すると、このとても長いベクトルは最初の行列の要素全てと、二番目の行列の要素全てと三番目の行列の要素全てだ。",
    "output": "If I display it, I find that this very long vector with all the elements of the first matrix, all the elements of the second matrix, then all the elements of the third matrix."
  },
  {
    "index": "F15521",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最初の行列を取り出したいとしたら、thetaVecをreshapeすれば良い。",
    "output": "And if I want to get back my original matrices, I can do reshape thetaVec."
  },
  {
    "index": "F15522",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の110の要素を取り出しそれを10x11行列にreshapeしよう。",
    "output": "Let's pull out the first 110 elements and reshape them to a 10 by 11 matrix."
  },
  {
    "index": "F15523",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に続く110要素を取り出す。",
    "output": "And if I then pull out the next 110 elements."
  },
  {
    "index": "F15524",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりインデックスで111から220まで。",
    "output": "So that's indices 111 to 220."
  },
  {
    "index": "F15525",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これで2を全て取り出し、そして残りの221から最後までの要素で、それは231番目の要素となるが、それを1x11にreshapeする。これでtheta3に戻せる。",
    "output": "And if I go from 221 up to the last element, which is element 231, and reshape to 1 by 11, I get back theta 3."
  },
  {
    "index": "F15526",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このプロセスをもっともっと具体的にすべく学習アルゴリズムを実装する時にアンロールのアイデアをどう使うかをここに示す。",
    "output": "To make this process really concrete, here's how we use the unrolling idea to implement our learning algorithm."
  },
  {
    "index": "F15527",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シータ1、シータ2、シータ3の何らかの初期値があるとする。",
    "output": "Let's say that you have some initial value of the parameters theta 1, theta 2, theta 3."
  },
  {
    "index": "F15528",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らがやりたいのはこれら全部を取り出して一つの長いベクトルにアンロールしたい。これをinitialThetaと呼ぼう。",
    "output": "What we're going to do is take these and unroll them into a long vector we're gonna call initial theta to pass in to fminunc as this initial setting of the parameters theta."
  },
  {
    "index": "F15529",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他にやらなきゃいけない事としては、コスト関数を実装する、という事。",
    "output": "The other thing we need to do is implement the cost function."
  },
  {
    "index": "F15530",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが私のコスト関数の実装だ。",
    "output": "Here's my implementation of the cost function."
  },
  {
    "index": "F15531",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コスト関数はthetaVecという入力を受け取る、これはパラメータのベクトルで、それはベクトルにアンロールされた形式で入っている。",
    "output": "The cost function is going to give us input, thetaVec, which is going to be all of my parameters vectors that in the form that's been unrolled into a vector."
  },
  {
    "index": "F15532",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから最初にやるべき事はthetaVecを使って、reshape関数を使い、thetaVecから要素を取り出しreshapeを使って元のパラメータ行列、シータ1、シータ2、シータ3を復元する。",
    "output": "So the first thing I'm going to do is I'm going to use thetaVec and I'm going to use the reshape functions. So I'll pull out elements from thetaVec and use reshape to get back my original parameter matrices, theta 1, theta 2, theta 3."
  },
  {
    "index": "F15533",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらが得られるであろう行列だ。",
    "output": "So these are going to be matrices that I'm going to get."
  },
  {
    "index": "F15534",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こうすることで、微分やコスト関数、Jのシータを計算する為にフォワードプロパゲーションやバックプロパゲーションを実行する為にこれらの行列を使う事が出来る。",
    "output": "So that gives me a more convenient form in which to use these matrices so that I can run forward propagation and back propagation to compute my derivatives, and to compute my cost function j of theta."
  },
  {
    "index": "F15535",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、微分をとって、シータをアンロールした時と要素が同じ順番になるようにアンロール出来る。",
    "output": "And finally, I can then take my derivatives and unroll them, to keeping the elements in the same ordering as I did when I unroll my thetas."
  },
  {
    "index": "F15536",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、今度はD1、D2、D3をアンロールする、コスト関数が返す事ができるgradientVecを得る為に。",
    "output": "But I'm gonna unroll D1, D2, D3, to get gradientVec which is now what my cost function can return."
  },
  {
    "index": "F15537",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これでこれらの微分のベクトルを返す事が出来る。",
    "output": "It can return a vector of these derivatives."
  },
  {
    "index": "F15538",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、ガウス分布が行列表現にしたりベクトル表現にしたり、の変換をどうやったらいいのかだいぶはっきり分かったんじゃないかな。",
    "output": "So, hopefully, you now have a good sense of how to convert back and forth between the matrix representation of the parameters versus the vector representation of the parameters."
  },
  {
    "index": "F15539",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "行列表現の利点はパラメータを行列に保存しておけばフォワードプロパゲーションやバックワードプロパゲーションを行う時により便利で、しかも実装をいわゆるベクトル化する時にも行列の方がやりやすい、という利点がある。",
    "output": "The advantage of the matrix representation is that when your parameters are stored as matrices it's more convenient when you're doing forward propagation and back propagation and it's easier when your parameters are stored as matrices to take advantage of the, sort of, vectorized implementations."
  },
  {
    "index": "F15540",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方対照的に、ベクトル表現の利点は、つまりthetaVecとかDVecにしておく利点はアドバンスドな最適化アルゴリズムを使う時だ。",
    "output": "Whereas in contrast the advantage of the vector representation, when you have like thetaVec or DVec is that when you are using the advanced optimization algorithms."
  },
  {
    "index": "F15541",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらのアルゴリズムはパラメータを一つの大きなベクトルにアンロールしてある事を仮定している事が多い。",
    "output": "Those algorithms tend to assume that you have all of your parameters unrolled into a big long vector."
  },
  {
    "index": "F15542",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以上見てきた事で、2つの間を手早く変換出来るようになった事でしょう。",
    "output": "And so with what we just went through, hopefully you can now quickly convert between the two as needed."
  },
  {
    "index": "F15543",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回までの一連のビデオでニューラルネットワークにおいて微分を計算するために、フォワードプロパゲーションとバックワードプロパゲーションを行うやり方を見てきた。",
    "output": "In the last few videos we talked about how to do forward propagation and back propagation in a neural network in order to compute derivatives."
  },
  {
    "index": "F15544",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがバックプロパはたくさんの細かい部分のあるアルゴリズムで実装するのにちょっとトリッキーな所がある。",
    "output": "But back prop as an algorithm has a lot of details and can be a little bit tricky to implement."
  },
  {
    "index": "F15545",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして一つの不運な特徴としてバックプロパは色々と微妙にバグる、というのがある。",
    "output": "And one unfortunate property is that there are many ways to have subtle bugs in back prop."
  },
  {
    "index": "F15546",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから最急降下法なり別の最適化アルゴリズムなりなどで実行すると、ぱっと見うまく行ってるように見えたりする。",
    "output": "So that if you run it with gradient descent or some other optimizational algorithm, it could actually look like it's working."
  },
  {
    "index": "F15547",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてあなたのコスト関数Jのシータが、結局は最急降下法の各イテレーションで減少していく場合があるが、それはあなたのバックプロパの実装にバグがあるのを見逃していても起こりうる。",
    "output": "And your cost function, J of theta may end up decreasing on every iteration of gradient descent. But this could prove true even though there might be some bug in your implementation of back prop."
  },
  {
    "index": "F15548",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、Jのシータは減少しているように見えるが、バグ無しの実装に比べて結局はより高レベルのエラーに見舞われる可能性があり、そんなパフォーマンスになってしまっている微妙なバグに単に気づいていないだけ、という事にもなりかねない。",
    "output": "So that it looks J of theta is decreasing, but you might just wind up with a neural network that has a higher level of error than you would with a bug free implementation."
  },
  {
    "index": "F15549",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこの事態にどう対処すれば良いか?",
    "output": "So, what can we do about this?"
  },
  {
    "index": "F15550",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "グラディアントチェッキングと呼ばれるアイデアがあり、それはこれらの問題のほとんどを駆逐してくれる。",
    "output": "There's an idea called gradient checking that eliminates almost all of these problems."
  },
  {
    "index": "F15551",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこんにちでは、バックプロパゲーションなりそれ以外でもそれなりに複雑なモデルの最急降下法を実装する時には、私は毎回、グラディアントチェッキングを実装するようにしている。",
    "output": "So, today every time I implement back propagation or a similar gradient to a on a neural network or any other reasonably complex model, I always implement gradient checking."
  },
  {
    "index": "F15552",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれを行えば、あなたのフォワードプロパゲートやバックワードプロパゲートやそれ以外のなんでも、とにかく実装した物が、100%正しい、と確認したり、深く確信したりするのを助けてくれる。",
    "output": "And if you do this, it will help you make sure and sort of gain high confidence that your implementation of four prop and back prop or whatever is 100% correct."
  },
  {
    "index": "F15553",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私はこの手法がバックプロパゲートの実装に関したあらゆるバグを駆逐してくれるのを見て来た。",
    "output": "And from what I've seen this pretty much eliminates all the problems associated with a sort of a buggy implementation as a back prop."
  },
  {
    "index": "F15554",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして前回のビデオでは、私はあなたに、デルタやDたちを計算する式を単に信じてくれ、と頼んだ。私はあなたにそれらの公式が実際にコスト関数の微分を計算している事を単に信じてくれ、と頼んだ。",
    "output": "And in the previous videos I asked you to take on faith that the formulas I gave for computing the deltas and the vs and so on, I asked you to take on faith that those actually do compute the gradients of the cost function."
  },
  {
    "index": "F15555",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがひとたびあなたが数値的なグラディアントチェッキングを実装すれば、それこそがこのビデオのトピックだが、そうすればあなた自身があなたの書いたコードが確かにコスト関数Jの微分を計算している事を確認する事が出来る。",
    "output": "But once you implement numerical gradient checking, which is the topic of this video, you'll be able to absolute verify for yourself that the code you're writing does indeed, is indeed computing the derivative of the cross function J."
  },
  {
    "index": "F15556",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのアイデアはこうだ。以下のような例を考えてみよう。",
    "output": "So here's the idea, consider the following example."
  },
  {
    "index": "F15557",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Jのシータがあるとして、そしてある値シータがあるとする。そしてこの例では、シータは単なる実数だと仮定しよう。",
    "output": "Suppose that I have the function J of theta and I have some value theta and for this example gonna assume that theta is just a real number."
  },
  {
    "index": "F15558",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの関数の、例えばこの点の微分を推計したいとしよう。すると微分は、この接線の傾きに等しい。",
    "output": "And let's say that I want to estimate the derivative of this function at this point and so the derivative is equal to the slope of that tangent one."
  },
  {
    "index": "F15559",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが、数値的に微分を近似する方法、あるいはむしろ微分を数値的に近似する手続きはこうだ:シータ+エプシロンを計算する、つまりちょっとだけ右の値だ。そしてシータ-エプシロンも計算する。",
    "output": "Here's how I'm going to numerically approximate the derivative, or rather here's a procedure for numerically approximating the derivative."
  },
  {
    "index": "F15560",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら二つの点を直線でつなげよう。",
    "output": "And I'm gonna compute theta minus epsilon and I'm going to look at those two points, And connect them by a straight line And I'm gonna connect these two points by a straight line, and I'm gonna use the slope of that little red line as my approximation to the derivative."
  },
  {
    "index": "F15561",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、真の微分の値はここの青い線の傾きだ。",
    "output": "Which is, the true derivative is the slope of that blue line over there."
  },
  {
    "index": "F15562",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ふむ、それはとても良い近似になりそうだ。",
    "output": "So, you know it seems like it would be a pretty good approximation."
  },
  {
    "index": "F15563",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "数学的には、この赤い直線の傾きは垂直方向の高さ割る事のこの水平方向の幅だ。",
    "output": "Mathematically, the slope of this red line is this vertical height divided by this horizontal width."
  },
  {
    "index": "F15564",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この上の点はJのシータ+エプシロン。",
    "output": "So this point on top is the J of (Theta plus Epsilon)."
  },
  {
    "index": "F15565",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの垂直の差はJのシータ+エプシロン引くことのJのシータ-エプシロン。そしてこの水平距離は2エプシロンだ。",
    "output": "This point here is J (Theta minus Epsilon), so this vertical difference is J (Theta plus Epsilon) minus J of theta minus epsilon and this horizontal distance is just 2 epsilon."
  },
  {
    "index": "F15566",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、私の近似は以下のようになる:Jのシータの、シータによる微分は、このシータの場所での微分は、だいたい近似的にはJのシータ+エプシロン引く事のJのシータ-エプシロン,割ることの2エプシロンだ。",
    "output": "So my approximation is going to be that the derivative respect of theta of J of theta at this value of theta, that that's approximately J of theta plus epsilon minus J of theta minus epsilon over 2 epsilon."
  },
  {
    "index": "F15567",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "普通私は、エプシロンにはとても小さい数字を用いる。エプシロンにだいたい10の-4乗とかそういうオーダーの数をセットしてる。",
    "output": "Usually, I use a pretty small value for epsilon, expect epsilon to be maybe on the order of 10 to the minus 4."
  },
  {
    "index": "F15568",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だいたいにおいて、うまく行くようなエプシロンの範囲は結構大きな範囲に渡る。",
    "output": "There's usually a large range of different values for epsilon that work just fine."
  },
  {
    "index": "F15569",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際に、エプシロンにとても小さい値を入れていくと数学的には、この項は実際に数学的に、微分となる。",
    "output": "And in fact, if you let epsilon become really small, then mathematically this term here, actually mathematically, it becomes the derivative."
  },
  {
    "index": "F15570",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この点における関数の完全な傾きになる。",
    "output": "It becomes exactly the slope of the function at this point."
  },
  {
    "index": "F15571",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんなに小さなエプシロンを使いたくない理由は、単に小さすぎるエプシロンは数値的な問題を引き起こすからというだけ。",
    "output": "It's just that we don't want to use epsilon that's too, too small, because then you might run into numerical problems."
  },
  {
    "index": "F15572",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから私はだいたい、エプシロンに10の-4乗あたりの値を使う。",
    "output": "So I usually use epsilon around ten to the minus four."
  },
  {
    "index": "F15573",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、あなたがたの中には微分を推計する別の式、こんな式を見た事がある人もいるかもしれない。",
    "output": "And by the way some of you may have seen an alternative formula for s meeting the derivative which is this formula."
  },
  {
    "index": "F15574",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この右側のは片側微分と呼ばれる物だ。一方で、左側の式は両側微分と呼ばれる。",
    "output": "This one on the right is called a one-sided difference, whereas the formula on the left, that's called a two-sided difference."
  },
  {
    "index": "F15575",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "両側微分は通常はわずかだがより良い推計を与えるので、私は通常はこの片側微分の代わりに両側微分を用いている。つまり、具体的に言えば、Octaveであなたが実装するのは、以下のような物だ。",
    "output": "The two sided difference gives us a slightly more accurate estimate, so I usually use that, rather than this one sided difference estimate."
  },
  {
    "index": "F15576",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "gradApproxを計算するコードは、こうなる、これは微分を近似する。それはこんな式だ:Jのシータ+エプシロン引くことのJのシータ-エプシロン割ることの2掛けるエプシロン。",
    "output": "So, concretely, when you implement an octave, is you implemented the following, you implement call to compute gradApprox, which is going to be our approximation derivative as just here this formula, J of theta plus epsilon minus J of theta minus epsilon divided by 2 times epsilon."
  },
  {
    "index": "F15577",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この式はこの点の微分の数値的な推計を与える。",
    "output": "And this will give you a numerical estimate of the gradient at that point."
  },
  {
    "index": "F15578",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの例では、これは極めて良い推計になっているようだ。",
    "output": "And in this example it seems like it's a pretty good estimate."
  },
  {
    "index": "F15579",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、前のスライドでは、シータが実数の場合を検討した。",
    "output": "Now on the previous slide, we considered the case of when theta was a rolled number."
  },
  {
    "index": "F15580",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでは、より一般的なケースとなる、シータがパラメータベクトルの場合を見てみよう。シータがRnとしよう。",
    "output": "Now let's look at a more general case of when theta is a vector parameter, so let's say theta is an R n."
  },
  {
    "index": "F15581",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはニューラルネットワークのパラメータをアンロールしたバージョンと考えても良い。",
    "output": "And it might be an unrolled version of the parameters of our neural network."
  },
  {
    "index": "F15582",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりシータはn個の要素を持つベクトルで、つまりシータ1からシータnまでで、これらそれぞれに関する偏微分の項についてさっきと似たような近似のアイデアを用いる事が出来る。",
    "output": "So theta is a vector that has n elements, theta 1 up to theta n. We can then use a similar idea to approximate all the partial derivative terms."
  },
  {
    "index": "F15583",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、コスト関数の最初のパラメータ、シータ1による偏微分は、以下のように求める事が出来る。",
    "output": "Concretely the partial derivative of a cost function with respect to the first parameter, theta one, that can be obtained by taking J and increasing theta one."
  },
  {
    "index": "F15584",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはJに対しシータ1を増加させて、つまりJのシータ1+エプシロンにして、そこから引く事のJのシータ1-エプシロンに、全体を2エプシロンで割る。",
    "output": "So you have J of theta one plus epsilon and so on."
  },
  {
    "index": "F15585",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二番目のパラメータシータ2に関しての偏微分は、だいたいこれと同じ事をするが、唯一の違いは、エプシロンだけ増加させるのがシータ2だという所。",
    "output": "Minus J of this theta one minus epsilon and divide it by two epsilon."
  },
  {
    "index": "F15586",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここは、シータ2をエプシロンだけ減少させる。",
    "output": "The partial derivative respect to the second parameter theta two, is again this thing except that you would take J of here you're increasing theta two by epsilon, and here you're decreasing theta two by epsilon and so on down to the derivative."
  },
  {
    "index": "F15587",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "などと、シータnに関する微分まで降りていく。そこではここにあるシータnをエプシロンだけ増加させたり減少させたりする。",
    "output": "With respect of theta n would give you increase and decrease theta and by epsilon over there."
  },
  {
    "index": "F15588",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、これらの等式はJの、各パラメータに対する偏微分を数値的に近似する方法を与える。",
    "output": "So, these equations give you a way to numerically approximate the partial derivative of J with respect to any one of your parameters theta i."
  },
  {
    "index": "F15589",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的にはあなたが実装するのは、以下のような物だ。",
    "output": "Completely, what you implement is therefore the following."
  },
  {
    "index": "F15590",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らはOctaveで以下のように実装して数値的に微分を求める。",
    "output": "We implement the following in octave to numerically compute the derivatives."
  },
  {
    "index": "F15591",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "fori=1からnまでの、、、ここでnはパラメータベクトル、シータの次元だ。",
    "output": "We say, for i = 1:n, where n is the dimension of our parameter of vector theta."
  },
  {
    "index": "F15592",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして私は普通、これをアンロールしたバージョンのパラメータでやる。",
    "output": "And I usually do this with the unrolled version of the parameter."
  },
  {
    "index": "F15593",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりシータは私のニューラルネットワークのパラメータの単なる長いリストに過ぎない。",
    "output": "So theta is just a long list of all of my parameters in my neural network, say."
  },
  {
    "index": "F15594",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "thetaPlusにthetaをセットし、thetaPlusのi番目の要素をEPSILONだけ増加させる。",
    "output": "I'm gonna set thetaPlus = theta, then increase thetaPlus of the (i) element by epsilon."
  },
  {
    "index": "F15595",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、thetaPlusは基本的にはthetaに等しい、thetaPlus(i)以外は。",
    "output": "And so this is basically thetaPlus is equal to theta except for thetaPlus(i) which is now incremented by epsilon."
  },
  {
    "index": "F15596",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、thetaPlusはtheta1,theta2,...などと等しくて、そしてtheta(i)の所では、EPSILONを足した物に等しい。",
    "output": "Epsilon, so theta plus is equal to, write theta 1, theta 2 and so on."
  },
  {
    "index": "F15597",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてさらにtheta(n)まで降りていく。",
    "output": "Then theta I has epsilon added to it and then we go down to theta N."
  },
  {
    "index": "F15598",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に、これら二つの行はthetaMinusに、上と似たような物を代入しているが、theta(i)+EPSILONの代わりにtheta(i)-EPSILONな所だけが違う。",
    "output": "And similar these two lines set theta minus to something similar except that this instead of theta I plus Epsilon, this now becomes theta I minus Epsilon."
  },
  {
    "index": "F15599",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、このgradApprox(i)を実装する。これがJのシータのシータiによる偏微分の近似を与える。",
    "output": "And then finally you implement this gradApprox (i) and this would give you your approximation to the partial derivative respect of theta i of J of theta."
  },
  {
    "index": "F15600",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれの使い方としては、ニューラルネットワークの実装において、ニューラルネットワークの各パラメータによるコスト関数の偏微分を求める為にこれを実装する、このforループを実装する。",
    "output": "And the way we use this in our neural network implementation is, we would implement this four loop to compute the top partial derivative of the cost function for respect to every parameter in that network, and we can then take the gradient that we got from backprop."
  },
  {
    "index": "F15601",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、バックプロパからグラディアントを取得出来る。",
    "output": "So DVec was the derivative we got from backprop."
  },
  {
    "index": "F15602",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりDVecはバックプロパから得た微分だ。",
    "output": "All right, so backprop, backpropogation, was a relatively efficient way to compute a derivative or a partial derivative."
  },
  {
    "index": "F15603",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、バックプロパ、バックプロパゲーションは微分を計算する比較的効率的な方法だ、より正確に言えばコスト関数の各パラメータによる偏微分を計算する為の。",
    "output": "Of a cost function with respect to all our parameters."
  },
  {
    "index": "F15604",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして私がよくやるのは、数値的に計算した微分に対して、それはこのここの上で得たgradApproxだが、これがbackpropで得た物と等しいかほとんど等しい事を確認する事だ。",
    "output": "And what I usually do is then, take my numerically computed derivative that is this gradApprox that we just had from up here."
  },
  {
    "index": "F15605",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "小さい数値的な丸めの範囲に収まっているかを。backpropで得たDVecと極めて近いかを。",
    "output": "And make sure that that is equal or approximately equal up to small values of numerical round up, that it's pretty close."
  },
  {
    "index": "F15606",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれら二つの方法で計算した微分の値が、同じ答えか、少なくともとても近い答えをはじきだしたなら、小数点以下数桁の範囲で近ければ、私のバックプロパの実装が正しい、という事によりしっかりと自信を持つ事が出来る。",
    "output": "And if these two ways of computing the derivative give me the same answer, or give me any similar answers, up to a few decimal places, then I'm much more confident that my implementation of backprop is correct."
  },
  {
    "index": "F15607",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうしてから、これらのDVecベクトルを最急降下法なり何らかのアドバンスドな最適化アルゴリズムに食わせれば、その時には微分をちゃんと計算していると自信を持っているから、自分のコードが正しく走るとも期待出来て、Jのシータを最適化するのに良い仕事をしてくれると期待出来る。",
    "output": "And when I plug these DVec vectors into gradient assent or some advanced optimization algorithm, I can then be much more confident that I'm computing the derivatives correctly, and therefore that hopefully my code will run correctly and do a good job optimizing J of theta."
  },
  {
    "index": "F15608",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、全部を合わせて数値的グラディアントチェッキングをどう実装するかをお話したい。",
    "output": "Finally, I wanna put everything together and tell you how to implement this numerical gradient checking."
  },
  {
    "index": "F15609",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私はいつも、こんな風にする。",
    "output": "Here's what I usually do."
  },
  {
    "index": "F15610",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初にやるのは、DVecを計算する為にバックプロパゲーションを実装する。",
    "output": "First thing I do is implement back propagation to compute DVec."
  },
  {
    "index": "F15611",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは以前のビデオで話したDVecを計算する手順となり、それはこれらの行列を展開したバージョンとなる。",
    "output": "So there's a procedure we talked about in the earlier video to compute DVec which may be our unrolled version of these matrices."
  },
  {
    "index": "F15612",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に私がやるのは、gradApproxを計算する為に数値的なグラディアントチェッキングを実装する事だ。",
    "output": "So then what I do, is implement a numerical gradient checking to compute gradApprox."
  },
  {
    "index": "F15613",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが私がこのビデオで話してきた所だ、前のスライドで話した奴。",
    "output": "So this is what I described earlier in this video and in the previous slide."
  },
  {
    "index": "F15614",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、DVecとgradApproxが似た値かどうかを確認する、たとえば小数点第二位とか第三位までで一致するかを見る。",
    "output": "Then should make sure that DVec and gradApprox give similar values, you know let's say up to a few decimal places."
  },
  {
    "index": "F15615",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、そしてこれは大切なステップなのだが、あなたのコードを実際に学習させ始める前に、真面目にネットワークをトレーニングする前に、グラディアントチェッキングを切るのが大切だ。",
    "output": "And finally and this is the important step, before you start to use your code for learning, for seriously training your network, it's important to turn off gradient checking and to no longer compute this gradApprox thing using the numerical derivative formulas that we talked about earlier in this video."
  },
  {
    "index": "F15616",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれ以降はこのビデオでやってきたgradApproxの数値的な微分の式での計算をしないように。",
    "output": "And the reason for that is the numeric code gradient checking code, the stuff we talked about in this video, that's a very computationally expensive, that's a very slow way to try to approximate the derivative."
  },
  {
    "index": "F15617",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その理由は、数値的なグラディアントチェッキングのコードは、このビデオで議論してきた内容は、計算量的にとても高価で、微分を近似しようとするのには凄い遅いやり方だ。",
    "output": "Whereas In contrast, the back propagation algorithm that we talked about earlier, that is the thing we talked about earlier for computing. You know, D1, D2, D3 for Dvec."
  },
  {
    "index": "F15618",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方で対照的に、以前に話したバックプロパゲーションのアルゴリズムはそれは前にD1とかD2とかD3とかDVecを計算するのに議論してきた物だが、そのバックプロパゲーションは微分を計算するのに、もっとずっと計算量的に効率的な方法だ。",
    "output": "Backprop is much more computationally efficient way of computing for derivatives. So once you've verified that your implementation of back propagation is correct, you should turn off gradient checking and just stop using that."
  },
  {
    "index": "F15619",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからひとたびあなたのバックプロパゲーションの実装が正しい、と確認した後には、グラディアントチェッキングは切るべきだ、単純に使うのをやめるべきだ。",
    "output": "So just to reiterate, you should be sure to disable your gradient checking code before running your algorithm for many iterations of gradient descent or for many iterations of the advanced optimization algorithms, in order to train your classifier."
  },
  {
    "index": "F15620",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたのアルゴリズムを最急降下法のたくさんの繰り返しで走らせる前には、あるいはアドバンスドな最適化アルゴリズムで分類器を訓練する為にたくさんの繰り返しを走らせる前には、グラディアントチェッキングのコードを切る事を忘れないようにしよう。",
    "output": "Concretely, if you were to run the numerical gradient checking on every single iteration of gradient descent."
  },
  {
    "index": "F15621",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、もし万が一最急降下法の各イテレーションで毎回数値的グラディアントチェッキングを走らせてしまったら、またはcostFunctionの内側のループで走らせてしまったら、あなたのコードはとてものろくなってしまうだろう。",
    "output": "Or if you were in the inner loop of your costFunction, then your code would be very slow."
  },
  {
    "index": "F15622",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら数値的なグラディアントチェッキングのコードはバックプロパゲーションのアルゴリズムに比べてずっと遅いからだ。つまりデルタ4、デルタ3、デルタ2などを計算する時に用いたバックプロパゲーションと比較するとだ。",
    "output": "Because the numerical gradient checking code is much slower than the backpropagation algorithm, than the backpropagation method where, you remember, we were computing delta(4), delta(3), delta(2), and so on."
  },
  {
    "index": "F15623",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはグラディアントチェッキングよりもずっと早い微分の計算方法だ。",
    "output": "That is a much faster way to compute derivates than gradient checking."
  },
  {
    "index": "F15624",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから準備が出来たら。一旦あなたのバックプロパゲーションの実装が正しいと確認出来たら、グラディアントチェッキングのコードを切るなりdisableするなりを確実に行おう、アルゴリズムをトレーニングする間は。",
    "output": "So when you're ready, once you've verified the implementation of back propagation is correct, make sure you turn off or you disable your gradient checking code while you train your algorithm, or else you code could run very slowly."
  },
  {
    "index": "F15625",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こうやって、あなたのバックプロパゲーションの実装が正しい、と検証する事が出来る。",
    "output": "So, that's how you take gradients numericaly, and that's how you can verify tha implementation of back propagation is correct."
  },
  {
    "index": "F15626",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私がバックプロパゲーションや、似たような複雑なモデルに対して最急降下法を実装する時にはいつでも、グラディアントチェッキングを使っている。これは自分のコードが正しいと確認する為の、本当に良い助けとなってくれるんだ。",
    "output": "Whenever I implement back propagation or similar gradient discerning algorithm for a complicated mode,l I always use gradient checking and this really helps me make sure that my code is correct."
  },
  {
    "index": "F15627",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではニューラルネットワークを実装し訓練するのに必要なほとんど全てのピースをまとめた。",
    "output": "In the previous video, we've put together almost all the pieces you need in order to implement and train in your network."
  },
  {
    "index": "F15628",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがまだもう一つ、最後のピースをあなたにも伝えなくてはいけない。それはランダム初期化と呼ばれるアイデアだ。",
    "output": "There's just one last idea I need to share with you, which is the idea of random initialization."
  },
  {
    "index": "F15629",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最急降下法なりアドバンスドな最適化アルゴリズムなりのようなアルゴリズムを走らせる時には、パラメータシータのある初期値を選ぶ必要がある。",
    "output": "When you're running an algorithm of gradient descent, or also the advanced optimization algorithms, we need to pick some initial value for the parameters theta."
  },
  {
    "index": "F15630",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アドバンスドな最適化アルゴリズムは、あなたがパラメータのシータのある初期値を渡すと想定している。",
    "output": "So for the advanced optimization algorithm, it assumes you will pass it some initial value for the parameters theta."
  },
  {
    "index": "F15631",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでは最急降下法を考えよう。",
    "output": "Now let's consider a gradient descent."
  },
  {
    "index": "F15632",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合でも、シータを何かしらには初期化しなくてはいけない。",
    "output": "For that, we'll also need to initialize theta to something, and then we can slowly take steps to go downhill using gradient descent."
  },
  {
    "index": "F15633",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそこから、ゆっくりと丘を最急降下法を使って一歩一歩降りていき、Jのシータの最小値まで降りていく。",
    "output": "To go downhill, to minimize the function j of theta."
  },
  {
    "index": "F15634",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではシータの初期値をどうセットしたらいいだろうか?",
    "output": "So what can we set the initial value of theta to?"
  },
  {
    "index": "F15635",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シータの初期値に全部0を入れる、というのが考えられる。",
    "output": "Is it possible to set the initial value of theta to the vector of all zeros?"
  },
  {
    "index": "F15636",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはロジスティック回帰の時にはうまく行ったが、パラメータの初期値を全て0にするのはニューラルネットワークをトレーニングする時にはうまく行かない。",
    "output": "Whereas this worked okay when we were using logistic regression, initializing all of your parameters to zero actually does not work when you are trading on your own network."
  },
  {
    "index": "F15637",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以下のニューラルネットワークをトレーニングする事を考えてみよう。そして全てのネットワークのパラメータを0に初期化したとしよう。",
    "output": "Consider trading the follow Neural network, and let's say we initialize all the parameters of the network to 0."
  },
  {
    "index": "F15638",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうすると、それの意味する事は、この青いウェイトを初期化する時に、、、ここでこのウェイトを青で色づけする。これらは共に0となる。",
    "output": "And if you do that, then what you, what that means is that at the initialization, this blue weight, colored in blue is gonna equal to that weight, so they're both 0."
  },
  {
    "index": "F15639",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのウェイト、赤でいろづけしておくと、このウェイトがこっちのウェイト、これも赤で色付けしておくが、これらが等しい。",
    "output": "And this weight that I'm coloring in in red, is equal to that weight, colored in red, and also this weight, which I'm coloring in green is going to equal to the value of that weight."
  },
  {
    "index": "F15640",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのウェイト、これは緑に色付けしよう、これはこっちのウェイトの値と等しくなる。",
    "output": "And what that means is that both of your hidden units, A1 and A2, are going to be computing the same function of your inputs."
  },
  {
    "index": "F15641",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その意味する所は、隠れユニット、a1とa2はどちらも同じ入力の関数を計算する事になる、という事。",
    "output": "And thus you end up with for every one of your training examples, you end up with A 2 1 equals A 2 2."
  },
  {
    "index": "F15642",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらに、あまり細かい話に首をつっこむ気は無いけれど、しかしこれらのウェイトが等しければ、デルタの値も等しくなる、という事を示す事が出来る。",
    "output": "And moreover because I'm not going to show this in too much detail, but because these outgoing weights are the same you can also show that the delta values are also gonna be the same."
  },
  {
    "index": "F15643",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらに数学を続けていくと、あなたのパラメータによる偏微分は以下を満たす事を示す事が出来る。コスト関数の偏微分、、、ここに書いている、ニューラルネットワークのこれら二つの青のウェイトによる偏微分。",
    "output": "So concretely you end up with delta 1 1, delta 2 1 equals delta 2 2, and if you work through the map further, what you can show is that the partial derivatives with respect to your parameters will satisfy the following, that the partial derivative of the cost function with respected to breaking out the derivatives respect to these two blue waves in your network."
  },
  {
    "index": "F15644",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら二つの偏微分はお互いに等しい事が分かる。",
    "output": "You find that these two partial derivatives are going to be equal to each other."
  },
  {
    "index": "F15645",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この意味する所は、一回最急降下法のアップデートが行われた後も、この青いウェイトを学習率掛けるこれでアップデートし、二番目の青いウェイトも学習率掛けるこれ、でアップデートする。",
    "output": "And so what this means is that even after say one greater descent update, you're going to update, say, this first blue rate was learning rate times this, and you're gonna update the second blue rate with some learning rate times this."
  },
  {
    "index": "F15646",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがその意味する所は、一回最急降下法のアップデートをかました後でも、これら二つの青のウェイトは、これら二つの青の色付けしたパラメータは結局相等しいままだ。",
    "output": "And what this means is that even after one created the descent update, those two blue rates, those two blue color parameters will end up the same as each other."
  },
  {
    "index": "F15647",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "なんらかの非0の値にはなるだろう、だがこの値はこっちの値と等しい。",
    "output": "So there'll be some nonzero value, but this value would equal to that value."
  },
  {
    "index": "F15648",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして同様に、一回最急降下法のアップデートを行った後でも、この値はこっちの値と等しい。",
    "output": "And similarly, even after one gradient descent update, this value would equal to that value."
  },
  {
    "index": "F15649",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "なんらかの非0の値にはなる。この二つの赤の値もお互いに相等しい。",
    "output": "There'll still be some non-zero values, just that the two red values are equal to each other."
  },
  {
    "index": "F15650",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして同様に、二つの緑のウェイトもそれらの値も共に変化するが、だが変化した結果はどちらも同じ値になる。",
    "output": "And similarly, the two green ways. Well, they'll both change values, but they'll both end up with the same value as each other."
  },
  {
    "index": "F15651",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから各アップデートの後でも、各入力から二つの隠れユニットへと至る二つのウェイトは、同一となる。",
    "output": "So after each update, the parameters corresponding to the inputs going into each of the two hidden units are identical."
  },
  {
    "index": "F15652",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは単に、二つの緑のウェイトが等しくなり、二つの赤のウェイトも等しくなり、二つの青のウェイトも等しいままだ、と言っているだけだ。",
    "output": "That's just saying that the two green weights are still the same, the two red weights are still the same, the two blue weights are still the same, and what that means is that even after one iteration of say, gradient descent and descent."
  },
  {
    "index": "F15653",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその意味する所は、例えば最急降下法などの一回のイテレーションの後でも、あなたの二つの隠れユニットが全く同一の入力の関数を計算し続ける事となる。",
    "output": "You find that your two headed units are still computing exactly the same functions of the inputs."
  },
  {
    "index": "F15654",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、a(1)2=a(2)2のままとなる。",
    "output": "You still have the a1(2) = a2(2)."
  },
  {
    "index": "F15655",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのケースに戻る。",
    "output": "And so you're back to this case."
  },
  {
    "index": "F15656",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最急降下法を走らせ続けても、この二つの青のウェイトはお互いに等しいままだ。",
    "output": "And as you keep running greater descent, the blue waves,, the two blue waves, will stay the same as each other."
  },
  {
    "index": "F15657",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二つの赤いウェイトはお互いに等しいままだ。",
    "output": "The two red waves will stay the same as each other and the two green waves will stay the same as each other."
  },
  {
    "index": "F15658",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この意味する所は、このニューラルネットワークはあまり面白い関数は計算出来ない、という事。",
    "output": "And what this means is that your neural network really can compute very interesting functions, right?"
  },
  {
    "index": "F15659",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二つの隠れユニットだけじゃなくてもっともっとたくさんの隠れユニットがある場合を考えてみよう。",
    "output": "Imagine that you had not only two hidden units, but imagine that you had many, many hidden units."
  },
  {
    "index": "F15660",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとこれの意味する所は、隠れユニットが全て完全に同じフィーチャーを計算する、という事になる。",
    "output": "Then what this is saying is that all of your headed units are computing the exact same feature."
  },
  {
    "index": "F15661",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "全ての隠れユニットが、全く同一の、入力の関数を計算する事になる。",
    "output": "All of your hidden units are computing the exact same function of the input."
  },
  {
    "index": "F15662",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならそれはつまりは、最後のロジスティック回帰のユニットは、実際は一つの入力があるだけに見えるから。",
    "output": "And this is a highly redundant representation because you find the logistic progression unit. It really has to see only one feature because all of these are the same."
  },
  {
    "index": "F15663",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならこれらは全て同じで、このことがあなたのニューラルネットワークが何かしら面白い学習をする事を妨げているから。",
    "output": "And this prevents you and your network from doing something interesting."
  },
  {
    "index": "F15664",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この問題を回避する為に、ニューラルネットワークのパラメータを初期化する方法は、ランダム初期化の方法だ。",
    "output": "In order to get around this problem, the way we initialize the parameters of a neural network therefore is with random initialization."
  },
  {
    "index": "F15665",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、前のスライドで我らが見た問題は対称ウェイトの問題と呼ばれる事もあり、それはウェイトが全て同じという事だ。",
    "output": "Concretely, the problem was saw on the previous slide is something called the problem of symmetric ways, that's the ways are being the same."
  },
  {
    "index": "F15666",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこのランダム初期化で対称性を破る訳だ。",
    "output": "So this random initialization is how we perform symmetry breaking."
  },
  {
    "index": "F15667",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らのやるべき事は、各シータの値を-エプシロンからエプシロンまでの間のランダムな値で初期化する。",
    "output": "So what we do is we initialize each value of theta to a random number between minus epsilon and epsilon."
  },
  {
    "index": "F15668",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは-エプシロンと+エプシロンの間の値を表す記法だ。",
    "output": "So this is a notation to b numbers between minus epsilon and plus epsilon."
  },
  {
    "index": "F15669",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Octaveでこれをやるコードを書く方法は、Theta1をイコールこれ、とする。",
    "output": "The way I write code to do this in octave is I've said Theta1 should be equal to this."
  },
  {
    "index": "F15670",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらは、連続的な実数の0と1の間のいかなる値でも取る事が出来る。",
    "output": "All the values are between 0 and 1, so these are going to be raw numbers that take on any continuous values between 0 and 1."
  },
  {
    "index": "F15671",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、0と1の間の数を取って、2*エプシロンを掛けて、そこからエプシロンを引く。",
    "output": "And so if you take a number between zero and one, multiply it by two times INIT_EPSILON then minus INIT_EPSILON, then you end up with a number that's between minus epsilon and plus epsilon."
  },
  {
    "index": "F15672",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると最終的には-エプシロンとエプシロンの間の数となる。",
    "output": "And the so that leads us, this epsilon here has nothing to do with the epsilon that we were using when we were doing gradient checking."
  },
  {
    "index": "F15673",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ついでに言っておくと、このエプシロンはグラディアントチェッキングで使ってたエプシロンとは全く関係が無い。",
    "output": "So when numerical gradient checking, there we were adding some values of epsilon and theta."
  },
  {
    "index": "F15674",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "数値的なグラディアントチェッキングを行っていた時は、シータにあるエプシロンという値を足していたが、この値は、そのエプシロンとは無関係だ。",
    "output": "This is your unrelated value of epsilon."
  },
  {
    "index": "F15675",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんな訳でこのエプシロンをINIT_EPSILONと書いている。これはグラディアントチェッキングで使ったエプシロンの値と区別する為だけの理由だ。",
    "output": "We just wanted to notate init epsilon just to distinguish it from the value of epsilon we were using in gradient checking."
  },
  {
    "index": "F15676",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様にもしシータ2を1x11のランダムの行列で初期化したければ、このコード辺でそれが行える。",
    "output": "And similarly if you want to initialize theta2 to a random 1 by 11 matrix you can do so using this piece of code here."
  },
  {
    "index": "F15677",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、まとめると、ニューラルネットワークをトレーニングする為には、ウェイトを小さな値、0のそばの-エプシロンから+エプシロンの間のどこかの値でランダムに初期化しなければならない。",
    "output": "So to summarize, to create a neural network what you should do is randomly initialize the waves to small values close to zero, between -epsilon and +epsilon say."
  },
  {
    "index": "F15678",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてバックプロパゲーションを実装し、グラディアントチェッキングをする。そして最急降下法なりアドバンスドな最適化アルゴリズムなりを用いて、Jのシータをパラメータシータの関数として、ランダムに初期化したパラメータから始めて最小化を試みる。",
    "output": "And then implement back propagation, do great in checking, and use either great in descent or 1b advanced optimization algorithms to try to minimize j(theta) as a function of the parameters theta starting from just randomly chosen initial value for the parameters."
  },
  {
    "index": "F15679",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして対称性の破れを行う事で、それはこのプロセスだが、最急降下法なりアドバンスドな最適化アルゴリズムなりが、シータの良い値を見つける事が期待出来るようになる。",
    "output": "And by doing symmetry breaking, which is this process, hopefully great gradient descent or the advanced optimization algorithms will be able to find a good value of theta."
  },
  {
    "index": "F15680",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークのアルゴリズムを見ていくのにたくさんのビデオを費やしてきた。",
    "output": "So, it's taken us a lot of videos to get through the neural network learning algorithm."
  },
  {
    "index": "F15681",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、全てのピースを一つに合わせて全体的なサマリー、あるいはより大きな鳥瞰図を提供したい、全てのピースがどう組み合わさりニューラルネットワークを全体としてどう実装したらいいのかについて。",
    "output": "In this video, what I'd like to do is try to put all the pieces together, to give a overall summary or a bigger picture view, of how all the pieces fit together and of the overall process of how to implement a neural network learning algorithm."
  },
  {
    "index": "F15682",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークをトレーニングする時は最初にやらなくてはいけない事はなんらかのネットワークアーキテクチャを選ぶ事だ。ここでアーキテクチャという言葉でニューロン同士の接続のパターンを意味している。",
    "output": "When training a neural network, the first thing you need to do is pick some network architecture and by architecture I just mean connectivity pattern between the neurons."
  },
  {
    "index": "F15683",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから例えば3つの入力ユニットに5つの隠れユニット、そして4つの出力を選ぶかもしれないし、対して3つ、5つの隠れ、5つの隠れ4つの出力を選ぶかもしれないし、そしてこれは3で、5,5,5ユニットが三つの隠れレイヤーにあって、そして4つの出力ユニット、これを選ぶかもしれない。これらが幾つの隠れユニットと幾つの隠れレイヤーを持つかの選択肢だ。",
    "output": "So, you know, we might choose between say, a neural network with three input units and five hidden units and four output units versus one of 3, 5 hidden, 5 hidden, 4 output and here are 3, 5, 5, 5 units in each of three hidden layers and four open units, and so these choices of how many hidden units in each layer and how many hidden layers, those are architecture choices."
  },
  {
    "index": "F15684",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの選択肢をどうやって選ぶのか?",
    "output": "So, how do you make these choices?"
  },
  {
    "index": "F15685",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず、入力ユニットの総数はかっちりと定義されている。",
    "output": "Well first, the number of input units well that's pretty well defined."
  },
  {
    "index": "F15686",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして一旦固定されたフィーチャーの集まりxを決定してしまえば、入力ユニットの数は、フィーチャーx(i)の次元によって決定される。",
    "output": "And once you decides on the fix set of features x the number of input units will just be, you know, the dimension of your features x(i) would be determined by that."
  },
  {
    "index": "F15687",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしマルチクラスの分類問題を扱っているなら、出力の数は、あなたの扱ってる分類問題のクラスの数によって決められる。",
    "output": "And if you are doing multiclass classifications the number of output of this will be determined by the number of classes in your classification problem."
  },
  {
    "index": "F15688",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "思い出してもらう為に触れておくとマルチクラスの分類問題の時には、例えばyが1から10の間の数を取るとすると、つまり10個のとりうるクラスがある事になるが、その時は、覚えているだろうか、出力のyはこれらのベクトルとなるのだった。",
    "output": "And just a reminder if you have a multiclass classification where y takes on say values between 1 and 10, so that you have ten possible classes."
  },
  {
    "index": "F15689",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれがクラス1で、それをこのベクトルに再コーディングする。二番目のクラスはこんなベクトルに再コーディングする。",
    "output": "So instead of clause one, you recode it as a vector like that, or for the second class you recode it as a vector like that."
  },
  {
    "index": "F15690",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、これらのうちの一つを例として挙げると、5番目のクラスの場合、yイコール5の時は、ニューラルネットワークにおいて見る事になるのはy=5という値では無く、その代わりにここでは出力レイヤで、この場合は10個の出力ユニットがあり、そこに5番目に1があってそれ以外が0で埋まっているようなベクトルを食わせることになる。",
    "output": "So if one of these apples takes on the fifth class, you know, y equals 5, then what you're showing to your neural network is not actually a value of y equals 5, instead here at the upper layer which would have ten output units, you will instead feed to the vector which you know with one in the fifth position and a bunch of zeros down here."
  },
  {
    "index": "F15691",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり入力ユニットと出力ユニットの総数の選択は割とまっすぐ決まると思う。",
    "output": "So the choice of number of input units and number of output units is maybe somewhat reasonably straightforward."
  },
  {
    "index": "F15692",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして隠れユニットと隠れレイヤーの数の選択についてはもっとも普通の基本形は隠れレイヤは一層。つまりこの左側に示したような隠れレイヤ一つのニューラルネットワークが恐らくもっとも一般的な物と言える。",
    "output": "And as for the number of hidden units and the number of hidden layers, a reasonable default is to use a single hidden layer and so this type of neural network shown on the left with just one hidden layer is probably the most common."
  },
  {
    "index": "F15693",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもし一つ以上の隠れレイヤを使うならここでも普通の基本形は各レイヤの隠れユニットは同じ数にする、という物。",
    "output": "Or if you use more than one hidden layer, again the reasonable default will be to have the same number of hidden units in every single layer."
  },
  {
    "index": "F15694",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりここでは2つの隠れレイヤがあり、これらの隠れレイヤはそれぞれ同じ隠れユニットの数である5つの隠れユニットを持ち、こちらは見ての通り3つの隠れレイヤがありそれらはそれぞれ、同じユニット数でそれぞれ5つの隠れユニットがある。",
    "output": "So here we have two hidden layers and each of these hidden layers have the same number five of hidden units and here we have, you know, three hidden layers and each of them has the same number, that is five hidden units."
  },
  {
    "index": "F15695",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこれらのネットワークアーキテクチャを使うまでもなくこの左側のアーキテクチャでも十分に悪くないデフォルトの選択肢だ。",
    "output": "Rather than doing this sort of network architecture on the left would be a perfect ably reasonable default."
  },
  {
    "index": "F15696",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして隠れユニットの数としては、通常は隠れユニットが多ければ多い程良いが隠れユニットがたくさんあると計算量的には高価になる。",
    "output": "And as for the number of hidden units - usually, the more hidden units the better; it's just that if you have a lot of hidden units, it can become more computationally expensive, but very often, having more hidden units is a good thing."
  },
  {
    "index": "F15697",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがしばしば、隠れユニットは多ければ多い程良い。",
    "output": "And usually the number of hidden units in each layer will be maybe comparable to the dimension of x, comparable to the number of features, or it could be any where from same number of hidden units of input features to maybe so that three or four times of that."
  },
  {
    "index": "F15698",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして通常は、各レイヤーの隠れユニットの総数はだいたいxの次元と同じ程度、フィーチャーの総数と同じ程度で、あるいは入力のフィーチャーの総数と同じ程度から2倍とか3倍とか4倍程度の適当な数が良い。",
    "output": "So having the number of hidden units is comparable."
  },
  {
    "index": "F15699",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから入力のフィーチャーと同じ程度の数か、その数倍程度の数でもやる価値のある事が多い。以上で、ニューラルネットワークのアーキテクチャのデフォルトの選択としてリーズナブルな選択肢を幾つか示せただろう。",
    "output": "You know, several times, or some what bigger than the number of input features is often a useful thing to do So, hopefully this gives you one reasonable set of default choices for neural architecture and and if you follow these guidelines, you will probably get something that works well, but in a later set of videos where I will talk specifically about advice for how to apply algorithms, I will actually say a lot more about how to choose a neural network architecture."
  },
  {
    "index": "F15700",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また、実際に隠れユニットや隠れレイヤーの総数の良い選び方について、などはたくさん話すつもりだ。",
    "output": "Or actually have quite a lot I want to say later to make good choices for the number of hidden units, the number of hidden layers, and so on."
  },
  {
    "index": "F15701",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、ニューラルネットワークをトレーニングする為に実装しなくてはいけない事はこれだ。6つのステップがある。",
    "output": "Next, here's what we need to implement in order to trade in neural network, there are actually six steps that I have; I have four on this slide and two more steps on the next slide."
  },
  {
    "index": "F15702",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初のステップはニューラルネットワークをセットアップして、ウェイトの値をランダムに初期化する。",
    "output": "First step is to set up the neural network and to randomly initialize the values of the weights."
  },
  {
    "index": "F15703",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ウェイトの初期化は普通、0付近の小さな値で初期化する。",
    "output": "And we usually initialize the weights to small values near zero."
  },
  {
    "index": "F15704",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にフォワードプロパゲーションを実装する、入力をニューラルネットワークに入れて予測が計算出来るように。つまりhのxを計算し、この出力ベクトルのyの値を得る為に。",
    "output": "Then we implement forward propagation so that we can input any excellent neural network and compute h of x which is this output vector of the y values."
  },
  {
    "index": "F15705",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にコスト関数であるJのシータを計算するコードを実装する。",
    "output": "We then also implement code to compute this cost function j of theta."
  },
  {
    "index": "F15706",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にバックプロパゲーションをバックプロパゲーションアルゴリズムを実装する、これらの偏微分の項を計算する為に、、、Jのシータの、パラメータによる偏微分を計算する為に。",
    "output": "And next we implement back-prop, or the back-propagation algorithm, to compute these partial derivatives terms, partial derivatives of j of theta with respect to the parameters."
  },
  {
    "index": "F15707",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが最初にバックプロパゲーションを実装する時には、ほぼ確実にforループのある実装から始めるべきだろう。",
    "output": "Concretely, to implement back prop. Usually we will do that with a fore loop over the training examples."
  },
  {
    "index": "F15708",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでトレーニング手本に渡ってイテレーションを行い、つまり、まずx1とy1に対して最初の手本に対してフォワードプロパケーションとバックプロパゲーションを行い、そして次にforループの二回目のイテレーションで二番目の手本に対しフォワードプロパケーションとバックワードプロパゲーションを行う、などなど。",
    "output": "Some of you may have heard of advanced, and frankly very advanced factorization methods where you don't have a four-loop over the m-training examples, that the first time you're implementing back prop there should almost certainly the four loop in your code, where you're iterating over the examples, you know, x1, y1, then so you do forward prop and back prop on the first example, and then in the second iteration of the four-loop, you do forward propagation and back propagation on the second example, and so on."
  },
  {
    "index": "F15709",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを最後の手本に至るまで進める。",
    "output": "Until you get through the final example."
  },
  {
    "index": "F15710",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、少なくとも初めて実装する時にはあなたのバックプロパゲーションの実装にはforループがあるはずだ。",
    "output": "So there should be a four-loop in your implementation of back prop, at least the first time implementing it."
  },
  {
    "index": "F15711",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして簡単に言うとなんか難しいやり方でforループ無しのもある、という事。だが私は心から最初にバックプロパケーションを実装する時にその複雑なバージョンにチャレンジするのは、おすすめしない。",
    "output": "And then there are frankly somewhat complicated ways to do this without a four-loop, but I definitely do not recommend trying to do that much more complicated version the first time you try to implement back prop."
  },
  {
    "index": "F15712",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、では具体的には、mトレーニング手本に渡るfor文があり、そしてfor文の中では、フォワードプロパゲートとバックワードプロパゲートをこの一つの手本だけに対して行う。",
    "output": "So concretely, we have a four-loop over my m-training examples and inside the four-loop we're going to perform fore prop and back prop using just this one example."
  },
  {
    "index": "F15713",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとニューラルネットワーク内の全てのレイヤーの全てのユニットのアクティベーション全部とデルタの項が全部得られるので、次に、まだforループの内部だが、、、よし、中括弧を書こう、forループのスコープを示す為に。これはもちろんOctaveのコードなんだが、だがC++とかJavaのコードのシーケンスみたいな意味でこれはfor文をここ全体に拡張する。",
    "output": "And what that means is that we're going to take x(i), and feed that to my input layer, perform forward-prop, perform back-prop and that will if all of these activations and all of these delta terms for all of the layers of all my units in the neural network then still inside this four-loop, let me draw some curly braces just to show the scope with the four-loop, this is in octave code of course, but it's more a sequence Java code, and a four-loop encompasses all this."
  },
  {
    "index": "F15714",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らはこれらのデルタの項を計算する訳だがその式は以前に既に得ている。",
    "output": "We're going to compute those delta terms, which are is the formula that we gave earlier."
  },
  {
    "index": "F15715",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "、、、足すことのデルタl+1掛けるaのlの転置に、さらにコードが続く。",
    "output": "Plus, you know, delta l plus one times a, l transpose of the code."
  },
  {
    "index": "F15716",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、これらのデルタ項がアキュームレーションの項が計算されたあとの外側ではさらに幾つかコードがあったあとで、これらの偏微分の項の計算が出来るようになる。",
    "output": "And then finally, outside the having computed these delta terms, these accumulation terms, we would then have some other code and then that will allow us to compute these partial derivative terms."
  },
  {
    "index": "F15717",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの偏微分の項は正規化項のラムダも同様に考慮に入れる必要がある。",
    "output": "Right and these partial derivative terms have to take into account the regularization term lambda as well."
  },
  {
    "index": "F15718",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらの式は前半のビデオで与えてあった。",
    "output": "And so, those formulas were given in the earlier video."
  },
  {
    "index": "F15719",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからそれを終えているなら、これらの偏微分項を計算するコードを既に手中に収めているはずだ。",
    "output": "So, how do you done that you now hopefully have code to compute these partial derivative terms."
  },
  {
    "index": "F15720",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次はステップ5。そこではグラディアントチェッキングを用いて計算した偏微分の項と比較する。",
    "output": "Next is step five, what I do is then use gradient checking to compare these partial derivative terms that were computed."
  },
  {
    "index": "F15721",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりバックプロパケーションを用いて計算したバージョンと、微分を数値計算的に推計した偏微分の数値推計を比較する。",
    "output": "So, I've compared the versions computed using back propagation versus the partial derivatives computed using the numerical estimates as using numerical estimates of the derivatives."
  },
  {
    "index": "F15722",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりグラディアントチェッキングをこれらの値が両方とも似通っている事を確認する為に使う。",
    "output": "So, I do gradient checking to make sure that both of these give you very similar values."
  },
  {
    "index": "F15723",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "グラディアントチェッキングで我らのバックプロパゲーションの実装が正しいと確認した後には、次のとても大事な事だが、グラディアントチェッキングをdisableする。何故ならグラディアントチェッキングのコードは計算量的にとても遅いからだ。",
    "output": "Having done gradient checking just now reassures us that our implementation of back propagation is correct, and is then very important that we disable gradient checking, because the gradient checking code is computationally very slow."
  },
  {
    "index": "F15724",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、最急降下法なりアドバンスドな最適化アルゴリズムのL-BFGSとかConjugateグラディアントとかfminuncで使われているアルゴリズムでもそれ以外のアルゴリズムでもとにかく何かしらを用いて、、、これらをバックプロパゲーションと共に用いる、つまりバックプロパゲーションがこれらの偏微分を我らに提供してくれる。",
    "output": "And finally, we then use an optimization algorithm such as gradient descent, or one of the advanced optimization methods such as LB of GS, contract gradient has embodied into fminunc or other optimization methods."
  },
  {
    "index": "F15725",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてコスト関数を計算する方法を知っていて、バックプロパゲーションを用いて偏微分を計算する方法も知っているので、これらの最適化手法のどれかを用いてJのシータをパラメータシータの関数として最小化する事を、試みる事が出来る。",
    "output": "We use these together with back propagation, so back propagation is the thing that computes these partial derivatives for us."
  },
  {
    "index": "F15726",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、ニューラルネットワークの場合、このコスト関数Jのシータは非凸関数、言い換えると凸関数では無いので、理論上はローカル最小に陥りうるし、実際、最急降下法やアドバンスドな最適化手法は、理論上はローカル最適に捕まってしまう事がありうる。だが実際には、通常はこれはそんなに大きな問題にならない。",
    "output": "And so, we know how to compute the cost function, we know how to compute the partial derivatives using back propagation, so we can use one of these optimization methods to try to minimize j of theta as a function of the parameters theta."
  },
  {
    "index": "F15727",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらのアルゴリズムがグローバル最小を探す、とは保証出来なくても、普通は最急降下法のようなアルゴリズムはこのコスト関数Jのシータを小さくする、という仕事をとてもうまくこなす。",
    "output": "And by the way, for neural networks, this cost function j of theta is non-convex, or is not convex and so it can theoretically be susceptible to local minima, and in fact algorithms like gradient descent and the advance optimization methods can, in theory, get stuck in local optima, but it turns out that in practice this is not usually a huge problem and even though we can't guarantee that these algorithms will find a global optimum, usually algorithms like gradient descent will do a very good job minimizing this cost function j of theta and get a very good local minimum, even if it doesn't get to the global optimum."
  },
  {
    "index": "F15728",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、ニューラルネットワークに対する最急降下法は、まだこれでもちょっと魔法っぽく見えるかもしれない。",
    "output": "Finally, gradient descents for a neural network might still seem a little bit magical."
  },
  {
    "index": "F15729",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもう一つ最急降下法がニューラルネットワークに何をしているかを示す図をお見せしたい。",
    "output": "So, let me just show one more figure to try to get that intuition about what gradient descent for a neural network is doing."
  },
  {
    "index": "F15730",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは最急降下法を以前説明した時に用いた図に実は似た物だ。",
    "output": "This was actually similar to the figure that I was using earlier to explain gradient descent."
  },
  {
    "index": "F15731",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あるコスト関数があり、ニューラルネットワークのパラメータがある。",
    "output": "So, we have some cost function, and we have a number of parameters in our neural network."
  },
  {
    "index": "F15732",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここではパラメータの値を二つ書いた。",
    "output": "In reality, of course, in the neural network, we can have lots of parameters with these."
  },
  {
    "index": "F15733",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実際には、もちろん、ニューラルネットワークの場合、もっとたくさんのパラメータを持ちうる。シータ1もシータ2も行列だ。",
    "output": "Theta one, theta two--all of these are matrices, right?"
  },
  {
    "index": "F15734",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからパラメータの次元はとても高次になりうる。",
    "output": "So we can have very high dimensional parameters but because of the limitations the source of parts we can draw."
  },
  {
    "index": "F15735",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがプロットして表示してみる側の制約の為に、ニューラルネットワークに二つのパラメータしか無いかのように話をする。",
    "output": "I'm pretending that we have only two parameters in this neural network. Although obviously we have a lot more in practice."
  },
  {
    "index": "F15736",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いま、このコスト関数Jのシータはそのニューラルネットワークがトレーニングデータにどれだけフィットしているかを測っている物だ。",
    "output": "Now, this cost function j of theta measures how well the neural network fits the training data."
  },
  {
    "index": "F15737",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこんな点を取るなら、この下側の、そこはJのシータがとても低い点に対応し、つまりこれは以下のようなシチュエーションのパラメータに対応する、つまりだいたいのトレーニング手本に対して仮説の出力がy(i)に極めて近くなるようなそういうシチュエーションのパラメータに対応する。",
    "output": "So, if you take a point like this one, down here, that's a point where j of theta is pretty low, and so this corresponds to a setting of the parameters."
  },
  {
    "index": "F15738",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この条件が真の時が、コスト関数が極めて低い、という状態に対応する。",
    "output": "There's a setting of the parameters theta, where, you know, for most of the training examples, the output of my hypothesis, that may be pretty close to y(i) and if this is true than that's what causes my cost function to be pretty low."
  },
  {
    "index": "F15739",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方、対照的に、こっちのような値を取る時には、この点はトレーニング手本の大多数が、ニューラルネットワークの出力が実際の値、実際の観測された値のy(i)から、大きく離れた値に対応した点となる。",
    "output": "Whereas in contrast, if you were to take a value like that, a point like that corresponds to, where for many training examples, the output of my neural network is far from the actual value y(i) that was observed in the training set."
  },
  {
    "index": "F15740",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの右側の点のような点はトレーニングセットに対する仮説の出力、ニューラルネットワークの出力が、y(i)から遠く離れた値となる点に対応する。",
    "output": "So points like this on the line correspond to where the hypothesis, where the neural network is outputting values on the training set that are far from y(i)."
  },
  {
    "index": "F15741",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方このような点は、コスト関数の値が低いというのはJのシータが低い所に対応しているので、つまりはニューラルネットワークがちょうど良く私のトレーニングセットにフィットする場所に対応する。何故ならこれこそがJのシータが小さくなる時に真である必要のある事だから。",
    "output": "So, it's not fitting the training set well, whereas points like this with low values of the cost function corresponds to where j of theta is low, and therefore corresponds to where the neural network happens to be fitting my training set well, because I mean this is what's needed to be true in order for j of theta to be small."
  },
  {
    "index": "F15742",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから最急降下法のやる事は、なんらかのランダムな初期点から始めて、たとえばこことか、そこから繰り返し丘を降りていく。",
    "output": "So what gradient descent does is we'll start from some random initial point like that one over there, and it will repeatedly go downhill."
  },
  {
    "index": "F15743",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりバックプロパゲーションが行うのはグラディアントの方向を計算して、そして最急降下法がやる事は、一歩一歩ちょっとずつ丘を降りていき、期待するのは、そしてこの場合は実際にそうだが、かなり良いローカル最適に至るまで進む訳だ。",
    "output": "And so what back propagation is doing is computing the direction of the gradient, and what gradient descent is doing is it's taking little steps downhill until hopefully it gets to, in this case, a pretty good local optimum."
  },
  {
    "index": "F15744",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、バックプロパゲーションを実装して、最急降下法やアドバンスドな最適化法の一つを使う時には、この図はアルゴリズムが何をするかの、いくらかの説明となっている。",
    "output": "So, when you implement back propagation and use gradient descent or one of the advanced optimization methods, this picture sort of explains what the algorithm is doing."
  },
  {
    "index": "F15745",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはニューラルネットワークの出力する値が、トレーニングセットにおける観測値のy(i)になるべく近いようなニューラルネットのパラメータを探す事を試みる。",
    "output": "It's trying to find a value of the parameters where the output values in the neural network closely matches the values of the y(i)'s observed in your training set."
  },
  {
    "index": "F15746",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これで、あなたもニューラルネットワークのそれぞれのピースが、どう組み合わさるのか分かったんじゃないかな。",
    "output": "So, hopefully this gives you a better sense of how the many different pieces of neural network learning fit together."
  },
  {
    "index": "F15747",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも、もしこのビデオが終わった後でも、これらの様々なピースの中にいまいちどうもしっくり来ない物があったり、あるいはそれらのうちの幾つかが完全にはっきりと分かったという訳で無くても、あるいはこれらのピースがどうくっつくのか全部は分からなくても、実際は問題無い。",
    "output": "In case even after this video, in case you still feel like there are, like, a lot of different pieces and it's not entirely clear what some of them do or how all of these pieces come together, that's actually okay."
  },
  {
    "index": "F15748",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワーク学習とバックプロパゲーションは複雑なアルゴリズムだ。",
    "output": "Neural network learning and back propagation is a complicated algorithm."
  },
  {
    "index": "F15749",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして私ですら、バックプロパゲーションの背後にある数学を長年見てきて、さらに自分で思うにはバックプロパゲーションをとても成功裡に何年も使い続けてきた私ですら、こんにちでも、まだ時々バックプロパゲーションが正確に何やってるのかをいつもしっかりつかんでる、という訳では無いと感じる事がある。",
    "output": "And even though I've seen the math behind back propagation for many years and I've used back propagation, I think very successfully, for many years, even today I still feel like I don't always have a great grasp of exactly what back propagation is doing sometimes."
  },
  {
    "index": "F15750",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてJのシータを最小化する最適化がどう進んでいくかもしっかりとつかんでないと感じる事がある。",
    "output": "And what the optimization process looks like of minimizing j if theta."
  },
  {
    "index": "F15751",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはより難しいアルゴリズムで、これはしっかり分かるのが、、、これが正確に何をやっているのかをしっかりと把握するのが、線形回帰とかロジスティック回帰に比べて難しい。",
    "output": "Much this is a much harder algorithm to feel like I have a much less good handle on exactly what this is doing compared to say, linear regression or logistic regression."
  },
  {
    "index": "F15752",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "線形回帰などの方が数学的にも概念的にもよりシンプルで、よりクリーンなアルゴリズムだ。",
    "output": "Which were mathematically and conceptually much simpler and much cleaner algorithms."
  },
  {
    "index": "F15753",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがバックプロパゲーションを実装してみれば、きっとこれがもっとも強力な学習アルゴリズムの一つだと分かるだろう。そしてもしこのアルゴリズム、バックプロパゲーションをこれらの最適化手法の一つを実装すれば、バックプロパゲーションがとても複雑で強力で非線型な関数であなたのデータにフィッティング出来てそしてこれがこんにちある中でも最も効率的な学習アルゴリズムの一つである事が分かるだろう。",
    "output": "But so in case if you feel the same way, you know, that's actually perfectly okay, but if you do implement back propagation, hopefully what you find is that this is one of the most powerful learning algorithms and if you implement this algorithm, implement back propagation, implement one of these optimization methods, you find that back propagation will be able to fit very complex, powerful, non-linear functions to your data, and this is one of the most effective learning algorithms we have today."
  },
  {
    "index": "F15754",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、楽しくて歴史的にも重要なニューラルネットワークの学習の例をお見せしたい。",
    "output": "In this video, I'd like to show you a fun and historically important example of neural networks learning of using a neural network for autonomous driving."
  },
  {
    "index": "F15755",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークを用いて自動運転をする例を、つまり車が自分で運転するように学習させるという事。",
    "output": "That is getting a car to learn to drive itself."
  },
  {
    "index": "F15756",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私がこのちょっと後に見せるビデオはDeanPomilieuから頂いた物だ。彼はカーネギーメロンでアメリカの東海岸で働いている同僚だ。",
    "output": "The video that I'll showed a minute was something that I'd gotten from Dean Pomerleau, who was a colleague who works out in Carnegie Mellon University out on the east coast of the United States."
  },
  {
    "index": "F15757",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ビデオの中では、このような可視化を目にする事になるので、ビデオを開始する前にこの可視化がどんな風になるのかを、ちょっと言っておきたい。",
    "output": "And in part of the video you see visualizations like this."
  },
  {
    "index": "F15758",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この左下の部分は、車が見ている車の前に何があるかの映像で、つまり、そこに映るのは、道路で、例えばちょっとだけ左に向かってたりちょっとだけ右に向かってたりする訳だ。",
    "output": "And I want to tell what a visualization looks like before starting the video."
  },
  {
    "index": "F15759",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの上のここにあるのは、この最初の水平バーは人間の運転手が選んだ方向。",
    "output": "Down here on the lower left is the view seen by the car of what's in front of it."
  },
  {
    "index": "F15760",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの白いバンドの場所は人間の運転手が選んだステアリングの方向だ。",
    "output": "And up here on top, this first horizontal bar shows the direction selected by the human driver."
  },
  {
    "index": "F15761",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この一番左の端のここの端は、ステアリングを一番左に切った状態に対応し、ここは、右側に強く切る事に対応する。",
    "output": "And this location of this bright white band that shows the steering direction selected by the human driver where you know here far to the left corresponds to steering hard left, here corresponds to steering hard to the right."
  },
  {
    "index": "F15762",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この位置は、ちょっとだけ左、真ん中のやや左のこの位置は、人間の運転手が、わずかに左にステアリングを切っている事を意味する。",
    "output": "And so this location which is a little bit to the left, a little bit left of center means that the human driver at this point was steering slightly to the left."
  },
  {
    "index": "F15763",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの二番目のバーは学習アルゴリズムによって選択されたステアリングの方向に対応している。これも、この種の白のバンドの位置が意味する事は、ニューラルネットワークはここでは、ステアリングの方向をちょっとだけ左と選んでいるという事。",
    "output": "And this second bot here corresponds to the steering direction selected by the learning algorithm and again the location of this sort of white band means that the neural network was here selecting a steering direction that's slightly to the left."
  },
  {
    "index": "F15764",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際は、ニューラルネットワークが学習を始める前には、見ての通り、ネットワークはグレーのバンドを出力している、一様にグレーな、この範囲全体に渡ったグレーのバンド、つまりこのグレーで一様でぼんやりした物は、ニューラルネットワークがランダムに初期化されている事に対応していて、最初の時点では、車をどう運転したらいいか、さっぱり分からない。",
    "output": "And in fact before the neural network starts leaning initially, you see that the network outputs a grey band, like a grey, like a uniform grey band throughout this region and sort of a uniform gray fuzz corresponds to the neural network having been randomly initialized. And initially having no idea how to drive the car."
  },
  {
    "index": "F15765",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あるいは最初の時点では、ステアリングをどちらに切ったらいいか分からない。",
    "output": "Or initially having no idea of what direction to steer in."
  },
  {
    "index": "F15766",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてしばらく学習した後になってはじめてしっかりとした白いバンドで、小さな領域のみを占有した物を出力するようになる、それは特定のステアリングの方向を選ぶ事に対応している。",
    "output": "And is only after it has learned for a while, that will then start to output like a solid white band in just a small part of the region corresponding to choosing a particular steering direction."
  },
  {
    "index": "F15767",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれは、ニューラルネットワークが特定のバンド、特定の場所を選ぶ事に確信を持った、という事に対応する、グレーでぼやけた出力では無く、白いバンド、これは一つのステアリングの方向をしっかりと選び続けている事に対応しているが、それを出力するという。",
    "output": "And that corresponds to when the neural network becomes more confident in selecting a band in one particular location, rather than outputting a sort of light gray fuzz, but instead outputting a white band that's more constantly selecting one's steering direction."
  },
  {
    "index": "F15768",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Albanは人工知能ニューラルネットワークのシステムで、人が運転しているのを見る事でステアリングを学習する。",
    "output": ">> ALVINN is a system of artificial neural networks that learns to steer by watching a person drive."
  },
  {
    "index": "F15769",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Albanはナトラブ2を制御するように設計されている、ナトラブ2は軍用ハンビーを改造した物で、センサーとコンピュータとアクチュエーターを、自動運転の実験の為に搭載した物だ。",
    "output": "ALVINN is designed to control the NAVLAB 2, a modified Army Humvee who had put sensors, computers, and actuators for autonomous navigation experiments."
  },
  {
    "index": "F15770",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Albanを設定する最初のステップは人間に運転させて、それをAlbanが観察する事でトレーニングする事だ。",
    "output": "The initial step in configuring ALVINN is creating a network just here. During training, a person drives the vehicle while ALVINN watches."
  },
  {
    "index": "F15771",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "毎秒に12回、Albanは前方の道路のビデオ画像をデジタイズして、そして人間のステアリングの方向を記録する。",
    "output": "Once every two seconds, ALVINN digitizes a video image of the road ahead, and records the person's steering direction."
  },
  {
    "index": "F15772",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このトレーニング画像は解像度で30x32pixelに縮小して、Albanの3層のネットワークの入力として供給される。",
    "output": "This training image is reduced in resolution to 30 by 32 pixels and provided as input to ALVINN's three layered network."
  },
  {
    "index": "F15773",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バックプロパゲーションの学習アルゴリズムを用いてAlbanはその画像に対して人間のドライバと同じステアリングの方向を出力するようにトレーニングされる。",
    "output": "Using the back propagation learning algorithm,ALVINN is training to output the same steering direction as the human driver for that image."
  },
  {
    "index": "F15774",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "初期には、ネットワークのステアリングの答えは、ランダムだ。",
    "output": "Initially the network steering response is random."
  },
  {
    "index": "F15775",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だいたい2分くらいトレーニングした後には、ネットワークは人間の運転手のステアリングの反応を正確に模倣するようになる。",
    "output": "After about two minutes of training the network learns to accurately imitate the steering reactions of the human driver."
  },
  {
    "index": "F15776",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これと同じトレーニングを他の種類の道路でも繰り返す。",
    "output": "This same training procedure is repeated for other road types."
  },
  {
    "index": "F15777",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ネットワークのトレーニングが終わったら、オペレータは実行スイッチを押して、そして運転を開始する。",
    "output": "After the networks have been trained the operator pushes the run switch and ALVINN begins driving."
  },
  {
    "index": "F15778",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一秒間に12回、Albanは画像をデジタイズしてニューラルネットワークに食わせる。",
    "output": "Twelve times per second, ALVINN digitizes the image and feeds it to its neural networks."
  },
  {
    "index": "F15779",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "各ネットワークを、並行に走らせて、ステアリングの方向を生成させて、その応答の信頼度を測る。",
    "output": "Each network, running in parallel, produces a steering direction, and a measure of its' confidence in its' response."
  },
  {
    "index": "F15780",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もっとも信頼度が高いネットワークからのステアリングの方向、この場合は1レーンの道でトレーニングされたネットワークが、乗り物を制御する為に使われる。",
    "output": "The steering direction, from the most confident network, in this network training for the one lane road, is used to control the vehicle."
  },
  {
    "index": "F15781",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "突然、交差点が乗り物の前に現れた。",
    "output": "Suddenly an intersection appears ahead of the vehicle."
  },
  {
    "index": "F15782",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "乗り物が交差点に近づくと1レーンのニューラルネットワークの信頼度が低下する。",
    "output": "As the vehicle approaches the intersection the confidence of the lone lane network decreases."
  },
  {
    "index": "F15783",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "交差点を渡ると、2レーンの道が前方に現れ、そして2レーンのネットワークの信頼度が上昇する。",
    "output": "As it crosses the intersection and the two lane road ahead comes into view, the confidence of the two lane network rises."
  },
  {
    "index": "F15784",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その信頼度が上昇すると、2レーンのネットワークがステアリングを選択し、安全に乗り物をレーン内に導く、2レーンの道の。",
    "output": "When its' confidence rises the two lane network is selected to steer. Safely guiding the vehicle into its lane onto the two lane road."
  },
  {
    "index": "F15785",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、最近だともっとモダンな、車の自動運転を目指すプロジェクトが、USとかヨーロッパに存在している。",
    "output": "Of course there are more recently more modern attempts to do autonomous driving."
  },
  {
    "index": "F15786",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらはここに示した物よりももっと頑強なコントローラーを用いているが、それでも私は、これはとても特筆すべきで、そしてとても驚きだと思う、シンプルなニューラルネットワークをバックプロパゲーションでトレーニングしただけの物が結構いい感じに車が運転出来る、という事が。",
    "output": "There are few projects in the US and Europe and so on, that are giving more robust driving controllers than this, but I think it's still pretty remarkable and pretty amazing how instant neural network trained with backpropagation can actually learn to drive a car somewhat well."
  },
  {
    "index": "F15787",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでに、あなたは様々な学習のアルゴリズムを見てきた。",
    "output": "By now you have seen a lot of different learning algorithms."
  },
  {
    "index": "F15788",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここまでのビデオと共に歩んできたあなたは、自分自身が既に、たくさんの芸術の域にまで達した機械学習のエキスパートである事を自覚すべき所まで来ている。",
    "output": "And if you've been following along these videos you should consider yourself an expert on many state-of-the-art machine learning techniques."
  },
  {
    "index": "F15789",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがある学習アルゴリズムを知っている人の間でも、本当にそのアルゴリズムを強力かつ効率的に適用する方法を知っている人と、私が今から語る事に、そこまでは慣れ親しんでいなく、これらのアルゴリズムをどう適用したらいいか分からなくて、うまく行くはずも無い事にトライしてたくさんの時間を無駄にする人とでは本当に大きな違いが存在する。",
    "output": "But even among people that know a certain learning algorithm."
  },
  {
    "index": "F15790",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私がお伝えしたいのは、もしあなたが機械学習のシステムを開発する時に、もっとも時間を有効に投資出来る道を確実に選べるようになる事です。",
    "output": "There's often a huge difference between someone that really knows how to powerfully and effectively apply that algorithm, versus someone that's less familiar with some of the material that I'm about to teach and who doesn't really understand how to apply these algorithms and can end up wasting a lot of their time trying things out that don't really make sense."
  },
  {
    "index": "F15791",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのビデオとつづくビデオでいくつかの実践的な提案、アドバイス、ガイドラインを、それが出来るように提供する。",
    "output": "What I would like to do is make sure that if you are developing machine learning systems, that you know how to choose one of the most promising avenues to spend your time pursuing."
  },
  {
    "index": "F15792",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、フォーカスしたい問題は、あなたは機械学習のシステムを開発するなり、パフォーマンスを改善したいなりするとする。",
    "output": "And on this and the next few videos I'm going to give a number of practical suggestions, advice, guidelines on how to do that."
  },
  {
    "index": "F15793",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その時に、次に試すのに約束された道をどうやって決定するのか?という問題。",
    "output": "And concretely what we'd focus on is the problem of, suppose you are developing a machine learning system or trying to improve the performance of a machine learning system, how do you go about deciding what are the proxy avenues to try next?"
  },
  {
    "index": "F15794",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを説明する為に、家の価格を予言する例をまた使っていこう。",
    "output": "To explain this, let's continue using our example of learning to predict housing prices."
  },
  {
    "index": "F15795",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして正規化した線形回帰を実装したとしよう。",
    "output": "And let's say you've implement and regularize linear regression."
  },
  {
    "index": "F15796",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "かくしてコスト関数Jを最小化した。",
    "output": "Thus minimizing that cost function j."
  },
  {
    "index": "F15797",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、学習したパラメータを用いて新しい家の集合の価格を予言したとする。そこで予言された価格がとても巨大な誤差を生み出していたとします。",
    "output": "Now suppose that after you take your learn parameters, if you test your hypothesis on the new set of houses, suppose you find that this is making huge errors in this prediction of the housing prices."
  },
  {
    "index": "F15798",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、ここで問題です。学習アルゴリズムを改善する為に、次に何をやりますか?",
    "output": "The question is what should you then try mixing in order to improve the learning algorithm?"
  },
  {
    "index": "F15799",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "学習アルゴリズムを改善しうる事はたくさん考えられる。",
    "output": "There are many things that one can think of that could improve the performance of the learning algorithm."
  },
  {
    "index": "F15800",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つには、より多くのトレーニング手本を使う、というのが考えられる。",
    "output": "One thing they could try, is to get more training examples."
  },
  {
    "index": "F15801",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、想像してみてくれ、電話による調査を立ち上げて、一軒一軒それぞれの家が幾らで売られているかより多くのデータを得ようとする事を。",
    "output": "And concretely, you can imagine, maybe, you know, setting up phone surveys, going door to door, to try to get more data on how much different houses sell for."
  },
  {
    "index": "F15802",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "悲しい事に、私はこれまでたくさんの人々がより多くのトレーニング手本を集めるのに、たくさんの時間を使っているのを見てきた。「もし二倍とか10倍のデータがあれば、きっと良くなるに違い無いでしょ?",
    "output": "And the sad thing is I've seen a lot of people spend a lot of time collecting more training examples, thinking oh, if we have twice as much or ten times as much training data, that is certainly going to help, right?"
  },
  {
    "index": "F15803",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもより多くのトレーニングデータを得る事が助けにならない事もある。次の一連のビデオでそれが何故なのか、そしてそれが役に立たない状況なのにたくさんのトレーニングデータを集めるのにたくさんの時間を費やしてしまうのをどうやって避けるのか、見ていく事にする。",
    "output": "But sometimes getting more training data doesn't actually help and in the next few videos we will see why, and we will see how you can avoid spending a lot of time collecting more training data in settings where it is just not going to help."
  },
  {
    "index": "F15804",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他に試す事としては、より少ない数のフィーチャーを使う、というのが考えられる。",
    "output": "Other things you might try are to well maybe try a smaller set of features."
  },
  {
    "index": "F15805",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、もし幾つかのフィーチャー、例えばx1、x2、x3、、、というようなのがひょっとしたらたくさんあるとする。",
    "output": "So if you have some set of features such as x1, x2, x3 and so on, maybe a large number of features."
  },
  {
    "index": "F15806",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その時は、それらのサブセットを時間をかけて慎重にオーバーフィッティングを避ける為に選ぶという事が考えられる。",
    "output": "Maybe you want to spend time carefully selecting some small subset of them to prevent overfitting."
  },
  {
    "index": "F15807",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または、追加のフィーチャーが必要かもしれない。",
    "output": "Or maybe you need to get additional features."
  },
  {
    "index": "F15808",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ひょっとしたら現在のフィーチャーの集まりは十分な情報を持っていなくて、だからより多くの種類のフィーチャーを得るという意味でより多くのデータを集めるべきかもしれない。",
    "output": "Maybe the current set of features aren't informative enough and you want to collect more data in the sense of getting more features."
  },
  {
    "index": "F15809",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれはある種、とても巨大なスケールに拡張しうるプロジェクトだ。電話調査でよりたくさんの家や追加の土地測量でよりたくさんの地価を集めたりするのを想像出来るだろうか?",
    "output": "And once again this is the sort of project that can scale up the huge projects can you imagine getting phone surveys to find out more houses, or extra land surveys to find out more about the pieces of land and so on, so a huge project."
  },
  {
    "index": "F15810",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり巨大プロジェクトなんです。そしてここでも、こんな事を実際に試す前にそれが役に立つかを前もって知れたらいいと思わない?",
    "output": "And once again it would be nice to know in advance if this is going to help before we spend a lot of time doing something like this."
  },
  {
    "index": "F15811",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また、さらなるフィーチャーの多項式を追加する事も出来る。例えばx1の二乗とかx2の二乗とかフィーチャーの積、x1x2など。",
    "output": "We can also try adding polynomial features things like x2 square x2 square and product features x1, x2."
  },
  {
    "index": "F15812",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんな事を考えるのにもたくさんの時間を使えるし、他にも試せる事としては、ラムダ、正規化のパラメータを減らしたり増やしたりも試せる。",
    "output": "We can still spend quite a lot of time thinking about that and we can also try other things like decreasing lambda, the regularization parameter or increasing lambda."
  },
  {
    "index": "F15813",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのような選択肢のメニューがあったとして、そのうちの幾つかは簡単に6ヶ月とかもっと長いプロジェクトにスケールアップ出来る。",
    "output": "Given a menu of options like these, some of which can easily scale up to six month or longer projects."
  },
  {
    "index": "F15814",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "残念なことに、これらから一つ選ぶのに人々がもっとも良く使うのはなんとなくのフィーリングだ。",
    "output": "Unfortunately, the most common method that people use to pick one of these is to go by gut feeling."
  },
  {
    "index": "F15815",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで多くの人がやってるのは適当にランダムに一つこれらの選択肢の中から選んで「よーし、もっと多くのトレーニングデータを集めに行こう!",
    "output": "In which what many people will do is sort of randomly pick one of these options and maybe say, \"Oh, lets go and get more training data.\" And easily spend six months collecting more training data or maybe someone else would rather be saying, \"Well, let's go collect a lot more features on these houses in our data set.\" And I have a lot of times, sadly seen people spend, you know, literally 6 months doing one of these avenues that they have sort of at random only to discover six months later that that really wasn't a promising avenue to pursue."
  },
  {
    "index": "F15816",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "」とか言ってる訳だ。そして簡単に六ヶ月とかより多くのトレーニングデータを集めるのに使っちゃう。",
    "output": "Fortunately, there is a pretty simple technique that can let you very quickly rule out half of the things on this list as being potentially promising things to pursue."
  },
  {
    "index": "F15817",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または別の誰かが代わりに、「よし、我らのデータセットの家に対して、もっと追加のフィーチャーを集めに行こうっと」とか言ってたりする。そして私は何度も見た!",
    "output": "And there is a very simple technique, that if you run, can easily rule out many of these options, and potentially save you a lot of time pursuing something that's just is not going to work."
  },
  {
    "index": "F15818",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "可哀想に見える人々が文字通り六ヶ月とかの時間を使ってようするにランダムに選んだこれらの選択肢の一つを進み六ヶ月後にはこれは未来のある道では無かったという事だけが分かった、という出来事を!",
    "output": "In the next two videos after this, I'm going to first talk about how to evaluate learning algorithms."
  },
  {
    "index": "F15819",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "幸運なことに、リストの半分を除外しつつ、将来のある手段は残すとてもシンプルなテクニックが存在する。",
    "output": "And in the next few videos after that, I'm going to talk about these techniques, which are called the machine learning diagnostics."
  },
  {
    "index": "F15820",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "しかもそれは、とてもシンプルなテクニックでもし実行すれば、簡単にこれらの選択肢の多くを排除出来て、単に機能しない何かをし続けるのにたくさんの時間を浪費するのを、防ぐことが出来る。これに続く2つのビデオで、まず学習アルゴリズムをどうやって評価するかを議論する。",
    "output": "And what a diagnostic is, is a test you can run, to get insight into what is or isn't working with an algorithm, and which will often give you insight as to what are promising things to try to improve a learning algorithm's performance."
  },
  {
    "index": "F15821",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれに続くビデオでこれらの機械学習診断と呼ばれるテクニックの議論に進む。",
    "output": "We'll talk about specific diagnostics later in this video sequence."
  },
  {
    "index": "F15822",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが前もって言っておきたい事とてしては、診断は実装するのに時間もかかるし、時にはただかかるだけじゃなくて、実装したり理解したりするのに凄い時間がかかる場合もある。でもそうやって時間を使うのは学習アルゴリズム開発する時にはとても良い時間の使い方だ。",
    "output": "But I should mention in advance that diagnostics can take time to implement and can sometimes, you know, take quite a lot of time to implement and understand but doing so can be a very good use of your time when you are developing learning algorithms because they can often save you from spending many months pursuing an avenue that you could have found out much earlier just was not going to be fruitful."
  },
  {
    "index": "F15823",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では次のビデオで最初にまず私は学習アルゴリズムをどう評価するかについて話し、そしてその後これらの診断の幾つかを議論することで、機械学習のシステムを改善するというゴールにより有益な物をより効率的に見つけられるようになる事を期待する。",
    "output": "So in the next few videos, I'm going to first talk about how evaluate your learning algorithms and after that I'm going to talk about some of these diagnostics which will hopefully let you much more effectively select more of the useful things to try mixing if your goal to improve the machine learning system."
  },
  {
    "index": "F15824",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、あなたのアルゴリズムで学習させた仮説をどうやって評価するか?について話す。",
    "output": "In this video, I would like to talk about how to evaluate a hypothesis that has been learned by your algorithm."
  },
  {
    "index": "F15825",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "後半のビデオで、これを用いてオーバーフィッティングとアンダーフィッティングをどう防止するかについて話す。",
    "output": "In later videos, we will build on this to talk about how to prevent in the problems of overfitting and underfitting as well."
  },
  {
    "index": "F15826",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "学習アルゴリズムのパラメータをフィットする時はトレーニングの誤差を最小にするようにパラメータを選ぼうとする。",
    "output": "When we fit the parameters of our learning algorithm we think about choosing the parameters to minimize the training error."
  },
  {
    "index": "F15827",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニングでの誤差をとっても小さくするのが良い事だ、と思う人も居るかもしれない。でも既に見たように、ただ仮説のトレーニング誤差が低いという事だけでは、それが良い仮説だという事は、必ずしも意味しない。",
    "output": "One might think that getting a really low value of training error might be a good thing, but we have already seen that just because a hypothesis has low training error, that doesn't mean it is necessarily a good hypothesis."
  },
  {
    "index": "F15828",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして既に、仮説がどうオーバーフィットしてしまうか、の例も見てきた。",
    "output": "And we've already seen the example of how a hypothesis can overfit."
  },
  {
    "index": "F15829",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその結果、トレーニングセットに無い新しいサンプルに対して、一般化する事にどう失敗するかを。",
    "output": "And therefore fail to generalize the new examples not in the training set."
  },
  {
    "index": "F15830",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではどのように仮説がオーバーフィットしている事を知る事が出来るか?",
    "output": "So how do you tell if the hypothesis might be overfitting."
  },
  {
    "index": "F15831",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この単純な例では、仮説h(x)をプロットして何が起きているかを見てみる事が出来る。",
    "output": "In this simple example we could plot the hypothesis h of x and just see what was going on."
  },
  {
    "index": "F15832",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが一般的には、フィーチャーが一つよりも多い問題に対しては、、、これらのようにたくさんのフィーチャーがある問題に対しては仮説をプロットするのが難しかったり、時には不可能だったりする。",
    "output": "But in general for problems with more features than just one feature, for problems with a large number of features like these it becomes hard or may be impossible to plot what the hypothesis looks like and so we need some other way to evaluate our hypothesis."
  },
  {
    "index": "F15833",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから仮説評価する他の手段が必要だ。学習の仮説を評価するスタンダードな方法は以下のようになる。",
    "output": "The standard way to evaluate a learned hypothesis is as follows."
  },
  {
    "index": "F15834",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんなデータセットがあるとする。",
    "output": "Suppose we have a data set like this."
  },
  {
    "index": "F15835",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここではトレーニング手本を10個しか示していないが通常は何十、何百、または何千ものトレーニング手本がある。",
    "output": "Here I have just shown 10 training examples, but of course usually we may have dozens or hundreds or maybe thousands of training examples."
  },
  {
    "index": "F15836",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "仮説を評価出来ている、という事を確認する為に、実行するのは、データを2つの部分に分ける、という事。",
    "output": "In order to make sure we can evaluate our hypothesis, what we are going to do is split the data we have into two portions."
  },
  {
    "index": "F15837",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の部分は通常のトレーニングセットで、二番目の部分はテストセットとなる。そしてこの分割の典型的なやり方は全体のデータを、トレーニングセットとテストセットがだいたい70%、30%になるように分ける。",
    "output": "The first portion is going to be our usual training set and the second portion is going to be our test set, and a pretty typical split of this all the data we have into a training set and test set might be around say a 70%, 30% split."
  },
  {
    "index": "F15838",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より多くのデータがトレーニングセットになり、相対的には少ない量がテストセットになるように。",
    "output": "Worth more today to grade the training set and relatively less to the test set."
  },
  {
    "index": "F15839",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりデータセットがあったとすると、データの70%だけをトレーニングセットに振り分ける。ここで「m」はいつも通り、トレーニング手本の数。",
    "output": "And so now, if we have some data set, we run a sine of say 70% of the data to be our training set where here \"m\" is as usual our number of training examples and the remainder of our data might then be assigned to become our test set."
  },
  {
    "index": "F15840",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここで、mの下付き添字のtestでテストのサンプルの数を示す。",
    "output": "And here, I'm going to use the notation m subscript test to denote the number of test examples."
  },
  {
    "index": "F15841",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今後は一般に、このtestという下付き添字でテストセットから来たサンプルを表す事にする。つまりx1の下付き添字test、y1の下付き添字のtestで最初のテストのサンプルを表し、この例だとここにあるサンプルとなる。",
    "output": "And so in general, this subscript test is going to denote examples that come from a test set so that x1 subscript test, y1 subscript test is my first test example which I guess in this example might be this example over here."
  },
  {
    "index": "F15842",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に一つ細かい話を。",
    "output": "Finally, one last detail whereas here I've drawn this as though the first 70% goes to the training set and the last 30% to the test set."
  },
  {
    "index": "F15843",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここではまるで最初の70%をトレーニングセットに、最後の30%をテストセットにするかのように線を引いたが、データにもし順番があるような物なら、ランダムに70%のデータをトレーニングセットに残りの30%をテストセットに選んだ方が良い。",
    "output": "If there is any sort of ordinary to the data."
  },
  {
    "index": "F15844",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたのデータが既にランダムに並んでいたらただ最初の70%と後ろの30%を取ればよろしい。",
    "output": "That should be better to send a random 70% of your data to the training set and a random 30% of your data to the test set."
  },
  {
    "index": "F15845",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしデータがランダムに並んでる訳では無ければ、トレーニングセットをランダムにシャッフルする、またはランダムに並べ替える方が良い。",
    "output": "So if your data were already randomly sorted, you could just take the first 70% and last 30% that if your data were not randomly ordered, it would be better to randomly shuffle or to randomly reorder the examples in your training set."
  },
  {
    "index": "F15846",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の70%をトレーニングセットに送り、後ろの30%をテストセットに送る前に。",
    "output": "Before you know sending the first 70% in the training set and the last 30% of the test set."
  },
  {
    "index": "F15847",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに、学習アルゴリズムと回帰の学習の訓練を実施する、きわめて典型的な手順を示す。",
    "output": "Here then is a fairly typical procedure for how you would train and test the learning algorithm and the learning regression."
  },
  {
    "index": "F15848",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず、トレーニングセットからパラメータのシータを学習して、通常のトレーニングの目的関数であるトレーニングの誤差、Jのシータを最小化する。ここでJのシータは全データの70%を使って定義した物。",
    "output": "First, you learn the parameters theta from the training set so you minimize the usual training error objective j of theta, where j of theta here was defined using that 70% of all the data you have."
  },
  {
    "index": "F15849",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニングデータだけについて計算した物ね。",
    "output": "There is only the training data."
  },
  {
    "index": "F15850",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、テストの誤差を計算する。",
    "output": "And then you would compute the test error."
  },
  {
    "index": "F15851",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてテストの誤差をJの下付き添字testで示す。",
    "output": "And I am going to denote the test error as j subscript test."
  },
  {
    "index": "F15852",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、何をするかといえば、トレーニングセットで学習したパラメータのシータをここに入れて、テストセットの誤差を計算する。それは以下のようになる。",
    "output": "And so what you do is take your parameter theta that you have learned from the training set, and plug it in here and compute your test set error."
  },
  {
    "index": "F15853",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは基本的にはテストセットに対して測った誤差の二乗の平均だ。",
    "output": "So this is basically the average squared error as measured on your test set."
  },
  {
    "index": "F15854",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たぶんご想像の通りでしょう。",
    "output": "It's pretty much what you'd expect."
  },
  {
    "index": "F15855",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりパラメータシータを入れた仮説でテストセットの要素一つずつを予言して、仮説の誤差をm下付き添字test個に対して計測する。",
    "output": "So if we run every test example through your hypothesis with parameter theta and just measure the squared error that your hypothesis has on your m subscript test, test examples."
  },
  {
    "index": "F15856",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、これは線形回帰を使っている時のテストセットの誤差の定義であって、二乗誤差の計量を用いている場合だ。",
    "output": "And of course, this is the definition of the test set error if we are using linear regression and using the squared error metric."
  },
  {
    "index": "F15857",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではもし分類問題を行なっていて、代わりに例えばロジスティック回帰を使っていた場合はどうだろう?",
    "output": "How about if we were doing a classification problem and say using logistic regression instead."
  },
  {
    "index": "F15858",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合でも、ロジスティック回帰でのトレーニングとテストの手続きはとても似た物だ。まず先にトレーニングデータからパラメータを学習する、データの最初の70%からね。",
    "output": "In that case, the procedure for training and testing say logistic regression is pretty similar first we will do the parameters from the training data, that first 70% of the data."
  },
  {
    "index": "F15859",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして以下のようにテスト誤差を計算する。",
    "output": "And it will compute the test error as follows."
  },
  {
    "index": "F15860",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは普段ロジスティック回帰に使っているのと同じ目的関数だ。",
    "output": "It's the same objective function as we always use but we just logistic regression, except that now is define using our m subscript test, test examples."
  },
  {
    "index": "F15861",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "違いはm下付き添字testに対して定義されている。つまりテストセットに対して。",
    "output": "While this definition of the test set error j subscript test is perfectly reasonable."
  },
  {
    "index": "F15862",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そちらの方が解釈が容易かも。それは誤判別の誤差だ。",
    "output": "Sometimes there is an alternative test sets metric that might be easier to interpret, and that's the misclassification error."
  },
  {
    "index": "F15863",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはまた、ゼロワン誤判別の誤差とも呼ばれる。ゼロワンはサンプルから正しい結果が得られたか、誤った結果が得られたかを表す。",
    "output": "It's also called the zero one misclassification error, with zero one denoting that you either get an example right or you get an example wrong."
  },
  {
    "index": "F15864",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこういう事だ。",
    "output": "Here's what I mean."
  },
  {
    "index": "F15865",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "予言の誤差を定義しよう。",
    "output": "Let me define the error of a prediction."
  },
  {
    "index": "F15866",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "予言とはh(x)だ。",
    "output": "That is h of x."
  },
  {
    "index": "F15867",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてラベルyをつけて、イコール1となるのは、仮説の出力が0.5以上の値で、なおかつy=0の時。または、仮説の出力値が0.5未満でy=1の時。",
    "output": "And given the label y as equal to one if my hypothesis outputs the value greater than equal to five and Y is equal to zero or if my hypothesis outputs a value of less than 0.5 and y is equal to one, right, so both of these cases basic respond to if your hypothesis mislabeled the example assuming your threshold at an 0.5."
  },
  {
    "index": "F15868",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりどちらのケースでも、基本的には仮説がサンプルを間違えてラベルづけした時となる、しきい値を0.5として。つまり、より1の可能性が高いと考えたが実際は0か仮説がより0の可能性が高いと考えたが実際は0という事。",
    "output": "So either thought it was more likely to be 1, but it was actually 0, or your hypothesis stored was more likely to be 0, but the label was actually 1."
  },
  {
    "index": "F15869",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれ以外の場合は、このエラー関数を0と定義する。",
    "output": "And otherwise, we define this error function to be zero."
  },
  {
    "index": "F15870",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし仮説が基本的には手本のyを正しく分類したら。",
    "output": "If your hypothesis basically classified the example y correctly."
  },
  {
    "index": "F15871",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、テスト誤差を誤判別の誤差計量を用いて、それのi=1からm下付き添字testまでのerrのhのxi_test、yiの和として定義出来る。",
    "output": "We could then define the test error, using the misclassification error metric to be one of the m tests of sum from i equals one to m subscript test of the error of h of x(i) test comma y(i)."
  },
  {
    "index": "F15872",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これをずばり書き下す方法は正確に我らの仮説がテストセットの手本を誤判別した割合という事になる。",
    "output": "And so that's just my way of writing out that this is exactly the fraction of the examples in my test set that my hypothesis has mislabeled."
  },
  {
    "index": "F15873",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がゼロワン誤判別計量を用いたテストセットの誤判別の誤差の定義だ。",
    "output": "And so that's the definition of the test set error using the misclassification error of the 0 1 misclassification metric."
  },
  {
    "index": "F15874",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が学習した仮説がどれだけ良いかを評価する標準的なテクニックだ。",
    "output": "So that's the standard technique for evaluating how good a learned hypothesis is."
  },
  {
    "index": "F15875",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、これらのアイデアを用いて、なんのフィーチャーを含めるべきか選んだり何次の多項式を学習アルゴリズムに含めるべきかを選んだり学習アルゴリズムの正規化パラメータを選ぶ助けとしていきます。",
    "output": "In the next video, we will adapt these ideas to helping us do things like choose what features like the degree polynomial to use with the learning algorithm or choose the regularization parameter for learning algorithm."
  },
  {
    "index": "F15876",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "データセットに何次の多項式まで含めてフィットさせたいか決めたい、としよう。",
    "output": "Suppose you're left to decide what degree of polynomial to fit to a data set."
  },
  {
    "index": "F15877",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり学習アルゴリズムになんのフィーチャーを含めるか、という話だ。",
    "output": "So that what features to include that gives you a learning algorithm."
  },
  {
    "index": "F15878",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または学習アルゴリズムの正規化パラメータ、ラムダを選びたいとしよう。",
    "output": "Or suppose you'd like to choose the regularization parameter longer for learning algorithm."
  },
  {
    "index": "F15879",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうやる?",
    "output": "How do you do that?"
  },
  {
    "index": "F15880",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをどう行うかの議論をするにあたり、データをどうトレーニングセットとテストセットに分割するかだけでなく、データを、やがて見る事になる、トレインバリデーションと呼ばれる物とテストセットにデータをどう分割するかを扱う。",
    "output": "Browsers, and in our discussion of how to do this, we'll talk about not just how to split your data into the train and test sets, but how to switch data into what we discover is called the train, validation, and test sets."
  },
  {
    "index": "F15881",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオの中で以上の事がいったいなんなのか、そしてそれらを使ってどうモデル選択をするかを見ていく。",
    "output": "We'll see in this video just what these things are, and how to use them to do model selection."
  },
  {
    "index": "F15882",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでに何度もオーバーフィティングの問題を見てきた。そこでは学習アルゴリズムがトレーニングセットに良くフィットしているからといって必ずしも良い仮説だとは限らないのだった。",
    "output": "We've already seen a lot of times the problem of overfitting, in which just because a learning algorithm fits a training set well, that doesn't mean it's a good hypothesis."
  },
  {
    "index": "F15883",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的には、これこそがトレーニングセットの誤差が新しいサンプルに対してどれだけ良い予測器かの良い指標にはならない理由だった。",
    "output": "More generally, this is why the training set's error is not a good predictor for how well the hypothesis will do on new example."
  },
  {
    "index": "F15884",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、あるパラメータの集まり、シータ0、シータ1、シータ2など、をトレーニングセットにフィットしようとしているとしよう。",
    "output": "Concretely, if you fit some set of parameters. Theta0, theta1, theta2, and so on, to your training set."
  },
  {
    "index": "F15885",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その時に仮説がトレーニングセットに対して良いという事実は、仮説が新たな、トレーニングセットに無いサンプルをどれだけうまく予言するかという観点からは、多くは意味しない、という事。",
    "output": "Well, this doesn't mean much in terms of predicting how well your hypothesis will generalize to new examples not seen in the training set. And a more general principle is that once your parameter is what fit to some set of data."
  },
  {
    "index": "F15886",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的な原則としては、一旦パラメータをあるデータの集合にフィッティングしたら、それがトレーニングセットであれそれ以外であれ、その同じデータセットに対して測った仮説の誤差というのは、たとえばトレーニング誤差などはそれは恐らく実際の一般のケースの誤差を推計するには良い物では無い。",
    "output": "Then the error of your hypothesis as measured on that same data set, such as the training error, that's unlikely to be a good estimate of your actual generalization error."
  },
  {
    "index": "F15887",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりどれだけ仮説が新しいサンプルに対してうまく一般化されているか、という事については。",
    "output": "That is how well the hypothesis will generalize to new examples."
  },
  {
    "index": "F15888",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、モデル選択問題を検討してみよう。",
    "output": "Now let's consider the model selection problem."
  },
  {
    "index": "F15889",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "データをフィットするのに、何次の多項式を含めるかを選ぼうとしている、としよう。",
    "output": "Let's say you're trying to choose what degree polynomial to fit to data."
  },
  {
    "index": "F15890",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、あなたは線形関数を選びたい、二次関数、三次関数、、、、と10乗の多項式まで。",
    "output": "So, should you choose a linear function, a quadratic function, a cubic function? All the way up to a 10th-order polynomial."
  },
  {
    "index": "F15891",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、それはまるで、一つ追加のパラメータ、それをdで表すが、何次の多項式まで含めるか、を表すパラメータがアルゴリズムにあるみたいな物だ。",
    "output": "So it's as if there's one extra parameter in this algorithm, which I'm going to denote d, which is, what degree of polynomial."
  },
  {
    "index": "F15892",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりまるで、パラメータのシータに追加してさらにもう一つ、パラメータdという物が追加されていて、それをデータセットを用いて決めたい、と考える。",
    "output": "So it's as if, in addition to the theta parameters, it's as if there's one more parameter, d, that you're trying to determine using your data set."
  },
  {
    "index": "F15893",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の選択肢はd=1でそれは線形関数となる。",
    "output": "So, the first option is d equals one, if you fit a linear function."
  },
  {
    "index": "F15894",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "d=2も、d=3も、、、とd=10までの選択肢がある。",
    "output": "We can choose d equals two, d equals three, all the way up to d equals 10."
  },
  {
    "index": "F15895",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの、ある意味で追加のパラメータについてフィッティングしたい、という訳だ。",
    "output": "So, we'd like to fit this extra sort of parameter which I'm denoting by d."
  },
  {
    "index": "F15896",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それをdで示していて、具体的には、モデルを選びたいと、これら10個のモデルから一つの多項式の次数を選びたいとする。",
    "output": "And concretely let's say that you want to choose a model, that is choose a degree of polynomial, choose one of these 10 models."
  },
  {
    "index": "F15897",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそのモデルをフィッティングしてそのフィッティングした仮説がどれくらいうまく、新しいサンプルに対して一般化されているかを評価する。",
    "output": "And fit that model and also get some estimate of how well your fitted hypothesis was generalize to new examples."
  },
  {
    "index": "F15898",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ、ありえる事としては、こんなのがある:最初のモデルを取ってトレーニング誤差を最小化し、それであるパラメータベクトルが得られる。",
    "output": "What you could, first take your first model and minimize the training error. And this would give you some parameter vector theta."
  },
  {
    "index": "F15899",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、二番目のモデル、二次関数をとってきて、トレーニングセットに対してーー別のパラメータベクトル、シータを得る事になる。",
    "output": "And you could then take your second model, the quadratic function, and fit that to your training set and this will give you some other. Parameter vector theta."
  },
  {
    "index": "F15900",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら別々のパラメータベクトルを区別する為に、下付き添字1、2、、、を使っていく。ここでシータの下付き添字1は、このモデルをトレーニングデータに対してフィッティングして得られたパラメータを意味するに過ぎない。",
    "output": "In order to distinguish between these different parameter vectors, I'm going to use a superscript one superscript two there where theta superscript one just means the parameters I get by fitting this model to my training data."
  },
  {
    "index": "F15901",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シータ下付き添字2は単にこの二次関数をトレーニングデータに対してフィッティングして得られたパラメータを表すだけ、などなど。",
    "output": "And theta superscript two just means the parameters I get by fitting this quadratic function to my training data and so on."
  },
  {
    "index": "F15902",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして三次のモデルをフィッティングする事で、パラメータ、シータ3を得て、これをシータ10まで続ける事が出来る。",
    "output": "By fitting a cubic model I get parenthesis three up to, well, say theta 10."
  },
  {
    "index": "F15903",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここまて来た後にやれる事としては、一つにはこれらのパラメータに対しテストセットの誤差を見ていく。",
    "output": "And one thing we ccould do is that take these parameters and look at test error."
  },
  {
    "index": "F15904",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、テストセットに対してJのtestのシータ1、Jのtestのシータ2、みたいに。",
    "output": "So I can compute on my test set J test of one, J test of theta two, and so on."
  },
  {
    "index": "F15905",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Jのtestのシータ3とか。",
    "output": "J test of theta three, and so on."
  },
  {
    "index": "F15906",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、対応する仮説を一つずつ見ていってテストセットに対するパフォーマンスをただ計測していくだけ。",
    "output": "So I'm going to take each of my hypotheses with the corresponding parameters and just measure the performance of on the test set."
  },
  {
    "index": "F15907",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこから出来る事としては、一つにはこれらのモデルを選ぶ為にどのモデルが一番低いテストセットの誤差となっているかを見る、というのがある。",
    "output": "Now, one thing I could do then is, in order to select one of these models, I could then see which model has the lowest test set error."
  },
  {
    "index": "F15908",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの例では、仮に5次の多項式を選ぶ事になったとしよう。",
    "output": "And let's just say for this example that I ended up choosing the fifth order polynomial."
  },
  {
    "index": "F15909",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまではリーズナブルっぽいね。",
    "output": "So, this seems reasonable so far."
  },
  {
    "index": "F15910",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがここで、フィットさせた仮説、この5次のモデルに対して、このモデルがどれだけうまく一般化されているか知りたいとする。",
    "output": "But now let's say I want to take my fifth hypothesis, this, this, fifth order model, and let's say I want to ask, how well does this model generalize?"
  },
  {
    "index": "F15911",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ考えられるのは、五次の多項式の仮説がどれだけテストセットに対してうまく機能するかを見る、という事だが、、、だがこれには問題がある。",
    "output": "One thing I could do is look at how well my fifth order polynomial hypothesis had done on my test set."
  },
  {
    "index": "F15912",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは我らの仮説がどれだけうまく一般化出来ているかを見積もるにはフェアなやり方じゃないって事だ。",
    "output": "But the problem is this will not be a fair estimate of how well my hypothesis generalizes."
  },
  {
    "index": "F15913",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その理由は、我らがやった事はそもそも、この追加のパラメータdをそれは多項式の次数だが、これをフィッティングたのだった。",
    "output": "And the reason is what we've done is we've fit this extra parameter d, that is this degree of polynomial."
  },
  {
    "index": "F15914",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのdは、テストセットを使ってフィッティングしたのだった。",
    "output": "And what fits that parameter d, using the test set, namely, we chose the value of d that gave us the best possible performance on the test set."
  },
  {
    "index": "F15915",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ようするに、我らはdの値をテストセットに対して最も高いパフォーマンスが出るように選んだのだった。",
    "output": "And so, the performance of my parameter vector theta5, on the test set, that's likely to be an overly optimistic estimate of generalization error."
  },
  {
    "index": "F15916",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、パラメータベクトルのシータ5のテストセットに対してのパフォーマンスはたぶん一般化の誤差を見積もるには過剰に楽観的になってしまうはず。でしょ?",
    "output": "Right, so, that because I had fit this parameter d to my test set is no longer fair to evaluate my hypothesis on this test set, because I fit my parameters to this test set, I've chose the degree d of polynomial using the test set."
  },
  {
    "index": "F15917",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だってパラメータdはテストセットに対してフィッティングしたんだから。",
    "output": "And specifically, what we did was, we fit this parameter d to the test set."
  },
  {
    "index": "F15918",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもはや、仮説をテストセットに対して用いるのはフェアじゃ無くなってしまっているよ。",
    "output": "And by having fit the parameter to the test set, this means that the performance of the hypothesis on that test set may not be a fair estimate of how well the hypothesis is, is likely to do on examples we haven't seen before."
  },
  {
    "index": "F15919",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だってパラメータのフィッティングにテストセットを使っちゃったんだから。",
    "output": "So when faced with a model selection problem like this, what we're going to do is, instead of using the test set to select the model, we're instead going to use the validation set, or the cross validation set, to select the model."
  },
  {
    "index": "F15920",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多項式の次数、dをテストセットを使って選んだんだった。",
    "output": "And finally, what this means is that that parameter d, remember d was the degree of polynomial, right?"
  },
  {
    "index": "F15921",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから我らの仮説はたぶんテストセットに対しての方がまだ見ぬ新しいサンプルに対してよりうまく振舞ってしまうだろう。",
    "output": "What we've done is we'll fit that parameter d and we'll say d equals four. And we did so using the cross-validation set."
  },
  {
    "index": "F15922",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその新しいサンプルに対してこそが知りたい事だ。",
    "output": "The machine learning, as of this practice today, there aren't many people that will do that early thing that I talked about, and said that, you know, it isn't such a good idea, of selecting your model using this test set."
  },
  {
    "index": "F15923",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前のスライドを繰り返すと、もしあるパラメータの集合、例えばシータ0とかシータ1とかを、なんらかのトレーニングセットに対してフィッティングしたら、そのトレーニングセットにフィットさせたモデルの(そのトレーニングセットに対する)パフォーマンスは新しいサンプルに対してどれだけうまく一般化出来た仮説となっているかを予測するのには使えない。",
    "output": "And then using the same test set to report the error as though selecting your degree of polynomial on the test set, and then reporting the error on the test set as though that were a good estimate of generalization error. That sort of practice is unfortunately many, many people do do it."
  },
  {
    "index": "F15924",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならこれらのパラメータは当然このトレーニングセットにはフィットするだろうから。",
    "output": "And it's considered better practice to have separate train validation and test sets."
  },
  {
    "index": "F15925",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれらはトレーニングセットに対しては良いだろうと思われる。",
    "output": "I just warned you to sometimes people to do, you know, use the same data for the purpose of the validation set, and for the purpose of the test set."
  },
  {
    "index": "F15926",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たとえそのパラメータが、他のサンプルには良くなくても。そしてこのスライドで記述した手順では、",
    "output": "You need a training set and a test set, and that's good, that's practice, though you will see some people do it."
  },
  {
    "index": "F15927",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし学習アルゴリズムを走らせて期待ほど良い結果で無ければ、だいたいいつもそれは高いバイアス問題か、高い分散問題のどちらかだ。言い換えると、それはアンダーフィット問題かオーバーフィット問題のどちらか、という事。",
    "output": "If you run a learning algorithm and it doesn't do as long as you are hoping, almost all the time, it will be because you have either a high bias problem or a high variance problem, in other words, either an underfitting problem or an overfitting problem."
  },
  {
    "index": "F15928",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの場合、これらの問題のどちらなのかを見分けるのは凄く重要。バイアスなのか分散なのか。",
    "output": "In this case, it's very important to figure out which of these two problems is bias or variance or a bit of both that you actually have."
  },
  {
    "index": "F15929",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故かといえば、これらのどちらが起きているのかを知ると、アルゴリズムを改善するのに将来のありそうな道がどちらかのとても強力な指標となるから。",
    "output": "Because knowing which of these two things is happening would give a very strong indicator for whether the useful and promising ways to try to improve your algorithm."
  },
  {
    "index": "F15930",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、バイアスと分散の問題についてより深くつっこんで行き、それらをより深く理解すると同時に我らの問題がバイアス問題なのか分散問題なのかを見分ける為に学習アルゴリズムをどう見ていくか、どう評価して診断していくかも探求していきたい。何故ならあなたの実装した学習アルゴリズムのパフォーマンスを改善する方法を知るのに必須だから。",
    "output": "In this video, I'd like to delve more deeply into this bias and variance issue and understand them better as was figure out how to look in a learning algorithm and evaluate or diagnose whether we might have a bias problem or a variance problem since this will be critical to figuring out how to improve the performance of a learning algorithm that you will implement."
  },
  {
    "index": "F15931",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あまりにも複雑過ぎる仮説にフィットさせるとそれはトレーニングセットに完璧に一致するかもしれないがデータにオーバーフィットするかもしれない。そしてこれは、中間の複雑さで、たとえば多項式の次数が高すぎもせず、低すぎもしない、という仮説の例。",
    "output": "So, you've already seen this figure a few times where if you fit two simple hypothesis like a straight line that underfits the data, if you fit a two complex hypothesis, then that might fit the training set perfectly but overfit the data and this may be hypothesis of some intermediate level of complexities of some maybe degree two polynomials or not too low and not too high degree that's like just right and gives you the best generalization error over these options."
  },
  {
    "index": "F15932",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今やトレーニング、バリデーション、テストセットという考え方で武装したので、バイアスと分散の概念をもうちょっと良く理解出来る。",
    "output": "Now that we're armed with the notion of chain training and validation in test sets, we can understand the concepts of bias and variance a little bit better."
  },
  {
    "index": "F15933",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的にいこう。トレーニング誤差とクロスバリデーション誤差を前回のビデオ同様に定義しよう。",
    "output": "Concretely, let's let our training error and cross validation error be defined as in the previous videos."
  },
  {
    "index": "F15934",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "誤差の自乗、トレーニングセットかクロスバリデーションセットに対して計測した誤差の自乗の平均の値。",
    "output": "Just say the squared error, the average squared error, as measured on the training sets or as measured on the cross validation set."
  },
  {
    "index": "F15935",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、以下の図をプロットしよう。",
    "output": "Now, let's plot the following figure."
  },
  {
    "index": "F15936",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "横軸には多項式の次数。",
    "output": "On the horizontal axis I'm going to plot the degree of polynomial."
  },
  {
    "index": "F15937",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり右に行くに連れてどんどん高い次元の多項式をフィッティングする事となる。",
    "output": "So, as I go to the right I'm going to be fitting higher and higher order polynomials."
  },
  {
    "index": "F15938",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの図の右側ではd=1の時など、とても簡単な関数でフィッティングする事となる。他方、横軸の右側ではより大きな数字となり、とても複雑な高い次元の多項式にフィッティングする事となる。",
    "output": "So where the left of this figure where maybe d equals one, we're going to be fitting very simple functions whereas we're here on the right of the horizontal axis, I have much larger values of ds, of a much higher degree polynomial."
  },
  {
    "index": "F15939",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりトレーニングセットとより複雑な関数でフィッティングする事となる。",
    "output": "So here, that's going to correspond to fitting much more complex functions to your training set."
  },
  {
    "index": "F15940",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではトレーニング誤差とクロスバリデーション誤差を見てみて、この図にプロットしてみよう。",
    "output": "Let's look at the training error and the cross validation error and plot them on this figure."
  },
  {
    "index": "F15941",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニング誤差から始めよう。",
    "output": "Let's start with the training error."
  },
  {
    "index": "F15942",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多項式の次元を上げるに連れてトレーニングセットへのフィッティングもますます良くなる。もしd=1なら高いトレーニング誤差となるし、もしとても高い次数の多項式ならば我らのトレーニング誤差は極めて低くなる。",
    "output": "As we increase the degree of the polynomial, we're going to be able to fit our training set better and better and so if d equals one, then there is high training error, if we have a very high degree of polynomial our training error is going to be really low, maybe even 0 because will fit the training set really well."
  },
  {
    "index": "F15943",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり多項式の次数を高めれば高める程、典型的にはトレーニングの誤差は減少していく。",
    "output": "So, as we increase the degree of polynomial, we find typically that the training error decreases."
  },
  {
    "index": "F15944",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからJ下付き添字trainのシータを書くとこんな感じ。何故ならトレーニング誤差はデータにフィットする多項式の次元の増加とともに減少していくから。",
    "output": "So I'm going to write J subscript train of theta there, because our training error tends to decrease with the degree of the polynomial that we fit to the data."
  },
  {
    "index": "F15945",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "仮にテストセットの誤差を見るとすると、そちらも似たような結果となるだろう。まるでクロスバリデーション誤差をプロットしたかのように。",
    "output": "Next, let's look at the cross-validation error or for that matter, if we look at the test set error, we'll get a pretty similar result as if we were to plot the cross validation error."
  },
  {
    "index": "F15946",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしd=1なら、とても簡単な関数にフィッティングする事になるのでトレーニングセットにアンダーフィットするかもしれない。",
    "output": "So, we know that if d equals one, we're fitting a very simple function and so we may be underfitting the training set and so it's going to be very high cross-validation error."
  },
  {
    "index": "F15947",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからクロスバリデーション誤差はとても高いはず。",
    "output": "If we fit an intermediate degree polynomial, we had d equals two in our example in the previous slide, we're going to have a much lower cross-validation error because we're finding a much better fit to the data."
  },
  {
    "index": "F15948",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし中間の次数の多項式をフィッティングさせたら前のスライドの例だとd=2のケースとなるが、この場合はより低いクロスバリデーション誤差となるだろう。何故なら我らはより良くデータにフィットするのを見つけたという事だから。",
    "output": "So if d took on say a value of four, then we're again overfitting, and so we end up with a high value for cross-validation error."
  },
  {
    "index": "F15949",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、逆にdが大きすぎる時は例えばdが4の時はふたたびオーバーフィットしてしまう。だから結果としては高いクロスバリデーション誤差となるだろう。",
    "output": "So, if you were to vary this smoothly and plot a curve, you might end up with a curve like that where that's JCV of theta."
  },
  {
    "index": "F15950",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれをなめらかにつなげて、曲線をプロットすると結局こんな曲線となる。",
    "output": "Again, if you plot J test of theta you get something very similar."
  },
  {
    "index": "F15951",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この種のプロットはまた、バイアスと分散という考え方をよりよく理解する助けともなる。",
    "output": "So, this sort of plot also helps us to better understand the notions of bias and variance."
  },
  {
    "index": "F15952",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうやって学習アルゴリズムが何の被害を、、、具体的に、学習アルゴリズムを適用したとする、そして期待通りにはパフォーマンスが出ていないとする。つまり、クロスバリデーションセット誤差とテストセット誤差が高い。",
    "output": "Concretely, suppose you have applied a learning algorithm and it's not performing as well as you are hoping, so if your cross-validation set error or your test set error is high, how can we figure out if the learning algorithm is suffering from high bias or suffering from high variance?"
  },
  {
    "index": "F15953",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クロスバリデーション誤差が高いという事なのでこのレジームかこのレジームに対応するはずだ。",
    "output": "So, the setting of a cross-validation error being high corresponds to either this regime or this regime."
  },
  {
    "index": "F15954",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この左のレジームは高バイアス問題に対応している、つまり、もし単純すぎる多項式、例えばd=1でフィッティングしていて、でも本当はもっと高い次数でフィッティングする必要があるようなデータの時。",
    "output": "So, this regime on the left corresponds to a high bias problem."
  },
  {
    "index": "F15955",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方、対照的に、このレジームは高分散の問題に対応する。",
    "output": "That is, if you are fitting a overly low order polynomial such as a d equals one when we really needed a higher order polynomial to fit to data, whereas in contrast this regime corresponds to a high variance problem."
  },
  {
    "index": "F15956",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこではd、つまり多項式の次数が我らのデータセットに対しては大きすぎるという事。そしてこの図が、これら2つのケースをどうやって見分けるか、に関する手がかりを与えてくれる。",
    "output": "That is, if d the degree of polynomial was too large for the data set that we have, and this figure gives us a clue for how to distinguish between these two cases."
  },
  {
    "index": "F15957",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に、高バイアスのケースではつまりアンダーフィットのケースでは、この図を見ると、クロスバリデーション誤差とトレーニング誤差の両方とも高くなっている事が分かる。",
    "output": "Concretely, for the high bias case, that is the case of underfitting, what we find is that both the cross validation error and the training error are going to be high."
  },
  {
    "index": "F15958",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしあなたのアルゴリズムがバイアスの問題を被っているなら、トレーニングセットの誤差も高くなるだろうし、またクロスバリデーションの誤差もまた高くなるはず。",
    "output": "So, if your algorithm is suffering from a bias problem, the training set error will be high and you might find that the cross validation error will also be high."
  },
  {
    "index": "F15959",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それら2つは、近い。トレーニング誤差よりちょっと高いだけかも。",
    "output": "It might be close, maybe just slightly higher, than the training error."
  },
  {
    "index": "F15960",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、この組み合わせを観測したらあなたのアルゴリズムが高バイアスの被害を被っているサインだ。",
    "output": "So, if you see this combination, that's a sign that your algorithm may be suffering from high bias."
  },
  {
    "index": "F15961",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "対照的にアルゴリズムが高分散の被害を被ってるならここを見るとJtrain、つまりトレーニング誤差が低くなっている事に気付くだろう。",
    "output": "In contrast, if your algorithm is suffering from high variance, then if you look here, we'll notice that J train, that is the training error, is going to be low."
  },
  {
    "index": "F15962",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、トレーニングセットにはとてもうまくフィッティング出来ている。一方、クロスバリデーション誤差はこれが最小化したい誤差の二乗だと仮定するとーーー他方で対照的に、クロスバリデーションセットに対する誤差はつまりクロスバリデーションセットに対するコスト関数はトレーニングセットの誤差よりも、ずっと大きくなるだろう。",
    "output": "That is, you're fitting the training set very well, whereas your cross validation error assuming that this is, say, the squared error which we're trying to minimize say, whereas in contrast your error on a cross validation set or your cross function or cross validation set will be much bigger than your training set error."
  },
  {
    "index": "F15963",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは「大なり大なり」の記号でそれは数学の記号でずっと大きい、を意味するもので2つの大なりの記号で示す。",
    "output": "So, this is a double greater than sign. That's the map symbol for much greater thans, denoted by two greater than signs."
  },
  {
    "index": "F15964",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしこの組み合わせを見たらそれはつまり、、、そしてつまりこの値の組み合わせ見たら、それは学習アルゴリズムが高分散を被っていて、オーバーフィットしてるという手がかりを得たという事。",
    "output": "So if you see this combination of values, then that's a clue that your learning algorithm may be suffering from high variance and might be overfitting."
  },
  {
    "index": "F15965",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれら2つのケースを分けるキーとなるのはもし高バイアスの問題ならトレーニングセットの誤差も高くなるはずで、何故なら仮説がトレーニングセットにうまくフィッティング出来ていないだけだから。",
    "output": "The key that distinguishes these two cases is, if you have a high bias problem, your training set error will also be high is your hypothesis just not fitting the training set well."
  },
  {
    "index": "F15966",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもし高分散の問題なら、トレーニングセットの誤差は通常は低く、クロスバリデーション誤差とくらべると大きく低いはず。",
    "output": "If you have a high variance problem, your training set error will usually be low, that is much lower than your cross-validation error."
  },
  {
    "index": "F15967",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、バイアスと分散の2つの問題が、いくらかでもより良く理解出来たら幸いです。",
    "output": "So hopefully that gives you a somewhat better understanding of the two problems of bias and variance."
  },
  {
    "index": "F15968",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次の幾つかのビデオで言います。のち程見るのは、学習アルゴリズムが高バイアスか高分散を被っているかを診断する事です。",
    "output": "I still have a lot more to say about bias and variance in the next few videos, but what we'll see later is that by diagnosing whether a learning algorithm may be suffering from high bias or high variance, I'll show you even more details on how to do that in later videos."
  },
  {
    "index": "F15969",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "学習アルゴリズムが被っているのが高バイアスか両方の組み合わせかを区別する事により、学習アルゴリズムを改善する為に何をやるのが成果が期待出来そうな道かを知る、より良いガイダンスを提供してくれる事を見ていきます。",
    "output": "But we'll see that by figuring out whether a learning algorithm may be suffering from high bias or high variance or combination of both, that that would give us much better guidance for what might be promising things to try in order to improve the performance of a learning algorithm."
  },
  {
    "index": "F15970",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "正規化がどうオーバーフィットを防止するかは、これまで見てきた。",
    "output": "You've seen how regularization can help prevent over-fitting."
  },
  {
    "index": "F15971",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもそれは、どんな風に学習アルゴリズムのバイアスと分散に影響を与えるか?",
    "output": "But how does it affect the bias and variances of a learning algorithm?"
  },
  {
    "index": "F15972",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、バイアスと分散の問題についてより深く見ていきたい。そして学習アルゴリズムの正規化とどう相互作用し、影響を与えるか議論していきたい。",
    "output": "In this video I'd like to go deeper into the issue of bias and variances and talk about how it interacts with and is affected by the regularization of your learning algorithm."
  },
  {
    "index": "F15973",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このような高次の多項式をフィッティングするが、そこでオーバーフィットを避けるためにここに示したような正規化を行うとする。",
    "output": "Suppose we're fitting a high auto polynomial, like that showed here, but to prevent over fitting we need to use regularization, like that shown here."
  },
  {
    "index": "F15974",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの正規化の項でパラメータを値を小さく保とうとするという訳。",
    "output": "So we have this regularization term to try to keep the values of the prem to small."
  },
  {
    "index": "F15975",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしていつも通り、正規化の和はjが1からmまで取り、jが0からmでは無い。",
    "output": "And as usual, the regularizations comes from J = 1 to m, rather than j = 0 to m."
  },
  {
    "index": "F15976",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では3つのケースを考えてみよう。",
    "output": "Let's consider three cases."
  },
  {
    "index": "F15977",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初のケースは正規化パラメータのラムダの値がとても大きな場合。例えばラムダ=10,000とか巨大な値の時。",
    "output": "The first is the case of the very large value of the regularization parameter lambda, such as if lambda were equal to 10,000."
  },
  {
    "index": "F15978",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合、これらのパラメータ全てシータ1、シータ2、シータ3などなど、が、とても重くペナルティを課される。",
    "output": "In this case, all of these parameters, theta 1, theta 2, theta 3, and so on would be heavily penalized and so we end up with most of these parameter values being closer to zero."
  },
  {
    "index": "F15979",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、これらのパラメータの値のほとんどがゼロになり、結果として、仮説はだいたいh(x)は単にだいたい近似としてはイコールシータ0となる。",
    "output": "And the hypothesis will be roughly h of x, just equal or approximately equal to theta zero."
  },
  {
    "index": "F15980",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから仮説は多かれ少なかれそんな感じに見える。これは多かれ少なかれフラットで、定数の直線。",
    "output": "So we end up with a hypothesis that more or less looks like that, more or less a flat, constant straight line."
  },
  {
    "index": "F15981",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから水平の直線は単にこれらのデータセットのとても良い仮説という訳では無い。",
    "output": "And so this hypothesis has high bias and it badly under fits this data set, so the horizontal straight line is just not a very good model for this data set."
  },
  {
    "index": "F15982",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "反対側の極端では、とても小さい値ラムダの時、たとえばラムダがイコール0の時。",
    "output": "At the other extreme is if we have a very small value of lambda, such as if lambda were equal to zero."
  },
  {
    "index": "F15983",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、高次の多項式でフィッティングしているなら、これは良くあるオーバーフィットの状況だ。",
    "output": "In that case, given that we're fitting a high order polynomial, this is a usual over-fitting setting."
  },
  {
    "index": "F15984",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、高次の多項式でフィッティングしていて、基本的には正規化無しかとても少ない正規化しかしないとすると、通常の高分散と、オーバーフィットの状況となる。",
    "output": "In that case, given that we're fitting a high-order polynomial, basically, without regularization or with very minimal regularization, we end up with our usual high-variance, over fitting setting."
  },
  {
    "index": "F15985",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならもしラムダがイコール0ならそれは単に正規化無しでフィッティングしているのだから、仮説はオーバーフィットする事となる。",
    "output": "This is basically if lambda is equal to zero, we're just fitting with our regularization, so that over fits the hypothesis."
  },
  {
    "index": "F15986",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしラムダの値を中間に大きすぎず、小さすぎずにパラメータを設定出来た時だけ、データに対してリーズナブルにフィッティング出来る。",
    "output": "And it's only if we have some intermediate value of longer that is neither too large nor too small that we end up with parameters data that give us a reasonable fit to this data."
  },
  {
    "index": "F15987",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、どうやったら正規化パラメータのラムダの良い値を自動的に選ぶ事が出来るだろうか?",
    "output": "So, how can we automatically choose a good value for the regularization parameter?"
  },
  {
    "index": "F15988",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに、我らのモデル、学習アルゴリズムの目的関数を再掲しておく。",
    "output": "Just to reiterate, here's our model, and here's our learning algorithm's objective."
  },
  {
    "index": "F15989",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "正規化を使っている状況として、Jtrainのシータを最適化の目的関数とは分けて定義しよう。正規化項を抜いて定義する。",
    "output": "For the setting where we're using regularization, let me define J train(theta) to be something different, to be the optimization objective, but without the regularization term."
  },
  {
    "index": "F15990",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以前は、前のビデオで正規化をしていない時はJtrainのシータをJのシータ、つまりコスト関数と同じ物として定義していた。だが正規化を用いるべく追加のラムダの項を足したら、Jtrain、つまりトレーニングセットの誤差をトレーニングセットの誤差の二乗の和だけで定義する、トレーニングセットの平均の誤差の二乗で正規化項を考慮に入れないで定義するのだ。",
    "output": "Previously, in an earlier video, when we were not using regularization I define J train of data to be the same as J of theta as the cause function but when we're using regularization when the six well under term we're going to define J train my training set to be just my sum of squared errors on the training set or my average squared error on the training set without taking into account that regularization."
  },
  {
    "index": "F15991",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして同様にクロスバリデーションセット誤差とテストセット誤差も以前と同様に定義する、つまりクロスバリデーションセットとテストセットの誤差の二乗の和の平均。まとめると、JtrainとJcvとJtestの定義は単なる誤差の二乗の平均、、、じゃなくてトレーニングセットとバリデーションセットとテストセットの誤差の二乗の平均で追加の正規化項が無しの物と定義する。",
    "output": "And similarly I'm then also going to define the cross validation sets error and to test that error as before to be the average sum of squared errors on the cross validation in the test sets so just to summarize my definitions of J train J CU and J test are just the average square there one half of the other square record on the training validation of the test set without the extra regularization term."
  },
  {
    "index": "F15992",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこんな風に正規化パラメータのラムダを自動で選ぶ。",
    "output": "So, this is how we can automatically choose the regularization parameter lambda."
  },
  {
    "index": "F15993",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "普通やるのは、試したいラムダの値の範囲をあらかじめ持っておく。",
    "output": "So what I usually do is maybe have some range of values of lambda I want to try out."
  },
  {
    "index": "F15994",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば正規化しない、というのもあるかもしれないし、幾つかの試してみたい値も入れてあるかもしれない。例えばラムダを0.01、0.02、0.04、、、などに渡って構築するかもしれない。",
    "output": "So I might be considering not using regularization or here are a few values I might try lambda considering lambda = 0.01, 0.02, 0.04, and so on."
  },
  {
    "index": "F15995",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして見ての通り、普通私は、これらのステップを2倍づつで進めて、ある程度の大きい数字までやる。これを二の倍数とすると、実際は10.24だ。",
    "output": "And I usually set these up in multiples of two, until some maybe larger value if I were to do these in multiples of 2 I'd end up with a 10.24."
  },
  {
    "index": "F15996",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ぴったり10じゃないけどだいたい同じだ。",
    "output": "It's 10 exactly, but this is close enough."
  },
  {
    "index": "F15997",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "桁数で三桁目とか四桁目の数は結果にはそんなに影響を与えない。",
    "output": "And the three to four decimal places won't effect your result that much."
  },
  {
    "index": "F15998",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは、えーと、12個の異なるモデルを与える。",
    "output": "So, this gives me maybe 12 different models."
  },
  {
    "index": "F15999",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私が試す事になるモデルの選択肢を、12個の異なる正規化パラメータに対応した。",
    "output": "And I'm trying to select a month corresponding to 12 different values of the regularization of the parameter lambda."
  }
]