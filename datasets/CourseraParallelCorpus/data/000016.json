[
  {
    "index": "F16000",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、0.01以下の値とか10より大きい値を試したっていい。でもここでは簡単のためにこんだけで切った。",
    "output": "And of course you can also go to values less than 0.01 or values larger than 10 but I've just truncated it here for convenience."
  },
  {
    "index": "F16001",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの12のモデルを所与とすると我らが行うのは以下の事だ:この最初のモデルラムダ=0を取り、コスト関数Jのシータを最小化する。",
    "output": "Given the issue of these 12 models, what we can do is then the following, we can take this first model with lambda equals zero and minimize my cost function J of data and this will give me some parameter of active data."
  },
  {
    "index": "F16002",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この結果、あるパラメータベクトルのシータが得られ前回のビデオと同様にこれをシータの上付き添字1で示す事にしよう。",
    "output": "And similar to the earlier video, let me just denote this as theta super script one."
  },
  {
    "index": "F16003",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、二番目のモデル、ラムダを0.01として取り、コスト関数を最小化する。",
    "output": "And then I can take my second model with lambda set to 0.01 and minimize my cost function now using lambda equals 0.01 of course."
  },
  {
    "index": "F16004",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今回は当然ラムダ=0.01を使ってさっきとは別のパラメータベクトル、シータを得る。",
    "output": "To get some different parameter vector theta."
  },
  {
    "index": "F16005",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次は三番目のモデルからシータ3を得て、それを以下同様に最後のモデルであるラムダ=10まで、または10.24までやり、結局このシータ12を得る。",
    "output": "And for that I end up with theta(3)."
  },
  {
    "index": "F16006",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "基本的にはこれらのパラメータ毎に、誤差の二乗の平均をクロスバリデーションセットに対して測る。",
    "output": "So if part for my third model. And so on until for my final model with lambda set to 10 or 10.24, I end up with this theta(12)."
  },
  {
    "index": "F16007",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれら12個のモデルから一番低いクロスバリデーションセット誤差が得られる物を選びだす。",
    "output": "Next, I can talk all of these hypotheses, all of these parameters and use my cross validation set to validate them so I can look at my first model, my second model, fit to these different values of the regularization parameter, and evaluate them with my cross validation set based in measure the average square error of each of these square vector parameters theta on my cross validation sets."
  },
  {
    "index": "F16008",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、ここで話を進める為に結果としてシータ5、つまり5次の多項式のモデルを選んだとしよう。",
    "output": "And I would then pick whichever one of these 12 models gives me the lowest error on the trans validation set."
  },
  {
    "index": "F16009",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならこれが一番クロスバリデーションの誤差が小さかったから、とする。",
    "output": "And let's say, for the sake of this example, that I end up picking theta 5, the 5th order polynomial, because that has the lowest cause validation error."
  },
  {
    "index": "F16010",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを終えて、最後にテストセットの誤差をレポートしたければ、やることは、パラメータのシータ5、これは私の選んだ物だが、それを取って、それがテストセットに対してとれくらい良いかを見る。",
    "output": "Having done that, finally what I would do if I wanted to report each test set error, is to take the parameter theta 5 that I've selected, and look at how well it does on my test set."
  },
  {
    "index": "F16011",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでも、このパラメータのシータはクロスバリデーションセットに対してフィッティングしたのだから、テストセットはそれとは別にとっておいて、初めて見る手本に対してどれくらいうまく一般化出来ているかを見積もるのをよりうまくやる為に使うのです。",
    "output": "So once again, here is as if we've fit this parameter, theta, to my cross-validation set, which is why I'm setting aside a separate test set that I'm going to use to get a better estimate of how well my parameter vector, theta, will generalize to previously unseen examples."
  },
  {
    "index": "F16012",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がモデル選択を正規化パラメータのラムダを選ぶのに適用した場合です。",
    "output": "So that's model selection applied to selecting the regularization parameter lambda."
  },
  {
    "index": "F16013",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオで最後にやりたい事は、正規化パラメータのラムダを変化させていくと、クロスバリデーションとトレーニングの誤差がどう変化していくかをより良く理解する事です。",
    "output": "The last thing I'd like to do in this video is get a better understanding of how cross validation and training error vary as we vary the regularization parameter lambda."
  },
  {
    "index": "F16014",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "備忘録として、これが元のコスト関数、Jのシータでした。",
    "output": "And so just a reminder right, that was our original cost on j of theta."
  },
  {
    "index": "F16015",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが今回の目的では、正規化パラメータ無しでトレーニング誤差とクロスバリデーション誤差を定義する、正規化パラメータ無しで。",
    "output": "But for this purpose we're going to define training error without using a regularization parameter, and cross validation error without using the regularization parameter."
  },
  {
    "index": "F16016",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "意図する所は、仮説がどれくらいトレーニングセットとクロスバリデーションセットに対して良いか、が、正規化パラメータのラムダを変化させていくとどう変わっていくかを見ていきたい。",
    "output": "And what I'd like to do is plot this Jtrain and plot this Jcv, meaning just how well does my hypothesis do on the training set and how does my hypothesis do when it cross validation sets. As I vary my regularization parameter lambda."
  },
  {
    "index": "F16017",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、以前に見たように、ラムダが小さい時は、あまり正規化をしない、という事なのでよりオーバーフィットの危険性にさらされる。他方、ラムダが大きくなると、つまり、この横軸の右側の部分だと、その時はより大きなラムダとなるので、バイアスの問題に遭遇するリスクが上がる。",
    "output": "So as we saw earlier if lambda is small then we're not using much regularization and we run a larger risk of over fitting whereas if lambda is large that is if we were on the right part of this horizontal axis then, with a large value of lambda, we run the higher risk of having a biased problem, so if you plot J train and J cv, what you find is that, for small values of lambda, you can fit the trading set relatively way cuz you're not regularizing."
  },
  {
    "index": "F16018",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから小さなラムダの値では正規化の項は基本的にはどっかに行ってしまうような物で、単に誤差の二乗を最小化しているのに、極めて近い事をしている。",
    "output": "So, for small values of lambda, the regularization term basically goes away, and you're just minimizing pretty much just gray arrows."
  },
  {
    "index": "F16019",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからラムダが小さい時は結果としてはJtrainは小さくなるが、他方ラムダが大きいと、高バイアス問題となりトレーニングセットにはうまくフィットしなくなる。だから値は結局、上昇する。",
    "output": "So when lambda is small, you end up with a small value for Jtrain, whereas if lambda is large, then you have a high bias problem, and you might not feel your training that well, so you end up the value up there."
  },
  {
    "index": "F16020",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、Jtrainのシータはラムダを増加させるとそれに連れて増加する傾向にある。というのは、ラムダの大きな値は高バイアスに対応していて、そこではトレーニングセットに対してすらうまくフィット出来ないだろう。",
    "output": "So Jtrain of theta will tend to increase when lambda increases, because a large value of lambda corresponds to high bias where you might not even fit your trainings that well, whereas a small value of lambda corresponds to, if you can really fit a very high degree polynomial to your data, let's say."
  },
  {
    "index": "F16021",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方で小さな値のラムダは考えてみれば分かるが自由にとても高い次数の多項式でデータにフィッティング出来る。",
    "output": "After the cost validation error we end up with a figure like this, where over here on the right, if we have a large value of lambda, we may end up under fitting, and so this is the bias regime."
  },
  {
    "index": "F16022",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クロスバリデーション誤差については、結局こんな図となる。",
    "output": "And so the cross validation error will be high."
  },
  {
    "index": "F16023",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "えーと、だから、クロスバリデーション誤差は高くなりちょっとラベルをつけておこう、これはJcvのシータで、高バイアスではフィッティングしない、、、クロスバリデーションセットに対しては良く無いだろう。他方、ここ、左の方では、これは高分散のレジームで、そこではラムダの値が小さすぎて、だからデータにオーバーフィットしている可能性がある。",
    "output": "Let me just leave all of that to this Jcv (theta) because so, with high bias, we won't be fitting, we won't be doing well in cross validation sets, whereas here on the left, this is the high variance regime, where we have two smaller value with longer, then we may be over fitting the data."
  },
  {
    "index": "F16024",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからデータにオーバーフィットしている事により、クロスバリデーション誤差も高くなるだろう。",
    "output": "And so by over fitting the data, then the cross validation error will also be high."
  },
  {
    "index": "F16025",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、これがクロスバリデーション誤差とトレーニング誤差がどんな見た目になるか、だ。パラメータのラムダを変更していった時に。",
    "output": "And so, this is what the cross validation error and what the trading error may look like on a trading stance as we vary the regularization parameter lambda."
  },
  {
    "index": "F16026",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "繰り返しておくと、しばしば、ある中間のラムダの値がいわゆる「ちょうどぴったし」のまたはクロスバリデーション誤差かテストセット誤差がどれだけ小さいかという観点で最良となる。",
    "output": "And so once again, it will often be some intermediate value of lambda that is just right or that works best In terms of having a small cross validation error or a small test theta."
  },
  {
    "index": "F16027",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところでここで描いた曲線はいくらか漫画的というか、理想化された物だ。だから実際のデータセットにおいては、得られるプロットの結果はもうちょっとごちゃごちやしていて、もっとノイジーかもしれない。",
    "output": "And whereas the curves I've drawn here are somewhat cartoonish and somewhat idealized so on the real data set the curves you get may end up looking a little bit more messy and just a little bit more noisy then this."
  },
  {
    "index": "F16028",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "データセットによっては、あまりトレンドらしき物が分からない事もある。全体やクロスバリデーション誤差のプロットを見る事で人力で、または自動でクロスバリデーション誤差を最小化する点を選び、そして低いクロスバリデーション誤差に対応するラムダの値を選ぶ。",
    "output": "For some data sets you will really see these for sorts of trends and by looking at a plot of the hold-out cross validation error you can either manual, automatically try to select a point that minimizes the cross validation error and select the value of lambda corresponding to low cross validation error."
  },
  {
    "index": "F16029",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "学習アルゴリズムの正規化パラメータラムダを選ぶ時はこのような図をプロットすることは何が起きているのか理解しやすくしてくれて、実際に正しい正規化パラメータの値を選んでいる、という事を確認しやすくしてくれる事が多い。",
    "output": "When I'm trying to pick the regularization parameter lambda for learning algorithm, often I find that plotting a figure like this one shown here helps me understand better what's going on and helps me verify that I am indeed picking a good value for the regularization parameter monitor."
  },
  {
    "index": "F16030",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、正規化とその学習アルゴリズムのバイアスや分散への影響に関する洞察を深めてくれてるといいな。",
    "output": "So hopefully that gives you more insight into regularization and it's effects on the bias and variance of a learning algorithm."
  },
  {
    "index": "F16031",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今や、バイアスと分散をたくさんの異なる視点から見てきた事になる。",
    "output": "By now you've seen bias and variance from a lot of different perspectives."
  },
  {
    "index": "F16032",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでやりたい事としては、ここまで見てきたたくさんの洞察を組み合わせてその上に学習曲線と言われる診断を作り上げたい、それは学習アルゴリズムがバイアス問題にあっているか、分散問題にあっているか、またはその両方かを診断する為に私が良く使うツールです。",
    "output": "And what we like to do in the next video is take all the insights we've gone through and build on them to put together a diagnostic that's called learning curves, which is a tool that I often use to diagnose if the learning algorithm may be suffering from a bias problem or a variance problem, or a little bit of both."
  },
  {
    "index": "F16033",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、学習曲線について議論したい。",
    "output": "In this video, I'd like to tell you about learning curves."
  },
  {
    "index": "F16034",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "学習曲線はプロットするととても便利な物で、アルゴリズムがちゃんと機能している、というサニティチェック(簡単な正当性チェック)をしたい時やアルゴリズムのパフォーマンスを改善したい時に役に立つ物だ。",
    "output": "If either you wanted to sanity check that your algorithm is working correctly, or if you want to improve the performance of the algorithm."
  },
  {
    "index": "F16035",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして学習曲線は実際私がバイアスやバリアンスやその混合の問題が起きてないかを診断したい時にしょっちゅう使っている物だ。",
    "output": "And learning curves is a tool that I actually use very often to try to diagnose if a physical learning algorithm may be suffering from bias, sort of variance problem or a bit of both."
  },
  {
    "index": "F16036",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その学習曲線とはこんな物だ。",
    "output": "Here's what a learning curve is."
  },
  {
    "index": "F16037",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "学習曲線をプロットするには普通私はJtrain、つまりトレーニングセットの二乗誤差の平均をプロットするかまたはJcv、つまりクロスバリデーションセットの平均の二乗誤差をプロットする。",
    "output": "To plot a learning curve, what I usually do is plot j train which is, say, average squared error on my training set or Jcv which is the average squared error on my cross validation set."
  },
  {
    "index": "F16038",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそのプロットはmの関数としてプロットする、つまりトレーニング手本の数の関数という事だ。",
    "output": "And I'm going to plot that as a function of m, that is as a function of the number of training examples I have."
  },
  {
    "index": "F16039",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "普通、mは定数で、例えば100トレーニング手本とかだ。だがここで私は、人工的にトレーニングセットのサイズを減らす、という事をやる訳だ。",
    "output": "And so m is usually a constant like maybe I just have, you know, a 100 training examples but what I'm going to do is artificially with use my training set exercise."
  },
  {
    "index": "F16040",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり自分で、自分自身に10とか20とか30とか40のトレーニング手本だけを使う、という制限を課す訳だ。そしてそれらのトレーニング誤差がどうなってるかそしてクロスバリデーションの誤差がどうなっているかをプロットする。",
    "output": "So, I deliberately limit myself to using only, say, 10 or 20 or 30 or 40 training examples and plot what the training error is and what the cross validation is for this smallest training set exercises."
  },
  {
    "index": "F16041",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではそのプロットがどんな感じか、見てみよう。",
    "output": "So let's see what these plots may look like."
  },
  {
    "index": "F16042",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニング手本が一つしか無いとしよう、こんな風に最初の手本だけ、そして二次関数をフィッティングしてるとしよう。",
    "output": "Suppose I have only one training example like that shown in this this first example here and let's say I'm fitting a quadratic function."
  },
  {
    "index": "F16043",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニング手本が一つだけなので完全にフィットさせる事が出来る。",
    "output": "Well, I have only one training example. I'm going to be able to fit it perfectly right?"
  },
  {
    "index": "F16044",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二次関数をフィットさせるだけで一つのトレーニング手本に対しては誤差0に出来る。",
    "output": "You know, just fit the quadratic function. I'm going to have 0 error on the one training example."
  },
  {
    "index": "F16045",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニング手本が2つの時は、、、この場合も二次関数はよくフィットさせられる。",
    "output": "If I have two training examples. Well the quadratic function can also fit that very well."
  },
  {
    "index": "F16046",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "正規化しててもたぶんかなり良くフィットさせられるだろう。",
    "output": "So, even if I am using regularization, I can probably fit this quite well."
  },
  {
    "index": "F16047",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもし正規化してなければこれに完璧にフィットさせられる。そしてトレーニング手本が3つの時も二次関数を完全にフィッティング出来る。",
    "output": "And if I am using no neural regularization, I'm going to fit this perfectly and if I have three training examples again."
  },
  {
    "index": "F16048",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、もしm=1かm=2かm=3ならこれらのトレーニングセットに対するトレーニング誤差は正規化してなければ0になると予想され、正規化してたら0よりちょっとだけ大きい値が予想される。",
    "output": "Yeah, I can fit a quadratic function perfectly so if m equals 1 or m equals 2 or m equals 3, my training error on my training set is going to be 0 assuming I'm not using regularization or it may slightly large in 0 if I'm using regularization and by the way if I have a large training set and I'm artificially restricting the size of my training set in order to J train."
  },
  {
    "index": "F16049",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、もしたくさんのトレーニングセットがあり、それをJtrain向けにトレーニングセットのサイズを制限したら、ここを、仮にm=3にしてみたら、そして3つの手本だけでトレーニングしてみたら、この図で3つの手本に対してだけの実際にフィッティングしてる対象に対してだけのトレーニング誤差を測る事になりだから100個のトレーニング手本がある訳だけど、m=3だけのトレーニング誤差をプロットしようという訳だ。",
    "output": "Here if I set M equals 3, say, and I train on only three examples, then, for this figure I am going to measure my training error only on the three examples that actually fit my data too and so even I have to say a 100 training examples but if I want to plot what my training error is the m equals 3."
  },
  {
    "index": "F16050",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、3つの手本だけに対して仮説をフィッティングしてそのトレーニング誤差を測るわけ。",
    "output": "What I'm going to do is to measure the training error on the three examples that I've actually fit to my hypothesis 2."
  },
  {
    "index": "F16051",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその他のトレーニング手本をわざと学習プロセスから抜くわけ。",
    "output": "And not all the other examples that I have deliberately omitted from the training process."
  },
  {
    "index": "F16052",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まとめると、ここまで見てきた事から、トレーニングセットのサイズが小さい時はトレーニング誤差も小さくなる。",
    "output": "So just to summarize what we've seen is that if the training set size is small then the training error is going to be small as well."
  },
  {
    "index": "F16053",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら、トレーニングセットが小さければとても簡単にフィッティング出来るから。トレーニングセットにとても良くフィットさせられる、時には完全に一致させられる事も。",
    "output": "Because you know, we have a small training set is going to be very easy to fit your training set very well may be even perfectly now say we have m equals 4 for example."
  },
  {
    "index": "F16054",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、ここでm=4となると、二次関数はもはや完全にはデータセットにフィットさせられなくなる。そしてm=5となると、、、まぁこの位ならそこそこフィットしたままかもしれん。",
    "output": "Well then a quadratic function can be a longer fit this data set perfectly and if I have m equals 5 then you know, maybe quadratic function will fit to stay there so so, then as my training set gets larger."
  },
  {
    "index": "F16055",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもそこからトレーニングセットを大きくしていくと全てのトレーニング手本の上を完全に通る二次関数を見つけるのはどんどん難しくなる。",
    "output": "It becomes harder and harder to ensure that I can find the quadratic function that process through all my examples perfectly."
  },
  {
    "index": "F16056",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこの図をプロットするとトレーニングセット誤差はそれは仮説の誤差の平均だが、それはmが増加するにつれて増加する。その直感的な理解を繰り返すと、mが小さい時は、トレーニング手本がとてもちょっとしか無い時には、トレーニング手本を一つ一つ完璧に通過するようにフィッティングするのはとても簡単だ。",
    "output": "So in fact as the training set size grows what you find is that my average training error actually increases and so if you plot this figure what you find is that the training set error that is the average error on your hypothesis grows as m grows and just to repeat when the intuition is that when m is small when you have very few training examples."
  },
  {
    "index": "F16057",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方、mが大きくなると全てのトレーニング手本を完璧に通るのはどんどん難しくなる。だからトレーニングセットの誤差はより大きくなる。",
    "output": "It's pretty easy to fit every single one of your training examples perfectly and so your error is going to be small whereas when m is larger then gets harder all the training examples perfectly and so your training set error becomes more larger now, how about the cross validation error."
  },
  {
    "index": "F16058",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クロスバリデーションの誤差とはこのクロスバリデーションセットでの誤差で、これはまだ見ていないデータだ。だからとても小さなトレーニングセットではしっかりと一般化は出来ない。",
    "output": "Well, the cross validation is my error on this cross validation set that I haven't seen and so, you know, when I have a very small training set, I'm not going to generalize well, just not going to do well on that."
  },
  {
    "index": "F16059",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの仮説はそんなにいい仮説じゃない。トレーニングセットをより大きくしていく事で初めて、、、初めていくらかデータにより良くフィットするような仮説が得られ始めるのだ。",
    "output": "So, right, this hypothesis here doesn't look like a good one, and it's only when I get a larger training set that, you know, I'm starting to get hypotheses that maybe fit the data somewhat better."
  },
  {
    "index": "F16060",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、クロスバリデーション誤差とテストセットの誤差は、トレーニングセットのサイズを大きくするにつれて、減少する傾向にある、なぜならデータが多くあればある程、新しいサンプルに対して一般化するのもうまく出来るだろうからだ。",
    "output": "So your cross validation error and your test set error will tend to decrease as your training set size increases because the more data you have, the better you do at generalizing to new examples."
  },
  {
    "index": "F16061",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ようするに、データを多く使えば使うほど、フィッティングで得られる仮説も良くなっていくはず。",
    "output": "So, just the more data you have, the better the hypothesis you fit."
  },
  {
    "index": "F16062",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからJtrainとJcvをプロットすると、こんな感じの物が得られるはず。",
    "output": "So if you plot j train, and Jcv this is the sort of thing that you get."
  },
  {
    "index": "F16063",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではここで、もし高バイアスだったり高バリアンスだったりといった問題を被った時に、どうなるか見てみよう。",
    "output": "Now let's look at what the learning curves may look like if we have either high bias or high variance problems."
  },
  {
    "index": "F16064",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "仮説が高バイアスだったとしよう、それを説明する為に、例として、あまり直線ではうまくフィット出来ないようなデータに対し直線をフィッティングさせる場合を考えてみよう。",
    "output": "Suppose your hypothesis has high bias and to explain this I'm going to use a, set an example, of fitting a straight line to data that, you know, can't really be fit well by a straight line."
  },
  {
    "index": "F16065",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると仮説は、こんな感じになる。",
    "output": "So we end up with a hypotheses that maybe looks like that."
  },
  {
    "index": "F16066",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでトレーニングセットのサイズを大きくしていくと、何が起こるか考えてみよう。",
    "output": "Now let's think what would happen if we were to increase the training set size."
  },
  {
    "index": "F16067",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりここに書いた5つだけの手本の代わりに、もっとたくさんのトレーニングの手本があると想像してみよう。",
    "output": "So if instead of five examples like what I've drawn there, imagine that we have a lot more training examples."
  },
  {
    "index": "F16068",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "うーん、これに直線をフィッティングしたら、どうなるだろう?",
    "output": "Well what happens, if you fit a straight line to this."
  },
  {
    "index": "F16069",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "結局の所得られる結果はほとんど同じような直線だろう。",
    "output": "What you find is that, you end up with you know, pretty much the same straight line."
  },
  {
    "index": "F16070",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、このデータにうまくフィットするのが不可能な直線はさらに大量のデータを増やしてもその直線はたいして変わらない、という事。",
    "output": "I mean a straight line that just cannot fit this data and getting a ton more data, well the straight line isn't going to change that much."
  },
  {
    "index": "F16071",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがこのデータにもっとも適合する直線だ。それでも、この直線はこのデータセットにそんなに良くフィットさせる事は出来ない。",
    "output": "This is the best possible straight-line fit to this data, but the straight line just can't fit this data set that well."
  },
  {
    "index": "F16072",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしクロスバリデーション誤差をプロットしたらこんな感じの結果となるだろう。",
    "output": "So, if you plot across validation error, this is what it will look like."
  },
  {
    "index": "F16073",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "グラフの左側では、トレーニングセットをすごく小さく、例えば1つのトレーニング手本に制限した場合で、そんなに良くは無いだろう。",
    "output": "Option on the left, if you have already a miniscule training set size like you know, maybe just one training example and is not going to do well."
  },
  {
    "index": "F16074",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがある程度の数のトレーニング手本の数に到達する頃には、ほとんど確実に、可能な範囲でベストな直線を得る事になる。そしてそこからさらにトレーニングセットのサイズを増やしたところで、つまりよりmの値を大きくした所で、基本的には同じ直線を得る事になる。",
    "output": "But by the time you have reached a certain number of training examples, you have almost fit the best possible straight line, and even if you end up with a much larger training set size, a much larger value of m, you know, you're basically getting the same straight line, and so, the cross-validation error - let me label that - or test set error or plateau out, or flatten out pretty soon, once you reached beyond a certain the number of training examples, unless you pretty much fit the best possible straight line."
  },
  {
    "index": "F16075",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではトレーニング誤差はどうだろう?",
    "output": "And how about training error?"
  },
  {
    "index": "F16076",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニング誤差は今回も小さい所から始まるが、高バイアスの場合、トレーニング誤差は最終的にはクロスバリデーション誤差に近くなる。",
    "output": "Well, the training error will again be small."
  },
  {
    "index": "F16077",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならあまりにもちょっとのパラメータしか無くデータはたくさんある、、、少なくともmが大きい所ではそうなので、トレーニングセットとクロスバリデーションセットの誤差は似たりよったりとなる。だからこんな感じの学習曲線が得られる事になる。",
    "output": "And what you find in the high bias case is that the training error will end up close to the cross validation error, because you have so few parameters and so much data, at least when m is large."
  },
  {
    "index": "F16078",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後になるが、高バイアスの問題はクロスバリデーション誤差とトレーニング誤差が両方とも高い、という形であらわれる。",
    "output": "The performance on the training set and the cross validation set will be very similar."
  },
  {
    "index": "F16079",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり最終的に比較的高いJcvとJtrainの値に落ち着く。これはまた、とても興味深い事を示唆している。",
    "output": "And so, this is what your learning curves will look like, if you have an algorithm that has high bias."
  },
  {
    "index": "F16080",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはもし学習アルゴリズムが高バイアスだと、もっとトレーニング手本を増やしていったとしても、つまりこの図の右側に移動していっても、クロスバリデーション誤差はたいして下がらない事が分かる。",
    "output": "And finally, the problem with high bias is reflected in the fact that both the cross validation error and the training error are high, and so you end up with a relatively high value of both Jcv and the j train."
  },
  {
    "index": "F16081",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは高い所でだいたい水平になってしまってる。だから学習アルゴリズムが実際に高バイアスの問題を被ってる時はトレーニングデータを増やす事それ自体はそんなには助けにならないだろう。",
    "output": "This also implies something very interesting, which is that, if a learning algorithm has high bias, as we get more and more training examples, that is, as we move to the right of this figure, we'll notice that the cross validation error isn't going down much, it's basically fattened up, and so if learning algorithms are really suffering from high bias."
  },
  {
    "index": "F16082",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この図によれば、この右側の図の例ではここでは5つのトレーニング手本しか無い。",
    "output": "Getting more training data by itself will actually not help that much,and as our figure example in the figure on the right, here we had only five training."
  },
  {
    "index": "F16083",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして何らかの直線をフィットさせてる。",
    "output": "examples, and we fill certain straight line."
  },
  {
    "index": "F16084",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそこに大量のトレーニングデータを追加しても、ほとんど同じ直線のまま。",
    "output": "And when we had a ton more training data, we still end up with roughly the same straight line."
  },
  {
    "index": "F16085",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから学習アルゴリズムが高バイアスな所にもっとたくさんのトレーニングデータを追加しても、テストセット誤差やクロスバリデーション誤差をたいして低下させない。",
    "output": "And so if the learning algorithm has high bias give me a lot more training data."
  },
  {
    "index": "F16086",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからあなたの学習アルゴリズムが高バイアスの問題を被っているかは知ると便利な事だと思う、何故ならそれを知る事で、役に立たない所でたくさんのトレーニングデータを集めてしまうという無駄を避ける事が出来るから。",
    "output": "That doesn't actually help you get a much lower cross validation error or test set error."
  },
  {
    "index": "F16087",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に学習アルゴリズムが高バリアンスの場合を見てみよう。とても小さなトレーニングセット、右の図だと5つしか無いような場合でとても高次の多項式、ここでは100次の多項式を描いた。",
    "output": "So knowing if your learning algorithm is suffering from high bias seems like a useful thing to know because this can prevent you from wasting a lot of time collecting more training data where it might just not end up being helpful."
  },
  {
    "index": "F16088",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは誰も使わないような物だが、例示の為に。この高次の多項式でフィッティングしたらトレーニング誤差がどうなるかを見てみよう。",
    "output": "Next let us look at the setting of a learning algorithm that may have high variance."
  },
  {
    "index": "F16089",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてとても小さなラムダの値、0では無いがとても小さな値を使えば、最終的にはこのデータにすこぶる良くフィットする事が出来て、ようするにそれはオーバーフィットする事になる。",
    "output": "Let us just look at the training error in a around if you have very smart training set like five training examples shown on the figure on the right and if we're fitting say a very high order polynomial, and I've written a hundredth degree polynomial which really no one uses, but just an illustration."
  },
  {
    "index": "F16090",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、トレーニングセットのサイズが小さい時にはトレーニング誤差、つまりJtrainのシータは小さくなるだろう。そしてトレーニングセットのサイズを少し増やしてもたぶんまだこのデータにオーバーフィットしたままだがでも多少はデータセットに完全にフィットさせるのは難しくなる。",
    "output": "And if we're using a fairly small value of lambda, maybe not zero, but a fairly small value of lambda, then we'll end up, you know, fitting this data very well that with a function that overfits this."
  },
  {
    "index": "F16091",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからトレーニングセットのサイズを増加させると、Jtrainも増えていくのが見られるだろう、何故ならトレーニングセットに完全にフィットさせるのはちょっと難しくなるだろうから、もっとトレーニング手本が増えると。",
    "output": "So, if the training set size is small, our training error, that is, j train of theta will be small."
  },
  {
    "index": "F16092",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも増えると言ってもトレーニングセット誤差はまだ極めて低いままだろう。",
    "output": "And as this training set size increases a bit, you know, we may still be overfitting this data a little bit but it also becomes slightly harder to fit this data set perfectly, and so, as the training set size increases, we'll find that j train increases, because it is just a little harder to fit the training set perfectly when we have more examples, but the training set error will still be pretty low."
  },
  {
    "index": "F16093",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、クロスバリデーション誤差はどうなるだろう?",
    "output": "Now, how about the cross validation error?"
  },
  {
    "index": "F16094",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "高バリアンスの設定では仮説はオーバーフィットしているのだからクロスバリデーション誤差は高いままに留まる、トレーニング手本の数をある程度増やしたとしても。だから、クロスバリデーション誤差はこんな感じとなる。",
    "output": "Well, in high variance setting, a hypothesis is overfitting and so the cross validation error will remain high, even as we get you know, a moderate number of training examples and, so maybe, the cross validation error may look like that."
  },
  {
    "index": "F16095",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "高バリアンスの問題が起こってる時に特徴的な診断ポイントとしてはトレーニング誤差とクロスバリデーション誤差との間にとても大きなギャップがある、というもの。",
    "output": "And the indicative diagnostic that we have a high variance problem, is the fact that there's this large gap between the training error and the cross validation error."
  },
  {
    "index": "F16096",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの図を見ると、もっとトレーニングデータを追加する事を考えるとこの図を外挿して右に伸ばすと、この2つのカーブは青いカーブとマゼンダのカーブは、同じ所に収束していくだろう事が分かる。",
    "output": "If we think about adding more training data, that is, taking this figure and extrapolating to the right, we can kind of tell that, you know the two curves, the blue curve and the magenta curve, are converging to each other."
  },
  {
    "index": "F16097",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、仮にこの図を右へと外挿し続けていくと、たぶん、トレーニング誤差は増加し続け、そしてクロスバリデーション誤差は減少し続ける。",
    "output": "And so, if we were to extrapolate this figure to the right, then it seems it likely that the training error will keep on going up and the cross-validation error would keep on going down."
  },
  {
    "index": "F16098",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして我らが本当に問題としてるのはクロスバリデーション誤差かテストセット誤差でしょ?",
    "output": "And the thing we really care about is the cross-validation error or the test set error, right?"
  },
  {
    "index": "F16099",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこの種の図では、トレーニング手本をさらに追加していく事で右に外挿してく事で、クロスバリデーション誤差は減少していく事が分かる。",
    "output": "So in this sort of figure, we can tell that if we keep on adding training examples and extrapolate to the right, well our cross validation error will keep on coming down."
  },
  {
    "index": "F16100",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから高バリアンスの状況では、トレーニングデータをさらに増やす事は実際に状況を改善する。",
    "output": "And, so, in the high variance setting, getting more training data is, indeed, likely to help."
  },
  {
    "index": "F16101",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこの場合もまた、学習アルゴリズムが高バリアンスの問題を被っているかを知るのは有益な事だと思う、何故ならそれを知る事で例えばもっと多くのデータを取りに行く価値があるかが分かるからだ。",
    "output": "And so again, this seems like a useful thing to know if your learning algorithm is suffering from a high variance problem, because that tells you, for example that it may be be worth your while to see if you can go and get some more training data."
  },
  {
    "index": "F16102",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前のスライドとこのスライドではかなりクリーンで理想化されたカーブを描いた。",
    "output": "Now, on the previous slide and this slide, I've drawn fairly clean fairly idealized curves."
  },
  {
    "index": "F16103",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実際の学習アルゴリズムに対してこれらの曲線をプロットしたら、時には私が描いたのにとても似たカーブを見る事もあると思うが、時にはもうちょっとノイズが入った、もっととっちらかったようなカーブを見る事もあるだろう。",
    "output": "If you plot these curves for an actual learning algorithm, sometimes you will actually see, you know, pretty much curves, like what I've drawn here."
  },
  {
    "index": "F16104",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこれらのような学習曲線をプロットする事はしばしばあなたの学習アルゴリズムがバイアスの問題を被ってるかバリアンスの問題を被ってるか、またはその両方がちょっとずつ混ざってるかを知る助けとなる。",
    "output": "Although, sometimes you see curves that are a little bit noisier and a little bit messier than this."
  },
  {
    "index": "F16105",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから学習アルゴリズムのパフォーマンスを改善しようと試みる時に、ほぼ必ずやる事としては、これらの学習曲線をプロットする、というのがある。",
    "output": "But plotting learning curves like these can often tell you, can often help you figure out if your learning algorithm is suffering from bias, or variance or even a little bit of both."
  },
  {
    "index": "F16106",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてだいたいは、バイアスかバリアンスの問題があるかについて、より良い感覚を与えてくれる。",
    "output": "So when I'm trying to improve the performance of a learning algorithm, one thing that I'll almost always do is plot these learning curves, and usually this will give you a better sense of whether there is a bias or variance problem."
  },
  {
    "index": "F16107",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、これがどのように、次に取るべきアクション、または取るべきでないアクションを考える助けとなるかを見ていく事にする。学習アルゴリズムのパフォーマンスを改善しようとする時に。",
    "output": "And in the next video we'll see how this can help suggest specific actions is to take, or to not take, in order to try to improve the performance of your learning algorithm."
  },
  {
    "index": "F16108",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまで、学習アルゴリズムをどう評価するか、モデルの選択、バイアスとバリアンスについて議論してきた。",
    "output": "We've talked about how to evaluate learning algorithms, talked about model selection, talked a lot about bias and variance."
  },
  {
    "index": "F16109",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこれらがどのように学習アルゴリズムのパフォーマンスを改善しようとする時に何が潜在的に実りが多そうで何はそんなに良く無さそうかを見分けるのに役立たせる事が出来るだろうか?",
    "output": "So how does this help us figure out what are potentially fruitful, potentially not fruitful things to try to do to improve the performance of a learning algorithm."
  },
  {
    "index": "F16110",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もとの動機づけ目的の例に戻り、その結果を見てみよう。",
    "output": "Let's go back to our original motivating example and go for the result."
  },
  {
    "index": "F16111",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、これは前に見た例で正規化した線形回帰をフィッティングして期待通りには振る舞ってない、と判明した物だ。",
    "output": "So here is our earlier example of maybe having fit regularized linear regression and finding that it doesn't work as well as we're hoping."
  },
  {
    "index": "F16112",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこんな選択肢のメニューがあると言った。",
    "output": "We said that we had this menu of options."
  },
  {
    "index": "F16113",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこの選択肢のうちどれが実り多そうな選択肢かを見分けるは方法が無いものか?",
    "output": "So is there some way to figure out which of these might be fruitful options?"
  },
  {
    "index": "F16114",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのうちの最初の物はもっと多くのトレーニング手本を得る、という物だった。",
    "output": "The first thing all of this was getting more training examples."
  },
  {
    "index": "F16115",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは高バリアンスを直すのに良い物だった。",
    "output": "What this is good for, is this helps to fix high variance."
  },
  {
    "index": "F16116",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして例えば、代わりに高バイアスの問題にかかっていて、そしてバリアンスの問題は無ければ、その場合は前のビデオで見たようにトレーニング手本の数を増やす事はそんなに役には立たないと思われる。",
    "output": "And concretely, if you instead have a high bias problem and don't have any variance problem, then we saw in the previous video that getting more training examples, while maybe just isn't going to help much at all."
  },
  {
    "index": "F16117",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この最初の選択肢が有用なのは学習曲線をプロットしてみて最低でもちょっとはバリアンスがある、と確認出来た場合のとにだけだ。つまり、クロスバリデーション誤差はトレーニングセット誤差よりもかなり大きい時という事。",
    "output": "So the first option is useful only if you, say, plot the learning curves and figure out that you have at least a bit of a variance, meaning that the cross-validation error is, you know, quite a bit bigger than your training set error."
  },
  {
    "index": "F16118",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フィーチャーの数を減らして試す、という件についてはどうだろう?",
    "output": "How about trying a smaller set of features?"
  },
  {
    "index": "F16119",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フィーチャーの数を減らして試す、というのはこれまた高バリアンスを治す為の物だ。",
    "output": "Well, trying a smaller set of features, that's again something that fixes high variance."
  },
  {
    "index": "F16120",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、学習曲線なりそれ以外の何かしらなりで高バイアスの問題だ、と分かったなら、おお、どうかより少ない数のフィーチャーにする為に何を残すか、を慎重に選びだす事に時間を使わないでください。",
    "output": "And in other words, if you figure out, by looking at learning curves or something else that you used, that have a high bias problem; then for goodness sakes, don't waste your time trying to carefully select out a smaller set of features to use."
  },
  {
    "index": "F16121",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならもし高バイアスの問題にかかっているならより少ない数のフィーチャーを使うというのは、きっと役には立たないから。",
    "output": "Because if you have a high bias problem, using fewer features is not going to help."
  },
  {
    "index": "F16122",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方で、学習曲線なりなんなりを見て高バリアンスの問題を見つけた時には、まさにフィーチャーを減らす事を試してみるべきだ。それはきっとあなたの時間の、とても有効な使い方に違いない。",
    "output": "Whereas in contrast, if you look at the learning curves or something else you figure out that you have a high variance problem, then, indeed trying to select out a smaller set of features, that might indeed be a very good use of your time."
  },
  {
    "index": "F16123",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フィーチャーを追加する、という選択肢はどうだろうか?フィーチャーを追加する、というのは必ずという訳では無いにせよ普通は高バイアスの問題の修正方法だとみなしている。",
    "output": "How about trying to get additional features, adding features, usually, not always, but usually we think of this as a solution for fixing high bias problems."
  },
  {
    "index": "F16124",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりフィーチャーを追加する時は普通は現在の仮説があまりにもシンプル過ぎる為、さらなるフィーチャーを追加する事で仮説がトレーニングセットにもっと良くフィットするようにしようとしたいからだ。",
    "output": "So if you are adding extra features it's usually because your current hypothesis is too simple, and so we want to try to get additional features to make our hypothesis better able to fit the training set."
  },
  {
    "index": "F16125",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に、多項式のフィーチャーを追加するのも、これはフィーチャーを追加するもう一つの方法で、つまり高バイアスの問題を修正するもう一つの方法がある訳だ。",
    "output": "And similarly, adding polynomial features; this is another way of adding features and so there is another way to try to fix the high bias problem."
  },
  {
    "index": "F16126",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして例えば、学習曲線が高バリアンスの問題を示していたら、その時はこの場合もあなたの時間の有効な使い方とはならないだろう。",
    "output": "And, if concretely if your learning curves show you that you still have a high variance problem, then, you know, again this is maybe a less good use of your time."
  },
  {
    "index": "F16127",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後にラムダを増減させる。",
    "output": "And finally, decreasing and increasing lambda."
  },
  {
    "index": "F16128",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは手早く、簡単に試す事が出来る。これらはあなたの人生の何ヶ月とかを無駄にする可能性は、まぁ無いだろう。",
    "output": "This are quick and easy to try, I guess these are less likely to be a waste of, you know, many months of your life."
  },
  {
    "index": "F16129",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがラムダを現象させるのは高バイアスを修正するという事を、既に知っている。",
    "output": "But decreasing lambda, you already know fixes high bias."
  },
  {
    "index": "F16130",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこれが当然、と感じられなければ、ひとまずビデオを止めて、ラムダを減らすのが高バイアスの問題を修正するのに有効で、ラムダを増加させるのか高バリアンスの問題を修正する事をしっかり納得出来るまでよく考えてみて欲しい。",
    "output": "In case this isn't clear to you, you know, I do encourage you to pause the video and think through this that convince yourself that decreasing lambda helps fix high bias, whereas increasing lambda fixes high variance."
  },
  {
    "index": "F16131",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうしてそうなるのかいまいち自信が持てなければ、ビデオを止めてそうだと納得出来るまで確認して欲しい。",
    "output": "And if you aren't sure why this is the case, do pause the video and make sure you can convince yourself that this is the case."
  },
  {
    "index": "F16132",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または前回のビデオの最後でプロットしたカーブを見てそしてこれらがそうだと言う事をしっかりと納得してくれ。",
    "output": "Or take a look at the curves that we were plotting at the end of the previous video and try to make sure you understand why these are the case."
  },
  {
    "index": "F16133",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、学んだ事全てをあわせて、それをニューラルネットワークに関連づけると、ニューラルネットワークの接続のパターンを、普段どうやって選んでいるかの実践的なアドバイスをしておく。",
    "output": "Finally, let us take everything we have learned and relate it back to neural networks and so, here is some practical advice for how I usually choose the architecture or the connectivity pattern of the neural networks I use."
  },
  {
    "index": "F16134",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ニューラルネットワークをフィッティングする時は考えられる一つの選択肢としてはとても小さなニューラルネット、相対的に少しの隠れユニットしか無いような、例えば一つの隠れユニットだけのような物にフィッティングする、というのが考えられる、、、ニューラルネットワークをフィッティングする時には考えられる一つの選択肢としては相対的に小さめのニューラルネットワーク、相対的に少しだけの、たとえば隠れレイヤが一つだけの物で隠れユニットの数も相対的に少しだけのネットワークにフィッティングするというのが考えられる。",
    "output": "If you're fitting a neural network, one option would be to fit a relatively small neural network with, say, relatively few, maybe only one hidden layer and maybe only a relatively few number of hidden units."
  },
  {
    "index": "F16135",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、このようなネットワークは相対的にはより少しのパラメータしか持たず、アンダーフィッティングしがちだ。",
    "output": "So, a network like this might have relatively few parameters and be more prone to underfitting."
  },
  {
    "index": "F16136",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの小さなネットワークの主な利点は計算量がより安上がり、という所。",
    "output": "The main advantage of these small neural networks is that the computation will be cheaper."
  },
  {
    "index": "F16137",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これに対して代替的な方法としては相対的により大きなニューラルネットワークを用いる事で、もっと多くの隠れユニットなり、この場合は一層内にたくさんの隠れユニットがあるとか。またはもっと多くの隠れレイヤがあるような物も考えられる。",
    "output": "An alternative would be to fit a, maybe relatively large neural network with either more hidden units--there's a lot of hidden in one there--or with more hidden layers."
  },
  {
    "index": "F16138",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとこれらのネットワークはより多くのパラメータを持つ傾向にあり、ゆえによりオーバーフィッティングしやすい。",
    "output": "And so these neural networks tend to have more parameters and therefore be more prone to overfitting."
  },
  {
    "index": "F16139",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つの欠点としてはしばしばそれほど大きな問題とはならないがたまに考える必要がある事としては、ネットワークに多くのニューロンがあると、より計算量的に高くつく場合がある、という事。",
    "output": "One disadvantage, often not a major one but something to think about, is that if you have a large number of neurons in your network, then it can be more computationally expensive."
  },
  {
    "index": "F16140",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが様々な理由から、これはたいていの場合は大きな問題とはならない。",
    "output": "Although within reason, this is often hopefully not a huge problem."
  },
  {
    "index": "F16141",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらより大きなニューラルネットワークの主要な問題点たりえるのは、よりオーバーフィッティングしやすい、という物で、ニューラルネットワークを用いてみると、ネットワークは大きければ大きいほど良い事が分かる。だがもしオーバーフィットしてしまったら正規化を用いてオーバーフィッティングの問題に対処出来る、通常は大きめのネットワークに正規化を適用してオーバーフィットに対処する方が小さめのニューラルネットワークを使うよりも効果的な事が多い。",
    "output": "The main potential problem of these much larger neural networks is that it could be more prone to overfitting and it turns out if you're applying neural network very often using a large neural network often it's actually the larger, the better but if it's overfitting, you can then use regularization to address overfitting, usually using a larger neural network by using regularization to address is overfitting that's often more effective than using a smaller neural network."
  },
  {
    "index": "F16142",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして主要な想定される短所としてはそちらの方が計算量的により高価になりえる、という所。",
    "output": "And the main possible disadvantage is that it can be more computationally expensive."
  },
  {
    "index": "F16143",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、決断すべき別の問題として、隠れレイヤの数を幾つにすべきか、というのがある。",
    "output": "And finally, one of the other decisions is, say, the number of hidden layers you want to have, right?"
  },
  {
    "index": "F16144",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり隠れレイヤは一つにすべきか、あるいはここに示したように3つの隠れレイヤにすべきか?",
    "output": "So, do you want one hidden layer or do you want three hidden layers, as we've shown here, or do you want two hidden layers?"
  },
  {
    "index": "F16145",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たが隠れレイヤの数を選ぼうという時に、もう一つ試せる事としては、トレーニングセット、クロスバリデーションセット、そしてテストセットに分けて、そして隠れレイヤ一つ、または隠れレイヤ二つ、または隠れレイヤ三つでニューラルネットワークをトレーニングしてみて、それらのネットワークのどれがクロスバリデーションセットで最も良いパフォーマンスを出すかを見てみる。",
    "output": "And usually, as I think I said in the previous video, using a single hidden layer is a reasonable default, but if you want to choose the number of hidden layers, one other thing you can try is find yourself a training cross-validation, and test set split and try training neural networks with one hidden layer or two hidden layers or three hidden layers and see which of those neural networks performs best on the cross-validation sets."
  },
  {
    "index": "F16146",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "隠れレイヤを一つ、二つ、三つのニューラルネットワークに対してクロスバリデーション誤差のJcvをそれら全てに対して計算してみて、そしてそれを用いてこれらのうちのどのニューラルネットワークを用いるのがベストかを選ぶのに使う事が出来る。",
    "output": "You take your three neural networks with one, two and three hidden layers, and compute the cross validation error at Jcv and all of them and use that to select which of these is you think the best neural network."
  },
  {
    "index": "F16147",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、バイアスとバリアンス、そして学習曲線などを用いてこれらの問題を診断する方法だ。",
    "output": "So, that's it for bias and variance and ways like learning curves, who tried to diagnose these problems."
  },
  {
    "index": "F16148",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを用いて、学習アルゴリズムのパフォーマンスを改善するのに何は実り多そうで何はあまり実りが無さそうかを判断出来る。",
    "output": "As far as what you think is implied, for one might be truthful or not truthful things to try to improve the performance of a learning algorithm."
  },
  {
    "index": "F16149",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたがもしここ数回のビデオの内容を理解出来ていたら、そしてそれを実践出来るのなら、あなたはすでに、、、学習アルゴリズムを問題に対して適用出来、しかもそれを効率的に行う事が出来る、しかもそれを、ここシリコンバレーでこんにちフルタイムの仕事としてやっているエンジニアのかなりの割合、ひょっとしたら大多数の実践家よりもうまく、だ。",
    "output": "If you understood the contents of the last few videos and if you apply them you actually be much more effective already and getting learning algorithms to work on problems and even a large fraction, maybe the majority of practitioners of machine learning here in Silicon Valley today doing these things as their full-time jobs."
  },
  {
    "index": "F16150",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、これらの診断の時の経験に対するアドバイスが、学習アルゴリズムを効率的に適用し、とてもうまく機能出来るようになる、助けとなるといいな。",
    "output": "So I hope that these pieces of advice on by experience in diagnostics will help you to much effectively and powerfully apply learning and get them to work very well."
  },
  {
    "index": "F16151",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "続くいくつかのビデオで機械学習システムのデザインについて話す。",
    "output": "In the next few videos I'd like to talk about machine learning system design."
  },
  {
    "index": "F16152",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのビデオは複雑な機械学習のシステムをあなたがデザインする時に直面するであろう問題に触れる、そしてどうやって複雑な機械学習のシステムを組み合わせるかについての戦略を提供する。",
    "output": "These videos will touch on the main issues that you may face when designing a complex machine learning system, and will actually try to give advice on how to strategize putting together a complex machine learning system."
  },
  {
    "index": "F16153",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この後に続く一連のビデオがいまいちまとまりが無く感じられたとしたら、それはこれらのビデオが複雑な機械学習のシステムをデザインする時に遭遇するであろう様々な問題に触れていくからだ。",
    "output": "In case this next set of videos seems a little disjointed that's because these videos will touch on a range of the different issues that you may come across when designing complex learning systems."
  },
  {
    "index": "F16154",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの後の一連のビデオはまた、いくらか、数学的で無い、と感じるかもしれないが、それでもこれらの題材はとても役に立つ、と分かる日が来るだろうと思う。そしてあなたが大きな機械学習のシステムを構築する時には巨大な時間の節約になってくれると思う。",
    "output": "And even though the next set of videos may seem somewhat less mathematical, I think that this material may turn out to be very useful, and potentially huge time savers when you're building big machine learning systems."
  },
  {
    "index": "F16155",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に言うと、次に何をするのに時間を費やすべきかのプライオリティ付けする、という問題について議論したい。スパム分類の例から始めよう。",
    "output": "Concretely, I'd like to begin with the issue of prioritizing how to spend your time on what to work on, and I'll begin with an example on spam classification."
  },
  {
    "index": "F16156",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スパム分類器を構築したいとする。",
    "output": "Let's say you want to build a spam classifier."
  },
  {
    "index": "F16157",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに、明らかにスパムのメールと明らかにスパムで無いメールが幾つかある。",
    "output": "Here are a couple of examples of obvious spam and non-spam emails."
  },
  {
    "index": "F16158",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "左側のは、物を売ろうとするメールだ。",
    "output": "if the one on the left tried to sell things."
  },
  {
    "index": "F16159",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スパマーはどう単語をミススペルしているか気づくだろう。たとえばMedicineの中には数字の1が混じってたり、M0rgagesとかだったり。",
    "output": "And notice how spammers will deliberately misspell words, like Vincent with a 1 there, and mortgages."
  },
  {
    "index": "F16160",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして右側は明らかにスパムで無い例。これは実際の、私の弟からのメールだ。",
    "output": "And on the right as maybe an obvious example of non-stamp email, actually email from my younger brother."
  },
  {
    "index": "F16161",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは何通かのスパムのメールとスパムでないメールからなるトレーニングセットがあり、それをラベルでy=1か0で示してあるとする。スパムと非スパムを区別する分類器を教師あり学習を用いてどうやって構築出来るだろうか?",
    "output": "Let's say we have a labeled training set of some number of spam emails and some non-spam emails denoted with labels y equals 1 or 0, how do we build a classifier using supervised learning to distinguish between spam and non-spam?"
  },
  {
    "index": "F16162",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "教師あり学習を適用する為には最初に決めなくてはならない事はxをどう表現するか、これはe-mailのフィーチャーだが、それを決める事だ。",
    "output": "In order to apply supervised learning, the first decision we must make is how do we want to represent x, that is the features of the email."
  },
  {
    "index": "F16163",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フィーチャーxとラベルyがトレーニングセットとして与えられたとすると、我らは分類器を、例えばロジスティック回帰を使うなどして、訓練する事が出来る。",
    "output": "Given the features x and the labels y in our training set, we can then train a classifier, for example using logistic regression."
  },
  {
    "index": "F16164",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは、我らのe-mailについてのフィーチャーを選ぶ一つの方法だ。",
    "output": "Here's one way to choose a set of features for our emails."
  },
  {
    "index": "F16165",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば我らはe-mailがスパムか非スパムかを判断出来ると思われる、100個の特徴的な単語を選び出せたとする。",
    "output": "We could come up with, say, a list of maybe a hundred words that we think are indicative of whether e-mail is spam or non-spam, for example, if a piece of e-mail contains the word 'deal' maybe it's more likely to be spam if it contains the word 'buy' maybe more likely to be spam, a word like 'discount' is more likely to be spam, whereas if a piece of email contains my name, Andrew, maybe that means the person actually knows who I am and that might mean it's less likely to be spam."
  },
  {
    "index": "F16166",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてある理由により、私はnowという単語が入っている物も非スパムを示すと考える。何故なら私はたくさんの緊急のe-mailを受け取るからだ。",
    "output": "And maybe for some reason I think the word \"now\" may be indicative of non-spam because I get a lot of urgent emails, and so on, and maybe we choose a hundred words or so."
  },
  {
    "index": "F16167",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "e-mailの一部分を与えられた時に、このe-mailの一部分を以下のようにフィーチャーベクトルへとエンコードする。",
    "output": "Given a piece of email, we can then take this piece of email and encode it into a feature vector as follows."
  },
  {
    "index": "F16168",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "100個の単語のリストに対し、それをアルファベット順に並べる。単語のリストはソートされてないかもしれないから。",
    "output": "I'm going to take my list of a hundred words and sort them in alphabetical order say."
  },
  {
    "index": "F16169",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "discountとかそのまま下まで行ってnowとか、などなど。そして右に示したようなe-mailの一部分が与えられた時に、これらの単語がそれぞれe-mailに現れるかをチェックしていき、そしてフィーチャーベクトルxを以下のように定義する:この右側のメールの一部分において、私の名前は現れてない、だからそこに0を入れる。",
    "output": "But, you know, here's a, here's my list of words, just count and so on, until eventually I'll get down to now, and so on and given a piece of e-mail like that shown on the right, I'm going to check and see whether or not each of these words appears in the e-mail and then I'm going to define a feature vector x where in this piece of an email on the right, my name doesn't appear so I'm gonna put a zero there."
  },
  {
    "index": "F16170",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうやって1か0を入れていく。Buyが二回現れても1を入れる。",
    "output": "The word \"by\" does appear, so I'm gonna put a one there and I'm just gonna put one's or zeroes."
  },
  {
    "index": "F16171",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単語が何回出てくるか、はカウントしない。",
    "output": "I'm not gonna recount how many times the word occurs."
  },
  {
    "index": "F16172",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単語dealは現れる、だから1を入れる。",
    "output": "The word \"due\" appears, I put a one there."
  },
  {
    "index": "F16173",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "discountは現れない。少なくともこのメールの範囲には。",
    "output": "The word \"discount\" doesn't appear, at least not in this this little short email, and so on."
  },
  {
    "index": "F16174",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "という風に続けていく。単語nowは現れる、などなど。",
    "output": "The word \"now\" does appear and so on."
  },
  {
    "index": "F16175",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、私はフィーチャーベクトルに0か1を、特定の単語が現れるかどうかに応じて入れていく。",
    "output": "So I put ones and zeroes in this feature vector depending on whether or not a particular word appears."
  },
  {
    "index": "F16176",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私が100を選んだからだが。もし100語の単語をこの表現に使う事としたとして、そして各フィーチャーxjは基本的には、ある単語、それを単語jと呼ぼう、それがe-mailにあったら1をとり、それ以外ではxjは0となる。",
    "output": "And in this example my feature vector would have to mention one hundred, if I have a hundred, if if I chose a hundred words to use for this representation and each of my features Xj will basically be 1 if you have a particular word that, we'll call this word j, appears in the email and Xj would be zero otherwise."
  },
  {
    "index": "F16177",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "オーケー。",
    "output": "Okay."
  },
  {
    "index": "F16178",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こうしてe-mail片のフィーチャーによる表現が得られる。",
    "output": "So that gives me a feature representation of a piece of email."
  },
  {
    "index": "F16179",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、このプロセスを手動で100個の単語を選ぶ、と記述したが、実際にはもっとも一般的な方法は、トレーニングセットを取り、それを見ていってトレーニングセットの中にもっとも頻繁に現れるn個の単語を選び出す、ここでnは通常は1万から5万程度の値で、それらをフィーチャーとして使う。",
    "output": "By the way, even though I've described this process as manually picking a hundred words, in practice what's most commonly done is to look through a training set, and in the training set depict the most frequently occurring n words where n is usually between ten thousand and fifty thousand, and use those as your features."
  },
  {
    "index": "F16180",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、手動で100個の単語を選ぶ代わりに、トレーニング手本を見ていって、もっとも良く見かける単語の上位1万から5万語くらいを選び、それらから構成されるフィーチャーをスパム分類器に対してe-mailを表現するフィーチャーとして用いる。",
    "output": "So rather than manually picking a hundred words, here you look through the training examples and pick the most frequently occurring words like ten thousand to fifty thousand words, and those form the features that you are going to use to represent your email for spam classification."
  },
  {
    "index": "F16181",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、もしあなたがスパム分類器を構築しているとすると、直面するであろう一つの問いとして、スパム分類器として高い正確性と低いエラーを得る為に何に時間を使うのが、もっとも賢い自分の時間の使い方か?",
    "output": "Now, if you're building a spam classifier one question that you may face is, what's the best use of your time in order to make your spam classifier have higher accuracy, you have lower error."
  },
  {
    "index": "F16182",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "自然に思い浮かぶ事としては、もっと多くのデータを集めるという事。",
    "output": "One natural inclination is going to collect lots of data."
  },
  {
    "index": "F16183",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でしょ?",
    "output": "Right?"
  },
  {
    "index": "F16184",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして現実でも、より多くのデータがあれば、アルゴリズムはよりうまくやってくれるに違いない、と考える傾向にある。",
    "output": "And in fact there's this tendency to think that, well the more data we have the better the algorithm will do."
  },
  {
    "index": "F16185",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際、e-mailのスパムの分野では、HoneyPot(ハチミツ壺)プロジェクト、と呼ばれるとても真剣なプロジェクトが存在していて、それは偽のe-mailアドレスを作ってその偽のアドレスをスパマーにつかませて、それを使って大量のスパムのe-mailを集める、という物だ。つまり、学習アルゴリズムを訓練するスパムのデータをたくさん集める、という事。",
    "output": "And in fact, in the email spam domain, there are actually pretty serious projects called Honey Pot Projects, which create fake email addresses and try to get these fake email addresses into the hands of spammers and use that to try to collect tons of spam email, and therefore you know, get a lot of spam data to train learning algorithms."
  },
  {
    "index": "F16186",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが我らは既にここまでの何本かのビデオで、データをもっと集める、というのは役に立つ事も確かにあるが、いつもそうだという訳じゃない、というのを見てきた。",
    "output": "But we've already seen in the previous sets of videos that getting lots of data will often help, but not all the time."
  },
  {
    "index": "F16187",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして多くの機械学習の問題において、パフォーマンスを改善する為に考えられる選択肢は他にもたくさんある。",
    "output": "But for most machine learning problems, there are a lot of other things you could usually imagine doing to improve performance."
  },
  {
    "index": "F16188",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スパムに関しては、一つ考えられる手としては、e-mailについてもっと洗練されたフィーチャーを開発する、というのが考えられる、例えばe-mailのルーティング情報に基づいたりとか。",
    "output": "For spam, one thing you might think of is to develop more sophisticated features on the email, maybe based on the email routing information."
  },
  {
    "index": "F16189",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはe-mailのヘッダに含まれている情報だ。",
    "output": "And this would be information contained in the email header."
  },
  {
    "index": "F16190",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スパマーがe-mailを送る時には、彼らはよく、どこから送ってるかをごまかそうとする。だから偽物のe-mailヘッダを使ったりする。",
    "output": "So, when spammers send email, very often they will try to obscure the origins of the email, and maybe use fake email headers."
  },
  {
    "index": "F16191",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "またはとても普通でないコンピューターサービス群を通して、とても普通で無いルートを通してスパムが届くようにする。",
    "output": "Through very unusual routes, in order to get the spam to you."
  },
  {
    "index": "F16192",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの情報の幾らかはe-mailのヘッダに反映される。",
    "output": "And some of this information will be reflected in the email header."
  },
  {
    "index": "F16193",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこんな風に考えられる。e-mailヘッダを見て、この種のe-mailのルーティング情報をうまくとらえるような、もっと洗練されたフィーチャーを開発して、それがスパムかどうかを判定する、という事を試みるという事を。",
    "output": "And so one can imagine, looking at the email headers and trying to develop more sophisticated features to capture this sort of email routing information to identify if something is spam."
  },
  {
    "index": "F16194",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他に考えられる事としてはe-mailのメッセージのボディ、つまりはe-mailのテキストを見て、もっと洗練したフィーチャーを構築しようと試みる、というのがある。",
    "output": "Something else you might consider doing is to look at the email message body, that is the email text, and try to develop more sophisticated features."
  },
  {
    "index": "F16195",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、discountとdiscountsは同じ単語とみなした方がいいだろうか?",
    "output": "For example, should the word 'discount' and the word 'discounts' be treated as the same words or should we have treat the words 'deal' and 'dealer' as the same word?"
  },
  {
    "index": "F16196",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例では片方は大文字で片方は小文字だけれども。",
    "output": "Maybe even though one is lower case and one in capitalized in this example."
  },
  {
    "index": "F16197",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または句読点などの記号についてのもっと複雑なフィーチャーが良いかもしれない。何故ならスパムはびっくりマーク(!",
    "output": "Or do we want more complex features about punctuation because maybe spam is using exclamation marks a lot more."
  },
  {
    "index": "F16198",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "知らないけどね。",
    "output": "I don't know."
  },
  {
    "index": "F16199",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同じような考えとして、故意のミススペルを検出する為のもっと洗練されたアルゴリズムを開発したい、と思うかもしれない。",
    "output": "And along the same lines, maybe we also want to develop more sophisticated algorithms to detect and maybe to correct to deliberate misspellings, like mortgage, medicine, watches."
  },
  {
    "index": "F16200",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならスパマーは実際にこれをやってるから。何故ならwatchesに4が入っていると、今話した単純なテクニックではスパム分類器はこれを\"watches\"と同じ物だとはみなせないかもしれない。",
    "output": "Because spammers actually do this, because if you have watches with a 4 in there then well, with the simple technique that we talked about just now, the spam classifier might not equate this as the same thing as the word \"watches,\" and so it may have a harder time realizing that something is spam with these deliberate misspellings."
  },
  {
    "index": "F16201",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてだからこそスパマーはそれをするのだ。",
    "output": "And this is why spammers do it."
  },
  {
    "index": "F16202",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "機械学習の問題にとりくんでいる時には、あなたはしょっちゅうこれらのように試してみたい事のリストをブレインストーミングする事が出来る。",
    "output": "While working on a machine learning problem, very often you can brainstorm lists of different things to try, like these."
  },
  {
    "index": "F16203",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、実のところ私はスパムの問題にしばらく従事していた事がある。",
    "output": "By the way, I've actually worked on the spam problem myself for a while."
  },
  {
    "index": "F16204",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私はかなりの時間、それに費やした。",
    "output": "And I actually spent quite some time on it."
  },
  {
    "index": "F16205",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして私はスパムの問題をかなり理解しているのだけれども、それについてかなり知っているのだけれども、それでもあなたにこの四つの選択肢のうちどれにあなたの時間を使うのがベストな選択なのか、と言うのは、凄く難しい。",
    "output": "And even though I kind of understand the spam problem, I actually know a bit about it, I would actually have a very hard time telling you of these four options which is the best use of your time so what happens, frankly what happens far too often is that a research group or product group will randomly fixate on one of these options."
  },
  {
    "index": "F16206",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして時には、誰かがランダムにこれらの選択肢のどれに固執し始めたかに応じてあなたがそれに時間を費やすのが実り多いかどうかが決まってしまうという事になる。",
    "output": "And sometimes that turns out not to be the most fruitful way to spend your time depending, you know, on which of these options someone ends up randomly fixating on."
  },
  {
    "index": "F16207",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、実のところ、試すべき様々な選択肢をブレインストーミングする所まで来ていたら、あなたはたぶん他人よりは前に進んでいる。",
    "output": "By the way, in fact, if you even get to the stage where you brainstorm a list of different options to try, you're probably already ahead of the curve."
  },
  {
    "index": "F16208",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "残念な事に、多くの人が代わりにやる事は、試すべき選択肢をリストに書き出そうとはせず、その代わりに多くの人々がやってしまう事は、ある朝起きて、何らかの理由で、例えば妙なガッツフィーリングで、「よし、巨大なハニーポットのプロジェクトを立ち上げて、もっと大量のデータを集めよう!」とか思い立ったりとか、とにかく何かしら変な理由で、ある朝起きてランダムに一つに決めてしまって、それに六ヶ月とか費やしてしまう。",
    "output": "Sadly, what most people do is instead of trying to list out the options of things you might try, what far too many people do is wake up one morning and, for some reason, just, you know, have a weird gut feeling that, \"Oh let's have a huge honeypot project to go and collect tons more data\" and for whatever strange reason just sort of wake up one morning and randomly fixate on one thing and just work on that for six months."
  },
  {
    "index": "F16209",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、きっと我らはもっとマシなやり方が出来る。",
    "output": "But I think we can do better."
  },
  {
    "index": "F16210",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、次のビデオで私が話すのはエラー分析という概念で、そしてもっとシステマティックな方法でさまざまな異なる選択肢からうまく行きそうな方法を選ぶ事を試みようとする時に使える方法で、つまりは実際に続く数週間、数日、または数ヶ月もの時間をあなたが費やすのにより良さそうな選択を行う方法だ。",
    "output": "And in particular what I'd like to do in the next video is tell you about the concept of error analysis and talk about the way where you can try to have a more systematic way to choose amongst the options of the many different things you might work, and therefore be more likely to select what is actually a good way to spend your time, you know for the next few weeks, or next few days or the next few months."
  },
  {
    "index": "F16211",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではエラー分析とエラーのメトリクスを持つ必要性を議論した。エラーのメトリクスとは学習アルゴリズムがどれだけうまくやっているかを語る単一の実数値による評価指標の事だった。",
    "output": "In the previous video, I talked about error analysis and the importance of having error metrics, that is of having a single real number evaluation metric for your learning algorithm to tell how well it's doing."
  },
  {
    "index": "F16212",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "評価とエラーのメトリクスという文脈においては、一つの重要なケースとしてあなたの学習アルゴリズムに対して適切なエラーの指標、適切な評価の指標を得るのがトリッキーになってまう場合というのがある。",
    "output": "In the context of evaluation and of error metrics, there is one important case, where it's particularly tricky to come up with an appropriate error metric, or evaluation metric, for your learning algorithm."
  },
  {
    "index": "F16213",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのケースとは、いわゆるスキューした(歪んだ)クラス、と言われる物だ。",
    "output": "That case is the case of what's called skewed classes."
  },
  {
    "index": "F16214",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがどういう事か、説明しよう。",
    "output": "Let me tell you what that means."
  },
  {
    "index": "F16215",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ガンの分類の問題を考えよう。医療患者のフィーチャーがあって、そして彼らがガンかどうかを判断したいとする。",
    "output": "Consider the problem of cancer classification, where we have features of medical patients and we want to decide whether or not they have cancer."
  },
  {
    "index": "F16216",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは以前に見た、腫瘍が悪性か良性かの分類の例に似ている。",
    "output": "So this is like the malignant versus benign tumor classification example that we had earlier."
  },
  {
    "index": "F16217",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ガンの患者をy=1と、そしてそれ以外の患者をy=0としよう。",
    "output": "So let's say y equals 1 if the patient has cancer and y equals 0 if they do not."
  },
  {
    "index": "F16218",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰の分類器をトレーニングしてテストセットに対して分類器をテストしてみたら、1パーセントのエラーを得たとする。",
    "output": "We have trained the progression classifier and let's say we test our classifier on a test set and find that we get 1 percent error."
  },
  {
    "index": "F16219",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "うーん、これはなかなか素晴らしい結果、、、かしら?",
    "output": "Seems like a really impressive result, right."
  },
  {
    "index": "F16220",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "99パーセントの場合は正しいんだから。",
    "output": "We're correct 99% percent of the time."
  },
  {
    "index": "F16221",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもここで、トレーニングとテストセットの0.5パーセントの患者しか実際にガンにかかってないとする。",
    "output": "But now, let's say we find out that only 0.5 percent of patients in our training test sets actually have cancer."
  },
  {
    "index": "F16222",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりたった1パーセントの半分の患者しか、スクリーニングプロセスを越えて実際にガンとならない。",
    "output": "So only half a percent of the patients that come through our screening process have cancer."
  },
  {
    "index": "F16223",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合、1パーセントのエラーはもはやそんなに素晴らしくは見えない。",
    "output": "In this case, the 1% error no longer looks so impressive."
  },
  {
    "index": "F16224",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、ここにあるコードなら、これは実の所学習すらしてないコードで、単に入力としてフィーチャーxを受け取りながらただそれを無視してy=0と言うだけのコード。",
    "output": "And in particular, here's a piece of code, here's actually a piece of non learning code that takes this input of features x and it ignores it."
  },
  {
    "index": "F16225",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてどんな時でも、誰もガンじゃない、と予測するという物。このアルゴリズムは実際に0.5パーセントのエラーを得る。",
    "output": "It just sets y equals 0 and always predicts, you know, nobody has cancer and this algorithm would actually get 0.5 percent error."
  },
  {
    "index": "F16226",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこんなのですらさっき得た1パーセントのエラーよりマシになってしまう。そしてこれは非学習アルゴリズムで見ての通り、単にどんな時でもy=0と予測するだけの物だ。",
    "output": "So this is even better than the 1% error that we were getting just now and this is a non learning algorithm that you know, it is just predicting y equals 0 all the time."
  },
  {
    "index": "F16227",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、このような状況、陽性の手本と陰性の手本の割合がとても両極端となるような状況では、その場合、陽性の手本の総数が陰性の手本の総数よりもずっと、ずっと少ない、何故ならy=1はとてもレアだから。",
    "output": "So this setting of when the ratio of positive to negative examples is very close to one of two extremes, where, in this case, the number of positive examples is much, much smaller than the number of negative examples because y equals one so rarely, this is what we call the case of skewed classes."
  },
  {
    "index": "F16228",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単にある一方のクラスが、他方のクラスよりもずっとたくさんあるような場合。",
    "output": "We just have a lot more of examples from one class than from the other class."
  },
  {
    "index": "F16229",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そういう場合は単にいつでもy=0と予測しておけば、またはいつでもy=1と予測しておけば、アルゴリズムはとてもうまく振る舞う事が出来る。",
    "output": "And by just predicting y equals 0 all the time, or maybe our predicting y equals 1 all the time, an algorithm can do pretty well."
  },
  {
    "index": "F16230",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、分類エラー、または分類の正確さを評価指標として使う際の問題点は、以下のようになる。",
    "output": "So the problem with using classification error or classification accuracy as our evaluation metric is the following."
  },
  {
    "index": "F16231",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたは手元に、99.2%の正確さが得られるアルゴリズムを持ってたとしよう。",
    "output": "Let's say you have one joining algorithm that's getting 99.2% accuracy."
  },
  {
    "index": "F16232",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりエラーは0.8%だ。",
    "output": "So, that's a 0.8% error."
  },
  {
    "index": "F16233",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてあなたは自分のアルゴリズムに変更を加えて、そして今やあなたは99.5%の正確さを得たとする。",
    "output": "Let's say you make a change to your algorithm and you now are getting 99.5% accuracy."
  },
  {
    "index": "F16234",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり0.5%のエラーだ。",
    "output": "That is 0.5% error."
  },
  {
    "index": "F16235",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこれは、アルゴリズムの改善と言えるだろうか?",
    "output": "So, is this an improvement to the algorithm or not?"
  },
  {
    "index": "F16236",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単一の実数評価指標を持っている事の利点の一つとしては、これが我らに、アルゴリズムに良い変更をしたかを素早く判断する助けとなるというのがある。",
    "output": "One of the nice things about having a single real number evaluation metric is this helps us to quickly decide if we just need a good change to the algorithm or not."
  },
  {
    "index": "F16237",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それとも我らのコードを単にy=0を単により多く予測するだけのコードに置き換えたのだろうか?",
    "output": "You know, did we just do something useful or did we just replace our code with something that just predicts y equals zero more often?"
  },
  {
    "index": "F16238",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、とてもスキューしたクラスの場合には、単に分類の正確さを使うというのはより難しくなる。何故ならとても高い分類の正確さを得る、またはとても低いエラーを得るという事は出来て、そしてそれがあなたの分類器を改善したかは、いつも明らかという訳では無い、何故ならどんな時でもy=0を予測する、というのは、そんなに良い分類器とは思えない。",
    "output": "So, if you have very skewed classes it becomes much harder to use just classification accuracy, because you can get very high classification accuracies or very low errors, and it's not always clear if doing so is really improving the quality of your classifier because predicting y equals 0 all the time doesn't seem like a particularly good classifier."
  },
  {
    "index": "F16239",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、単にy=0と、より多く予測するだけで、エラーを減少させる事が出来てしまい、これは0.5%まで減らす事が出来る。",
    "output": "But just predicting y equals 0 more often can bring your error down to, you know, maybe as low as 0.5%."
  },
  {
    "index": "F16240",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スキューしたクラスに直面している時には、もっと別のエラーの指標、あるいは別の評価指標が、欲しくなる。",
    "output": "When we're faced with such a skewed classes therefore we would want to come up with a different error metric or a different evaluation metric."
  },
  {
    "index": "F16241",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんな評価指標の一つには、Preicision(精度)とRecall(再現率)という物がある。",
    "output": "One such evaluation metric are what's called precision recall."
  },
  {
    "index": "F16242",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが何なのかを説明していこう。",
    "output": "Let me explain what that is."
  },
  {
    "index": "F16243",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "テストセットに対して分類器を評価しているとしよう。",
    "output": "Let's say we are evaluating a classifier on the test set."
  },
  {
    "index": "F16244",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばテストセットにある手本の実際のクラスが1か0のどちらかとする。つまりバイナリ分類問題だ。",
    "output": "For the examples in the test set the actual class of that example in the test set is going to be either one or zero, right, if there is a binary classification problem."
  },
  {
    "index": "F16245",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして我らの学習アルゴリズムが行う事は、そのクラスの何らかの値を予測する事で、だから我らの学習アルゴリズムはテストセットの各手本に対して値を予測し、その予測する値もまた1か0のどちらかだ。",
    "output": "And what our learning algorithm will do is it will, you know, predict some value for the class and our learning algorithm will predict the value for each example in my test set and the predicted value will also be either one or zero."
  },
  {
    "index": "F16246",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで以下のように2x2のテーブルを書いてみよう。これらのエントリは、実際のクラスと予測されたクラスに基づいて埋める。",
    "output": "So let me draw a two by two table as follows, depending on a full of these entries depending on what was the actual class and what was the predicted class."
  },
  {
    "index": "F16247",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし我らが、実際のクラスが1で予測されたクラスが1の手本を持つ時には、それはtruepositive(真陽性)と呼ばれる手本である。その意味する所は、我らのアルゴリズムはそれが陽性だと予測して、しかも実際にも陽性の手本だった場合。",
    "output": "If we have an example where the actual class is one and the predicted class is one then that's called an example that's a true positive, meaning our algorithm predicted that it's positive and in reality the example is positive."
  },
  {
    "index": "F16248",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし我らのアルゴリズムがある物を陰性、クラス0と予測して、そして実際のクラスもまたクラス0だった時には、それはtruenegative(真陰性)と呼ばれる。",
    "output": "If our learning algorithm predicted that something is negative, class zero, and the actual class is also class zero then that's what's called a true negative."
  },
  {
    "index": "F16249",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは0と予測し、実際に0だ。",
    "output": "We predicted zero and it actually is zero."
  },
  {
    "index": "F16250",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その他の二つの箱は、我らの学習アルゴリズムがクラス1だと予測したが、実際のクラスが0の時には、これはfalsepositive(偽陽性)と呼ばれる。",
    "output": "To find the other two boxes, if our learning algorithm predicts that the class is one but the actual class is zero, then that's called a false positive."
  },
  {
    "index": "F16251",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その意味する所は、我らのアルゴリズムは、ある患者がガンを持っていると予測しておきながら、実際には患者はガンを持っていなかった。",
    "output": "So that means our algorithm for the patient is cancelled out in reality if the patient does not."
  },
  {
    "index": "F16252",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後の箱は、0、1。",
    "output": "And finally, the last box is a zero, one."
  },
  {
    "index": "F16253",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはfalsenegative(偽陰性)と呼ぶ。何故なら我らのアルゴリズムは0を予測し、しかし実際のクラスは1だから。",
    "output": "That's called a false negative because our algorithm predicted zero, but the actual class was one."
  },
  {
    "index": "F16254",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こうして、我らは小さなある種の2x2のテーブルを得た、それは実際のクラスと予測されたクラスに基づいた物だ。",
    "output": "And so, we have this little sort of two by two table based on what was the actual class and what was the predicted class."
  },
  {
    "index": "F16255",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここに、我らのアルゴリズムのパフォーマンスを評価する別の方法がある。",
    "output": "So here's a different way of evaluating the performance of our algorithm."
  },
  {
    "index": "F16256",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは二つの数を計算する事にする。",
    "output": "We're going to compute two numbers."
  },
  {
    "index": "F16257",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の物は、Precision(精度)と呼ばれる物。それが伝える事は、我らがガンだと予測した全ての患者のうち、どれだけの割合の人が実際にガンだったのか?",
    "output": "The first is called precision - and what that says is, of all the patients where we've predicted that they have cancer, what fraction of them actually have cancer?"
  },
  {
    "index": "F16258",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを書き下してみよう。分類器のPrecisionとは、truepositiveの総数を陽性と予測した総数で割った物だ。",
    "output": "So let me write this down, the precision of a classifier is the number of true positives divided by the number that we predicted as positive, right?"
  },
  {
    "index": "F16259",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、我らが実際に赴いて、「あなたはガンだと、我らは思ってる」と告げた患者のうち、それらの患者全員の中で、実際にガンを持ってる割合はどれだけか?",
    "output": "So of all the patients that we went to those patients and we told them, \"We think you have cancer.\" Of all those patients, what fraction of them actually have cancer?"
  },
  {
    "index": "F16260",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがPrecision(精度)と言われる物だ。",
    "output": "So that's called precision."
  },
  {
    "index": "F16261",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれの別の書き方としては、truepositiveとそして分母は、陽性と予測された総数、それはつまりテーブルの最初の行のエントリの和だ。",
    "output": "And another way to write this would be true positives and then in the denominator is the number of predicted positives, and so that would be the sum of the, you know, entries in this first row of the table."
  },
  {
    "index": "F16262",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれは、truepositiveを割る事のtruepositiveに...ここでpositiveをPOS、と略す事にしよう、そしてそこに足す事のfalsepositive、再びpositiveをPOSと省略して書く。",
    "output": "I'm going to abbreviate positive as POS and then plus false positives, again abbreviating positive using POS."
  },
  {
    "index": "F16263",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがPrecision(精度)と呼ばれる物だ。見て分かるように、高いPrecisionは良い事だ。",
    "output": "So that's called precision, and as you can tell high precision would be good."
  },
  {
    "index": "F16264",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その意味する所は、我らが赴き、「大変残念ですが、あなたはガンだと思います」と告げた患者全てに対して、高いPrecisionが意味する事は、その患者のグループの大多数が、我らが正しく予測を行った事になり、つまり実際にガンを持っている。",
    "output": "That means that all the patients that we went to and we said, \"You know, we're very sorry. We think you have cancer,\" high precision means that of that group of patients most of them we had actually made accurate predictions on them and they do have cancer."
  },
  {
    "index": "F16265",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らが計算する二番目の数値はRecall(再現率)と呼ばれる物で、Recallが言う事は、テストセットなりクロスバリデーションセットなり、とにかくデータセットのうち全てのガンの患者に対して、彼らのうちどれだけの割合を正しくガンだと、検出出来たか?",
    "output": "The second number we're going to compute is called recall, and what recall say is, if all the patients in, let's say, in the test set or the cross-validation set, but if all the patients in the data set that actually have cancer, what fraction of them that we correctly detect as having cancer."
  },
  {
    "index": "F16266",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "という事。つまり、全ての患者がガンだったとして、それらのうち、何人の元に我らは実際におもむき、正しくも「あなた方には治療が必要だと我らは思っている」と告げる事が出来るだろうか?",
    "output": "So if all the patients have cancer, how many of them did we actually go to them and you know, correctly told them that we think they need treatment."
  },
  {
    "index": "F16267",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これを書き下すと、Recall(再現率)は以下のように定義出来る:陽性の、、、truepositiveの総数、つまり、実際にガンを持ってる患者の中から、我らが正しくガンを持っていると予測出来た総数だが、それを割る事の、実際に陽性である患者の総数で割る。",
    "output": "So, writing this down, recall is defined as the number of positives, the number of true positives, meaning the number of people that have cancer and that we correctly predicted have cancer and we take that and divide that by, divide that by the number of actual positives, so this is the right number of actual positives of all the people that do have cancer."
  },
  {
    "index": "F16268",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どれだけの割合にフラグ立てして処置に送れるか?",
    "output": "What fraction do we directly flag and you know, send the treatment."
  },
  {
    "index": "F16269",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これを違う形に書き直すと、分母は実際の陽性の数だから、つまりそれはここの最初の列のエントリの和だ。",
    "output": "So, to rewrite this in a different form, the denominator would be the number of actual positives as you know, is the sum of the entries in this first column over here."
  },
  {
    "index": "F16270",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、別の書き方で書くと、これはつまり、truepositiveの総数をtruepositiveの総数足す事のfalsenegativeで割った物、という事になる。",
    "output": "And so writing things out differently, this is therefore, the number of true positives, divided by the number of true positives plus the number of false negatives."
  },
  {
    "index": "F16271",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここでも、高いRecall(再現率)になるのは、良い事だ。",
    "output": "And so once again, having a high recall would be a good thing."
  },
  {
    "index": "F16272",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてPrecision(精度)とRecall(再現率)を計算する事により、我らは分類器がどれだけうまく振舞っているかについて、より良い感覚を得られる。",
    "output": "So by computing precision and recall this will usually give us a better sense of how well our classifier is doing."
  },
  {
    "index": "F16273",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして例えば、いつもy=0を予測するような学習アルゴリズムだとすると、それが一回もガンだと予測しないとすると、その場合、この分類器はRecall(再現率)=0となる。何故ならそこには、truepositiveは全く無いから。",
    "output": "And in particular if we have a learning algorithm that predicts y equals zero all the time, if it predicts no one has cancer, then this classifier will have a recall equal to zero, because there won't be any true positives and so that's a quick way for us to recognize that, you know, a classifier that predicts y equals 0 all the time, just isn't a very good classifier."
  },
  {
    "index": "F16274",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはとても良い分類器、とは言えなかろう。そしてより一般的に言って、とてもスキューしたクラスのセッティングにおいても、アルゴリズムがある種のチート(ずる)をして、とても高いPrecision(精度)ととても高いRecall(再現率)をいつもy=0と予測する、というような何かしら簡単な方法で達成するのは不可能だ。",
    "output": "And more generally, even for settings where we have very skewed classes, it's not possible for an algorithm to sort of \"cheat\" and somehow get a very high precision and a very high recall by doing some simple thing like predicting y equals 0 all the time or predicting y equals 1 all the time."
  },
  {
    "index": "F16275",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、高いPrecisionまたは高いRecallの分類器というのは、実際に良い分類器だという事により確信を持てる。そしてこれは我らに、より有用な評価指標で我らのアルゴリズムがうまい事やっているかをより直接的に理解させてくれるような物を、与えてくれる。",
    "output": "And so we're much more sure that a classifier of a high precision or high recall actually is a good classifier, and this gives us a more useful evaluation metric that is a more direct way to actually understand whether, you know, our algorithm may be doing well."
  },
  {
    "index": "F16276",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、Precision(精度)とRecall(再現率)の定義の最後のコメントとして、我らはPrecisionとRecallを普通はy=1が普通、より稀なクラスが存在している、という方をコンベンションを用いる。",
    "output": "So one final note in the definition of precision and recall, that we would define precision and recall, usually we use the convention that y is equal to 1, in the presence of the more rare class."
  },
  {
    "index": "F16277",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり我らが検出しようとするそのレアなクラスが存在する、という場合に基づいて。",
    "output": "So if we are trying to detect."
  },
  {
    "index": "F16278",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてPrecision(精度)とRecall(再現率)を使う事で、起こる事というと、とてもスキューしたクラスの場合だとしても、アルゴリズムが「チート」(ずる)して、いかなる時もy=1を予測したり、あるいはいかなる時でもy=0を予測する事で、高いPrecision(精度)とかRecall(再現率)を得る事は不可能である、という事だ。",
    "output": "rare conditions such as cancer, hopefully that's a rare condition, precision and recall are defined setting y equals 1, rather than y equals 0, to be sort of that the presence of that rare class that we're trying to detect."
  },
  {
    "index": "F16279",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして特に、もし分類器で高いPrecisionとRecallが得られたなら、我らはアルゴリズムがとてもうまく機能しているだろう事に実際にしっかりと確信が持てる、たとえスキューしたクラスだったとしても。",
    "output": "And by using precision and recall, we find, what happens is that even if we have very skewed classes, it's not possible for an algorithm to you know, \"cheat\" and predict y equals 1 all the time, or predict y equals 0 all the time, and get high precision and recall."
  },
  {
    "index": "F16280",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、スキューしたクラスの問題に関しては、PrecisionとRecallは我らにアルゴリズムが実際にどうなってるか、についてのより直接的な洞察を与えてくれて、そしてこれはしばしば、我らの学習アルゴリズムを評価する、より良い方法だ。単に分類エラーや分類の正確さを見るだけに比べると。",
    "output": "And in particular, if a classifier is getting high precision and high recall, then we are actually confident that the algorithm has to be doing well, even if we have very skewed classes."
  },
  {
    "index": "F16281",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クラスがとてもスキューしている時には。",
    "output": "So for the problem of skewed classes precision recall gives us more direct insight into how the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms, than looking at classification error or classification accuracy, when the classes are very skewed."
  },
  {
    "index": "F16282",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオで、我らは評価メトリクス(指標)に関して議論してきた。",
    "output": "In the previous video, we talked about evaluation metrics."
  },
  {
    "index": "F16283",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオではトラックをちょっと変更して機械学習のシステムのデザインにおいてしばしば表面化するもう一つの重要な一面であるところの、どれだけのデータを試すか?",
    "output": "In this video, I'd like to switch tracks a bit and touch on another important aspect of machine learning system design, which will often come up, which is the issue of how much data to train on."
  },
  {
    "index": "F16284",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以前のビデオでは、盲目的に外に出て大量のデータを集めるという事を戒めてきた。何故ならそれは役に立つ場合と立たない場合があるからだ。",
    "output": "Now, in some earlier videos, I had cautioned against blindly going out and just spending lots of time collecting lots of data, because it's only sometimes that that would actually help."
  },
  {
    "index": "F16285",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "しかし、ある条件下では、そしてその条件が何なのかはこのビデオで伝えるが、ある種の学習アルゴリズムに対してなら大量のデータを得る事は、学習アルゴリズムにとても良いパフォーマンスで動かす為のとても効率的な方法たりえる。",
    "output": "But it turns out that under certain conditions, and I will say in this video what those conditions are, getting a lot of data and training on a certain type of learning algorithm, can be a very effective way to get a learning algorithm to do very good performance."
  },
  {
    "index": "F16286",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの事から、とてもしばしばあなたの問題においてこれらの条件を真のまま維持出来てそしてもしあなたが大量のデータを得る事が出来れば、これはとても高いパフォーマンスの学習アルゴリズムを得るとても良い方法になり得る。",
    "output": "And this arises often enough that if those conditions hold true for your problem and if you're able to get a lot of data, this could be a very good way to get a very high performance learning algorithm."
  },
  {
    "index": "F16287",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこのビデオで、この事についてもっと議論していこう。",
    "output": "So in this video, let's talk more about that."
  },
  {
    "index": "F16288",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まずこんなストーリーから始めよう。",
    "output": "Let me start with a story."
  },
  {
    "index": "F16289",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何年も前の事、二人の研究者、MichelloBankoとEricBrillは以下のような魅力的な研究を行った。",
    "output": "Many, many years ago, two researchers that I know, Michelle Banko and Eric Broule ran the following fascinating study."
  },
  {
    "index": "F16290",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "彼らはややこしい単語間での分類問題を検討した。例えばこんな文の中では:ForbreakfastIateなら、入る単語はtwoだろうか?",
    "output": "They were interested in studying the effect of using different learning algorithms versus trying them out on different training set sciences, they were considering the problem of classifying between confusable words, so for example, in the sentence: for breakfast I ate, should it be to, two or too?"
  },
  {
    "index": "F16291",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例の場合、ForbreakfastIateで「two」つまり「2」個の玉子を食べる。",
    "output": "Well, for this example, for breakfast I ate two, 2 eggs."
  },
  {
    "index": "F16292",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これがややこしい単語の集合の一例で、これもまた別の集合の例だ。",
    "output": "So, this is one example of a set of confusable words and that's a different set."
  },
  {
    "index": "F16293",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして彼らはこのような機械学習の問題、ある種の教師あり学習の問題であるところの、英語の文の中のある位置に来る単語はどの単語が適切かを分類しようとする問題に対して、彼らは幾つか異なる学習アルゴリズムを用いてーーそれらはその当時、彼らが研究を行った2001年だが、その当時最先端とみなされていた物達で、彼らはだいたいロジスティック回帰の変種のような物である所のPerceptronと、その当時は結構良く使われてたが昨今はあまり使われてないようなWinnowアルゴリズムと、これまたロジスティック回帰に類似した物たが多少違う物でたくさん使われていたが今はあまり使われていないMemory-Basedと呼ばれる学習アルゴリズムで、最近はあまり使われてないがあとでちょっとだけこれについては触れる。",
    "output": "They took a few different learning algorithms which were, you know, sort of considered state of the art back in the day, when they ran the study in 2001, so they took a variance, roughly a variance on logistic regression called the Perceptron."
  },
  {
    "index": "F16294",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてナイーブなベイズアルゴリズム、これは彼らがこのコースで実際に語ってくれる。",
    "output": "And they used a naive based algorithm, which is something they'll actually talk about in this course."
  },
  {
    "index": "F16295",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのアルゴリズムの正確な詳細は、重要では無い。",
    "output": "The exact algorithms of these details aren't important."
  },
  {
    "index": "F16296",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この事は、単に4つの異なる分類アルゴリズムを選んだ、と考えるべきだ。アルゴリズムが実際になんなのかは重要では無い。",
    "output": "Think of this as, you know, just picking four different classification algorithms and really the exact algorithms aren't important."
  },
  {
    "index": "F16297",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "彼らがやった事は、トレーニングセットのサイズを変えて、これらの学習アルゴリズムをそのトレーニングセットサイズの範囲のトレーニングセット群に対して試してみた。そしてその結果がこれだ。",
    "output": "But what they did was they varied the training set size and tried out these learning algorithms on the range of training set sizes and that's the result they got."
  },
  {
    "index": "F16298",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その傾向ははっきりしてる。まず、これらのアルゴリズムはほとんど、驚くほど似たようなパフォーマンスを与える。",
    "output": "And the trends are very clear right first most of these outer rooms give remarkably similar performance."
  },
  {
    "index": "F16299",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二番目に、トレーニングセットのサイズが増加していくにつれて、ここで横軸が100万を単位としたトレーニングセットのサイズで、見ての通り10万から1000の100万、つまり10億のトレーニング手本までの範囲をとっている。",
    "output": "And second, as the training set size increases, on the horizontal axis is the training set size in millions go from you know a hundred thousand up to a thousand million that is a billion training examples."
  },
  {
    "index": "F16300",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アルゴリズムのパフォーマンスは全てだいたい単調に増加していて、そしてあなたがどんなアルゴリズムを選ぼうと、たとえ、いわゆる「劣ったアルゴリズム」を選ぼうと、その「劣ったアルゴリズム」により多くのデータを与えさえすれば、これらの例から分かる事は、より「優れたアルゴリズム」を打ち負かすだろう。",
    "output": "The performance of the algorithms all pretty much monotonically increase and the fact that if you pick any algorithm may be pick a \"inferior algorithm\" but if you give that \"inferior algorithm\" more data, then from these examples, it looks like it will most likely beat even a \"superior algorithm\"."
  },
  {
    "index": "F16301",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、多くの異なる学習アルゴリズムは時には詳細に依存する事もあるが、傾向としてはだいたい似たような範囲のパフォーマンスを示し、本当にパフォーマンスを先導するのはアルゴリズムに大量のトレーニングデータを与える事が出来るかどうかだ、という結果を示している。",
    "output": "So since this original study which is very influential, there's been a range of many different studies showing similar results that show that many different learning algorithms you know tend to, can sometimes, depending on details, can give pretty similar ranges of performance, but what can really drive performance is you can give the algorithm a ton of training data."
  },
  {
    "index": "F16302",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでこれらの結果は機械学習においてはこう言われる事になる:機械学習においてしばしば勝者になるのは、もっとも良いアルゴリズムを持つ者では無く、もっとも多くのデータを持つ者だ、と。ではそれが事実であるのはどういう時で、それが事実で無いのはどういう時だろう?",
    "output": "And this is, results like these has led to a saying in machine learning that often in machine learning it's not who has the best algorithm that wins, it's who has the most data So when is this true and when is this not true?"
  },
  {
    "index": "F16303",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では大量のトレーニングセットを持つ事が役に立ちそうと思えるような一連の条件を並べてみよう。",
    "output": "Let's try to lay out a set of assumptions under which having a massive training set we think will be able to help."
  },
  {
    "index": "F16304",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らの機械学習の問題においては、フィーチャーxがyを正確に予測するのに十分な情報を持っている事を仮定しよう。",
    "output": "Let's assume that in our machine learning problem, the features x have sufficient information with which we can use to predict y accurately."
  },
  {
    "index": "F16305",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、もし前のスライドのややこしい単語の例の場合を考えるとすると、フィーチャーxが我らが埋めようとしている空白の回りの単語を捕捉しているとすると、つまりフィーチャーは「Forbreakfast,Ihave空白eggs」という文を捉えているとすると、その場合は、うん、私がこの間に欲しい単語はtwoである、と分かるだけのそしてtoでもtooでも無いと分かるだけの十分な情報がある。",
    "output": "For example, if we take the confusable words all of them that we had on the previous slide."
  },
  {
    "index": "F16306",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、フィーチャーがこれらの回りの単語を捉えているなら、ラベルyが何なのか?という事をかなり曖昧さ無しで決める為に十分な情報を持っている。",
    "output": "Let's say that it features x capture what are the surrounding words around the blank that we're trying to fill in."
  },
  {
    "index": "F16307",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ラベルyが何かという事を言い換えると、これら三つのややこしい単語のどれを使って空白を埋めるべきか?という事だ。",
    "output": "So the features capture, you know, one of these surrounding words then that gives me enough information to pretty unambiguously decide what is the label y or in other words what is the word that I should be using to fill in that blank out of this set of three confusable words."
  },
  {
    "index": "F16308",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、フィーチャーxが特定のyについて十分な情報を持っている例だ。",
    "output": "So that's an example what the future ex has sufficient information for specific y. For a counter example."
  },
  {
    "index": "F16309",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これの反対の例としては、住宅の価格を予測する時に住宅のサイズだけで他のフィーチャーが何も無いような予測の問題を考えてみよう。",
    "output": "Consider a problem of predicting the price of a house from only the size of the house and from no other features."
  },
  {
    "index": "F16310",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私があなたに住宅の価格は500平方フィートだ、とだけ伝えてそれ以外の情報を何も伝えなかったとする。",
    "output": "So if you imagine I tell you that a house is, you know, 500 square feet but I don't give you any other features. I don't tell you that the house is in an expensive part of the city."
  },
  {
    "index": "F16311",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その住居が町の高い地区にあるとも言わないし、あるいは私はあなたにその住居の部屋の数も言わないし、あるいはどれくらい素晴らしい家具を備えているかも言わないし、その住居が新しいか古いかも言わない。",
    "output": "Or if I don't tell you that the house, the number of rooms in the house, or how nicely furnished the house is, or whether the house is new or old."
  },
  {
    "index": "F16312",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし私が、この家が500平方フィートという情報以外を何も言わなければ、住居の価格に影響を与える要因は住居のサイズ以外にもあまりにもたくさんあるので、あなたがサイズしか知らなければ、その価格を正確に予測するのはとても困難だ。",
    "output": "If I don't tell you anything other than that this is a 500 square foot house, well there's so many other factors that would affect the price of a house other than just the size of a house that if all you know is the size, it's actually very difficult to predict the price accurately."
  },
  {
    "index": "F16313",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから以上は、この仮定、フィーチャーが望む水準の正確さで価格を推測するのに十分な情報を持っている、という仮定の反例と言えると思う。",
    "output": "So that would be a counter example to this assumption that the features have sufficient information to predict the price to the desired level of accuracy."
  },
  {
    "index": "F16314",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私がこの仮定をテストする方法は、私が良くやる方法の一つに、良く自分自身に問うのは、入力のフィーチャーxが与えられた時にこのフィーチャーが与えられた時、学習アルゴリズムと同様の情報が入手可能だった時に、仮にこのドメインの人間のエキスパートの元に赴いたとすると、実際に人間のエキスパートが予想をする事が、あるいは実際に人間のエキスパートが確信を持ってyの値を予想する事が出来るだろうか?",
    "output": "The way I think about testing this assumption, one way I often think about it is, how often I ask myself."
  },
  {
    "index": "F16315",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この最初の例の場合、人間の英語話者の専門家の所に行って、、、英語をうまく話せる人の所に行けば、英語の専門家の人間なら、単に読むだけで、あなたとか私みたいな多くの人々なら、ここに入るのが何であるのかを予測出来るだろう、英語の得意な話者なら、これをうまく予測出来る。",
    "output": "For this first example if we go to, you know an expert human English speaker."
  },
  {
    "index": "F16316",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれで私は、xでyを正しく予測出来る、ということに、確信が持てる。",
    "output": "You go to someone that speaks English well, right, then a human expert in English just read most people like you and me will probably we would probably be able to predict what word should go in here, to a good English speaker can predict this well, and so this gives me confidence that x allows us to predict y accurately, but in contrast if we go to an expert in human prices."
  },
  {
    "index": "F16317",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこれに対して我らがもし住居の価格の専門家、例えば住宅を売る不動産屋とかとにかく住居を売る事で生計を立ててる人の所に行き、そしれ彼らに家のサイズを伝えて、そして彼らに価格は幾らか?",
    "output": "Like maybe an expert realtor, right, someone who sells houses for a living."
  },
  {
    "index": "F16318",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "と聞けば、たとえ住宅の価格や販売のエキスパートであっても、私に言う事は出来ないだろう、つまりこれは、住宅の価格の例で、サイズを知るだけでは住宅の価格を予測するのに十分な情報では無い、というサインである。",
    "output": "If I just tell them the size of a house and I tell them what the price is well even an expert in pricing or selling houses wouldn't be able to tell me and so this is fine that for the housing price example knowing only the size doesn't give me enough information to predict the price of the house."
  },
  {
    "index": "F16319",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで、この前提を維持したままで、大量のデータを得る事が助けとなるかを見てみよう。",
    "output": "Let's see then, when having a lot of data could help."
  },
  {
    "index": "F16320",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "yの値を予測するのに十分な情報を持つフィーチャーを得ていたとしよう。",
    "output": "Suppose the features have enough information to predict the value of y."
  },
  {
    "index": "F16321",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてたくさんのパラメータの学習アルゴリズムを用いる事にしよう。それはロジスティック回帰かもしれないし、たくさんのフィーチャーの線形回帰かもしれない。",
    "output": "And let's suppose we use a learning algorithm with a large number of parameters so maybe logistic regression or linear regression with a large number of features."
  },
  {
    "index": "F16322",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あるいは、私が時々やる事として、、、私が実際に良くやる事としては、たくさんの隠れユニットを持ったニューラルネットワークを使う、というやり方もある。",
    "output": "Or one thing that I sometimes do, one thing that I often do actually is using neural network with many hidden units."
  },
  {
    "index": "F16323",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これもまたたくさんのパラメータを持つ学習アルゴリズムと言える。",
    "output": "That would be another learning algorithm with a lot of parameters."
  },
  {
    "index": "F16324",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これらは全て、たくさんのパラメータを持つ強力なアルゴリズムであり、とても複雑な関数にフィッティング出来る。",
    "output": "So these are all powerful learning algorithms with a lot of parameters that can fit very complex functions."
  },
  {
    "index": "F16325",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで私はこれらを、低バイアスのアルゴリズムと呼び、そうみなしていく事にする。何故ならとても複雑な関数にフィッティング出来るから。",
    "output": "So, I'm going to call these, I'm going to think of these as low-bias algorithms because you know we can fit very complex functions and because we have a very powerful learning algorithm, they can fit very complex functions."
  },
  {
    "index": "F16326",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たぶん、これらのアルゴリズムをデータセットに対して実行すると、トレーニングセットに良くフィットするように出来るだろう。つまり、トレーニング誤差は小さくなる事が期待出来る。",
    "output": "Chances are, if we run these algorithms on the data sets, it will be able to fit the training set well, and so hopefully the training error will be slow."
  },
  {
    "index": "F16327",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、大量の、本当に大量のトレーニングセットを用いる事にしよう。その場合、我らに巨大なトレーニングセットがあれば、たとえたくさんのパラメータがあっても、パラメータの数に対してでさえ十分に大量のトレーニングセットであれば、これらのアルゴリズムはオーバーフィットしそうには無い。",
    "output": "Now let's say, we use a massive, massive training set, in that case, if we have a huge training set, then hopefully even though we have a lot of parameters but if the training set is sort of even much larger than the number of parameters then hopefully these albums will be unlikely to overfit."
  },
  {
    "index": "F16328",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら我らはそんなにも巨大なトレーニングセットを持っているのだから。そしてオーバーフィットしなさそう、という事はトレーニング誤差はテスト誤差と近い事が期待される、という事を意味する。",
    "output": "Right because we have such a massive training set and by unlikely to overfit what that means is that the training error will hopefully be close to the test error."
  },
  {
    "index": "F16329",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、これら二つをあわせると、トレーニングセット誤差は小さくて、テストセット誤差はトレーニング誤差と近くなる、これら二つをあわせると、テストセット誤差も小さくなるだろう事が期待される。",
    "output": "Finally putting these two together that the train set error is small and the test set error is close to the training error what this two together imply is that hopefully the test set error will also be small."
  },
  {
    "index": "F16330",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これのもう一つ別の考え方としては、高いパフォーマンスの学習アルゴリズムを得る為に、それが高バイアスでも高バリアンスでも無い事を望む。",
    "output": "Another way to think about this is that in order to have a high performance learning algorithm we want it not to have high bias and not to have high variance."
  },
  {
    "index": "F16331",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでバイアスの問題に対して、我らは学習アルゴリズムがたくさんのパラメータを持つ事で低バイアスのアルゴリズムとなるようにしつつ、一方でとても大量のトレーニングセットを用いる事で、これはバリアンスの問題が無い事を保証してくれる。",
    "output": "So the bias problem we're going to address by making sure we have a learning algorithm with many parameters and so that gives us a low bias alorithm and by using a very large training set, this ensures that we don't have a variance problem here."
  },
  {
    "index": "F16332",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり我らのアルゴリズムにバリアンスの問題が無い事が期待出来て、そしてこれら二つを合わせる事で、結局は低バイアス、低バリアンスの学習アルゴリズムとなる。そしてこれは、テストセットにおいてとても良く振舞ってくれる。",
    "output": "So hopefully our algorithm will have no variance and so is by pulling these two together, that we end up with a low bias and a low variance learning algorithm and this allows us to do well on the test set."
  },
  {
    "index": "F16333",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは本質的には、鍵となる想定は、フィーチャーが十分な情報を持っている事、そしてリッチなクラスの関数である事で低バイアスである事を保証し、そして次に大量のトレーニングセットを持つ事で低バリアンスである事を保証する訳だ。つまり、これが我らに以下のような一連の条件を持った問題:より多くのデータを持ってたくさんのパラメータの学習アルゴリズムを訓練するような物。",
    "output": "And fundamentally it's a key ingredients of assuming that the features have enough information and we have a rich class of functions that's why it guarantees low bias, and then it having a massive training set that that's what guarantees more variance."
  },
  {
    "index": "F16334",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは高いパフォーマンスの学習アルゴリズムを得る為の良い方法たりえる。そして実際に、私がキーだと思うテストとして、自分自身にも良く問う物としては、まず一つ目の問いは、人間のエキスパートがフィーチャーxを見た時に、yの値を確信を持って予測出来るか、という事。",
    "output": "So this gives us a set of conditions rather hopefully some understanding of what's the sort of problem where if you have a lot of data and you train a learning algorithm with lot of parameters, that might be a good way to give a high performance learning algorithm and really, I think the key test that I often ask myself are first, can a human experts look at the features x and confidently predict the value of y."
  },
  {
    "index": "F16335",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならそれは、フィーチャーxからyが正確に予測出来る、という保証となるからだ。そして二番目の問いは、我らは実際に大量のトレーニングセットを得る事が出来て、たくさんのパラメータの学習アルゴリズムをそのトレーニングセットでトレーニング出来るか?",
    "output": "Because that's sort of a certification that y can be predicted accurately from the features x and second, can we actually get a large training set, and train the learning algorithm with a lot of parameters in the training set and if you can't do both then that's more often give you a very kind performance learning algorithm."
  },
  {
    "index": "F16336",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでで、様々な学習アルゴリズムを見てきた。",
    "output": "By now, you've seen a range of difference learning algorithms."
  },
  {
    "index": "F16337",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "教師有り学習の中では、それぞれの学習アルゴリズム同士のパフォーマンスはとても似通っていて、学習アルゴリズムAを使うか学習アルゴリズムBを使うかの違いは重要で無い事が多い。",
    "output": "With supervised learning, the performance of many supervised learning algorithms will be pretty similar, and what matters less often will be whether you use learning algorithm a or learning algorithm b, but what matters more will often be things like the amount of data you create these algorithms on, as well as your skill in applying these algorithms."
  },
  {
    "index": "F16338",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それよりも重要になる事が多いのはこれらのアルゴリズムを適用する対象のデータの量とか、または学習アルゴリズムを適用するスキル、例えば学習アルゴリズムに与えるフィーチャーの選択とか正規化パラメータをどう選ぶかとかそういった物の方が重要な事が多い。",
    "output": "Things like your choice of the features you design to give to the learning algorithms, and how you choose the colorization parameter, and things like that."
  },
  {
    "index": "F16339",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、それでももう一つ、とても強力で、業界でもアカデミアでも良く使われているアルゴリズムがある。",
    "output": "But, there's one more algorithm that is very powerful and is very widely used both within industry and academia, and that's called the support vector machine."
  },
  {
    "index": "F16340",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはサポートベクターマシーンと呼ばれていて、ロジスティック回帰やニューラルネットワークと比べて、サポートベクターマシーン、またの名をSVMは、複雑な非線形の関数を学習する方法として、場合によってはより明解で、よりパワフルな事がある。",
    "output": "And compared to both logistic regression and neural networks, the Support Vector Machine, or SVM sometimes gives a cleaner, and sometimes more powerful way of learning complex non-linear functions."
  },
  {
    "index": "F16341",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから次のビデオで、それについて話したい。",
    "output": "And so let's take the next videos to talk about that."
  },
  {
    "index": "F16342",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このコースの後半で、様々な教師有り学習アルゴリズムの簡単なサーベイを行い、とても簡潔にそれらを紹介するつもりだ。",
    "output": "Later in this course, I will do a quick survey of a range of different supervisory algorithms just as a very briefly describe them."
  },
  {
    "index": "F16343",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがサポートベクターマシーンはその人気があまりにも大きいので、教師有り学習アルゴリズムの最後として、このコースの中のそれなりの時間を費やしたいと思う。ここまでの学習アルゴリズムの開発と同様、最適化の目的関数から始めたいと思う。",
    "output": "But the support vector machine, given its popularity and how powerful it is, this will be the last of the supervisory algorithms that I'll spend a significant amount of time on in this course as with our development other learning algorithms, we're gonna start by talking about the optimization objective."
  },
  {
    "index": "F16344",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこのアルゴリズムを始めよう。",
    "output": "So, let's get started on this algorithm."
  },
  {
    "index": "F16345",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "サポートベクターマシーンを記述する為に、まずはロジスティック回帰から始めて、それをちょこっと変更して本質的にはサポートベクターマシーンが得られるやり方をお見せしたい。",
    "output": "In order to describe the support vector machine, I'm actually going to start with logistic regression, and show how we can modify it a bit, and get what is essentially the support vector machine."
  },
  {
    "index": "F16346",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではロジスティック回帰において、今や見慣れた仮説の形がこれで、そしてsigmoidアクティベーション関数が右に示してある。",
    "output": "So in logistic regression, we have our familiar form of the hypothesis there and the sigmoid activation function shown on the right."
  },
  {
    "index": "F16347",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして数学をいくつか説明する為に、zをシータ転置のxを表すのに使う。",
    "output": "And in order to explain some of the math, I'm going to use z to denote theta transpose axiom."
  },
  {
    "index": "F16348",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、ロジスティック回帰において我らが何をするかを見てみよう。",
    "output": "Now let's think about what we would like logistic regression to do."
  },
  {
    "index": "F16349",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "手本があって、y=1とする、これの意味は、トレーニングセットなりテストセットなりクロスバリデーションセットにおいて、y=1という事で、それはようするに、h(x)が1に近い事を期待する、という事を意味する。",
    "output": "If we have an example with y equals one and by this I mean an example in either the training set or the test set or the cross-validation set, but when y is equal to one then we're sort of hoping that h of x will be close to one."
  },
  {
    "index": "F16350",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、手本を正しく分類する事を望んでいて、h(x)が1に近いことは、シータ転置のxが0よりもずっと大きくなければならない事を意味する。",
    "output": "Right, we're hoping to correctly classify that example. And what having x subscript 1, what that means is that theta transpose x must be must larger than 0."
  },
  {
    "index": "F16351",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは大なり大なりの記号で、0よりも、とってもとっても大きい事を意味する。",
    "output": "So there's greater than, greater than sign that means much, much greater than 0."
  },
  {
    "index": "F16352",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれはとりもなさず、z、つまりシータ転置のxが0よりもずっと大きい時この図で遥か右に位置するという事で、ロジスティック回帰の出力は1に近くなるという事を意味する。",
    "output": "And that's because it is z, the theta of transpose x is when z is much bigger than 0 is far to the right of the sphere."
  },
  {
    "index": "F16353",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならそれに対応する仮説の出力の値は0に近いから。",
    "output": "That the outputs of logistic progression becomes close to one."
  },
  {
    "index": "F16354",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでロジスティック回帰のコスト関数を見てみると、見られる結果は各手本、x、yがこのような項として全体のコストに貢献している。",
    "output": "Conversely, if we have an example where y is equal to zero, then what we're hoping for is that the hypothesis will output a value close to zero."
  },
  {
    "index": "F16355",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりコスト関数の全体としては、普通はトレーニング手本全体に渡る和があり、さらに1/mの項もある。",
    "output": "And that corresponds to theta transpose x of z being much less than zero because that corresponds to a hypothesis of putting a value close to zero."
  },
  {
    "index": "F16356",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがここのこの式は、これこそが、一つのトレーニング手本の寄与の項だ、ロジスティック回帰の目的関数全体への。",
    "output": "If you look at the cost function of logistic regression, what you'll find is that each example (x,y) contributes a term like this to the overall cost function, right?"
  },
  {
    "index": "F16357",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、この仮説の定義の式を取り、ここに代入する。得られた物は、各トレーニング手本の寄与はこの項だ。",
    "output": "So for the overall cost function, we will also have a sum over all the chain examples and the 1 over m term, that this expression here, that's the term that a single training example contributes to the overall objective function so we can just rush them."
  },
  {
    "index": "F16358",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1/mは無視してるが、この項がロジスティック回帰の全体のコスト関数への寄与だ。",
    "output": "Now if I take the definition for the fall of my hypothesis and plug it in over here, then what I get is that each training example contributes this term, ignoring the one over M but it contributes that term to my overall cost function for logistic regression."
  },
  {
    "index": "F16359",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、2つの場合を考えてみよう:y=1の時とy=0の時。",
    "output": "Now let's consider two cases of when y is equal to one and when y is equal to zero."
  },
  {
    "index": "F16360",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初のケースとして、y=1の時を考えよう。",
    "output": "In the first case, let's suppose that y is equal to 1."
  },
  {
    "index": "F16361",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合は、この目的関数の最初の項だけが重要だ、何故ならこの1-yの項は0になるから、y=1の時は。",
    "output": "In that case, only this first term in the objective matters, because this one minus y term would be equal to zero if y is equal to one."
  },
  {
    "index": "F16362",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりy=1の時は手本x,yの、yが1の時には、我らが得るのはこの項、-logの1+eの-z乗分の一。",
    "output": "So when y is equal to one, when in our example x comma y, when y is equal to 1 what we get is this term.."
  },
  {
    "index": "F16363",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで一つ前のスライドと同様、zをシータ転置xを表すのに使っている。もちろん、コストでは実際はこの-yがあるはずだが、今言ったように、y=1の場合だ。",
    "output": "Minus log one over one, plus E to the negative Z where as similar to the last line I'm using Z to denote data transposed X and of course in a cost I should have this minus line that we just had if Y is equal to one so that's equal to one I just simplify in a way in the expression that I have written down here."
  },
  {
    "index": "F16364",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの関数をzの関数として、プロットすると、こんな曲線がこの左下に描いたこの線が見られる。",
    "output": "And if we plot this function as a function of z, what you find is that you get this curve shown on the lower left of the slide."
  },
  {
    "index": "F16365",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こうして、zがとても大きい時はつまりシータ転置xが大きい場合は、とても小さい値、コスト関数にちょっとしか寄与しないzに対応する。",
    "output": "And thus, we also see that when z is equal to large, that is, when theta transpose x is large, that corresponds to a value of z that gives us a fairly small value, a very, very small contribution to the consumption."
  },
  {
    "index": "F16366",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは、何故ロジスティック回帰において、陽性の手本でy=1を見たらシータ転置xにとても大きな値を入れたがる、ある種の説明になっている。何故ならそれは対応するコスト関数の中のこの項が、とても小さくなる事を意味するから。",
    "output": "And this kinda explains why, when logistic regression sees a positive example, with y=1, it tries to set theta transport x to be very large because that corresponds to this term, in the cross function, being small."
  },
  {
    "index": "F16367",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、サポートベクターマシンを構築する為に、これがやるべき事だ。",
    "output": "Now, to fill the support vec machine, here's what we're going to do."
  },
  {
    "index": "F16368",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このコスト関数取って、この-logの1足すeの-z乗分の一を、ちょびっと変更する。",
    "output": "We're gonna take this cross function, this minus log 1 over 1 plus e to negative z, and modify it a little bit."
  },
  {
    "index": "F16369",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この点、ここにある1を取り、今後使うコスト関数を書いてみよう、新しいコスト関数はここからフラットになり、そして成長の仕方は直線で描く、ロジスティック回帰に似ているが、しかしこれは直線。",
    "output": "Let me take this point 1 over here, and let me draw the cross functions you're going to use. The new pass functions can be flat from here on out, and then we draw something that grows as a straight line, similar to logistic regression."
  },
  {
    "index": "F16370",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、マゼンタで今描いた曲線。",
    "output": "But this is going to be a straight line at this portion."
  },
  {
    "index": "F16371",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "紫というかマゼンダで描いた曲線。つまりこれは、ロジスティック回帰で使っていたコスト関数に極めて近い近似となっている。",
    "output": "So the curve that I just drew in magenta, and the curve I just drew purple and magenta, so if it's pretty close approximation to the cross function used by logistic regression."
  },
  {
    "index": "F16372",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "2つの線分から構成されている所が違うが。右側にはこのフラットな部分があり、そしてこの左側には直線の部分がある。",
    "output": "Except it is now made up of two line segments, there's this flat portion on the right, and then there's this straight line portion on the left."
  },
  {
    "index": "F16373",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして直線部分の傾きについてはあんま気にしないでくれ。",
    "output": "And don't worry too much about the slope of the straight line portion."
  },
  {
    "index": "F16374",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がy=1のときに使う事になる新しいコスト関数だ。そして想像できると思うがロジスティック回帰と極めて似た事をやっていく事になる。",
    "output": "But that's the new cost function we're going to use for when y is equal to one, and you can imagine it should do something pretty similar to logistic regression."
  },
  {
    "index": "F16375",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがやがて明らかになるが、これはサポートベクターマシンの計算的な優位である、より簡単な最適化問題をあとで与えてくれる事となる。",
    "output": "But turns out, that this will give the support vector machine computational advantages and give us, later on, an easier optimization problem that would be easier for software to solve."
  },
  {
    "index": "F16376",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまではy=1の場合だけを話してきた。",
    "output": "We just talked about the case of y equals one."
  },
  {
    "index": "F16377",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もう一方のケース、y=0、この場合は、コスト関数を見てみると、この二番目の項だけが適用される、何故なら最初の項は、y=0の時は消え去るから。",
    "output": "The other case is if y is equal to zero. In that case, if you look at the cost, then only the second term will apply because the first term goes away, right?"
  },
  {
    "index": "F16378",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりここは0となる。だから上の式で二番目の項だけが残る。",
    "output": "If y is equal to zero, then you have a zero here, so you're left only with the second term of the expression above."
  },
  {
    "index": "F16379",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから手本のコスト、つまりコスト関数へと寄与はここの、この項で与えられる。",
    "output": "And so the cost of an example, or the contribution of the cost function, is going to be given by this term over here."
  },
  {
    "index": "F16380",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれをzの関数としてプロットすると、、、だからここで横軸にzを取って、最終的にはこのカーブとなる。",
    "output": "And if you plot that as a function of z, to have pure z on the horizontal axis, you end up with this one."
  },
  {
    "index": "F16381",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてサポートベクターマシンの為、ふたたびこの青い線を似たような物で置き換える。そして新しいコストでそれを置き換えると、ここは平坦となる。",
    "output": "And for the support vector machine, once again, we're going to replace this blue line with something similar and at the same time we replace it with a new cost, this flat out here, this 0 out here."
  },
  {
    "index": "F16382",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここは0で、その後は直線で増加していく。こんな感じ。",
    "output": "And that then grows as a straight line, like so."
  },
  {
    "index": "F16383",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、これら2つの関数に名前をつけよう。",
    "output": "So let me give these two functions names."
  },
  {
    "index": "F16384",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この左の関数をcost下付き添字1のzと呼ぶ。そしてこの右側の関数を、cost下付き添字0のzと呼ぶ。",
    "output": "This function on the left I'm going to call cost subscript 1 of z, and this function of the right I'm gonna call cost subscript 0 of z."
  },
  {
    "index": "F16385",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで下付き添字は単にy=1に対応しているコストか、またはy=0に対応しているコストかを示しているに過ぎない。",
    "output": "And the subscript just refers to the cost corresponding to when y is equal to 1, versus when y Is equal to zero."
  },
  {
    "index": "F16386",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの定義で武装したので、サポートベクターマシンを構築する準備は整った!",
    "output": "Armed with these definitions, we're now ready to build a support vector machine."
  },
  {
    "index": "F16387",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはロジスティック回帰のコスト関数、Jのシータだ。",
    "output": "Here's the cost function, j of theta, that we have for logistic regression."
  },
  {
    "index": "F16388",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この方程式がちょっと見慣れない、と感じたとしたら、それは前回はマイナスの符号を外に置いていた。だがここでは、マイナスの符号をこの式の中に移動した。",
    "output": "In case this equation looks a bit unfamiliar, it's because previously we had a minus sign outside, but here what I did was I instead moved the minus signs inside these expressions, so it just makes it look a little different."
  },
  {
    "index": "F16389",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "サポートベクターマシンの為に我らがやる事は本質的にはここを、cost1のzで置き換える。",
    "output": "For the support vector machine what we're going to do is essentially take this and replace this with cost1 of z, that is cost1 of theta transpose x."
  },
  {
    "index": "F16390",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはcost1のシータ転置x。そしてこれをcost0のzで置き換える。",
    "output": "And we're going to take this and replace it with cost0 of z, that is cost0 of theta transpose x."
  },
  {
    "index": "F16391",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはcost0のシータ転置xで、ここでcost1関数は前のスライドで見た奴で、こんな感じで、cost0関数もまた、前のスライドで見た奴で、こんな感じの奴。",
    "output": "Where the cost one function is what we had on the previous slide that looks like this."
  },
  {
    "index": "F16392",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "サポートベクターマシンにおいて、我らがやるのは、1/mの和をトレーニング手本に渡って取ることの、y(i)掛けるcost1のシータ転置x(i)足すことの1-y(i)掛けるcost0のシータ転置x(i)。そしてさらに、いつもの正規化パラメータ、こんな感じ。",
    "output": "So what we have for the support vector machine is a minimization problem of one over M, the sum of Y I times cost one, theta transpose X I, plus one minus Y I times cause zero of theta transpose X I, and then plus my usual regularization parameter."
  },
  {
    "index": "F16393",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでサポートベクターマシーンの慣例により、実際にはちょっと違った書き方をする。",
    "output": "Now, by convention, for the support of vector machine, we're actually write things slightly different."
  },
  {
    "index": "F16394",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず、1/mの項を取り除く。これは単にこれは単にロジスティック回帰とはちょっとだけ異なるコンベンションをサポートベクターマシンでは人々が偶然使っていた、というだけ。",
    "output": "First, we're going to get rid of the 1 over m terms, and this just this happens to be a slightly different convention that people use for support vector machines compared to or just a progression."
  },
  {
    "index": "F16395",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこういう事だ。",
    "output": "But here's what I mean."
  },
  {
    "index": "F16396",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり単純に1/mの項を取り除く。これは最適なシータの値には違いをうまないはず。",
    "output": "You're one way to do this, we're just gonna get rid of these one over m terms and this should give you me the same optimal value of beta right?"
  },
  {
    "index": "F16397",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら1/mは単に定数だから。",
    "output": "Because one over m is just as constant so whether I solved this minimization problem with one over n in front or not."
  },
  {
    "index": "F16398",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこの最小化問題を前に1/mを置いて解こうが置かないで解こうが得られる結果は同じ最適なシータの値となる。",
    "output": "I should end up with the same optimal value for theta."
  },
  {
    "index": "F16399",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこういう事だ。具体例を見よう。",
    "output": "Here's what I mean, to give you an example, suppose I had a minimization problem."
  },
  {
    "index": "F16400",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実数のuを、(u-5)の二乗+1を最小化するように選ぶ。",
    "output": "Minimize over a long number U of U minus five squared plus one."
  },
  {
    "index": "F16401",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合、最小になるのはこれが最小になるのはu=5の時だ。",
    "output": "Well, the minimum of this happens to be U equals five."
  },
  {
    "index": "F16402",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、この目的関数に対し、これを10掛けるとするとこの場合、最小化問題は10掛ける(u-5)の二乗足すことの10を最小にするuだ。",
    "output": "Now if I were to take this objective function and multiply it by 10."
  },
  {
    "index": "F16403",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを最小にするuはu=5のままだ。",
    "output": "Well the value of U that minimizes this is still U equals five right?"
  },
  {
    "index": "F16404",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、最小化したい物に、何か定数を掛けても、この場合は10を掛けた訳だが、その事はこの関数を最小にするuの値を、変える事は無い。",
    "output": "So multiply something that you're minimizing over, by some constant, 10 in this case, it does not change the value of U that gives us, that minimizes this function."
  },
  {
    "index": "F16405",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり同様に、このmを取り除く為にやったのは、私がやったのは、目的関数にある定数、mを掛けただけだ。だからそれは、最小になるシータを変化させる事は無い。",
    "output": "So the same way, what I've done is by crossing out the M is all I'm doing is multiplying my objective function by some constant M and it doesn't change the value of theta."
  },
  {
    "index": "F16406",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "2つ目のちょっとしたノーテーションの変更はロジスティック回帰の代わりにSVMを使う時に、もっとも一般的なコンベンションだが、それは以下のような物だ。",
    "output": "The second bit of notational change, which is just, again, the more standard convention when using SVMs instead of logistic regression, is the following."
  },
  {
    "index": "F16407",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰の時は、目的関数に2つの項があった。",
    "output": "So for logistic regression, we add two terms to the objective function."
  },
  {
    "index": "F16408",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ目の項はトレーニングセットから来るコストだった。二番目のこの項は、正規化の項。",
    "output": "The first is this term, which is the cost that comes from the training set and the second is this row, which is the regularization term."
  },
  {
    "index": "F16409",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして我らがやらなくてはいけなかったのは、これらの間のトレードオフを制御する事だった。つまり、最小化したいのはA足すことの、正規化のパラメータ、ラムダに掛けるなんかの項、Bだ。",
    "output": "And what we had was we had a, we control the trade-off between these by saying, what we want is A plus, and then my regularization parameter lambda."
  },
  {
    "index": "F16410",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでAをこの最初の項を指すのに使い、そしてBをこの二番目の項を指すのに使う。ラムダは抜きで。",
    "output": "And then times some other term B, where I guess I'm using your A to denote this first term, and I'm using B to denote the second term, maybe without the lambda."
  },
  {
    "index": "F16411",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのAとBの優先度を考える代わりに、我らがやったのは、この正規化のパラメータ、ラムダに異なる値をセットしていったのだった。我らは相対的にどれだけトレーニングセットに良くフィットさせるか、つまりどれだけAを最小化するかと、とれだけパラメータの値を小さく保つ事を気にするか、それがパラメータBだが、それらの間のトレードオフを取る。",
    "output": "And instead of prioritizing this as A plus lambda B, and so what we did was by setting different values for this regularization parameter lambda, we could trade off the relative weight between how much we wanted the training set well, that is, minimizing A, versus how much we care about keeping the values of the parameter small, so that will be, the parameter is B for the support vector machine, just by convention, we're going to use a different parameter."
  },
  {
    "index": "F16412",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一番目の項と二番目の項の間の重みをコントロールする為にラムダを使う代わりに、そこでもまだパラメータを使う事になるのだが、それはコンベンションでCと呼ばれる。",
    "output": "So instead of using lambda here to control the relative waiting between the first and second terms."
  },
  {
    "index": "F16413",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、代わりにC掛けるA足すBを最小化する。",
    "output": "We're instead going to use a different parameter which by convention is called C and is set to minimize C times a + B."
  },
  {
    "index": "F16414",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰の時はとても大きなラムダの値を用いると、それの意味する所はBにとても大きな重みを付与する、という事だ。",
    "output": "So for logistic regression, if we set a very large value of lambda, that means you will give B a very high weight."
  },
  {
    "index": "F16415",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今回は、もしCにとても小さな値をセットすると、それがBに、Aとくらべてとても大きな重みを付与する事になる。",
    "output": "Here is that if we set C to be a very small value, then that responds to giving B a much larger rate than C, than A."
  },
  {
    "index": "F16416",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これはトレードオフをコントロールする単なる別のやり方、またはどれだけ最初の項を最適化するか、vsどれだけ二番目の項の最適化を重視するか、をパラメトライズする異なるやり方に過ぎない。",
    "output": "So this is just a different way of controlling the trade off, it's just a different way of prioritizing how much we care about optimizing the first term, versus how much we care about optimizing the second term."
  },
  {
    "index": "F16417",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "必要に応じてこれをパラメータCが、1/ラムダと似たよう役割の物とみなすことができる。",
    "output": "And if you want you can think of this as the parameter C playing a role similar to 1 over lambda."
  },
  {
    "index": "F16418",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこれら2つの方程式、2つの式が同じになるという訳では無く、C=1/ラムダというのはそういう場合という訳では無く、もしCが1/ラムダと等しいとこれら2つの最適化の目的関数は同じ結果の値を与える、という事。",
    "output": "This equals 1 over lambda, that's not the case."
  },
  {
    "index": "F16419",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを踏まえると、ラムダを消して、定数Cをここに書く。",
    "output": "It's rather that if C is equal to 1 over lambda, then these two optimization objectives should give you the same value the same optimal value for theta so we just filling that in I'm gonna cross out lambda here and write in the constant C there."
  },
  {
    "index": "F16420",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上の操作で、サポートベクターマシンの全体の最適化の目的関数が得られる。",
    "output": "So that gives us our overall optimization objective function for the support vector machine."
  },
  {
    "index": "F16421",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの関数を最適化すれば、SVMにより学習したパラメータを得られる。",
    "output": "And if you minimize that function, then what you have is the parameters learned by the SVM."
  },
  {
    "index": "F16422",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、ロジスティック回帰と異なり、サポートベクターマシンは確率を出力する訳では無い。その代わりに、我らが得るのはこのコスト関数を最小化するパラメータ、シータで、サポートベクターマシンがやるのは、yが1か0かの予言を直接行う。",
    "output": "Finally unlike logistic regression, the support vector machine doesn't output the probability is that what we have is we have this cost function, that we minimize to get the parameter's data, and what a support vector machine does is it just makes a prediction of y being equal to one or zero, directly."
  },
  {
    "index": "F16423",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、仮説は、シータ転置のxが0より大きければ1を予言し、それ以外なら0を予言する。つまり、学習したパラメータのシータを得た後は、これがサポートベクターマシンの仮説の形だ。",
    "output": "So the hypothesis will predict one if theta transpose x is greater or equal to zero, and it will predict zero otherwise and so having learned the parameters theta, this is the form of the hypothesis for the support vector machine."
  },
  {
    "index": "F16424",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、サポートベクターマシンが何をするかの数学的な定義だ。",
    "output": "So that was a mathematical definition of what a support vector machine does."
  },
  {
    "index": "F16425",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に続く幾つかのビデオて、この最適化の目的関数が何を意味するかを直感的に把握する事を目指します。そしてSVMが学習する仮説の元とどう修正したら、より複雑な、非線形の関数が学習出来るかも話します。",
    "output": "In the next few videos, let's try to get back to intuition about what this optimization objective leads to and whether the source of the hypotheses SVM will learn and we'll also talk about how to modify this just a little bit to the complex nonlinear functions."
  },
  {
    "index": "F16426",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ときどき皆さんはSVMを大きなマージンの分類器だといいますが、今回、みなさんにはこの意味と、皆さんに役立つであろうSVMの仮定がどんなものかという全体像についてお話します。これがサポートベクターマシンでのコスト関数です。",
    "output": "Sometimes people talk about support vector machines, as large margin classifiers, in this video I'd like to tell you what that means, and this will also give us a useful picture of what an SVM hypothesis may look like."
  },
  {
    "index": "F16427",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで左にcost1(z)の関数をプロットした、これは陽性の手本に対して使う物だ。そして右にはcost0(z)をプロットした、ここでzは横軸に取った。",
    "output": "Here's my cost function for the support vector machine where here on the left I've plotted my cost 1 of z function that I used for positive examples and on the right I've plotted my zero of 'Z' function, where I have 'Z' here on the horizontal axis."
  },
  {
    "index": "F16428",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、これらのコスト関数を小さくするとどうなるかを考えてみよう。",
    "output": "Now, let's think about what it takes to make these cost functions small."
  },
  {
    "index": "F16429",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし陽性の手本があったとして、つまりy=1の時、cost1のzはzが1以上の時にだけ0となる。",
    "output": "If you have a positive example, so if y is equal to 1, then cost 1 of Z is zero only when Z is greater than or equal to 1."
  },
  {
    "index": "F16430",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、陽性の手本に対しては、シータ転置xが1以上であって欲しい、という事。そして反対に、もしy=0の時はこのcost0(z)関数を見ると、この領域の時だけつまりzが1以下の時だけ(訳注:-1の間違いか)cost0(z)はゼロとなる。",
    "output": "So in other words, if you have a positive example, we really want theta transpose x to be greater than or equal to 1 and conversely if y is equal to zero, look this cost zero of z function, then it's only in this region where z is less than equal to 1 we have the cost is zero as z is equals to zero, and this is an interesting property of the support vector machine right, which is that, if you have a positive example so if y is equal to one, then all we really need is that theta transpose x is greater than equal to zero."
  },
  {
    "index": "F16431",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし陽性の手本、つまりy=1の時は本当に必要なのはシータ転置xが0以上であれば十分なはずだ。",
    "output": "And that would mean that we classify correctly because if theta transpose x is greater than zero our hypothesis will predict zero."
  },
  {
    "index": "F16432",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だってシータ転置xが0より大きければ仮説は0を予言するんだから(訳注:1の間違いか)同様に、陰性の手本があったらあなたが望むのはただシータ転置xがゼロより小さければ本当は良くて、それだけで手本で正解出来ている。",
    "output": "And similarly, if you have a negative example, then really all you want is that theta transpose x is less than zero and that will make sure we got the example right."
  },
  {
    "index": "F16433",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、サポートベクターマシンは、それよりももうちょっと多くを要求する。",
    "output": "But the support vector machine wants a bit more than that."
  },
  {
    "index": "F16434",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはぎりぎり手本が正しければ良いだけにとどまらず、つまり単にゼロよりちょっとでも大きければ良いというのではなく、それが要求するのは、ゼロよりもかなり大きいという事。",
    "output": "It says, you know, don't just barely get the example right."
  },
  {
    "index": "F16435",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1よりもちょっと大きい、と言っている。",
    "output": "So then don't just have it just a little bit bigger than zero."
  },
  {
    "index": "F16436",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれは0よりもずっと小さくしたい。",
    "output": "What i really want is for this to be quite a lot bigger than zero say maybe bit greater or equal to one and I want this to be much less than zero."
  },
  {
    "index": "F16437",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たとえば-1以下とかにしたい。",
    "output": "Maybe I want it less than or equal to -1."
  },
  {
    "index": "F16438",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、追加のセーフティーファクターをまたはセーフティーマージンをサポートベクターマシンに組み込むと言える。",
    "output": "And so this builds in an extra safety factor or safety margin factor into the support vector machine."
  },
  {
    "index": "F16439",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰ももちろん似たような事をしていたが、何が起こるか見てみよう。またはサポートベクターマシンの文脈ではこれはとういう結果になるのかを見てみよう。",
    "output": "Logistic regression does something similar too of course, but let's see what happens or let's see what the consequences of this are, in the context of the support vector machine."
  },
  {
    "index": "F16440",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、次に私がやりたいのは、この定数Cにとても大きな値をセットしてみたい、というもの。ではCにとてもおおきな値、例えば何十万もの値、なんらかの巨大な値をセットしのを想像してみよう。",
    "output": "Concretely, what I'd like to do next is consider a case case where we set this constant C to be a very large value, so let's imagine we set C to a very large value, may be a hundred thousand, some huge number."
  },
  {
    "index": "F16441",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "サポートベクターマシンが何をするか、見てみよう。",
    "output": "Let's see what the support vector machine will do."
  },
  {
    "index": "F16442",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしCがとっても、とっても大きいと、その場合はこの最適化の目的関数を最小化する時に、値を選ぶ時にこの項がゼロになるように、とても高く動機づけされる。",
    "output": "If C is very, very large, then when minimizing this optimization objective, we're going to be highly motivated to choose a value, so that this first term is equal to zero."
  },
  {
    "index": "F16443",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では目的関数のこの最初の項を0にしよう、というコンテキストで最適化問題はどうなるかを理解していこう。その理由は、我らはCをとても大きな定数に設定すると言った。",
    "output": "So let's try to understand the optimization problem in the context of, what would it take to make this first term in the objective equal to zero, because you know, maybe we'll set C to some huge constant, and this will hope, this should give us additional intuition about what sort of hypotheses a support vector machine learns."
  },
  {
    "index": "F16444",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが、サポートベクターマシンの仮説がどんな感じか、さらなる直感を与えてくれる事を期待している。既に見たように、トレーニング手本のラベルy=1の時はいつでも最初の項を0にしたいならやるべき事はシータ転置xが1以上になるようなシータを探すという事。",
    "output": "So we saw already that whenever you have a training example with a label of y=1 if you want to make that first term zero, what you need is is to find a value of theta so that theta transpose x i is greater than or equal to 1."
  },
  {
    "index": "F16445",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に、ラベル0の手本の時はいつでもcost0が、、、cost0(z)がそのコストが0に確実になるには、シータ転置xを-1以下にしなくてはならない。つまり、我らの最適化問題を実際にパラメータを選んでこの最初の項をゼロとしたら、その後に残るのは以下の最適化問題だ。",
    "output": "And similarly, whenever we have an example, with label zero, in order to make sure that the cost, cost zero of Z, in order to make sure that cost is zero we need that theta transpose x i is less than or equal to -1."
  },
  {
    "index": "F16446",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らが最小化するのは、最初の項が0なので、C掛ける0だ。",
    "output": "So, if we think of our optimization problem as now, really choosing parameters and show that this first term is equal to zero, what we're left with is the following optimization problem."
  },
  {
    "index": "F16447",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならそれが0になるようにパラメータを選ぶのだから。",
    "output": "We're going to minimize that first term zero, so C times zero, because we're going to choose parameters so that's equal to zero, plus one half and then you know that second term and this first term is 'C' times zero, so let's just cross that out because I know that's going to be zero."
  },
  {
    "index": "F16448",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれは、シータ転置x(i)が1以上という制約条件に従う。",
    "output": "And this will be subject to the constraint that theta transpose x(i) is greater than or equal to one, if y(i) Is equal to one and theta transpose x(i) is less than or equal to minus one whenever you have a negative example and it turns out that when you solve this optimization problem, when you minimize this as a function of the parameters theta you get a very interesting decision boundary."
  },
  {
    "index": "F16449",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "陰性の手本の時にはいつでも。",
    "output": "Concretely, if you look at a data set like this with positive and negative examples, this data is linearly separable and by that, I mean that there exists, you know, a straight line, altough there is many a different straight lines, they can separate the positive and negative examples perfectly."
  },
  {
    "index": "F16450",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして結局、この最適化問題を解くと、パラメータシータの関数としてこれを最小化すると、とても興味深い決定境界が得られる。",
    "output": "For example, here is one decision boundary that separates the positive and negative examples, but somehow that doesn't look like a very natural one, right?"
  },
  {
    "index": "F16451",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に、このようなデータセットを見た時、そこには陽性と陰性のサンプルがある訳だが、このデータは線形で分離可能。",
    "output": "Or by drawing an even worse one, you know here's another decision boundary that separates the positive and negative examples but just barely."
  },
  {
    "index": "F16452",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それの意味するところは、ある直線ーーたくさんの異なる直線が有り得るが、それらが陽性と陰性のサンプルを完璧に分離する、という事。",
    "output": "But neither of those seem like particularly good choices."
  },
  {
    "index": "F16453",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "サポートベクターマシンはそうではなく、この決定境界を選ぶ、この黒で描いた奴。",
    "output": "The Support Vector Machines will instead choose this decision boundary, which I'm drawing in black."
  },
  {
    "index": "F16454",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれは、マゼンタや緑で描いた決定境界のどちらよりも、ずっとマシっぽい。",
    "output": "And that seems like a much better decision boundary than either of the ones that I drew in magenta or in green."
  },
  {
    "index": "F16455",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "黒い線の方が、よりロバストな分離器に見える。こっちの方が陽性と陰性のサンプルを分けるという仕事をうまくこなしてる。",
    "output": "The black line seems like a more robust separator, it does a better job of separating the positive and negative examples."
  },
  {
    "index": "F16456",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "数学的には、それの意味する所はこの黒い決定境界の方が大きな距離を持っているという事。",
    "output": "And mathematically, what that does is, this black decision boundary has a larger distance."
  },
  {
    "index": "F16457",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この2つの追加の線を描いてみると分かるように、黒の決定境界は、トレーニング手本の中のサンプルへの最小距離がより大きい。他方マゼンタや緑の線はトレーニング手本に恐ろしいほど近い。",
    "output": "That distance is called the margin, when I draw up this two extra blue lines, we see that the black decision boundary has some larger minimum distance from any of my training examples, whereas the magenta and the green lines they come awfully close to the training examples."
  },
  {
    "index": "F16458",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからそちらの方が、陽性と陰性を分離するには黒の線に比べるといまいちな仕事しかしていない感じがする。",
    "output": "and then that seems to do a less a good job separating the positive and negative classes than my black line."
  },
  {
    "index": "F16459",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれがSVMに、ある程度のロバストさを与えている。何故ならそれは、データを出来るだけ大きなマージンになるように分離しようとするから。",
    "output": "And so this distance is called the margin of the support vector machine and this gives the SVM a certain robustness, because it tries to separate the data with as a large a margin as possible."
  },
  {
    "index": "F16460",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからサポートベクターマシンはたまに大きなマージン分類器とも呼ばれている。そしてこれは実は、前のスライドで書いた最適化問題の帰結だ。",
    "output": "So the support vector machine is sometimes also called a large margin classifier and this is actually a consequence of the optimization problem we wrote down on the previous slide."
  },
  {
    "index": "F16461",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "おっと分かってるって。前のスライドに書いた最適化問題がどうなってこの大きなマージン分類器になってるかって思ってるんでしょ?",
    "output": "I know that you might be wondering how is it that the optimization problem I wrote down in the previous slide, how does that lead to this large margin classifier."
  },
  {
    "index": "F16462",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それをまだ説明してないってのは分かってるよ。",
    "output": "I know I haven't explained that yet."
  },
  {
    "index": "F16463",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは次のビデオでさっきの最適化問題がなんで大きなマージン分類器になるのかの、ちょっとした直感の説明をするよ。",
    "output": "And in the next video I'm going to sketch a little bit of the intuition about why that optimization problem gives us this large margin classifier."
  },
  {
    "index": "F16464",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこれは、SVMはどんな仮説を選ぶのか?を理解したければ、心にとめておく価値のある特徴だ。",
    "output": "But this is a useful feature to keep in mind if you are trying to understand what are the sorts of hypothesis that an SVM will choose."
  },
  {
    "index": "F16465",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、陽性と陰性のサンプルを、できるだけ大きなマージンになるように分離しようとする、という事。",
    "output": "That is, trying to separate the positive and negative examples with as big a margin as possible."
  },
  {
    "index": "F16466",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この直感的な考え方だと、我らが書き下した大きなマージン分類器は、C、つまり正規化の定数が凄く大きな場合の話だった。たぶん何十万とかその辺をセットした気がする。",
    "output": "I want to say one last thing about large margin classifiers in this intuition, so we wrote out this large margin classification setting in the case of when C, that regularization concept, was very large, I think I set that to a hundred thousand or something."
  },
  {
    "index": "F16467",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんなデータセットが与えられたら、陽性と陰性のサンプルを大きなマージンになるようの分離する決定境界選ぶかもしれない。",
    "output": "So given a dataset like this, maybe we'll choose that decision boundary that separate the positive and negative examples on large margin."
  },
  {
    "index": "F16468",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "SVMは実際はこの大きなマージンという見方から考えられる物よりはもうちょっと洗練されている。",
    "output": "Now, the SVM is actually sligthly more sophisticated than this large margin view might suggest."
  },
  {
    "index": "F16469",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "とりわけ、大きなマージンという特徴だけの分類器を使っている場合は、ハズレ値にその学習アルゴリズムは、より敏感となる。つまり、画面に示したような陽性のサンプルを追加してみましょう。",
    "output": "And in particular, if all you're doing is use a large margin classifier then your learning algorithms can be sensitive to outliers, so lets just add an extra positive example like that shown on the screen."
  },
  {
    "index": "F16470",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしサンプルを一つ追加したら、データを大きなマージンで分離しようとすると、こんな決定境界を学習する事になる。",
    "output": "If he had one example then it seems as if to separate data with a large margin, maybe I'll end up learning a decision boundary like that, right?"
  },
  {
    "index": "F16471",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このマゼンタの線。でも一つの外れ値、一つのサンプルに基づいて、決定境界を黒の物からマゼンダの物へと変更するのが本当に良い事なのかは結構怪しい。",
    "output": "that is the magenta line and it's really not clear that based on the single outlier based on a single example and it's really not clear that it's actually a good idea to change my decision boundary from the black one over to the magenta one."
  },
  {
    "index": "F16472",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしCが、、、正規化パラメータのCがとても大きければ、その場合はこれがSVMが実際に行う事となる。それは決定境界を黒の物からマゼンタの物へ変更する。",
    "output": "So, if C, if the regularization parameter C were very large, then this is actually what SVM will do, it will change the decision boundary from the black to the magenta one but if C were reasonably small if you were to use the C, not too large then you still end up with this black decision boundary."
  },
  {
    "index": "F16473",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、もちろん、もしデータが直線で分離出来ない場合、例えばある陽性のサンプルがここにある場合とか、または陰性のサンプルがここにある場合とか、そういう場合もSVMは正しい事をする。",
    "output": "And of course if the data were not linearly separable so if you had some positive examples in here, or if you had some negative examples in here then the SVM will also do the right thing."
  },
  {
    "index": "F16474",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの大きなマージン分類器の絵は正規化パラメータCがとても大きい場合にだけ正しい直感を与えてくれる絵だ。そして繰り返すと、この、Cは1/ラムダに対応していて、このラムダは以前にあった正規化パラメータだ。",
    "output": "And so this picture of a large margin classifier that's really, that's really the picture that gives better intuition only for the case of when the regulations parameter C is very large, and just to remind you this corresponds C plays a role similar to one over Lambda, where Lambda is the regularization parameter we had previously."
  },
  {
    "index": "F16475",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この絵は1/ラムダがとても大きい時、つまりラムダがとても小さい時にだけこのマゼンタの決定境界を得る結果となる。",
    "output": "And so it's only of one over Lambda is very large or equivalently if Lambda is very small that you end up with things like this Magenta decision boundary, but in practice when applying support vector machines, when C is not very very large like that, it can do a better job ignoring the few outliers like here."
  },
  {
    "index": "F16476",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも実際にサポートベクターマシンを適用する時はCはそんなに凄く凄く大きい値をセットしたりはしないので、このようなちょっとの外れ値を無視するにはもっと良い仕事をしてくれます。",
    "output": "And also do fine and do reasonable things even if your data is not linearly separable."
  },
  {
    "index": "F16477",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがサポートベクターマシンの文脈でバイアスと分散の話をする時に、それはちょっと後でやる予定ですが、そのあかつきには、正規化パラメータにまつわるトレードオフはもっとクリアになってるといいな。",
    "output": "But when we talk about bias and variance in the context of support vector machines which will do a little bit later, hopefully all of of this trade-offs involving the regularization parameter will become clearer at that time."
  },
  {
    "index": "F16478",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がサポートベクターマシンの関数が大きなマージン分類器として与えられたデータを大きなマージンでどのように分離しようとするかについて、ある程度の直感を与えてくれるといいな。技術的にはこの見方はパラメータのCがとても大きな時にしか成り立たないけど、その考え方はサポートベクターマシンを考える上でとても有用な考え方だ。",
    "output": "So I hope that gives some intuition about how this support vector machine functions as a large margin classifier that tries to separate the data with a large margin, technically this picture of this view is true only when the parameter C is very large, which is a useful way to think about support vector machines."
  },
  {
    "index": "F16479",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオではそこはやってない。次のビデオでは、背後にある数学をもう少しスケッチしてみる事で我らの書き下した最適化の問題がどうやって大きなマージン分類器となっているかを説明したいと思います。",
    "output": "There was one missing step in this video which is, why is it that the optimization problem we wrote down on these slides, how does that actually lead to the large margin classifier, I didn't do that in this video, in the next video I will sketch a little bit more of the math behind that to explain that separate reasoning of how the optimization problem we wrote out results in a large margin classifier."
  },
  {
    "index": "F16480",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは大きなマージンによる分類の背後にある数学について、ちょっと話しておきたい。",
    "output": "In this video, I'd like to tell you a bit about the math behind large margin classification."
  },
  {
    "index": "F16481",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオはオプショナルです。だからどうぞご自由にスキップしちゃって下さい。",
    "output": "This video is optional, so please feel free to skip it."
  },
  {
    "index": "F16482",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、また、サポートベクターマシンの最適化問題が、どのように大きなマージン分類器を導くのかについてのより良い直感も与えたいと思う。",
    "output": "It may also give you better intuition about how the optimization problem of the support vex machine, how that leads to large margin classifiers."
  },
  {
    "index": "F16483",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "始めるにあたってまずベクトルの内積について幾つかの性質を思い出しておこう。",
    "output": "In order to get started, let me first remind you of a couple of properties of what vector inner products look like."
  },
  {
    "index": "F16484",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "2つのベクトル、uとvがあったとする。こんな感じの。",
    "output": "Let's say I have two vectors U and V, that look like this."
  },
  {
    "index": "F16485",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりどちらも二次元ベクトルだ。",
    "output": "So both two dimensional vectors."
  },
  {
    "index": "F16486",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてu転置のvがどんな風になるか見てみよう。",
    "output": "Then let's see what U transpose V looks like."
  },
  {
    "index": "F16487",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "u転置vはまた、ベクトルuとvとの内積、とも言われる。",
    "output": "And U transpose V is also called the inner products between the vectors U and V."
  },
  {
    "index": "F16488",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二次元のベクトルを使う事でこの図にプロットする事が出来る。",
    "output": "Use a two dimensional vector, so I can on plot it on this figure."
  },
  {
    "index": "F16489",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、これがベクトルuとしよう。",
    "output": "So let's say that's the vector U."
  },
  {
    "index": "F16490",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これの意味する所は、水平軸上で、何かしらの値u1を取り、そして垂直軸上ではその高さは何かしらの値u2を取る、これがベクトルuの第二成分である、という事。",
    "output": "And what I mean by that is if on the horizontal axis that value takes whatever value U1 is and on the vertical axis the height of that is whatever U2 is the second component of the vector U."
  },
  {
    "index": "F16491",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、得ておくと良い一つの量として、ベクトルuのノルムがある。",
    "output": "Now, one quantity that will be nice to have is the norm of the vector U."
  },
  {
    "index": "F16492",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの、左と右の両方にある二重線が、uのノルム、言い換えると長さを表す。",
    "output": "So, these are, you know, double bars on the left and right that denotes the norm or length of U."
  },
  {
    "index": "F16493",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは単に、ベクトルuのユークリッド距離を表すに過ぎない。",
    "output": "So this just means; really the euclidean length of the vector U."
  },
  {
    "index": "F16494",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、ピタゴラスの定理により、イコールu1の二乗に足すことのu2の二乗、これをルートを取る。",
    "output": "And this is Pythagoras theorem is just equal to U1 squared plus U2 squared square root, right?"
  },
  {
    "index": "F16495",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれはベクトルuの長さで、それは実数だ。",
    "output": "And this is the length of the vector U."
  },
  {
    "index": "F16496",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この長さ、ここのこのベクトルの長さ。",
    "output": "Just say you know, what is the length of this, what is the length of this vector down here."
  },
  {
    "index": "F16497",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに今書いた矢印の長さ、これがuのノルムだ。",
    "output": "What is the length of this arrow that I just drew, is the normal view?"
  },
  {
    "index": "F16498",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さてここで、戻って、ベクトルvを見てみよう、何故なら内積を計算したいのだから。",
    "output": "Now let's go back and look at the vector V because we want to compute the inner product."
  },
  {
    "index": "F16499",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、vもまた何らかの別のベクトルで、何らかの値、v1とv2がある。",
    "output": "So V will be some other vector with, you know, some value V1, V2."
  },
  {
    "index": "F16500",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからベクトルvもこんな感じに、vベクトル。",
    "output": "And so, the vector V will look like that, towards V like so."
  },
  {
    "index": "F16501",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、戻って、uとvの内積を計算する方法を見てみよう。",
    "output": "Now let's go back and look at how to compute the inner product between U and V."
  },
  {
    "index": "F16502",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこんなやり方で出来る。",
    "output": "Here's how you can do it."
  },
  {
    "index": "F16503",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ベクトルvに対してそれをベクトルu上に射影しよう。",
    "output": "Let me take the vector V and project it down onto the vector U."
  },
  {
    "index": "F16504",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、直角に射影する、あるいは90度に射影する、そしてそれをu上に下ろす。",
    "output": "So I'm going to take a orthogonal projection or a 90 degree projection, and project it down onto U like so."
  },
  {
    "index": "F16505",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてやるべき事は、この赤い線、たった今引いたこれの長さを測る。",
    "output": "And what I'm going to do measure length of this red line that I just drew here."
  },
  {
    "index": "F16506",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この赤い線の長さをpと呼ぶ事にしよう。",
    "output": "So, I'm going to call the length of that red line P."
  },
  {
    "index": "F16507",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとpはベクトルvをベクトルuに射影した時の長さ、大きさである。",
    "output": "So, P is the length or is the magnitude of the projection of the vector V onto the vector U."
  },
  {
    "index": "F16508",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに書き記しておこう。",
    "output": "Let me just write that down."
  },
  {
    "index": "F16509",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりpはベクトルvをベクトルuに射影した時の長さである。",
    "output": "So, P is the length of the projection of the vector V onto the vector U."
  },
  {
    "index": "F16510",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして内積であるu転置vはp掛けるuのノルム、あるいは長さに等しくなる、という事を示す事が出来る。",
    "output": "And it is possible to show that unit product U transpose V, that this is going to be equal to P times the norm or the length of the vector U."
  },
  {
    "index": "F16511",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これは内積を計算する一つの方法である。",
    "output": "So, this is one way to compute the inner product."
  },
  {
    "index": "F16512",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際に幾何学を用いるとpが何なのか、uのノルムが何なのかを知る事が出来る。",
    "output": "And if you actually do the geometry figure out what P is and figure out what the norm of U is."
  },
  {
    "index": "F16513",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この結果は内積の別の計算方法と同じ答えとなるべきだ。",
    "output": "This should give you the same way, the same answer as the other way of computing unit product."
  },
  {
    "index": "F16514",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはuの転置vで、u転置はu1,u2。これは1x2行列。",
    "output": "Which is if you take U transpose V then U transposes this U1 U2, its a one by two matrix, 1 times V."
  },
  {
    "index": "F16515",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、線形代数の定理としてこれら二つの式は同じ答えとなる。",
    "output": "And so the theorem of linear algebra that these two formulas give you the same answer."
  },
  {
    "index": "F16516",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、uの転置vは、v転置uとも等しい。",
    "output": "And by the way, U transpose V is also equal to V transpose U."
  },
  {
    "index": "F16517",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから同様のプロセスを反対に行えば、vをuに射影する代わりにuをvに射影出来て、同様のプロセスだがuとvの役割を反転させると、実際にも同じ数字を得る、その数字がなんであれ。",
    "output": "Then, you know, do the same process, but with the rows of U and V reversed. And you would actually, you should actually get the same number whatever that number is."
  },
  {
    "index": "F16518",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この等式で何が起こっているのかをもう少し明らかにしておく。uのノルムは実数でpもまた実数だ。",
    "output": "And just to clarify what's going on in this equation the norm of U is a real number and P is also a real number."
  },
  {
    "index": "F16519",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからu転置vは、単なる二つの実数の、普通の掛け算で、pの長さ掛けるuのノルム。",
    "output": "And so U transpose V is the regular multiplication as two real numbers of the length of P times the normal view."
  },
  {
    "index": "F16520",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に一つ細かい話を。pのノルムを見る時には、pは実際は符合つきだ。",
    "output": "Just one last detail, which is if you look at the norm of P, P is actually signed so to the right."
  },
  {
    "index": "F16521",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはプラスにもマイナスにもなりうる。",
    "output": "And it can either be positive or negative."
  },
  {
    "index": "F16522",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ようするに、ベクトルuがこのようなベクトルだとして、vがこんなベクトルだとすると、つまりuとvのなす角が90度より大きければ、vをu上に射影すると、得られる射影はこんな感じとなり、これがpの長さとなる。",
    "output": "Then if I project V onto U, what I get is a projection it looks like this and so that length P."
  },
  {
    "index": "F16523",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの場合、u転置vは基本的には、依然としてp掛けるuのノルム、となるが、しかしこの場合は、pが負になる所だけが違う。",
    "output": "And in this case, I will still have that U transpose V is equal to P times the norm of U."
  },
  {
    "index": "F16524",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、内積においては、uとvのなす角が90度未満の時は、pは赤い線の正の長さとなり、一方でここの角度が90度以上の時は、ここのpはマイナスの長さ、マイナスの、ここの線分の長さとなる。",
    "output": "So, you know, in inner products if the angle between U and V is less than ninety degrees, then P is the positive length for that red line whereas if the angle of this angle of here is greater than 90 degrees then P here will be negative of the length of the super line of that little line segment right over there."
  },
  {
    "index": "F16525",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、二つのベクトルの内積はマイナスになる事がある、もしなす角が90度より大きければ。",
    "output": "So the inner product between two vectors can also be negative if the angle between them is greater than 90 degrees."
  },
  {
    "index": "F16526",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がベクトルの内積がどう機能するかだ。",
    "output": "So that's how vector inner products work."
  },
  {
    "index": "F16527",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのベクトルの内積の性質を用いて、ここのサポートベクターマシンの最適化の目的関数を理解しよう。",
    "output": "We're going to use these properties of vector inner product to try to understand the support vector machine optimization objective over there."
  },
  {
    "index": "F16528",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが以前に見た、サポートベクターマシンの最適化の目的関数だ。",
    "output": "Here is the optimization objective for the support vector machine that we worked out earlier."
  },
  {
    "index": "F16529",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このスライドの目的の為に一つ単純化しておこう、つまり、目的関数を分析しやすくする為に切片項を無視する事にする。",
    "output": "Just for the purpose of this slide I am going to make one simplification or once just to make the objective easy to analyze and what I'm going to do is ignore the indeceptrums."
  },
  {
    "index": "F16530",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりシータ0を無視する、つまりイコール0とする。",
    "output": "So, we'll just ignore theta 0 and set that to be equal to 0."
  },
  {
    "index": "F16531",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "またプロットしやすいように、n、つまりフィーチャーの数を2とする。",
    "output": "To make things easier to plot, I'm also going to set N the number of features to be equal to 2."
  },
  {
    "index": "F16532",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、x1とx2だけとする。",
    "output": "So, we have only 2 features, X1 and X2."
  },
  {
    "index": "F16533",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、目的関数を見てみよう。",
    "output": "Now, let's look at the objective function."
  },
  {
    "index": "F16534",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "SVMの目的関数を。",
    "output": "The optimization objective of the SVM."
  },
  {
    "index": "F16535",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フィーチャーは二つだけなので、何故ならn=2だから、これは以下のように書ける:1/2のシータ1の二乗足すことのシータ2の二乗、と。",
    "output": "This can be written, one half of theta one squared plus theta two squared."
  },
  {
    "index": "F16536",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら我らは、二つのパラメータだけ、シータ1とシータ2だけしか持たないから。",
    "output": "Because we only have two parameters, theta one and thetaa two."
  },
  {
    "index": "F16537",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれをちょっと書き換える。",
    "output": "What I'm going to do is rewrite this a bit."
  },
  {
    "index": "F16538",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを、1/2のシータ1の二乗に足す事のシータ2の二乗、そしてこれらのルートを取る。",
    "output": "I'm going to write this as one half of theta one squared plus theta two squared and the square root squared."
  },
  {
    "index": "F16539",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな事が出来る理由というのは、任意の数wに対して、wを二乗して、それ全体のルートを取ると、それはちょうどwと等しくなるから。",
    "output": "And the reason I can do that, is because for any number, you know, W, right, the square roots of W and then squared, that's just equal to W."
  },
  {
    "index": "F16540",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ルートして二乗は同じ物を返す。",
    "output": "So square roots and squared should give you the same thing."
  },
  {
    "index": "F16541",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで気づく事として、この中の項は、シータのノルムと言い換えるとベクトルシータの長さと等しい。",
    "output": "What you may notice is that this term inside is that's equal to the norm or the length of the vector theta and what I mean by that is that if we write out the vector theta like this, as you know theta one, theta two."
  },
  {
    "index": "F16542",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれによって意味する事は、ベクトルシータをこんな風に書き下すと、つまり、シータ1,シータ2,と書くと、するとこの今赤で下線を引いたこの項は、これは正確にベクトルシータの長さ、ノルムである。",
    "output": "Then this term that I've just underlined in red, that's exactly the length, or the norm, of the vector theta."
  },
  {
    "index": "F16543",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前のスライドにあった、ベクトルのノルムの定義に等しい。",
    "output": "We are calling the definition of the norm of the vector that we have on the previous line."
  },
  {
    "index": "F16544",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際に、これはベクトルシータの長さに等しい。",
    "output": "And in fact this is actually equal to the length of the vector theta, whether you write it as theta zero, theta 1, theta 2."
  },
  {
    "index": "F16545",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たとえあなたがシータ0,シータ1,シータ2で書いても、ここで仮定したようにシータ0が0だから、または単にこれはシータ1,シータ2の長さでもあるが、このスライドではシータ0を無視するのだったから、シータをこんな風にシータの、このシータ1,シータ2のノルムをこんな風に書いてみよう。",
    "output": "Or just the length of theta 1, theta 2; but for this line I am going to ignore theta 0. So let me just, you know, treat theta as this, let me just write theta, the normal theta as this theta 1, theta 2 only, but the math works out either way, whether we include theta zero here or not."
  },
  {
    "index": "F16546",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれは以下の導出には、影響しない。",
    "output": "So it's not going to matter for the rest of our derivation."
  },
  {
    "index": "F16547",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最終的に、これは私の最適化の目的関数が1/2のシータのノルムの二乗に等しい事を意味する。",
    "output": "And so finally this means that my optimization objective is equal to one half of the norm of theta squared."
  },
  {
    "index": "F16548",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりサポートベクターマシンがやる事の全ては、最適化の目的関数として、パラメータベクトルのシータのノルムの二乗、つまり長さの二乗を最小化しようと試みる。",
    "output": "So all the support vector machine is doing in the optimization objective is it's minimizing the squared norm of the square length of the parameter vector theta."
  },
  {
    "index": "F16549",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで私はこれらの項を見て、シータ転置xを見て、それらが何をやっているのかをもっと良く理解していこう。",
    "output": "Now what I'd like to do is look at these terms, theta transpose X and understand better what they're doing."
  },
  {
    "index": "F16550",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、パラメータベクトルのシータが与えられたとして、手本のxが与えられたとすると、これは何に等しいだろうか?",
    "output": "So given the parameter vector theta and given and example x, what is this is equal to?"
  },
  {
    "index": "F16551",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前のスライドでは、u転置vとはどんな物なのかを見ていった、uとvは別のベクトル。",
    "output": "And on the previous slide, we figured out what U transpose V looks like, with different vectors U and V."
  },
  {
    "index": "F16552",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで我らはそれらの定義を持ってきて、シータとx(i)にuとvの役割をさせてみよう。",
    "output": "And so we're going to take those definitions, you know, with theta and X(i) playing the roles of U and V."
  },
  {
    "index": "F16553",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとどんな絵となるかを見てみよう。",
    "output": "And let's see what that picture looks like."
  },
  {
    "index": "F16554",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、プロットし、、、さて、たった一つのトレーニング手本を見てみるとする。",
    "output": "So, let's say I plot."
  },
  {
    "index": "F16555",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "陽性の手本が一つあるとして、その場所に十字を描いておくとする。",
    "output": "Let's say I look at just a single training example."
  },
  {
    "index": "F16556",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを手本x(i)としよう。それが実際に意味する事は、横軸にある値、x(i)1、そして縦軸にある値x(i)2をプロットした、という事。",
    "output": "Let's say I have a positive example the drawing was across there and let's say that is my example X(i), what that really means is plotted on the horizontal axis some value X(i) 1 and on the vertical axis X(i) 2."
  },
  {
    "index": "F16557",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、ここまでの所、これが実際にはベクトルであるとは考えてこなかったが、これは本当は原点、0,0からこのトレーニング手本の位置までのベクトルだ。",
    "output": "And although we haven't been really thinking of this as a vector, what this really is, this is a vector from the origin from 0, 0 out to the location of this training example."
  },
  {
    "index": "F16558",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして今、パラメータベクトルがあるとして、それも同様にベクトルとしてプロットする。",
    "output": "And now let's say we have a parameter vector and I'm going to plot that as vector, as well."
  },
  {
    "index": "F16559",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どういう事かというと、シータ1をここに、シータ2をここにプロットすると、内積であるシータ転置x(i)はどうなるだろうか?",
    "output": "What I mean by that is if I plot theta 1 here and theta 2 there so what is the inner product theta transpose X(i)."
  },
  {
    "index": "F16560",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以前の手法を用いると、それを計算する方法は、手本に対して、それをパラメータベクトルのシータの上に射影する。",
    "output": "While using our earlier method, the way we compute that is we take my example and project it onto my parameter vector theta."
  },
  {
    "index": "F16561",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にこの線分の長さを見る、今赤で描いているこの長さだ。",
    "output": "And then I'm going to look at the length of this segment that I'm coloring in, in red."
  },
  {
    "index": "F16562",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれを、p上付き添字iと呼ぼう、これはi番目のトレーニング手本のパラメータベクトル、シータに対する射影を表す。",
    "output": "And I'm going to call that P superscript I to denote that this is a projection of the i-th training example onto the parameter vector theta."
  },
  {
    "index": "F16563",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると我らが得るのは、シータ転置x(i)は前のスライドにあった、以下に等しい。これはp掛けるベクトルシータのノルム、長さに等しくなる。",
    "output": "And so what we have is that theta transpose X(i) is equal to following what we have on the previous slide, this is going to be equal to P times the length of the norm of the vector theta."
  },
  {
    "index": "F16564",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれはもちろん、シータ1x1足すことのシータ2x2に等しくなる。",
    "output": "And this is of course also equal to theta 1 x1 plus theta 2 x2."
  },
  {
    "index": "F16565",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらはそれぞれどれも、シータとx(i)の内積を計算する同じように正統な方法である。",
    "output": "So each of these is, you know, an equally valid way of computing the inner product between theta and X(i)."
  },
  {
    "index": "F16566",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "オーケー。",
    "output": "Okay."
  },
  {
    "index": "F16567",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これの意味する所は、この制約条件、シータ転置x(i)が1以上か-1以下という条件、この意味する所は、制約に使われている式をp(i)掛けるxが1以上、に置き換える事が出来る。",
    "output": "What this means is that, this constrains that theta transpose X(i) be greater than or equal to one or less than minus one."
  },
  {
    "index": "F16568",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならシータ転置x(i)はp(i)掛けることのシータのノルムに等しいから。",
    "output": "Because theta transpose X(i) is equal to P(i) times the norm of theta."
  },
  {
    "index": "F16569",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、以上を最適化の目的関数に書きこむと、これが得られる、ここではシータ転置x(i)の代わりに、今度はp(i)掛けるシータのノルムとなっている。",
    "output": "So writing that into our optimization objective. This is what we get where I have, instead of theta transpose X(i), I now have this P(i) times the norm of theta."
  },
  {
    "index": "F16570",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、思い出そう、前に見てみたように、この最適化の目的関数は、1/2掛けるシータのノルムの二乗、とも書けるのだった。",
    "output": "And just to remind you we worked out earlier too that this optimization objective can be written as one half times the norm of theta squared."
  },
  {
    "index": "F16571",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では下にあるようなトレーニング手本を考えてみよう、そしてここでも、シータ0が0と等しいという単純化を継続する。",
    "output": "So, now let's consider the training example that we have at the bottom and for now, continuing to use the simplification that theta 0 is equal to 0."
  },
  {
    "index": "F16572",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "サポートベクターマシンがどんな決定境界を選ぶかを見てみよう。",
    "output": "Let's see what decision boundary the support vector machine will choose."
  },
  {
    "index": "F16573",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここにそんな選択肢の一つがある。サポートベクターマシンが仮にこの決定境界を選んだ、と仮定してみよう。",
    "output": "Here's one option, let's say the support vector machine were to choose this decision boundary."
  },
  {
    "index": "F16574",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはとても良い選択という訳では無さそうだ。何故ならマージンがとても小さいから。",
    "output": "This is not a very good choice because it has very small margins."
  },
  {
    "index": "F16575",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この決定境界はトレーニング手本のとても近くを通ってる。",
    "output": "This decision boundary comes very close to the training examples."
  },
  {
    "index": "F16576",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故サポートベクターマシンがこれを行わないかを見ていこう。",
    "output": "Let's see why the support vector machine will not do this."
  },
  {
    "index": "F16577",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このパラメータの選択において、パラメータベクトルは実際に決定境界と90度の角度をなす事が証明出来る。",
    "output": "For this choice of parameters it's possible to show that the parameter vector theta is actually at 90 degrees to the decision boundary."
  },
  {
    "index": "F16578",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この緑の決定境界はこの方向を向いたパラメートルベクトルのシータに対応している。",
    "output": "And so, that green decision boundary corresponds to a parameter vector theta that points in that direction."
  },
  {
    "index": "F16579",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、シータ0がイコール0だという単純化は、単に決定境界が原点、ここの(0,0)を通らないといけない、という事を意味しているに過ぎない。",
    "output": "And by the way, the simplification that theta 0 equals 0 that just means that the decision boundary must pass through the origin, (0,0) over there."
  },
  {
    "index": "F16580",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではここで、これが最適化の目的関数に対して何を意味しているかを見ていこう。",
    "output": "So now, let's look at what this implies for the optimization objective."
  },
  {
    "index": "F16581",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この手本がここにあったとする。",
    "output": "Let's say that this example here."
  },
  {
    "index": "F16582",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが私の最初の手本、x(1)だとしよう。",
    "output": "Let's say that's my first example, you know, X1."
  },
  {
    "index": "F16583",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この手本のパラメータシータに対する射影を見てみると、これが射影となる。",
    "output": "If we look at the projection of this example onto my parameters theta."
  },
  {
    "index": "F16584",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの赤い線分。",
    "output": "And so that little red line segment."
  },
  {
    "index": "F16585",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはイコールp1だ。",
    "output": "That is equal to P1."
  },
  {
    "index": "F16586",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれは極めて小さい。でしょ?",
    "output": "And that is going to be pretty small, right."
  },
  {
    "index": "F16587",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして同様に、このここの手本を見ると、これをx2とすると、これが二番目の手本という事だが、この手本のシータに対する射影を見てみよう。",
    "output": "And similarly, if this example here, if this happens to be X2, that's my second example."
  },
  {
    "index": "F16588",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "お分かりの通り、これをマゼンタで描く事にすると、この小さなマゼンタの線分、これがp(2)となる。",
    "output": "Then, let me draw this one in magenta. This little magenta line segment, that's going to be P2."
  },
  {
    "index": "F16589",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが二番目の手本をパラメータベクトルシータに射影した物で、それはこんな感じになる。",
    "output": "That's the projection of the second example onto my, onto the direction of my parameter vector theta which goes like this."
  },
  {
    "index": "F16590",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この小さな射影の線分は、極めて小さくなってしまう。",
    "output": "And so, this little projection line segment is getting pretty small."
  },
  {
    "index": "F16591",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "p(2)は実際には負の数だから、p(2)は反対の向きになる。",
    "output": "P2 will actually be a negative number, right so P2 is in the opposite direction."
  },
  {
    "index": "F16592",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このベクトルはパラメータベクトルから90度より大きな角度となるので、0以下となる。",
    "output": "This vector has greater than 90 degree angle with my parameter vector theta, it's going to be less than 0."
  },
  {
    "index": "F16593",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると分かる事は、これらの項、p(i)は極めて小さな数となるという事だ。",
    "output": "And so what we're finding is that these terms P(i) are going to be pretty small numbers."
  },
  {
    "index": "F16594",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり最適化の目的関数を見ると、陽性の手本に対してはp(i)掛けるシータのノルムが1以上である必要があるが、ここのp(i)、ここのp(1)がとても小さければ、シータのノルムはとても大きくならないといけない。",
    "output": "So if we look at the optimization objective and see, well, for positive examples we need P(i) times the norm of theta to be bigger than either one."
  },
  {
    "index": "F16595",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "p(1)が小さくて、p(1)掛けるシータのノルムを1以上にしたいのだから、この条件を真にする唯一の方法は、この二つの数が大きくなるには、p(1)が小さいとするなら、シータのノルムが大きくなる必要がある。",
    "output": "But if P(i) over here, if P1 over here is pretty small, that means that we need the norm of theta to be pretty large, right?"
  },
  {
    "index": "F16596",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に陰性の手本に対しては、p(2)掛けることのシータのノルムが、-1未満でなくてはならない。",
    "output": "If P1 of theta is small and we want P1 you know times in all of theta to be bigger than either one, well the only way for that to be true for the profit that these two numbers to be large if P1 is small, as we said we want the norm of theta to be large."
  },
  {
    "index": "F16597",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの手本に対してはp(2)は極めて小さい負の数である事を既に見た。だからその条件が満たされる唯一の方法も、同様に、シータのノルムが大きくなる、という事だ。",
    "output": "And similarly for our negative example, we need P2 times the norm of theta to be less than or equal to minus one."
  },
  {
    "index": "F16598",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが我らが行っている最適化の目的関数は、以下のような条件を満たしたシータの組を探すという事だった:その条件とはシータのノルムが小さくなる、という物、つまり、これはパラメータベクトルのシータにとって良い方向では無さそうだ。",
    "output": "And we saw in this example already that P2 is going pretty small negative number, and so the only way for that to happen as well is for the norm of theta to be large, but what we are doing in the optimization objective is we are trying to find a setting of parameters where the norm of theta is small, and so you know, so this doesn't seem like such a good direction for the parameter vector and theta."
  },
  {
    "index": "F16599",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これと比較して、別の決定境界を見てみよう。",
    "output": "In contrast, just look at a different decision boundary."
  },
  {
    "index": "F16600",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今度は、このSVMはこんな決定境界を選んだとしよう。",
    "output": "Here, let's say, this SVM chooses that decision boundary."
  },
  {
    "index": "F16601",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんどは随分と違う絵となる。",
    "output": "Now the is going to be very different."
  },
  {
    "index": "F16602",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが決定境界だとすると、それに対応したシータの方向はこうなる。",
    "output": "If that is the decision boundary, here is the corresponding direction for theta."
  },
  {
    "index": "F16603",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、この決定境界、垂直な直線に、対応した決定境界は、線形代数を用いて証明する事が出来るのだが、この緑の決定境界を得る方法は、それと直角のシータベクトルを持てば良い。",
    "output": "So, with the direction boundary you know, that vertical line that corresponds to it is possible to show using linear algebra that the way to get that green decision boundary is have the vector of theta be at 90 degrees to it, and now if you look at the projection of your data onto the vector x, lets say its before this example is my example of x1."
  },
  {
    "index": "F16604",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてあなたのデータをベクトルシータに射影した物を見ると、前と同様にこの手本を手本x(1)とすると、これをシータに射影すると、これがp(1)となる。",
    "output": "So when I project this on to x, or onto theta, what I find is that this is P1."
  },
  {
    "index": "F16605",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この長さがp(1)。",
    "output": "That length there is P1."
  },
  {
    "index": "F16606",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "別の手本、この手本にも同様の射影を行って、この長さ、ここが、p(2)となり、これは0未満となる。",
    "output": "The other example, that example is and I do the same projection and what I find is that this length here is a P2 really that is going to be less than 0."
  },
  {
    "index": "F16607",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして見て分かるように、ここでのp(1)とp(2)、これら射影の長さは、もっと大きくなっている。",
    "output": "And you notice that now P1 and P2, these lengths of the projections are going to be much bigger, and so if we still need to enforce these constraints that P1 of the norm of theta is phase number one because P1 is so much bigger now."
  },
  {
    "index": "F16608",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、この制約、p(1)シータのノルムが1以上、という制約がここでも満たされている必要があるとすると、p(1)はいまやもっと大きくなったのでノルムは小さくなれる。",
    "output": "The normal can be smaller."
  },
  {
    "index": "F16609",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、この意味する所は、決定境界を、左に示したような物では無く右側に示したような物を選ぶ事によって、SVMはパラメータシータのノルムをもっと小さくする事が出来る。",
    "output": "And so, what this means is that by choosing the decision boundary shown on the right instead of on the left, the SVM can make the norm of the parameters theta much smaller."
  },
  {
    "index": "F16610",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、シータのノルムを小さくする事が出来ると、それはシータのノルムの二乗も小さくなるので、それこそが、SVMが右側の仮説の方を選ぶであろう理由となる。",
    "output": "So, if we can make the norm of theta smaller and therefore make the squared norm of theta smaller, which is why the SVM would choose this hypothesis on the right instead."
  },
  {
    "index": "F16611",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのようにして、SVMは大きなマージンの分類の効果を生み出す。",
    "output": "And this is how the SVM gives rise to this large margin certification effect."
  },
  {
    "index": "F16612",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、この緑の直線を見ると、この緑の仮説を見ると、我らは、陽性と陰性の手本のシータに対する射影を大きくしたい。",
    "output": "Mainly, if you look at this green line, if you look at this green hypothesis we want the projections of my positive and negative examples onto theta to be large, and the only way for that to hold true this is if surrounding the green line."
  },
  {
    "index": "F16613",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを真に保つ、唯一の方法は、緑の線を囲むとすると、この大きなマージンが存在する事、この陽性と陰性の手本を分離する時に大きなギャップが存在する事、実際にこのギャップの大きさ、このマージンの大きさは、実際にp(1)、p(2)、p(3)などの値である。",
    "output": "There's this large margin, there's this large gap that separates positive and negative examples is really the magnitude of this gap."
  },
  {
    "index": "F16614",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのマージンを大きくする事で、これらp(1)、p(2)、p(3)などを大きくしようとする事で、SVMは結果としてシータのノルムを小さくする事が出来る、そしてそれこそが目的関数でやろうとしていた事だった。",
    "output": "The magnitude of this margin is exactly the values of P1, P2, P3 and so on."
  },
  {
    "index": "F16615",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな訳で、サポートベクターマシンは分類器のマージンを広げる事になる、何故なら、それはp(1)のノルム、それは決定境界から手本までの距離だが、それを最大化しようとするからだ。",
    "output": "And so by making the margin large, by these tyros P1, P2, P3 and so on that's the SVM can end up with a smaller value for the norm of theta which is what it is trying to do in the objective."
  },
  {
    "index": "F16616",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、我らはこれらの導出を全てパラメータのシータ0が、イコール0という単純化の元で行なってきた。その効果を簡単に伝えておく。",
    "output": "And this is why this machine ends up with enlarge margin classifiers because itss trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary."
  },
  {
    "index": "F16617",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シータ0が0と等しい時というのは、それの意味する所は、決定境界をいつも、原点を通る物だけで決めるという事に対応する。",
    "output": "Finally, we did this whole derivation using this simplification that the parameter theta 0 must be equal to 0."
  },
  {
    "index": "F16618",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな風に原点を通る。もしシータ0に、0以外の値を許すとすると、その意味する所は、原点を通らない決定境界も試していくという事を意味する。",
    "output": "The effect of that as I mentioned briefly, is that if theta 0 is equal to 0 what that means is that we are entertaining decision boundaries that pass through the origins of decision boundaries pass through the origin like that, if you allow theta zero to be non 0 then what that means is that you entertain the decision boundaries that did not cross through the origin, like that one I just drew."
  },
  {
    "index": "F16619",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今ここに描いたような物も、だ。",
    "output": "And I'm not going to do the full derivation that."
  },
  {
    "index": "F16620",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これの完全な証明をする気は無いが、まさにそっくりの大きなマージンの証明がほとんど同様に成立する。",
    "output": "It turns out that this same large margin proof works in pretty much in exactly the same way."
  },
  {
    "index": "F16621",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、ここまで長々とやってきた議論を一般化出来て、シータ0が非ゼロでも、SVMが試みるのはーーこの最適化の目的関数を持つ時に、これもふたたび、Cがとても大きい場合に対応するが、ここでも、シータ0が0で無くてもこのサポートベクターマシンは同様に陽性と陰性の手本を分離するような、大きなマージンの分離を探そうとする、という事を示す事が出来る。",
    "output": "And there's a generalization of this argument that we just went through them long ago through that shows that even when theta 0 is non 0, what the SVM is trying to do when you have this optimization objective. Which again corresponds to the case of when C is very large."
  },
  {
    "index": "F16622",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、サポートベクターマシンがどのように大きなマージンの分類器であるのか、の説明だ。",
    "output": "So that explains how this support vector machine is a large margin classifier."
  },
  {
    "index": "F16623",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、これらのSVMの考え方の幾つかを用いて、それを複雑な非線型の分類器を構築する為に適用していく方法を見ていこう。",
    "output": "In the next video we will start to talk about how to take some of these SVM ideas and start to apply them to build a complex nonlinear classifiers."
  },
  {
    "index": "F16624",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、複雑な非線形の分類器を構築する為にサポートベクタマシンを用いる事を開始しよう。",
    "output": "In this video, I'd like to start adapting support vector machines in order to develop complex nonlinear classifiers."
  },
  {
    "index": "F16625",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを行う主要なテクニックは、カーネルと呼ばれる物だ。",
    "output": "The main technique for doing that is something called kernels."
  },
  {
    "index": "F16626",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "カーネルとは何で、それをどう使うのかを見ていこう。",
    "output": "Let's see what this kernels are and how to use them."
  },
  {
    "index": "F16627",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんなトレーニングセットがあったとして、陽性と陰性の手本を区別する非線型な決定境界を見つけたいとする。たとえばこんな感じの決定境界だ。",
    "output": "If you have a training set that looks like this, and you want to find a nonlinear decision boundary to distinguish the positive and negative examples, maybe a decision boundary that looks like that."
  },
  {
    "index": "F16628",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを行う方法として一つ考えられるのは、複雑な多項式のフィーチャーを用いる事だ。",
    "output": "One way to do so is to come up with a set of complex polynomial features, right?"
  },
  {
    "index": "F16629",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを書くもう一つの方法として、後で使う事になるちょっとしたノーテーションを導入しておくと、仮説を、これを用いて決定境界を計算する物と考える事が出来る、つまり、シータ0+シータ1f1+シータ2f2+シータ3f3、などなど。",
    "output": "And another way of writing this, to introduce a level of new notation that I'll use later, is that we can think of a hypothesis as computing a decision boundary using this."
  },
  {
    "index": "F16630",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで私はこの新しいノーテーションf1,f2,f3などを、今計算している、ある種の新しいフィーチャーを示すのに用いていて、つまりf1は単にx1で、f2はイコールx2で、f3はこれに等しい。",
    "output": "Where I'm going to use this new denotation f1, f2, f3 and so on to denote these new sort of features that I'm computing, so f1 is just X1, f2 is equal to X2, f3 is equal to this one here."
  },
  {
    "index": "F16631",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "f4はイコールx1の二乗で、f5はx2の二乗、などとなり、そして以前に見たように、これらの高次の多項式を含めていくのは、もっと多くのフィーチャーにする為の一つの方法だ。ここで疑問に思うのは、この高次の多項式以外のフィーチャーの選択が無いものか?",
    "output": "So, f4 is equal to X1 squared where f5 is to be x2 squared and so on and we seen previously that coming up with these high order polynomials is one way to come up with lots more features, the question is, is there a different choice of features or is there better sort of features than this high order polynomials because you know it's not clear that this high order polynomial is what we want, and what we talked about computer vision talk about when the input is an image with lots of pixels."
  },
  {
    "index": "F16632",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら、これらの高次の多項式は、我らの欲しい物かいまいち分からないし、コンピュータビジョンの、入力が画像のピクセルの場合などを議論したが、そこでは高次の多項式を用いるのは計算量的にとても高くつく事を見た、何故ならその場合は、大量の高次の多項式が存在する事になるから。",
    "output": "We also saw how using high order polynomials becomes very computationally expensive because there are a lot of these higher order polynomial terms."
  },
  {
    "index": "F16633",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、もっと別の、もっと良いフィーチャーの選択肢で、この種の仮説の形に代入出来るような物は無いものか?",
    "output": "So, is there a different or a better choice of the features that we can use to plug into this sort of hypothesis form."
  },
  {
    "index": "F16634",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでこれが、新しいフィーチャーf1,f2,f3を定義する一つのやり方だ。",
    "output": "So, here is one idea for how to define new features f1, f2, f3."
  },
  {
    "index": "F16635",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このスライドでは、三つのフィーチャーだけを定義するが、現実の問題では、もっと多くの数のフィーチャーを定義する事になる。",
    "output": "On this line I am going to define only three new features, but for real problems we can get to define a much larger number."
  },
  {
    "index": "F16636",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがここでは、フィーチャーx1とx2の張る空間に対し、ここでx0、切片項のx0は省略する事にするが、このx1x2の空間の中で、幾つかの点を手動で選ぶ、そしてこれらの点を、これをl(1)と呼び、さらに別の点を選んで、これをl(2)と呼ぶ、そして三番目を選んで、これをl(3)と呼ぶ事にする。ここでは、これら三つの点は手動で選ぶ事にする。",
    "output": "But here's what I'm going to do in this phase of features X1, X2, and I'm going to leave X0 out of this, the interceptor X0, but in this phase X1 X2, I'm going to just, you know, manually pick a few points, and then call these points l1, we are going to pick a different point, let's call that l2 and let's pick the third one and call this one l3, and for now let's just say that I'm going to choose these three points manually."
  },
  {
    "index": "F16637",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの点をランドマークと呼ぶ事にする。つまりランドマークの1,2,3。",
    "output": "I'm going to call these three points line ups, so line up one, two, three."
  },
  {
    "index": "F16638",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ある手本xが与えられた時に、最初のフィーチャーf1をなんらかの類似度の指標、トレーニング手本xと最初のランドマークとの類似度を定義する、そしてここでは具体的に以下のような類似度の指標を用いる、eの指数乗のマイナスのx-l1の距離を二乗して、2シグマ二乗で割る。",
    "output": "What I'm going to do is define my new features as follows, given an example X, let me define my first feature f1 to be some measure of the similarity between my training example X and my first landmark and this specific formula that I'm going to use to measure similarity is going to be this is E to the minus the length of X minus l1, squared, divided by two sigma squared."
  },
  {
    "index": "F16639",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のオプショナルのビデオをあなたが見ているかどうかは分からないが、このノーテーション、これはベクトルwの長さを表す。",
    "output": "So, depending on whether or not you watched the previous optional video, this notation, you know, this is the length of the vector W."
  },
  {
    "index": "F16640",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこの、ここのこれは、このx-l(1)は、これは実体は単なるユークリッド距離の二乗で、点xとランドマークl1とのユークリッド距離だ。",
    "output": "And so, this thing here, this X minus l1, this is actually just the euclidean distance squared, is the euclidean distance between the point x and the landmark l1."
  },
  {
    "index": "F16641",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この事は後でもうちょっと詳しく見る事にする。",
    "output": "We will see more about this later."
  },
  {
    "index": "F16642",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがとにかく、これが最初のフィーチャーだ。二番目のフィーチャーf2はl(2)とxがどれだけ類似しているかを測るsimilarity関数で、これは以下のような関数で定義される。",
    "output": "But that's my first feature, and my second feature f2 is going to be, you know, similarity function that measures how similar X is to l2 and the game is going to be defined as the following function."
  },
  {
    "index": "F16643",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはeの、以下による指数乗で、その指数はマイナスのxと二番目のランドマークとの間の距離の二乗を分子として、割ることの2シグマ二乗が指数となる。同様にf3はxとl3の間の類似度で、それはイコールまた同様の式となる。",
    "output": "This is E to the minus of the square of the euclidean distance between X and the second landmark, that is what the enumerator is and then divided by 2 sigma squared and similarly f3 is, you know, similarity between X and l3, which is equal to, again, similar formula."
  },
  {
    "index": "F16644",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのsimilarity関数とは数学的な用語では、カーネル関数と呼ばれる物だ。",
    "output": "And what this similarity function is, the mathematical term for this, is that this is going to be a kernel function."
  },
  {
    "index": "F16645",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここで具体的にカーネルとして使っている関数は実際にはガウスカーネルと呼ばれる。",
    "output": "And the specific kernel I'm using here, this is actually called a Gaussian kernel."
  },
  {
    "index": "F16646",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この式、この具体的なsimilarity関数の選択を、ガウスカーネルと呼ぶ。",
    "output": "And so this formula, this particular choice of similarity function is called a Gaussian kernel."
  },
  {
    "index": "F16647",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが用語的には、これらの様々なsimilarity関数を抽象的にカーネルと呼び、similarity関数には様々な物がありうる。そしてこの例で私が与えた物は、ガウスカーネルと呼ばれる。",
    "output": "But the way the terminology goes is that, you know, in the abstract these different similarity functions are called kernels and we can have different similarity functions and the specific example I'm giving here is called the Gaussian kernel."
  },
  {
    "index": "F16648",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは後に、別のカーネルの例を見る事になるだろう。",
    "output": "We'll see other examples of other kernels."
  },
  {
    "index": "F16649",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが現時点では、これらは単なる類似度の関数と思っておけば良い。",
    "output": "But for now just think of these as similarity functions."
  },
  {
    "index": "F16650",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな事情により、xとlの間のsimilarity(類似度)と書く代わりに、時々これを、カーネルを表す小文字のkを用いて、xとランドマークの間のカーネル、と記述する事もある。",
    "output": "And so, instead of writing similarity between X and l, sometimes we also write this a kernel denoted you know, lower case k between x and one of my landmarks all right."
  },
  {
    "index": "F16651",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではカーネルが実際に何をやるかを見てみよう、そして何故この種の類似度関数が、これらの式が筋が通っているのかを見てみよう。",
    "output": "So let's see what a criminals actually do and why these sorts of similarity functions, why these expressions might make sense."
  },
  {
    "index": "F16652",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでまず最初のランドマーク、ランドマークl(1)に対して、これはさっきの図で私が選んだ点の一つだが、xとl(1)の間のカーネルの類似度は、この式で与えられる。",
    "output": "My landmark l1, which is one of those points I chose on my figure just now. So the similarity of the kernel between x and l1 is given by this expression."
  },
  {
    "index": "F16653",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この分子が実際にどうなるのかを一応書いておくと、分子はこのように、j=1からnまでの、ある種の距離の和となる。",
    "output": "Just to make sure, you know, we are on the same page about what the numerator term is, the numerator can also be written as a sum from J equals 1 through N on sort of the distance."
  },
  {
    "index": "F16654",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、xとベクトルlの各要素に渡って取った距離だ。",
    "output": "So this is the component wise distance between the vector X and the vector l."
  },
  {
    "index": "F16655",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここでも、これらのスライド上では、x0を無視する。",
    "output": "And again for the purpose of these slides I'm ignoring X0."
  },
  {
    "index": "F16656",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり切片項のx0を、これはいつも=1だが、単に無視する事にする。",
    "output": "So just ignoring the intercept term X0, which is always equal to 1."
  },
  {
    "index": "F16657",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、これがxとランドマークの類似度を用いてカーネルを計算する方法だ。",
    "output": "So, you know, this is how you compute the kernel with similarity between X and a landmark."
  },
  {
    "index": "F16658",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこの関数が何をするかを見ていこう。",
    "output": "So let's see what this function does."
  },
  {
    "index": "F16659",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "xがランドマークの一つと近いとしてみよう。",
    "output": "Suppose X is close to one of the landmarks."
  },
  {
    "index": "F16660",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、この分子にあるユークリッド距離の式は0に近い値となる。",
    "output": "Then this euclidean distance formula and the numerator will be close to 0, right."
  },
  {
    "index": "F16661",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この項、このxとl(1)の距離、この距離は0に近い値となる。するとf1、フィーチャーは、だいたい近似的にeの指数乗である所の-0の二乗が分子で、2シグマ二乗が分母。",
    "output": "So, that is this term here, the distance was great, the distance using X and 0 will be close to zero, and so f1, this is a simple feature, will be approximately E to the minus 0 and then the numerator squared over 2 is equal to squared so that E to the 0, E to minus 0, E to 0 is going to be close to one."
  },
  {
    "index": "F16662",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここに近似の記号を用いたのは距離は厳密に0では無いかもしれないからだが、しかしxがランドマークに近ければ、この項は0に近くなり、f1は1に近くなる。",
    "output": "And I'll put the approximation symbol here because the distance may not be exactly 0, but if X is closer to landmark this term will be close to 0 and so f1 would be close 1."
  },
  {
    "index": "F16663",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "逆に、xがl(1)から遠く離れている時には、この最初のフィーチャーf1はeの指数乗である所の何らかの大きな数字の二乗に、割ることの2シグマ二乗で、つまりeのマイナスの、大きな数による累乗は0に近い値となる。",
    "output": "Conversely, if X is far from 01 then this first feature f1 will be E to the minus of some large number squared, divided divided by two sigma squared and E to the minus of a large number is going to be close to 0."
  },
  {
    "index": "F16664",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらのフィーチャーが行う事は、xとランドマークの一つとがどれだけ似ているかを測っているのだ。そしてフィーチャーfはxがランドマークに近い時に1に近い値となり、xがランドマークから遥か離れていると、0に近い値となる。",
    "output": "So what these features do is they measure how similar X is from one of your landmarks and the feature f is going to be close to one when X is close to your landmark and is going to be 0 or close to zero when X is far from your landmark."
  },
  {
    "index": "F16665",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前のスライドにあったこれらのランドマークそれぞれに対し、私は三つのランドマーク、l(1)、l(2)、l(3)を描いたのだが、これらのランドマークそれぞれに対し、新しいフィーチャーf1,f2,f3を定義する。",
    "output": "On the previous line, I drew three landmarks, l1, l2,l3. Each of these landmarks, defines a new feature f1, f2 and f3."
  },
  {
    "index": "F16666",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ある所与のトレーニング手本xに対し三つのフィーチャーf1,f2,f3を計算出来る、さっき書いた三つのランドマークが所与の時には。",
    "output": "That is, given the the training example X, we can now compute three new features: f1, f2, and f3, given, you know, the three landmarks that I wrote just now."
  },
  {
    "index": "F16667",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まずこの類似度(similarity)関数から見ていこう。そしてそれをプロットしてみて、それが実際にどんな形かをもっと良く理解していこう。",
    "output": "But first, let's look at this exponentiation function, let's look at this similarity function and plot in some figures and just, you know, understand better what this really looks like."
  },
  {
    "index": "F16668",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例では、二つのフィーチャーx1とx2があるとする。",
    "output": "For this example, let's say I have two features X1 and X2."
  },
  {
    "index": "F16669",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最初のランドマークのl(1)は、地点3,5にあるとする。",
    "output": "And let's say my first landmark, l1 is at a location, 3 5."
  },
  {
    "index": "F16670",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてシグマ二乗はここでは1としよう。",
    "output": "So and let's say I set sigma squared equals one for now."
  },
  {
    "index": "F16671",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこのフィーチャーをプロットすると、この図が得られる。",
    "output": "If I plot what this feature looks like, what I get is this figure."
  },
  {
    "index": "F16672",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この垂直の軸、表面の高さは、f1の値だ。",
    "output": "So the vertical axis, the height of the surface is the value of f1 and down here on the horizontal axis are, if I have some training example, and there is x1 and there is x2."
  },
  {
    "index": "F16673",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この下の、水平軸達は、あるトレーニング手本があった時に、それはx1とx2を持っている訳だが、あるトレーニング手本が与えられた時に、このx1とx2の値のトレーニング手本、この点の上の曲面までの高さが、対応するf1の値を示している、そしてこの下に、同じ図を等高線プロットを用いて描いた物を示しておいた。x1が横軸でx2が縦軸。",
    "output": "Given a certain training example, the training example here which shows the value of x1 and x2 at a height above the surface, shows the corresponding value of f1 and down below this is the same figure I had showed, using a quantifiable plot, with x1 on horizontal axis, x2 on horizontal axis and so, this figure on the bottom is just a contour plot of the 3D surface."
  },
  {
    "index": "F16674",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "見て分かるように、xがぴったり3,5の時には、f1の値は1を取る、何故ならそこが最大となる点だからだ。そしてxが離れていくと、xが離れていくにつれて、このフィーチャーは0に近い値を取る。",
    "output": "You notice that when X is equal to 3 5 exactly, then we the f1 takes on the value 1, because that's at the maximum and X moves away as X goes further away then this feature takes on values that are close to 0."
  },
  {
    "index": "F16675",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれは実際に、f1はxが最初のランドマークとどれだけ近いかを測る指標、フィーチャーで、それは0と1の間の値をxが最初のランドマークl(1)がどれだけ近いかに応じて取る。",
    "output": "And so, this is really a feature, f1 measures, you know, how close X is to the first landmark and if varies between 0 and one depending on how close X is to the first landmark l1."
  },
  {
    "index": "F16676",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、もう一つこのスライドで見せたい事は、このパラメータ、シグマ二乗を変えた時の効果だ。",
    "output": "Now the other was due on this slide is show the effects of varying this parameter sigma squared."
  },
  {
    "index": "F16677",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シグマ二乗はガウスカーネルのパラメータで、それを変えていく事で、ちょっと違った効果が得られる。",
    "output": "So, sigma squared is the parameter of the Gaussian kernel and as you vary it, you get slightly different effects."
  },
  {
    "index": "F16678",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シグマ二乗をイコール0.5にセットしよう。そしてどうなるか見てみよう。",
    "output": "Let's set sigma squared to be equal to 0.5 and see what we get."
  },
  {
    "index": "F16679",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シグマ二乗に0.5をセットすると、カーネルは基本的には似たような形だが、こぶの幅が狭くなる。",
    "output": "We set sigma square to 0.5, what you find is that the kernel looks similar, except for the width of the bump becomes narrower."
  },
  {
    "index": "F16680",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "等高線もちょっと縮む。",
    "output": "The contours shrink a bit too."
  },
  {
    "index": "F16681",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりシグマ二乗がイコール0.5なら、xイコール35から始まって、そこから離れていくにつれて、フィーチャーf1は0に、より急速に落ちていく。",
    "output": "So if sigma squared equals to 0.5 then as you start from X equals 3 5 and as you move away, then the feature f1 falls to zero much more rapidly and conversely, if you has increase since where three in that case and as I move away from, you know l."
  },
  {
    "index": "F16682",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "逆に、シグマ2乗を3に増加させると、その場合は、点lから離れていくと、この点がlだが、これはl(1)で、これは座標35だ。",
    "output": "So this point here is really l, right, that's l1 is at location 3 5, right."
  },
  {
    "index": "F16683",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはここ。",
    "output": "So it's shown up here."
  },
  {
    "index": "F16684",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてシグマ二乗が大きくなると、するとl(1)から離れていくに連れてフィーチャーの値はよりゆっくりと、低下していく。",
    "output": "And if sigma squared is large, then as you move away from l1, the value of the feature falls away much more slowly."
  },
  {
    "index": "F16685",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このフィーチャーの定義が与えられたとして、どんな仮説が学習出来るか見てみよう。",
    "output": "So, given this definition of the features, let's see what source of hypothesis we can learn."
  },
  {
    "index": "F16686",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ある手本xが与えられたとして、これらのフィーチャーf1,f2,f3を計算する、そして仮説はシータ0+シータ1f1+シータ2f2...が0以上の時に1を予測する。",
    "output": "Given the training example X, we are going to compute these features f1, f2, f3 and a hypothesis is going to predict one when theta 0 plus theta 1 f1 plus theta 2 f2, and so on is greater than or equal to 0."
  },
  {
    "index": "F16687",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この具体例だと、既に学習アルゴリズムを見つけていて、どうにかして既にこれらのパラメータの値を得ていたとする。",
    "output": "For this particular example, let's say that I've already found a learning algorithm and let's say that, you know, somehow I ended up with these values of the parameter."
  },
  {
    "index": "F16688",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてシータ0=-0.5、シータ1=1,シータ2=1,シータ3=0だったとする。そして私がやりたい事は、以下のようなマゼンダの点の位置の手本の時には、何が起こるか?",
    "output": "So if theta 0 equals minus 0.5, theta 1 equals 1, theta 2 equals 1, and theta 3 equals 0 And what I want to do is consider what happens if we have a training example that takes has location at this magenta dot, right where I just drew this dot over here."
  },
  {
    "index": "F16689",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たった今描いたここの点。トレーニング手本xがあったら、仮説は何を予測するだろうか?",
    "output": "So let's say I have a training example X, what would my hypothesis predict?"
  },
  {
    "index": "F16690",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この式を見てみると、トレーニング手本xはl(1)の近くなので、f1は1に近い値となる、トレーニング手本xはl(2)とl(3)からは遠く離れているから、f2は0に近い値で、f3も0に近い値だから、この式を見ると、シータ0+シータ1掛ける1+シータ2掛ける何かしらの値で、その値は完全に0では無いが、0に近いとしよう。",
    "output": "Because my training example X is close to l1, we have that f1 is going to be close to 1 the because my training example X is far from l2 and l3 I have that, you know, f2 would be close to 0 and f3 will be close to 0."
  },
  {
    "index": "F16691",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして足すことのシータ3掛ける何かしら0に近い値。",
    "output": "Then plus theta 3 times something close to 0."
  },
  {
    "index": "F16692",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはイコール、これらの値を代入すると、すると-0.5+1*1でこれは1、などと以下同様に、これはイコール0.5となり、これは0以上だ。",
    "output": "So, that gives minus 0.5 plus 1 times 1 which is 1, and so on. Which is equal to 0.5 which is greater than or equal to 0."
  },
  {
    "index": "F16693",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この点はy=1と予測する事になる、何故ならこれが0以上となったから。",
    "output": "So, at this point, we're going to predict Y equals 1, because that's greater than or equal to zero."
  },
  {
    "index": "F16694",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、別の点をとってみよう。",
    "output": "Now let's take a different point."
  },
  {
    "index": "F16695",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今度は別の点、これを別の色でシアン色で描く事にしよう、それをここの点とする。これがトレーニング手本xだとすると、同様の計算を行っていくと、f1,f2,f3は全て0に近い値となる、すると、シータ0+シータ1f1+...と続いていき、これはイコール-0.5となる。",
    "output": "Now lets' say I take a different point, I'm going to draw this one in a different color, in cyan say, for a point out there, if that were my training example X, then if you make a similar computation, you find that f1, f2, Ff3 are all going to be close to 0."
  },
  {
    "index": "F16696",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならシータ0は-0.5で、f1,f2,f3は全てゼロだから。",
    "output": "And so, we have theta 0 plus theta 1, f1, plus so on and this will be about equal to minus 0.5, because theta 0 is minus 0.5 and f1, f2, f3 are all zero."
  },
  {
    "index": "F16697",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれは0.5となり、これは0未満だ。",
    "output": "So this will be minus 0.5, this is less than zero."
  },
  {
    "index": "F16698",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ここの点は、y=0と予測する事になる。",
    "output": "And so, at this point out there, we're going to predict Y equals zero."
  },
  {
    "index": "F16699",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしあなたが自分で、さまざまな点で計算してみると、l(2)に近い点のトレーニング手本は、これもまたy=1を予測する事になる、と納得出来ると思う。",
    "output": "And if you do this yourself for a range of different points, be sure to convince yourself that if you have a training example that's close to L2, say, then at this point we'll also predict Y equals one."
  },
  {
    "index": "F16700",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして結局、最終的にあなたがやってるのは、この空間を見回して、l(1)とl(2)に近い点なら、陽性と予測する事になる。",
    "output": "And in fact, what you end up doing is, you know, if you look around this boundary, this space, what we'll find is that for points near l1 and l2 we end up predicting positive."
  },
  {
    "index": "F16701",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてl(1)とl(2)から遠い点は、これら二つのランドマークの両方から遠い点は、そのクラスはイコール0だと予測する事になる。",
    "output": "And for points far away from l1 and l2, that's for points far away from these two landmarks, we end up predicting that the class is equal to 0."
  },
  {
    "index": "F16702",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから最終的にやってる事は、この仮説の決定境界はこんな感じの物となり、この赤い決定境界の内側をy=1と予測し、外側をy=0と予測する。",
    "output": "As so, what we end up doing,is that the decision boundary of this hypothesis would end up looking something like this where inside this red decision boundary would predict Y equals 1 and outside we predict Y equals 0."
  },
  {
    "index": "F16703",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり以上が、ランドマークとカーネル関数を定義する事で、我らは極めて複雑な、非線型の決定境界を学習させる事が出来る、例えば私がさっき描いたような、二つのランドマークに近い時に陽性と予想し、どのランドマークからも遠く離れている時には陰性と予想するような。",
    "output": "We can learn pretty complex non-linear decision boundary, like what I just drew where we predict positive when we're close to either one of the two landmarks. And we predict negative when we're very far away from any of the landmarks."
  },
  {
    "index": "F16704",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がカーネルという考え方と、それをサポートベクターマシンで使う方法だ、それはランドマークを用いて、これらの新しいフィーチャーを定義し、類似度関数を用いてより複雑な非線型の分類器を学習する。",
    "output": "And so this is part of the idea of kernels of and how we use them with the support vector machine, which is that we define these extra features using landmarks and similarity functions to learn more complex nonlinear classifiers."
  },
  {
    "index": "F16705",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上でカーネルとそれを用いてサポートベクターマシンで新しいフィーチャーを定義する方法が分かっただろうか。",
    "output": "So hopefully that gives you a sense of the idea of kernels and how we could use it to define new features for the Support Vector Machine."
  },
  {
    "index": "F16706",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、まだ幾つか答えてない疑問がある。",
    "output": "But there are a couple of questions that we haven't answered yet."
  },
  {
    "index": "F16707",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ目は、どうやってこれらのランドマークを得たらいいだろうか?",
    "output": "One is, how do we get these landmarks?"
  },
  {
    "index": "F16708",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのランドマークを、どうやって選んだらいいだろう?",
    "output": "How do we choose these landmarks?"
  },
  {
    "index": "F16709",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもう一つは、別の類似度関数として、我らが話してきた物、ガウスカーネル以外にはどんな物が使えるだろうか?",
    "output": "And another is, what other similarity functions, if any, can we use other than the one we talked about, which is called the Gaussian kernel."
  },
  {
    "index": "F16710",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオではこれらの問いに対する解答を与えていく。そして全てを組み合わせてサポートベクターマシンがカーネルとあわせてどのように複雑な非線形の関数を学習する事が出来るかを見ていく。",
    "output": "In the next video we give answers to these questions and put everything together to show how support vector machines with kernels can be a powerful way to learn complex nonlinear functions."
  },
  {
    "index": "F16711",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでの所、我らはSVMについてかなり抽象的なレベルで議論してきた。",
    "output": "So far we've been talking about SVMs in a fairly abstract level."
  },
  {
    "index": "F16712",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、SVMを実際に走らせるにあたり、あるいは使うにあたり実際にやらなくてはいけない事について議論したい。",
    "output": "In this video I'd like to talk about what you actually need to do in order to run or to use an SVM."
  },
  {
    "index": "F16713",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "サポートベクタマシンアルゴリズムは特定の最適化問題を導く。",
    "output": "The support vector machine algorithm poses a particular optimization problem."
  },
  {
    "index": "F16714",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが以前のビデオで簡単に述べた通り、パラメータシータを解くソフトウェアを自力で書く事は、全く推奨しない。",
    "output": "But as I briefly mentioned in an earlier video, I really do not recommend writing your own software to solve for the parameter's theta yourself."
  },
  {
    "index": "F16715",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんにちでは、我々の中には、自分で行列の逆行列を求めるコードや数のルートを計算するコードを書こう、とする人はかなり少数派、いやほとんど居ないと言っても良かろう。",
    "output": "So just as today, very few of us, or maybe almost essentially none of us would think of writing code ourselves to invert a matrix or take a square root of a number, and so on."
  },
  {
    "index": "F16716",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは、これを行うライブラリ関数を呼ぶ。",
    "output": "We just, you know, call some library function to do that."
  },
  {
    "index": "F16717",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に、SVMの最適化問題を解くソフトウェアは、とても複雑だ。そして、本質的には数値計算の最適化だけを何年もやってるような研究者達、というのも存在している。",
    "output": "In the same way, the software for solving the SVM optimization problem is very complex, and there have been researchers that have been doing essentially numerical optimization research for many years."
  },
  {
    "index": "F16718",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、あなたはこれを行ってくれる素晴らしいソフトウェアライブラリやソフトウェアパッケージを見つける事が出来る。",
    "output": "So you come up with good software libraries and good software packages to do this."
  },
  {
    "index": "F16719",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "であるから、自分で実装しよう、なんて考えないでそれらの高度に最適化されたソフトウェアライブラリのどれかを使う事を強く推奨する。",
    "output": "And then strongly recommend just using one of the highly optimized software libraries rather than trying to implement something yourself."
  },
  {
    "index": "F16720",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "良いソフトウェアライブラリは、たくさんあるから。",
    "output": "And there are lots of good software libraries out there."
  },
  {
    "index": "F16721",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私がもっとも良く使う物を二つ上げておくと、liblinearとlibsvmだが、これを行うソフトウェアライブラリはこれ以外にもたくさんあって、あなたが学習アルゴリズムを実装している言語にリンク出来る物もたくさんあるはずだ。",
    "output": "The two that I happen to use the most often are the linear SVM but there are really lots of good software libraries for doing this that you know, you can link to many of the major programming languages that you may be using to code up learning algorithm."
  },
  {
    "index": "F16722",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたはSVMの最適化ソフトウェアを独自実装すべきでは無いのだけれども、あたながやるべき事も、ちょっとは存在する。",
    "output": "Even though you shouldn't be writing your own SVM optimization software, there are a few things you need to do, though."
  },
  {
    "index": "F16723",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず最初に、パラメータCを選ばなくてはいけない。",
    "output": "First is to come up with with some choice of the parameter's C."
  },
  {
    "index": "F16724",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この事については前回のビデオの中で、バイアス/バリアンスの性質の話の時に言及した。",
    "output": "We talked a little bit of the bias/variance properties of this in the earlier video."
  },
  {
    "index": "F16725",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二番目に、あなたが使うカーネル、あるいは類似度関数を選ぶ必要がある。",
    "output": "Second, you also need to choose the kernel or the similarity function that you want to use."
  },
  {
    "index": "F16726",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "選択の一例としては、カーネルを使わない、という決断もあり得る。",
    "output": "So one choice might be if we decide not to use any kernel."
  },
  {
    "index": "F16727",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてカーネルを使わない、というアイデアは線形(リニア)カーネルとも呼ばれる。",
    "output": "And the idea of no kernel is also called a linear kernel."
  },
  {
    "index": "F16728",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、誰かが「俺はSVMをリニアカーネルで使うぜ」と言ったら、それの意味する事は、SVMを、カーネル無しで使う、という事。そしてそれは、SVMを単にシータ転置xで使うというSVMのバージョンという事になる。",
    "output": "So if someone says, I use an SVM with a linear kernel, what that means is you know, they use an SVM without using without using a kernel and it was a version of the SVM that just uses theta transpose X, right, that predicts 1 theta 0 plus theta 1 X1 plus so on plus theta N, X N is greater than equals 0."
  },
  {
    "index": "F16729",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この「線形」カーネルという言葉は、これは標準的な「線形」の分類器を与えるSVMのバージョンだ、と考える事が出来る。",
    "output": "This term linear kernel, you can think of this as you know this is the version of the SVM that just gives you a standard linear classifier."
  },
  {
    "index": "F16730",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、これはある種の問題にはリーズナブルな選択となる。そして世の中にはたくさんのソフトウェアのライブラリ、たくさんあるそんなソフトウェアライブラリの中の一つの例としては、liblinearのような物とかが挙げられるが、とにかく、SVMをカーネル無しで、またの名を線形カーネルでトレーニング出来るソフトウェアライブラリはたくさんある。",
    "output": "So that would be one reasonable choice for some problems, and you know, there would be many software libraries, like linear, was one example, out of many, one example of a software library that can train an SVM without using a kernel, also called a linear kernel."
  },
  {
    "index": "F16731",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、何故これを使いたい、と思うのだろうか?",
    "output": "So, why would you want to do this?"
  },
  {
    "index": "F16732",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたが、大量のフィーチャーを持っていて、つまりnが大きくて、そしてm、トレーニング手本の数が、小さければ、その時はつまり、あなたは大量のフィーチャーを持っているという事で、xはRnとかRn+1という事だ。",
    "output": "If you have a large number of features, if N is large, and M the number of training examples is small, then you know you have a huge number of features that if X, this is an X is an Rn, Rn +1."
  },
  {
    "index": "F16733",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから既にたくさんのフィーチャーがあって、トレーニングセットのサイズは小さい、この場合は単に線形の決定境界へのフィッティングを望むかもしれない。そしてあまり複雑な非線型の関数にフィッティングしたいとは思わないだろう。",
    "output": "So if you have a huge number of features already, with a small training set, you know, maybe you want to just fit a linear decision boundary and not try to fit a very complicated nonlinear function, because might not have enough data."
  },
  {
    "index": "F16734",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら、十分なデータが無いと、オーバーフィッティングのリスクがあるからだ。とても次数の高いフィーチャー空間上のとても複雑な関数にフィッティングしようとしていて、しかもトレーニングセットのサイズが小さい時には。",
    "output": "And you might risk overfitting, if you're trying to fit a very complicated function in a very high dimensional feature space, but if your training set sample is small."
  },
  {
    "index": "F16735",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこそ、この状況はカーネルを使わない、または同じ事だが線形カーネルと呼ばれる物を使う、という選択をするのが合理的となる状況だ。",
    "output": "So this would be one reasonable setting where you might decide to just not use a kernel, or equivalents to use what's called a linear kernel."
  },
  {
    "index": "F16736",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたが行いそうな二番目の選択肢はこのガウスカーネルだ。これは以前に得た物だ。",
    "output": "A second choice for the kernel that you might make, is this Gaussian kernel, and this is what we had previously."
  },
  {
    "index": "F16737",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしあなたがこれを選ぶなら、あなたが行わなくてはいけないさらなる選択としては、パラメータであるシグマ二乗を選ぶ事だ。これはバイアス-バリアンスのトレードオフの話でちょっと触れたが、シグマ二乗が大きければ、高バイアス、低バリアンスの傾向の分類器となり、逆にシグマ二乗が小さければ、高バリアンス、低バイアスの傾向の分類器となる。",
    "output": "And if you do this, then the other choice you need to make is to choose this parameter sigma squared when we also talk a little bit about the bias variance tradeoffs of how, if sigma squared is large, then you tend to have a higher bias, lower variance classifier, but if sigma squared is small, then you have a higher variance, lower bias classifier."
  },
  {
    "index": "F16738",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、どんな時にガウスカーネルを使う事になるだろう?",
    "output": "So when would you choose a Gaussian kernel?"
  },
  {
    "index": "F16739",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはだね。あなたの元々のフィーチャーxが、Rnとして、nが小さい時、そして理想的には、mが大きい時。",
    "output": "Well, if your omission of features X, I mean Rn, and if N is small, and, ideally, you know, if n is large, right, so that's if, you know, we have say, a two-dimensional training set, like the example I drew earlier."
  },
  {
    "index": "F16740",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、例えば二次元のトレーニングセットだとして、これは以前に書いた物と同様という事だが、つまりn=2だとして、だがトレーニングセットの総数は多いという場合。",
    "output": "So n is equal to 2, but we have a pretty large training set."
  },
  {
    "index": "F16741",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、かなりたくさんのトレーニング手本を描いた。するとその場合、もっと複雑な非線形の決定境界にフィットするようなカーネルを使いたくなる場合がある。",
    "output": "So, you know, I've drawn in a fairly large number of training examples, then maybe you want to use a kernel to fit a more complex nonlinear decision boundary, and the Gaussian kernel would be a fine way to do this."
  },
  {
    "index": "F16742",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてガウスカーネルはこれを行う良い方法だ。このビデオの最後の方で線形カーネルを使う場合はどういう時かガウスカーネルを選ぶのはどういう場合か、などについて、話す機会がある。",
    "output": "I'll say more towards the end of the video, a little bit more about when you might choose a linear kernel, a Gaussian kernel and so on."
  },
  {
    "index": "F16743",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、もし具体的にガウスカーネルを使う、と決めたとすると、あなたがやるべき事はこれだ。",
    "output": "But if concretely, if you decide to use a Gaussian kernel, then here's what you need to do."
  },
  {
    "index": "F16744",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたの使うサポートベクタマシンのソフトウェアパッケージによっては、カーネル関数を、あるいは類似度関数をあなたが実装する必要があるものもある。",
    "output": "Depending on what support vector machine software package you use, it may ask you to implement a kernel function, or to implement the similarity function."
  },
  {
    "index": "F16745",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたがoctaveやMATLABのSVMの実装を用いるなら、カーネルの具体的なフィーチャーを計算する関数をあなたが提供する必要があるかもしれない。",
    "output": "So if you're using an octave or MATLAB implementation of an SVM, it may ask you to provide a function to compute a particular feature of the kernel."
  },
  {
    "index": "F16746",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは本当は、fの下付き添字iをある特定の値に対して計算する、ここでこのfは単一の実数値にすぎない、だからここでは、f(i)と書くべきかもしれない。",
    "output": "So this is really computing f subscript i for one particular value of i, where f here is just a single real number, so maybe I should move this better written f(i), but what you need to do is to write a kernel function that takes this input, you know, a training example or a test example whatever it takes in some vector X and takes as input one of the landmarks and but only I've come down X1 and X2 here, because the landmarks are really training examples as well."
  },
  {
    "index": "F16747",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがあなたがしなくてはいけない事は、これ、トレーニング手本やテスト手本や、とにかくなんであれあるベクトルxを入力として受け取り、そしてランドマークの一つを入力として受け取るが、ここでは私は単にこれらをx1,x2と呼ぶ、何故ならランドマークも実際はトレーニング手本だから。",
    "output": "But what you need to do is write software that takes this input, you know, X1, X2 and computes this sort of similarity function between them and return a real number."
  },
  {
    "index": "F16748",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたがやらなくてはいけない事は、この入力、x1とx2をうけとりそれらの間の、この種の類似度関数を計算して一つの実数を返すソフトウェアを書く事だ。",
    "output": "And so what some support vector machine packages do is expect you to provide this kernel function that take this input you know, X1, X2 and returns a real number."
  },
  {
    "index": "F16749",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで幾つかのサポートベクタマシンのパッケージはあなたがこの種のカーネル関数、x1とx2を入力に取って実数を返すような関数を提供する事を期待していて、そしてそこから、自動的に全てのフィーチャーを生成して、つまり自動的に、xに対しあなたの書いたこの関数を用いてf1,f2,...とf(m)まで、マッピングする。そして全てのフィーチャーを生成して、そこからサポートベクタマシンを訓練する。",
    "output": "And then it will take it from there and it will automatically generate all the features, and so automatically take X and map it to f1, f2, down to f(m) using this function that you write, and generate all the features and train the support vector machine from there."
  },
  {
    "index": "F16750",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、あなたは時々、この関数を自分で提供してやる必要がある。",
    "output": "But sometimes you do need to provide this function yourself."
  },
  {
    "index": "F16751",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ガウスカーネルを使う時でも、幾つかのSVM実装はガウスカーネルも含んでいて、そしてそれ以外にも幾つかのカーネルを含んでいる物だ。",
    "output": "Other if you are using the Gaussian kernel, some SVM implementations will also include the Gaussian kernel and a few other kernels as well, since the Gaussian kernel is probably the most common kernel."
  },
  {
    "index": "F16752",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ガウスカーネルは恐らくもっとも一般的なカーネルなので、ガウスカーネルと線形カーネルは本当にぶっちぎりで大人気の二大カーネルなので、たぶんあるだろう。",
    "output": "Gaussian and linear kernels are really the two most popular kernels by far. Just one implementational note."
  },
  {
    "index": "F16753",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたのフィーチャーが、とても異なるスケールだったら、ガウスカーネルを使う前にフィーチャースケーリングをするのが大切だ。",
    "output": "If you have features of very different scales, it is important to perform feature scaling before using the Gaussian kernel."
  },
  {
    "index": "F16754",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたがxとlの間のノルムを計算している所を、想像してみよう。このここの項は、こっちの分子の項だ。",
    "output": "If you imagine the computing the norm between X and l, right, so this term here, and the numerator term over there."
  },
  {
    "index": "F16755",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが行う事は、xとlの間のノルムで、それは実際には、、、、ベクトルvを計算してみよう。vはイコールx-lだとする。",
    "output": "What this is doing, the norm between X and l, that's really saying, you know, let's compute the vector V, which is equal to X minus l."
  },
  {
    "index": "F16756",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、このベクトルvのノルムを計算する。それはxとlとの間の差だ。",
    "output": "And then let's compute the norm does vector V, which is the difference between X."
  },
  {
    "index": "F16757",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでxはRn、いや、Rn+1だが、x0は無視する事にする。",
    "output": "Because here X is in Rn, or Rn plus 1, but I'm going to ignore, you know, X0."
  },
  {
    "index": "F16758",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでxはRnだとしよう、左側を二乗するとこれは正しくなる。",
    "output": "So, let's pretend X is an Rn, square on the left side is what makes this correct."
  },
  {
    "index": "F16759",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "別の書き方で書くと、これはx1-l(1)の二乗に、足すことのx2-l(2)の二乗に、足すことの点点点、、、と足すことのxn-l(n)の二乗。",
    "output": "So this is equal to that, right? And so written differently, this is going to be X1 minus l1 squared, plus x2 minus l2 squared, plus dot dot dot plus Xn minus ln squared."
  },
  {
    "index": "F16760",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして今、あなたのフィーチャーがとても異なったレンジの値を取るとしよう。",
    "output": "And now if your features take on very different ranges of value."
  },
  {
    "index": "F16761",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "住宅の価格を予測する例で考えてみると、データは住宅に関するデータで、xは、最初のフィーチャーx1に関しては何千平方フィートの範囲を取り、しかし二番目のフィーチャーx2は寝室の数とすると、寝室の数は範囲としては1部屋から5部屋程度だろう、するとx1-l(1)は巨大になりえて、1000平方フィートとかに成り得るが、一方でx2-l(2)はもっと小さくなり、その場合には、この項、これらの距離は、本質的にはほとんど住居のサイズで支配されてしまい、寝室の数はほとんど無視されてしまう。",
    "output": "But if your second feature, X2 is the number of bedrooms."
  },
  {
    "index": "F16762",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうする事で、SVMが様々なフィーチャーについて同じような程度の関心を払うようにし、この例の住居のサイズのように他のフィーチャーを塗りつぶしてしまうような事が無い事を、保証出来る。",
    "output": "So if this is in the range of one to five bedrooms, then X1 minus l1 is going to be huge."
  },
  {
    "index": "F16763",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたがサポートベクタマシンを試みる時に、おそらくもっとも一般的であろう二つのカーネルは、線形(linear)カーネル、つまりカーネル無しか、または既に話したガウスカーネルだろう。",
    "output": "This could be like a thousand squared, whereas X2 minus l2 is going to be much smaller and if that's the case, then in this term, those distances will be almost essentially dominated by the sizes of the houses and the number of bathrooms would be largely ignored. As so as, to avoid this in order to make a machine work well, do perform future scaling."
  },
  {
    "index": "F16764",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてちょっと注意を。",
    "output": "And that will sure that the SVM gives, you know, comparable amount of attention to all of your different features, and not just to in this example to size of houses were big movement here the features."
  },
  {
    "index": "F16765",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "類似度関数ならば、どんな物でもカーネルとして使える、という訳では無い。",
    "output": "When you try a support vector machines chances are by far the two most common kernels you use will be the linear kernel, meaning no kernel, or the Gaussian kernel that we talked about."
  },
  {
    "index": "F16766",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてガウスカーネル、線形カーネル、そしてたまに他の人が使ってるのを見かけるその他のカーネルでも、それらはある技術的条件を満たす必要がある。",
    "output": "And just one note of warning which is that not all similarity functions you might come up with are valid kernels."
  },
  {
    "index": "F16767",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはMercerの定理と言われる物だ。これが必要となる理由は、サポートベクタマシンのアルゴリズム、全てのSVMの実装は、大量の賢い数値計算的な最適化のトリックを使ってる、パラメータのシータについて効率的に解く為に。",
    "output": "And the Gaussian kernel and the linear kernel and other kernels that you sometimes others will use, all of them need to satisfy a technical condition."
  },
  {
    "index": "F16768",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてオリジナルのSVMの設計において、主な関心を、Mercerの定理と呼ばれる技術的条件を満たすカーネルだけに集中する、という決定がなされた。",
    "output": "It's called Mercer's Theorem and the reason you need to this is because support vector machine algorithms or implementations of the SVM have lots of clever numerical optimization tricks."
  },
  {
    "index": "F16769",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうする事で、それらSVMパッケージの全てで、それら全てのSVMソフトウェアパッケージで大きなクラスの最適化を用いる事が出来るようになり、パラメータのシータをとても早く得られるようになる。",
    "output": "In order to solve for the parameter's theta efficiently and in the original design envisaged, those are decision made to restrict our attention only to kernels that satisfy this technical condition called Mercer's Theorem."
  },
  {
    "index": "F16770",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、結局の所、ほとんどの人が使うのは線形カーネルかガウスカーネルだ。",
    "output": "And what that does is, that makes sure that all of these SVM packages, all of these SVM software packages can use the large class of optimizations and get the parameter theta very quickly."
  },
  {
    "index": "F16771",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがそれ以外にも幾つかMercerの定理を満たすカーネルが存在する。そして他の人々がそれらを使ってる所をあなたがみかける事もあるかもしれない。",
    "output": "So, what most people end up doing is using either the linear or Gaussian kernel, but there are a few other kernels that also satisfy Mercer's theorem and that you may run across other people using, although I personally end up using other kernels you know, very, very rarely, if at all."
  },
  {
    "index": "F16772",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私個人としては、それ以外のカーネルを使う事ってほんとにほんとーに稀で、全く無いって事は無いにせよほとんど無いけど。",
    "output": "Just to mention some of the other kernels that you may run across."
  },
  {
    "index": "F16773",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、xとlの間の類似度は、どう定義されるかというと、たくさんの選択肢があるが、xの転置lの二乗という定義がありうる。",
    "output": "And for that the similarity between X and l is defined as, there are a lot of options, you can take X transpose l squared."
  },
  {
    "index": "F16774",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは、xとlがどれだけ類似してるかを測る、一つの指標である。",
    "output": "So, here's one measure of how similar X and l are."
  },
  {
    "index": "F16775",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "xとlがお互いにとても近いと、内積は大きくなる傾向にある。",
    "output": "If X and l are very close with each other, then the inner product will tend to be large."
  },
  {
    "index": "F16776",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このカーネルはちょっと珍しいカーネルで、そんなには使われていない。",
    "output": "And so, you know, this is a slightly unusual kernel."
  },
  {
    "index": "F16777",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがたまには、使ってる人に出くわす事もあるかもしれない。",
    "output": "That is not used that often, but you may run across some people using it."
  },
  {
    "index": "F16778",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは多項式カーネルの一つのバージョンだ。",
    "output": "This is one version of a polynomial kernel."
  },
  {
    "index": "F16779",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もう一つ別の物としては、x転置lの三乗。",
    "output": "Another is X transpose l cubed."
  },
  {
    "index": "F16780",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらは全て、多項式カーネルの例だ。",
    "output": "These are all examples of the polynomial kernel."
  },
  {
    "index": "F16781",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして4乗。つまり、多項式カーネルは実際には二つのパラメータがある。",
    "output": "X transpose l plus maybe a number different then one 5 and, you know, to the power of 4 and so the polynomial kernel actually has two parameters."
  },
  {
    "index": "F16782",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つはどの数をここに足すか?",
    "output": "One is, what number do you add over here?"
  },
  {
    "index": "F16783",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは0でも良い。",
    "output": "It could be 0."
  },
  {
    "index": "F16784",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に、ここの、多項式の次数(degree)がある。つまり累乗の指数。",
    "output": "This is really plus 0 over there, as well as what's the degree of the polynomial over there."
  },
  {
    "index": "F16785",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの数字。",
    "output": "So the degree power and these numbers."
  },
  {
    "index": "F16786",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多項式カーネルのより一般的な形としては、x転置l足すことのある定数(const)そして指数のある次数(degree)。つまりこれら二つの両方が多項式カーネルのパラメータだ。",
    "output": "And the more general form of the polynomial kernel is X transpose l, plus some constant and then to some degree in the X1 and so both of these are parameters for the polynomial kernel."
  },
  {
    "index": "F16787",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多項式カーネルは、ほとんどいつでもガウスカーネルよりもより劣ったパフォーマンスを示す。だからそんなに使われない。",
    "output": "So the polynomial kernel almost always or usually performs worse."
  },
  {
    "index": "F16788",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもたまに出くわす事がある物ではある。通常、これはxとlが全て0より大きい、非負のデータに対してのみ使われる。",
    "output": "And the Gaussian kernel does not use that much, but this is just something that you may run across."
  },
  {
    "index": "F16789",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうする事で、これらの内積が決して負にならない事を担保している。",
    "output": "Usually it is used only for data where X and l are all strictly non negative, and so that ensures that these inner products are never negative."
  },
  {
    "index": "F16790",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれは、xとlがとてもお互いに近いとそれら同士の内積が大きくなるだろう、という直感を捕捉している。",
    "output": "And this captures the intuition that X and l are very similar to each other, then maybe the inter product between them will be large."
  },
  {
    "index": "F16791",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他の性質もあるんだが、何にせよ人々はあまり使ってない。",
    "output": "They have some other properties as well but people tend not to use it much."
  },
  {
    "index": "F16792",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてさらに、あなたがやっている事に応じて、ある種より難解なカーネルも存在して、あなたがそれらに遭遇する事もあるかもしれない。",
    "output": "And then, depending on what you're doing, there are other, sort of more esoteric kernels as well, that you may come across."
  },
  {
    "index": "F16793",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "文字列カーネルというのがある、これはあなたの入力データがテキスト文字列なり、それ以外でも文字列の時には、時々使われる事がある。",
    "output": "You know, there's a string kernel, this is sometimes used if your input data is text strings or other types of strings."
  },
  {
    "index": "F16794",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "カイ二乗カーネルという物もあり、ヒストグラム交差カーネルという物などもある。",
    "output": "There are things like the chi-square kernel, the histogram intersection kernel, and so on."
  },
  {
    "index": "F16795",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらは異なるオブジェクト同士の類似度を測る事が出来るある種より難解なカーネルだ。",
    "output": "There are sort of more esoteric kernels that you can use to measure similarity between different objects."
  },
  {
    "index": "F16796",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、あなたがある種のテキスト分類の問題に挑んでいる時には、入力xが文字列となるので、我らは二つの文字列の間の類似度を、文字列カーネルを用いる事で知りたいかもしれない。だが私個人としては、これらのより難解なカーネルは全く見ないとは言わないが、とても稀にしか見ない。",
    "output": "So for example, if you're trying to do some sort of text classification problem, where the input x is a string then maybe we want to find the similarity between two strings using the string kernel, but I personally you know end up very rarely, if at all, using these more esoteric kernels."
  },
  {
    "index": "F16797",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "カイ2乗カーネルを、たぶん私は生涯で、一回しか使ったことが無いと思うし、ヒストグラムカーネルは人生で一回か二回だったと思う。",
    "output": "I think I might have use the chi-square kernel, may be once in my life and the histogram kernel, may be once or twice in my life."
  },
  {
    "index": "F16798",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私は実は、文字列カーネルを自分で使った事は無い。",
    "output": "I've actually never used the string kernel myself."
  },
  {
    "index": "F16799",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "しかしもしあなたが別のアプリケーションでこれらに遭遇したら、ちょろっとwebを検索して、Googleで軽く検索するなり、Bingで軽く検索すれば、これらのカーネルの定義を見つける事が出来るはずだ。",
    "output": "But in case you've run across this in other applications. You know, if you do a quick web search we do a quick Google search or quick Bing search you should have found definitions that these are the kernels as well."
  },
  {
    "index": "F16800",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオで話しておきたい細かい話が最後に二つ。",
    "output": "So just two last details I want to talk about in this video."
  },
  {
    "index": "F16801",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つはマルチクラスの分類問題について。",
    "output": "One in multiclass classification."
  },
  {
    "index": "F16802",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "4つのクラスがあったとしよう、あるいはもっと一般的にk個のクラスの出力があったとしよう。",
    "output": "So, you have four classes or more generally 3 classes output some appropriate decision bounday between your multiple classes."
  },
  {
    "index": "F16803",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ほとんどのSVM、とは言い過ぎだが多くのSVMパッケージは、既にマルチクラスの分類の機能がビルドインされている。",
    "output": "Most SVM, many SVM packages already have built-in multiclass classification functionality."
  },
  {
    "index": "F16804",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしあなたがそのようなパッケージを使っているなら、あなたは単に、ビルドインの機能を使うだけで、うまくいくはずだ。",
    "output": "So if your using a pattern like that, you just use the both that functionality and that should work fine."
  },
  {
    "index": "F16805",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうで無ければ、これを行う方法の一つには、1vsallの手法を使う事だ、これについてはロジスティック回帰を作ってる時に議論した。",
    "output": "Otherwise, one way to do this is to use the one versus all method that we talked about when we are developing logistic regression."
  },
  {
    "index": "F16806",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたがやる事は、K個のSVMをトレーニングする、もしあなたがKクラスあったとしてだが、一度に一つのクラスをそれ以外のクラスと区別するように。",
    "output": "So what you do is you trade kSVM's if you have k classes, one to distinguish each of the classes from the rest."
  },
  {
    "index": "F16807",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、あなたにパラメータベクトルのシータ1を与え、これはクラスy=1をそれ以外のクラスから区別しようと試みる物で、そして次に二番目のパラメータベクトル、シータ2を与え、これはy=2を陽性のクラスとした時にそしてそれ以外全てを陰性のクラスとした時に得られるパラメータで、そうやってパラメータベクトルシータKまで、これは最後のクラスKをそれ以外と区別するパラメータベクトルだ。",
    "output": "And this would give you k parameter vectors, so this will give you, upi lmpw."
  },
  {
    "index": "F16808",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、これはまさにロジスティック回帰でやった1vsALL法だ。",
    "output": "theta 1, which is trying to distinguish class y equals one from all of the other classes, then you get the second parameter, vector theta 2, which is what you get when you, you know, have y equals 2 as the positive class and all the others as negative class and so on up to a parameter vector theta k, which is the parameter vector for distinguishing the final class key from anything else, and then lastly, this is exactly the same as the one versus all method we have for logistic regression."
  },
  {
    "index": "F16809",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこではあなたはシータ転置xが最大になったクラスiを予想とするのだった。",
    "output": "Where we you just predict the class i with the largest theta transpose X."
  },
  {
    "index": "F16810",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がマルチクラスの分類の方法だ。",
    "output": "So let's multiclass classification designate."
  },
  {
    "index": "F16811",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもっと一般的なケースとしては、だいたいは、かなり良い確率で、あなたが何のソフトウェアパッケージを使ってるにせよ、だいたいは、かなりの確率で、そこには既にマルチクラスの機能がビルドインされている。だからこの事について思い悩む必要は無い。",
    "output": "For the more common cases that there is a good chance that whatever software package you use, you know, there will be a reasonable chance that are already have built in multiclass classification functionality, and so you don't need to worry about this result."
  },
  {
    "index": "F16812",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、我らはサポートベクタマシンをロジスティック回帰から始めて、コスト関数をちょっと変更していく事で開発してきた。",
    "output": "Finally, we developed support vector machines starting off with logistic regression and then modifying the cost function a little bit."
  },
  {
    "index": "F16813",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオで最後にやりたい事は、これらの二つのアルゴリズムのどちらをいつ使うのか、についてちょっと話しておきたい。",
    "output": "The last thing we want to do in this video is, just say a little bit about."
  },
  {
    "index": "F16814",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "nをフィーチャーの数として、mをトレーニング手本の数とする。",
    "output": "when you will use one of these two algorithms, so let's say n is the number of features and m is the number of training examples."
  },
  {
    "index": "F16815",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、いつどちらのアルゴリズムを使い、いつもう一つのアルゴリズムを使うべきだろう?",
    "output": "So, when should we use one algorithm versus the other?"
  },
  {
    "index": "F16816",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしnがトレーニングセットサイズとの相対的に大きければ、例を挙げると、フィーチャーの数がこれがmよりずっと大きくて、そしてこれが、例えば、テキスト分類の問題だとすると、フィーチャーのベクトルの次元は、知らんけど例えば1万とかになりがちで、そしてトレーニングセットのサイズが10から、せいぜい1000くらいまでの間。",
    "output": "Well, if n is larger relative to your training set size, so for example, if you take a business with a number of features this is much larger than m and this might be, for example, if you have a text classification problem, where you know, the dimension of the feature vector is I don't know, maybe, 10 thousand."
  },
  {
    "index": "F16817",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スパム分類の問題を想像してみよう。e-mailスパムでは、1万の単語に対応した1万個のフィーチャーがあるとする、でも例えば10通からせいぜい1000通とかのトレーニング手本しか無いとする。",
    "output": "So, imagine a spam classification problem, where email spam, where you have 10,000 features corresponding to 10,000 words but you have, you know, maybe 10 training examples or maybe up to 1,000 examples."
  },
  {
    "index": "F16818",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、nはmとの相対で考えると大きい。その場合に私が普段やるのは、ロジスティック回帰を使うか、カーネル無しの、あるいは線形カーネルのSVMを使う。",
    "output": "So if n is large relative to m, then what I would usually do is use logistic regression or use it as the m without a kernel or use it with a linear kernel."
  },
  {
    "index": "F16819",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら、そんなにたくさんのフィーチャーで、トレーニングセットが小さいと、線形関数はたぶんいい感じだと思う、そしてとても複雑な非線形の関数をフィッティングするには十分なデータを持ってない。",
    "output": "Because, if you have so many features with smaller training sets, you know, a linear function will probably do fine, and you don't have really enough data to fit a very complicated nonlinear function."
  },
  {
    "index": "F16820",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、もしnが小さくて、mが中くらいの大きさだと、ここでnとしては1から1000くらいをイメージしてて、1はとても小さい場合だが、せいぜい1000フィーチャーくらいまで、そしてトレーニング手本の総数が10から1万手本くらいの間のどこか位。",
    "output": "Now if is n is small and m is intermediate what I mean by this is n is maybe anywhere from 1 - 1000, 1 would be very small."
  },
  {
    "index": "F16821",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "5万くらいまでの間でもいいかもしれない。",
    "output": "Maybe up to 50,000 examples."
  },
  {
    "index": "F16822",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "mがとても大きければ、1万くらい。だが100万は行かない。",
    "output": "If m is pretty big like maybe 10,000 but not a million."
  },
  {
    "index": "F16823",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、mがもし中くらいのサイズの時は、しばしばSVMに線形カーネルが、うまく機能するだろう。",
    "output": "So if m is an intermediate size then often an SVM with a linear kernel will work well."
  },
  {
    "index": "F16824",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合については以前にも話した、一つの具体例で。",
    "output": "We talked about this early as well, with the one concrete example, this would be if you have a two dimensional training set."
  },
  {
    "index": "F16825",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは、もしあなたが二次元のトレーニングセットの時に、つまり、もしn=2の時で、たくさんのトレーニング手本が描いてある時などは、ガウスカーネルは、陽性と陰性のクラスを分離するのにきわめて良い働きをするだろう。",
    "output": "So, if n is equal to 2 where you have, you know, drawing in a pretty large number of training examples."
  },
  {
    "index": "F16826",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "興味のある三番目の状況としては、nが小さくてmが大きい時。",
    "output": "One third setting that's of interest is if n is small but m is large."
  },
  {
    "index": "F16827",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばnは1から1000までとか、もうちょっと大きくてもいいかもしれないが、しかしmは5万くらいから100とかまでとか、5万から10万、100万、200万、と、とても大きなトレーニングセットのサイズの時、この場合は、ガウスカーネルのSVMは走らせるといくらか遅い。",
    "output": "You have very very large training set sizes, right. So if this is the case, then a SVM of the Gaussian Kernel will be somewhat slow to run."
  },
  {
    "index": "F16828",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんにちのSVMパッケージでは、ガウスカーネルを使うと、5万くらいなら、たぶん問題無い。",
    "output": "Today's SVM packages, if you're using a Gaussian Kernel, tend to struggle a bit."
  },
  {
    "index": "F16829",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、100万個のトレーニング手本だと、または10万個のトレーニング手本で、かつnが大きい値の時には、こんにちのSVMパッケージはとても良い物だが、それでも大量の、本当に大量のトレーニングセットの時にはガウスカーネルを使うとちょっとだけ苦戦するかもしれない。",
    "output": "If you have, you know, maybe 50 thousands okay, but if you have a million training examples, maybe or even a 100,000 with a massive value of m."
  },
  {
    "index": "F16830",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合には、私が普段やるのは、手動でフィーチャーを増やしてロジスティック回帰を使うか、カーネル無しのSVMを試す。",
    "output": "Today's SVM packages are very good, but they can still struggle a little bit when you have a massive, massive trainings that size when using a Gaussian Kernel."
  },
  {
    "index": "F16831",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしあなたがこのスライドを見てロジスティック回帰とカーネル無しのSVMがこれらの場所がいつもペアで一緒に出てきてると思ったなら、それには理由があるのだ。",
    "output": "So in that case, what I would usually do is try to just manually create have more features and then use logistic regression or an SVM without the Kernel."
  },
  {
    "index": "F16832",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰もカーネル無しのSVMも通常は極めて似た振る舞いをする。そして極めて似たパフォーマンスを与える。",
    "output": "And in case you look at this slide and you see logistic regression or SVM without a kernel."
  },
  {
    "index": "F16833",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが実装の詳細によっては、片方がもう片方よりも効率的だったりはするかもしれない。",
    "output": "There's a reason for that, is that logistic regression and SVM without the kernel, those are really pretty similar algorithms and, you know, either logistic regression or SVM without a kernel will usually do pretty similar things and give pretty similar performance, but depending on your implementational details, one may be more efficient than the other."
  },
  {
    "index": "F16834",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、これらのアルゴリズムの一つを適用する時、ロジスティック回帰とカーネル無しのSVMのどちらも、だいたい同様に機能する。",
    "output": "But, where one of these algorithms applies, logistic regression where SVM without a kernel, the other one is to likely to work pretty well as well."
  },
  {
    "index": "F16835",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがSVMの威力は、複雑な非線型の関数を学習する為に別のカーネルを使う時に発揮される。",
    "output": "But along with the power of the SVM is when you use different kernels to learn complex nonlinear functions."
  },
  {
    "index": "F16836",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの形態では、1万手本くらいまでとか、5万手本くらいまでとかで、そしてフィーチャーの数は、ちょうど良い程度に多い、これはとても良くある形態だ。",
    "output": "And this regime, you know, when you have maybe up to 10,000 examples, maybe up to 50,000. And your number of features, this is reasonably large."
  },
  {
    "index": "F16837",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの状況こそ、カーネルと共にサポートベクタマシンを用いる事が、光り輝く形態だ。",
    "output": "That's a very common regime and maybe that's a regime where a support vector machine with a kernel kernel will shine."
  },
  {
    "index": "F16838",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ロジスティック回帰を使ってではもっと大変になるような事をやる事が出来る。",
    "output": "You can do things that are much harder to do that will need logistic regression."
  },
  {
    "index": "F16839",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、ニューラルネットワークが適合するのはどういう形態か?",
    "output": "And finally, where do neural networks fit in?"
  },
  {
    "index": "F16840",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "うーん、これら全ての問題に対して、これらの別々の形態全てに対して、良く設計されたニューラルネットワークは、うまく機能するだろう。",
    "output": "Well for all of these problems, for all of these different regimes, a well designed neural network is likely to work well as well."
  },
  {
    "index": "F16841",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つの欠点としては、あるいはニューラルネットワークを使わない事がある理由の一つには、これらの問題の中には、ニューラルネットワークは訓練するのに遅いという場合がある。",
    "output": "The one disadvantage, or the one reason that might not sometimes use the neural network is that, for some of these problems, the neural network might be slow to train."
  },
  {
    "index": "F16842",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもしとても良いSVM実装パッケージがあるなら、そちらの方が速く走りうる、ニューラルネットワークよりずっと速く走る。",
    "output": "But if you have a very good SVM implementation package, that could run faster, quite a bit faster than your neural network."
  },
  {
    "index": "F16843",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、これは証明しなかったが、SVMの持つ最適化の問題は、凸最適化問題である事が知られている。だから良いSVM最適化ソフトウェアパッケージは必ずグローバル最小か、それに近い値を探してくれる。",
    "output": "And, although we didn't show this earlier, it turns out that the optimization problem that the SVM has is a convex optimization problem and so the good SVM optimization software packages will always find the global minimum or something close to it."
  },
  {
    "index": "F16844",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからSVMでは、ローカル最適の問題を心配する必要は無い。",
    "output": "And so for the SVM you don't need to worry about local optima."
  },
  {
    "index": "F16845",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "現実問題としては、ニューラルネットワークでもローカル最適はそんなに大きな問題では無い。だけど、、、あー、とにかくSVMを使えば、心配事が一つ減ると言えば減る。",
    "output": "In practice local optima aren't a huge problem for neural networks but they all solve, so this is one less thing to worry about if you're using an SVM."
  },
  {
    "index": "F16846",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして問題によっては、ニューラルネットワークの方が遅いかもしれない、特にこの形態の問題では、SVMよりも遅いかもしれない。",
    "output": "And depending on your problem, the neural network may be slower, especially in this sort of regime than the SVM."
  },
  {
    "index": "F16847",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに示したガイドラインがちょっと曖昧だなぁ、と思っても、もしあなたが何かの問題に際して、ガイドラインがいまいち曖昧だなぁ、と思っても、私もいまだに完全には確信が持てる訳では無い、私はこっちのアルゴリズムを使うべきか?",
    "output": "In case the guidelines they gave here, seem a little bit vague and if you're looking at some problems, you know, the guidelines are a bit vague, I'm still not entirely sure, should I use this algorithm or that algorithm, that's actually okay."
  },
  {
    "index": "F16848",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは実際には問題無い。私が機械学習の問題に直面する時には、時々、どのアルゴリズムを使うのがベストなのか、はっきりしない事がある。",
    "output": "When I face a machine learning problem, you know, sometimes its actually just not clear whether that's the best algorithm to use, but as you saw in the earlier videos, really, you know, the algorithm does matter, but what often matters even more is things like, how much data do you have."
  },
  {
    "index": "F16849",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが以前のビデオで見た通り、アルゴリズムは確かに重要だけど、しかししばしばもっと重要なのは、どれだけの量のデータを持ってるか、そしてどれだけあなたの技術力が高いか、エラー分析や学習アルゴリズムをデバッグするのをどれだけうまく行えるか、新しいフィーチャーをどうデザインする方法をどれだけうまく見いだせるか、学習アルゴリズムに渡す別のフィーチャーをどれだけ上手く見いだせるか、などだったりする。",
    "output": "And how skilled are you, how good are you at doing error analysis and debugging learning algorithms, figuring out how to design new features and figuring out what other features to give you learning algorithms and so on."
  },
  {
    "index": "F16850",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてしばしば、これらの事項はあなたがロジスティック回帰を使うかSVMを使うか、よりももっと重要な事だろう。",
    "output": "And often those things will matter more than what you are using logistic regression or an SVM."
  },
  {
    "index": "F16851",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、言ってきたように、SVMはいまだに、もっとも強力な学習アルゴリズムの一つだと広く受け止められている。そしてSVMが複雑な非線型の関数をとても良く学習出来るような事態がある。",
    "output": "But having said that, the SVM is still widely perceived as one of the most powerful learning algorithms, and there is this regime of when there's a very effective way to learn complex non linear functions."
  },
  {
    "index": "F16852",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして私も実際、、、ロジスティック回帰、ニューラルネットワーク、SVM、これら三つの学習アルゴリズムを使えるなら、私が思うにあなたはとても広範な応用に対し最先端の機械学習システムを構築するのにとても良い位置に居る。そしてこれはもう一つ、あなたの武器庫に備えておくのに良い、とてもパワフルなツールという訳だ。",
    "output": "And so I actually, together with logistic regressions, neural networks, SVM's, using those to speed learning algorithms you're I think very well positioned to build state of the art you know, machine learning systems for a wide region for applications and this is another very powerful tool to have in your arsenal."
  },
  {
    "index": "F16853",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この一つは、シリコンバレー中で業界中で、アカデミックな世界で、多くの高パフォーマンスの機械学習システムを作るのに使われている物だ。",
    "output": "One that is used all over the place in Silicon Valley, or in industry and in the Academia, to build many high performance machine learning system."
  },
  {
    "index": "F16854",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、クラスタリングについて議論しはじめたい。",
    "output": "In this video, I'd like to start to talk about clustering."
  },
  {
    "index": "F16855",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはエキサイティングな話だ、というのはこれが我らの初めての教師なし学習の例だからだ。そこではラベル付けされたデータではなく、ラベル付けされていないデータから学習する。",
    "output": "This will be exciting, because this is our first unsupervised learning algorithm, where we learn from unlabeled data instead from labelled data."
  },
  {
    "index": "F16856",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、教師なし学習ってなんだろう?",
    "output": "So, what is unsupervised learning?"
  },
  {
    "index": "F16857",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この講義の始めの所で簡単に述べたけど、ここで再び教師有り学習と対比してみるのは有益に思う。",
    "output": "I briefly talked about unsupervised learning at the beginning of the class but it's useful to contrast it with supervised learning."
  },
  {
    "index": "F16858",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その為に、これは典型的な教師有り問題だ、そこではラベルづけされたトレーニングセットが与えられてゴールは陽性の手本と陰性の手本を分離する決定境界を探す事。",
    "output": "So, here's a typical supervised learning problem where we're given a labeled training set and the goal is to find the decision boundary that separates the positive label examples and the negative label examples."
  },
  {
    "index": "F16859",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合、教師有り学習の問題はラベルの集まりを与えられて、それに仮説をフィットさせる、という事になる。",
    "output": "So, the supervised learning problem in this case is given a set of labels to fit a hypothesis to it."
  },
  {
    "index": "F16860",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "対して、教師なし学習では、関連したラベルが無いデータが与えられる。",
    "output": "In contrast, in the unsupervised learning problem we're given data that does not have any labels associated with it."
  },
  {
    "index": "F16861",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこんな感じのデータを与えられる。",
    "output": "So, we're given data that looks like this."
  },
  {
    "index": "F16862",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに点の集まりがあり、ラベルが無い。つまり、トレーニングセットは単にx(1)、x(2)、、、とx(m)まで書かれているだけで、いかなるラベルyも存在しない。",
    "output": "Here's a set of points add in no labels, and so, our training set is written just x1, x2, and so on up to x m and we don't get any labels y."
  },
  {
    "index": "F16863",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから上の図にプロットされた点はラベルが無いのだ。",
    "output": "And that's why the points plotted up on the figure don't have any labels with them."
  },
  {
    "index": "F16864",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり教師なし学習においては、こんな感じのラベル付けされていないトレーニングセットがある時に、アルゴリズムにこう尋ねるのだ、データのある構造を探し出してちょうだいな、と。",
    "output": "So, in unsupervised learning what we do is we give this sort of unlabeled training set to an algorithm and we just ask the algorithm find some structure in the data for us."
  },
  {
    "index": "F16865",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このデータセットがあった時に、アルゴリズムに探させる構造として一つ考えられるのは、データセットは点を2つのクラスタのグループに分けられそう。",
    "output": "Given this data set one type of structure we might have an algorithm find is that it looks like this data set has points grouped into two separate clusters and so an algorithm that finds clusters like the ones I've just circled is called a clustering algorithm."
  },
  {
    "index": "F16866",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれは、我らの最初の教師なし学習の最初の例となるだろう。他の種類の教師なし学習、それはあとで扱うが、つまり別の種類の構造またはパターンをデータから見つける物だ、クラスタ以外の。",
    "output": "And this would be our first type of unsupervised learning, although there will be other types of unsupervised learning algorithms that we'll talk about later that finds other types of structure or other types of patterns in the data other than clusters."
  },
  {
    "index": "F16867",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、クラスタリングは何に使える?",
    "output": "So, what is clustering good for?"
  },
  {
    "index": "F16868",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このクラスの最初の方で、幾つかの応用例に触れた。",
    "output": "Early in this class I already mentioned a few applications."
  },
  {
    "index": "F16869",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでは顧客のデータベースがあって、彼らを異なるマーケットセグメントごとにグルーピングしたい。異なるセグメントごとに別々に売ったり、セグメントに合わせて対応する事でより良いサービスを提供したり出来るように。",
    "output": "One is market segmentation where you may have a database of customers and want to group them into different marker segments so you can sell to them separately or serve your different market segments better."
  },
  {
    "index": "F16870",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ソーシャル・ネットワーク分析、というのをやっている人々もいて、たとえば人々のグループを調べる、SNSつまりFacebookやGoogle+やあなたがもっともたくさんメールを送っているか、もっともemailを送り合っているのは誰か?",
    "output": "There are actually groups have done this things like looking at a group of people's social networks."
  },
  {
    "index": "F16871",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "という情報から人々の同質なグループを見つけ出す。",
    "output": "So, things like Facebook, Google+, or maybe information about who other people that you email the most frequently and who are the people that they email the most frequently and to find coherence in groups of people."
  },
  {
    "index": "F16872",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これもある種のクラスタリングのアルゴリズムと言えるかもしれない、そこではソーシャル・ネットワークの中の友達の同質なグループを見つけ出したい、という。",
    "output": "So, this would be another maybe clustering algorithm where you know want to find who are the coherent groups of friends in the social network?"
  },
  {
    "index": "F16873",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは私の友人が実際にやってる例で、コンピュータのクラスタやデータセンターをより良く構成する為にクラスタリングを使う。",
    "output": "Here's something that one of my friends actually worked on which is, use clustering to organize computer clusters or to organize data centers better."
  },
  {
    "index": "F16874",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "というのはどのコンピュータがデータセンターにある他のクラスタと一緒に仕事をする傾向にある、と分かれば、それを用いてリソースやネットワークのレイアウトやデータセンターやそのコミュニケーションのデザインをするのに使えるからだ。",
    "output": "Because if you know which computers in the data center in the cluster tend to work together, you can use that to reorganize your resources and how you layout the network and how you design your data center communications."
  },
  {
    "index": "F16875",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後にもう一つ、実際に私がやってた仕事で、銀河の形成を理解するのにクラスタリングアルゴリズムを使う、というのがある。それを用いて、天文学的な詳細を理解する方法。",
    "output": "And lastly, something that actually another friend worked on using clustering algorithms to understand galaxy formation and using that to understand astronomical data."
  },
  {
    "index": "F16876",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がクラスタリング、我らの最初の教師なし学習の例となるアルゴリズムだ。",
    "output": "So, that's clustering which is our first example of an unsupervised learning algorithm."
  },
  {
    "index": "F16877",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、具体的なクラスタリングアルゴリズムについて扱っていく。",
    "output": "In the next video we'll start to talk about a specific clustering algorithm."
  },
  {
    "index": "F16878",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クラスタリングの問題では、ラベル付けされていないデータセットが渡されて、アルゴリズムに自動で互いに密接なサブセット、または互いに密接なクラスタにグループ分けして欲しい。",
    "output": "In the clustering problem we are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us."
  },
  {
    "index": "F16879",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-Meansアルゴリズムはずば抜けて人気のある、ずば抜けて広く使われているクラスタリングアルゴリズムだ。このビデオでは、K-Meansアルゴリズムとは何か、それがどう機能するかを話していきたい。",
    "output": "The K Means algorithm is by far the most popular, by far the most widely used clustering algorithm, and in this video I would like to tell you what the K Means Algorithm is and how it works."
  },
  {
    "index": "F16880",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-Meansクラスタリングアルゴリズムは絵で表すのが一番。",
    "output": "The K means clustering algorithm is best illustrated in pictures."
  },
  {
    "index": "F16881",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに見せたようなラベルの無いデータセットがあるとしよう。そしてこのデータを2つのクラスタにグループ分けしたい。",
    "output": "Let's say I want to take an unlabeled data set like the one shown here, and I want to group the data into two clusters."
  },
  {
    "index": "F16882",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-Meansアルゴリズムを実行するとしたらこれがやるべきことだ。",
    "output": "If I run the K Means clustering algorithm, here is what I'm going to do."
  },
  {
    "index": "F16883",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の一歩はランダムに2つの点を選ぶ、これはクラスタの重心と呼ばれる。",
    "output": "The first step is to randomly initialize two points, called the cluster centroids."
  },
  {
    "index": "F16884",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そう、これら2つのバッテンが、クラスタ重心と呼ばれる物だ。そしてそれが2つなのは、データを2つのクラスタにグループ分けしたいから。",
    "output": "So, these two crosses here, these are called the Cluster Centroids and I have two of them because I want to group my data into two clusters."
  },
  {
    "index": "F16885",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-Meansはイテレーティブなアルゴリズムで、2つの事をする。",
    "output": "K Means is an iterative algorithm and it does two things."
  },
  {
    "index": "F16886",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初はクラスタの割り付けステップ。二番目は重心移動ステップ。",
    "output": "First is a cluster assignment step, and second is a move centroid step."
  },
  {
    "index": "F16887",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらが何を意味するか解説していこう。",
    "output": "So, let me tell you what those things mean."
  },
  {
    "index": "F16888",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-Meansのループの中の2つのステップの内、最初の方は、クラスタ割り付けのステップだ。",
    "output": "The first of the two steps in the loop of K means, is this cluster assignment step."
  },
  {
    "index": "F16889",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その意味する所は各手本を見ていって、ここで示したのだと、この緑のドットを見ていき、赤のクラスタ重心と青のクラスタ重心のどちらと近いかによって、各データポイントを2つのクラスタのうちのどちらかに割り振る。",
    "output": "What that means is that, it's going through each of the examples, each of these green dots shown here and depending on whether it's closer to the red cluster centroid or the blue cluster centroid, it is going to assign each of the data points to one of the two cluster centroids."
  },
  {
    "index": "F16890",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的にその意味する所を見ると、データセットを見ていって、各点を赤か青に色付けしていく、赤のクラスタ重心に近いか青のクラスタ重心に近いかによって。この図ではそれを実際にやってみた。",
    "output": "Specifically, what I mean by that, is to go through your data set and color each of the points either red or blue, depending on whether it is closer to the red cluster centroid or the blue cluster centroid, and I've done that in this diagram here."
  },
  {
    "index": "F16891",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がクラスタ割り付けステップ。",
    "output": "So, that was the cluster assignment step."
  },
  {
    "index": "F16892",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-Meansのループ内を構成するもう一方は重心の移動ステップだ。我らがやるべきことは、2つのクラスタの重心を、つまり、赤のバッテンと青のバッテンを、同じ色で色付けされた点の平均へと移動する。",
    "output": "The other part of K means, in the loop of K means, is the move centroid step, and what we are going to do is, we are going to take the two cluster centroids, that is, the red cross and the blue cross, and we are going to move them to the average of the points colored the same colour."
  },
  {
    "index": "F16893",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり我らがやる事は、全ての赤の点を見て、平均を計算し、それは真に全ての赤い点の平均だが、赤のクラスタの重心をそこへ移動する。",
    "output": "So what we are going to do is look at all the red points and compute the average, really the mean of the location of all the red points, and we are going to move the red cluster centroid there."
  },
  {
    "index": "F16894",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同じ事を青のクラスタ重心にも行う。青い点を全て見て、平均を計算し、青のクラスタ重心をそこへ移動する。",
    "output": "And the same things for the blue cluster centroid, look at all the blue dots and compute their mean, and then move the blue cluster centroid there."
  },
  {
    "index": "F16895",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではやってみよう。",
    "output": "So, let me do that now."
  },
  {
    "index": "F16896",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クラスタ重心を以下のように動かし、それらは新しい平均へと移動した。赤いのはこんな感じで動き、青いのはこんな感じで動いた。",
    "output": "We're going to move the cluster centroids as follows and I've now moved them to their new means."
  },
  {
    "index": "F16897",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして赤いのはこんな感じで動いた。",
    "output": "The red one moved like that and the blue one moved like that and the red one moved like that."
  },
  {
    "index": "F16898",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にあらたなクラスタ割り当てステップに戻る。つまりまたラベルづけされていない手本を全て見ていって、青と赤のどちらの重心に近いかによって、それらを赤か青に色付けする。",
    "output": "And then we go back to another cluster assignment step, so we're again going to look at all of my unlabeled examples and depending on whether it's closer the red or the blue cluster centroid, I'm going to color them either red or blue."
  },
  {
    "index": "F16899",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "各点に2つのクラスタ重心のどちらかを割り振る、という事なので、やってみよう。",
    "output": "I'm going to assign each point to one of the two cluster centroids, so let me do that now."
  },
  {
    "index": "F16900",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "幾つかの点の色は変わった。",
    "output": "And so the colors of some of the points just changed."
  },
  {
    "index": "F16901",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてまた、重心移動のステップに進む。",
    "output": "And then I'm going to do another move centroid step."
  },
  {
    "index": "F16902",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり全ての青の点の平均を計算し、全ての赤の点の平均を計算し、クラスタ重心をこんな感じで移動する。やってみよう。",
    "output": "So I'm going to compute the average of all the blue points, compute the average of all the red points and move my cluster centroids like this, and so, let's do that again."
  },
  {
    "index": "F16903",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クラスタ割り振りステップをもう一回やってみよう。",
    "output": "Let me do one more cluster assignment step."
  },
  {
    "index": "F16904",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "各点を赤か青に色分けする、とちらに近いかに基づいてそして次に重心移動のステップを行う。",
    "output": "So colour each point red or blue, based on what it's closer to and then do another move centroid step and we're done."
  },
  {
    "index": "F16905",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実の所、ここからさらにK-Meansのイテレーションを走らせ続けても、クラスタ重心はこれ以上移動しない。そして点の色もこれ以上は変わらない。",
    "output": "And in fact if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further."
  },
  {
    "index": "F16906",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これがこここそが、K-Meansが収束する点だ。そしてそれは、このデータの2つのクラスタを探すには、かなり良い仕事をしている。",
    "output": "And so, this is the, at this point, K means has converged and it's done a pretty good job finding the two clusters in this data."
  },
  {
    "index": "F16907",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではK-Meansのアルゴリズムをよりフォーマルに記述しよう。",
    "output": "Let's write out the K means algorithm more formally."
  },
  {
    "index": "F16908",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-Meansアルゴリズムは2つの入力を取る。",
    "output": "The K means algorithm takes two inputs."
  },
  {
    "index": "F16909",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ目はパラメータK、それはデータの中から見つけたいクラスタの数。",
    "output": "One is a parameter K, which is the number of clusters you want to find in the data."
  },
  {
    "index": "F16910",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あとでどうやってこのKをどうやって選んだらいいかの話をするつもりだが、今のところはあるクラスタの数を既に決めた、としておこう。そして幾つのクラスタがデータセットにあるかをアルゴリズムに伝えるとする。",
    "output": "I'll later say how we might go about trying to choose k, but for now let's just say that we've decided we want a certain number of clusters and we're going to tell the algorithm how many clusters we think there are in the data set."
  },
  {
    "index": "F16911",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単なるxだけ。これは教師なし学習だから、もうラベルyは無い。",
    "output": "And then K means also takes as input this sort of unlabeled training set of just the Xs and because this is unsupervised learning, we don't have the labels Y anymore."
  },
  {
    "index": "F16912",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてK-Meansの教師なし学習に対してはxiをRnのベクトルを表すというコンベンションを用いることにする。",
    "output": "And for unsupervised learning of the K means I'm going to use the convention that XI is an RN dimensional vector."
  },
  {
    "index": "F16913",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんな訳でトレーニング手本はいまや、n+1次元では無くn次元のベクトルとなる。",
    "output": "And that's why my training examples are now N dimensional rather N plus one dimensional vectors. This is what the K means algorithm does."
  },
  {
    "index": "F16914",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがK-Meansアルゴリズムがやる事だ:最初のステップはランダムにK個の重心を選ぶ、それをミュー1、ミュー2、、、ミューkと名付けるつまり、前の図だと、クラスタ重心は赤のバッテンと青のバッテンの場所に対応してた。",
    "output": "And so in the earlier diagram, the cluster centroids corresponded to the location of the red cross and the location of the blue cross."
  },
  {
    "index": "F16915",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれは、2つのクラスタ重心があったという事で、たとえば赤のバッテンがミュー1で、青のバッテンがミュー2だった、という事。",
    "output": "So there we had two cluster centroids, so maybe the red cross was mu 1 and the blue cross was mu 2, and more generally we would have k cluster centroids rather than just 2."
  },
  {
    "index": "F16916",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして今度はより一般的に2つだけじゃなくて、K個のクラスタ重心を考えていく。",
    "output": "Then the inner loop of k means does the following, we're going to repeatedly do the following."
  },
  {
    "index": "F16917",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとK-Meansの内側のループは以下を実行する、つまり以下を繰り返し実行する事になる:まず、各トレーニング手本に対してこの変数c(i)をxiに一番近いクラスタ重心のインデックスをセットする。",
    "output": "First for each of my training examples, I'm going to set this variable CI to be the index 1 through K of the cluster centroid closest to XI."
  },
  {
    "index": "F16918",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、クラスタ割り付けステップにあたる。そこでは各サンプルに対してそれを赤か青か、そのどちらの重心に近いかに基づいて色分けしていく。",
    "output": "So this was my cluster assignment step, where we took each of my examples and coloured it either red or blue, depending on which cluster centroid it was closest to."
  },
  {
    "index": "F16919",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりc(i)は1からKまでの数で、その値は我らにそれが赤のバッテンか青のバッテンか、どちらに近いかを教えてくれる。これの他の書き方としては、ciを計算するのに、、、i番目の手本xiを取ってきてそれと個々のクラスタ重心との距離を測る。",
    "output": "So CI is going to be a number from 1 to K that tells us, you know, is it closer to the red cross or is it closer to the blue cross, and another way of writing this is I'm going to, to compute Ci, I'm going to take my Ith example Xi and and I'm going to measure it's distance to each of my cluster centroids, this is mu and then lower-case k, right, so capital K is the total number centroids and I'm going to use lower case k here to index into the different centroids."
  },
  {
    "index": "F16920",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてciをkの値に関して最小化する、そしてこのxiとクラスタ重心の距離を最小化するkを探す。そして、あー、これを最小化する値kを、そのkをciに代入する。",
    "output": "But so, Ci is going to, I'm going to minimize over my values of k and find the value of K that minimizes this distance between Xi and the cluster centroid, and then, you know, the value of k that minimizes this, that's what gets set in Ci."
  },
  {
    "index": "F16921",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がciとは何なのか?を記述するもう一つの書き方。",
    "output": "So, here's another way of writing out what Ci is."
  },
  {
    "index": "F16922",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "xiマイナスミューkのノルム、と書いた時は、これはi番目のトレーニング手本とクラスター重心のミュー下付き添字kとの距離を表す。これ、このこれは小文字のk。",
    "output": "If I write the norm between Xi minus Mu-k, then this is the distance between my ith training example Xi and the cluster centroid Mu subscript K, this is--this here, that's a lowercase K."
  },
  {
    "index": "F16923",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり大文字のKはクラスタ重心の総数を表すのに使う。",
    "output": "So uppercase K is going to be used to denote the total number of cluster centroids, and this lowercase K's a number between one and capital K."
  },
  {
    "index": "F16924",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの小文字のkは1から大文字のKまでの間の数字で異なるクラスタ重心同士を識別するのに小文字のkを使う。",
    "output": "I'm just using lower case K to index into my different cluster centroids."
  },
  {
    "index": "F16925",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それが小文字のk。",
    "output": "Next is lower case k."
  },
  {
    "index": "F16926",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがサンプルとクラスタ重心の距離で、我らがやるべき事はこれを最小化するk、このkは小文字のkだが、それを探す。そしてその最小化するkの値をciにセットする。",
    "output": "So that's the distance between the example and the cluster centroid and so what I'm going to do is find the value of K, of lower case k that minimizes this, and so the value of k that minimizes you know, that's what I'm going to set as Ci, and by convention here I've written the distance between Xi and the cluster centroid, by convention people actually tend to write this as the squared distance."
  },
  {
    "index": "F16927",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりciを、トレーニング手本xiとの二乗距離が最小になるクラスタ重心を選びとった物と考える事が出来る。",
    "output": "So we think of Ci as picking the cluster centroid with the smallest squared distance to my training example Xi."
  },
  {
    "index": "F16928",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもちろん、距離の二乗を最小化しようと、距離を最小化しようと、同じciの値になるはず。でも普通は二乗をつける。",
    "output": "But of course minimizing squared distance, and minimizing distance that should give you the same value of Ci, but we usually put in the square there, just as the convention that people use for K means."
  },
  {
    "index": "F16929",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がクラスタ割り付けステップ。",
    "output": "So that was the cluster assignment step."
  },
  {
    "index": "F16930",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-Meansのループの中の他の仕事は重心移動のステップだ。",
    "output": "The other in the loop of K means does the move centroid step."
  },
  {
    "index": "F16931",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで行うのは、クラスタ重心に対し、つまり小文字のkを1から大文字のKまでの範囲で、ミューkにその重心に関連付けられた点たちの平均を代入する。",
    "output": "And what that does is for each of my cluster centroids, so for lower case k equals 1 through K, it sets Mu-k equals to the average of the points assigned to cluster."
  },
  {
    "index": "F16932",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体例としては、クラスタ重心の一つ、クラスタ重心2としよう、それがトレーニング手本を持ってるとして、それは1、5、6、10に、それが割り振られているとする。",
    "output": "So as a concrete example, let's say that one of my cluster centroids, let's say cluster centroid two, has training examples, you know, 1, 5, 6, and 10 assigned to it."
  },
  {
    "index": "F16933",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その意味するところは、c1=c5=c6=...と、c10も同様にイコールだ。",
    "output": "And what this means is, really this means that C1 equals to C5 equals to C6 equals to and similarly well c10 equals, too, right?"
  },
  {
    "index": "F16934",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クラスタ割り振りのステップでそれを得たとすると、それはつまり、手本1、5、6、10はクラスタ重心2が割り付けられている。",
    "output": "If we got that from the cluster assignment step, then that means examples 1,5,6 and 10 were assigned to the cluster centroid two."
  },
  {
    "index": "F16935",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にこの重心移動のステップでは、そこでやるべきは単にこれら4つの平均を取るという事。",
    "output": "Then in this move centroid step, what I'm going to do is just compute the average of these four things."
  },
  {
    "index": "F16936",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして今、それらを平均したいのだから、このクラスタには点が4つ割り振られているのだから、1/4を取る。",
    "output": "And now I'm going to average them so here I have four points assigned to this cluster centroid, just take one quarter of that."
  },
  {
    "index": "F16937",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとミュー2はn次元ベクトルとなる。",
    "output": "And now Mu2 is going to be an n-dimensional vector."
  },
  {
    "index": "F16938",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら各手本、x1、x5、x6、x10は、どれもn次元ベクトルだったから。そしてこれらを足し合わせて、4で割ってる、だってこのクラスタ重心には4つの点が割り振られているから。",
    "output": "Because each of these example x1, x5, x6, x10 each of them were an n-dimensional vector, and I'm going to add up these things and, you know, divide by four because I have four points assigned to this cluster centroid, I end up with my move centroid step, for my cluster centroid mu-2."
  },
  {
    "index": "F16939",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはミュー2をここに列挙した4つの点の平均となる。",
    "output": "This has the effect of moving mu-2 to the average of the four points listed here."
  },
  {
    "index": "F16940",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "よく質問される事として、今、ミューkをクラスタに割り振られた点の平均にしよう、と言ったが、もし点を割り振られないクラスタ重心、点が0個しか割り振られないクラスタ重心があったら、どうしたらいい?",
    "output": "One thing that I've asked is, well here we said, let's let mu-k be the average of the points assigned to the cluster."
  },
  {
    "index": "F16941",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、一番普通の対応はたんにそのクラスタ重心を取り除く。",
    "output": "In that case the more common thing to do is to just eliminate that cluster centroid."
  },
  {
    "index": "F16942",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうすると、最終結果はK個のクラスタではなくてK-1個のクラスタとなる。時々、ほんとうにK個のクラスタが必要な場合もある。",
    "output": "And if you do that, you end up with K minus one clusters instead of k clusters."
  },
  {
    "index": "F16943",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合にやる別の手段としては、もし点が割り振られないクラスタ重心があったら、単にそのクラスタ重心をランダムに再初期化する。でも単に取り除く方が普通だね。",
    "output": "Sometimes if you really need k clusters, then the other thing you can do if you have a cluster centroid with no points assigned to it is you can just randomly reinitialize that cluster centroid, but it's more common to just eliminate a cluster if somewhere during K means it with no points assigned to that cluster centroid, and that can happen, altthough in practice it happens not that often."
  },
  {
    "index": "F16944",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がK-Meansアルゴリズム。",
    "output": "So that's the K means Algorithm."
  },
  {
    "index": "F16945",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオのまとめに入る前に、もう一つの良くあるK-Meansの応用を話しておきたい。それは、あまり綺麗に分かれていないクラスタの問題だ。",
    "output": "Before wrapping up this video I just want to tell you about one other common application of K Means and that's to the problems with non well separated clusters."
  },
  {
    "index": "F16946",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこんな意味だ。",
    "output": "Here's what I mean."
  },
  {
    "index": "F16947",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてアルゴリズムに3つのクラスタを探させてきた。",
    "output": "So far we've been picturing K Means and applying it to data sets like that shown here where we have three pretty well separated clusters, and we'd like an algorithm to find maybe the 3 clusters for us."
  },
  {
    "index": "F16948",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがK-Meansはこんな感じのデータセットに対してもとても良く適用されている、そこでは幾つかのクラスタに綺麗に分けられるようには見えない。",
    "output": "But it turns out that very often K Means is also applied to data sets that look like this where there may not be several very well separated clusters."
  },
  {
    "index": "F16949",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはTシャツのサイズに関する適用の例だ。",
    "output": "Here is an example application, to t-shirt sizing."
  },
  {
    "index": "F16950",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたはTシャツ作ってる会社だとしよう、あなたは自分たちがTシャツを売りたい、と思っている母集団に対して、たくさんのサンプルの身長と体重のデータを集めた、つまり、えー、たぶん身長と体重は正の相関があるだろうから、こんな感じのデータセットになるだろう。",
    "output": "Let's say you are a t-shirt manufacturer you've done is you've gone to the population that you want to sell t-shirts to, and you've collected a number of examples of the height and weight of these people in your population and so, well I guess height and weight tend to be positively highlighted so maybe you end up with a data set like this, you know, with a sample or set of examples of different peoples heights and weight."
  },
  {
    "index": "F16951",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Tシャツのサイズを決めたいと思ってるとしよう。",
    "output": "Let's say you want to size your t shirts."
  },
  {
    "index": "F16952",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "3つのサイズ、S、M、LのTシャツのデザインをして売りたい、としよう。",
    "output": "Let's say I want to design and sell t shirts of three sizes, small, medium and large."
  },
  {
    "index": "F16953",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではその時、Sはどのくらいの大きさにすべきだろう?",
    "output": "So how big should I make my small one?"
  },
  {
    "index": "F16954",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを決める一つの方法としては、右に示したこのデータセットに対しK-Meansクラスタリングアルゴリズムを適用する、というのがある。するとたぶんK-Meansが行う事は、これら全部の点を一つのクラスタに、これらの点全部を二番目のクラスタに、そしてこれらの点全部を三番目のクラスタにグループ分けする、という事だ。",
    "output": "One way to do that would to be to run my k means clustering logarithm on this data set that I have shown on the right and maybe what K Means will do is group all of these points into one cluster and group all of these points into a second cluster and group all of those points into a third cluster."
  },
  {
    "index": "F16955",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、もともとのデータセットが3つの異なるクラスタに分かれているようには見えないのにも関わらず、K-Meansは複数のクラスタに分けてくれるのだ。",
    "output": "So, even though the data, you know, before hand it didn't seem like we had 3 well separated clusters, K Means will kind of separate out the data into multiple pluses for you."
  },
  {
    "index": "F16956",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこから可能な事としては、この最初の人々を彼らを見て、彼らの身長と体重を見て、そしてSのTシャツをデザインする、という事。",
    "output": "And what you can do is then look at this first population of people and look at them and, you know, look at the height and weight, and try to design a small t-shirt so that it kind of fits this first population of people well and then design a medium t-shirt and design a large t-shirt."
  },
  {
    "index": "F16957",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはつまり、マーケットセグメンテーションの例となっている、そこではK-Meansを使って、マーケットを3つのセグメントに分けている。",
    "output": "And this is in fact kind of an example of market segmentation where you're using K Means to separate your market into 3 different segments."
  },
  {
    "index": "F16958",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれで、S、M、Lに分けてTシャツをデザイン出来る、3つのサブ集団のニーズに良く合うように。",
    "output": "So you can design a product separately that is a small, medium, and large t-shirts, that tries to suit the needs of each of your 3 separate sub-populations well."
  },
  {
    "index": "F16959",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がK-Meansアルゴリズムです。",
    "output": "So that's the K Means algorithm."
  },
  {
    "index": "F16960",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでで、もうK-Meansをどうやって実装するか、そしてどんな問題に適用出来るかを理解したはずだ。",
    "output": "And by now you should know how to implement the K Means Algorithm and kind of get it to work for some problems."
  },
  {
    "index": "F16961",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが次に続くいくつかのビデオでK-Meansの要点をより深く話していくのと実際にとてもうまくやる為に必要な事をちょろっと話していきたい。",
    "output": "But in the next few videos what I want to do is really get more deeply into the nuts and bolts of K means and to talk a bit about how to actually get this to work really well."
  },
  {
    "index": "F16962",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これまで見て来た教師あり学習アルゴリズム、線形回帰やロジスティック回帰などは、それらは全て、最適化の為の目的関数、またの名をコスト関数を持っていて、それを最小化しようとしていた。",
    "output": "Most of the supervised learning algorithms we've seen, things like linear regression, logistic regression, and so on, all of those algorithms have an optimization objective or some cost function that the algorithm was trying to minimize."
  },
  {
    "index": "F16963",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-meansもまた、最適化の目的関数、またはコスト関数を持っていて、それを最小化しようとする。",
    "output": "It turns out that k-means also has an optimization objective or a cost function that it's trying to minimize."
  },
  {
    "index": "F16964",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの動画では、最適化の目的関数が何かを説明する。",
    "output": "And in this video I'd like to tell you what that optimization objective is."
  },
  {
    "index": "F16965",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをやりたい理由としては、二つの目的にこれは有用だからだ。",
    "output": "And the reason I want to do so is because this will be useful to us for two purposes."
  },
  {
    "index": "F16966",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず、K-meansの最適化の目的関数がなんなのかを知る事は、学習アルゴリズムをデバッグする助けとなる。K-meansがちゃんと走ってるか確認も出来る。",
    "output": "First, knowing what is the optimization objective of k-means will help us to debug the learning algorithm and just make sure that k-means is running correctly."
  },
  {
    "index": "F16967",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二番目に、そしてたぶんこっちの方が重要だが、後半のビデオで、これをどう用いてK-meansがより良いクラスタを見つける助けに出来るか、そしてどう局所最適を避ける事が出来るかを議論する。",
    "output": "And second, and perhaps more importantly, in a later video we'll talk about how we can use this to help k-means find better costs for this and avoid the local ultima."
  },
  {
    "index": "F16968",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもそれはこのビデオの後に続くビデオでね。",
    "output": "But we do that in a later video that follows this one."
  },
  {
    "index": "F16969",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "思い出してもらう為に言っておくと、K-meansを実行している間、我らは二つの種類の変数を管理していく。",
    "output": "Just as a quick reminder while k-means is running we're going to be keeping track of two sets of variables."
  },
  {
    "index": "F16970",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはx(i)が現在どのクラスタに割り振られているかのインデックスをトラックする為の変数だ。",
    "output": "First is the ci's and that keeps track of the index or the number of the cluster, to which an example xi is currently assigned."
  },
  {
    "index": "F16971",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもう一方の管理する変数はミューの下付き添字kだ。それはクラスター重心Kの場所を表す。",
    "output": "And then the other set of variables we use is mu subscript k, which is the location of cluster centroid k."
  },
  {
    "index": "F16972",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もう一度言っておくと、K-meansでは大文字のKをクラスタの総数を表すのに使う。",
    "output": "Again, for k-means we use capital K to denote the total number of clusters."
  },
  {
    "index": "F16973",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの小文字のkでクラスタ重心のインデックスを表す。つまり小文字のkは1からKまでの間の数字となる。",
    "output": "And here lower case k is going to be an index into the cluster centroids and so, lower case k is going to be a number between one and capital K."
  },
  {
    "index": "F16974",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらにもう一つ追加の記法として、ミューの下付き添字c(i)という物でこれはクラスターの重心のうち、サンプルx(i)に割り振られている物を表す。",
    "output": "Now here's one more bit of notation, which is gonna use mu subscript ci to denote the cluster centroid of the cluster to which example xi has been assigned, right?"
  },
  {
    "index": "F16975",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この記法についてもうちょっと説明しよう。x(i)がクラスタ重心5に割り振られているとしよう。",
    "output": "And to explain that notation a little bit more, lets say that xi has been assigned to cluster number five."
  },
  {
    "index": "F16976",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それの意味する所はc(i)、このiはx(i)のインデックスだが、c(i)はイコール5となる。",
    "output": "What that means is that ci, that is the index of xi, that that is equal to five."
  },
  {
    "index": "F16977",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でしょ?",
    "output": "Right?"
  },
  {
    "index": "F16978",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だってc(i)=5となるのがサンプルx(i)がクラスタナンバー5に割り振らたという事だから。",
    "output": "Because having ci equals five, if that's what it means for the example xi to be assigned to cluster number five."
  },
  {
    "index": "F16979",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからミュー下付き添字のc(i)はイコール、ミュー下付き添字の5となる。",
    "output": "And so mu subscript ci is going to be equal to mu subscript 5."
  },
  {
    "index": "F16980",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このミュー下付き添字c(i)はクラスターナンバー5のクラスタ重心で、それがサンプルx(i)が割り振られている物だ。",
    "output": "And so this mu subscript ci is the cluster centroid of cluster number five, which is the cluster to which my example xi has been assigned."
  },
  {
    "index": "F16981",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この記法で、我らはK-meansのクラスタリングアルゴリズムの最適化の目的関数を書き下す、準備が出来た事になる。それはこうだ。",
    "output": "Out with this notation, we're now ready to write out what is the optimization objective of the k-means clustering algorithm and here it is."
  },
  {
    "index": "F16982",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-meansが最小化するコスト関数はこれらのパラメータ全ての関数Jだ、c1からcmまでと、ミュー1からミューKまでの。",
    "output": "The cost function that k-means is minimizing is a function J of all of these parameters, c1 through cm and mu 1 through mu K."
  },
  {
    "index": "F16983",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-meansは実行していく過程でこれらを変更していく。",
    "output": "That k-means is varying as the algorithm runs."
  },
  {
    "index": "F16984",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最適化の目的関数は、右に示した物で、平均としての1/mの、和を取る事のi=1からmまでの、この項で、今赤の箱でくくったこれ。",
    "output": "And the optimization objective is shown to the right, is the average of 1 over m of sum from i equals 1 through m of this term here."
  },
  {
    "index": "F16985",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "サンプルx(i)とx(i)に割り振られたクラスタ重心の位置との間の二乗距離。",
    "output": "The square distance between each example xi and the location of the cluster centroid to which xi has been assigned."
  },
  {
    "index": "F16986",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ちょっと描いて、これを説明しよう。",
    "output": "So let's draw this and just let me explain this."
  },
  {
    "index": "F16987",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはトレーニングサンプルのx(i)の位置で、これがサンプルx(i)が割り振られたクラスタ重心の位置とする。",
    "output": "Right, so here's the location of training example xi and here's the location of the cluster centroid to which example xi has been assigned."
  },
  {
    "index": "F16988",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを図で説明する為に、x1とx2があって、この点、ここがサンブルx(i)とすると、つまりこれがサンプルx(i)とイコールだとする。",
    "output": "So to explain this in pictures, if here's x1, x2, and if a point here is my example xi, so if that is equal to my example xi, and if xi has been assigned to some cluster centroid, I'm gonna denote my cluster centroid with a cross, so if that's the location of mu 5, let's say."
  },
  {
    "index": "F16989",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてx(i)があるクラスタ重心に割り振られているとすると、ところでクラスタの重心は十字で表す事にする。",
    "output": "If x i has been assigned cluster centroid five as in my example up there, then this square distance, that's the square of the distance between the point xi and this cluster centroid to which xi has been assigned."
  },
  {
    "index": "F16990",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、それがミュー5の場所で、この例ではx(i)がクラスター重心5に割り振られてるとすると。",
    "output": "And what k-means can be shown to be doing is that it is trying to define parameters ci and mu i."
  },
  {
    "index": "F16991",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてK-Meansがやる事は、つまり、パラメータであるc(i)とミューiを探そうとする、cとミューで、コスト関数Jを最小化する物を探そうとする。",
    "output": "Trying to find c and mu to try to minimize this cost function J."
  },
  {
    "index": "F16992",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このコスト関数はまた、ディストーション(歪み)コスト関数、またはK-meansアルゴリズムのディストーションと呼ばれる。",
    "output": "This cost function is sometimes also called the distortion cost function, or the distortion of the k-means algorithm."
  },
  {
    "index": "F16993",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もうちょっと詳細を見ると、これがK-meansのアルゴリズムだ。",
    "output": "And just to provide a little bit more detail, here's the k-means algorithm."
  },
  {
    "index": "F16994",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは前のスライドにあったのと全く同じ物を実際の形にした物だ。",
    "output": "Here's exactly the algorithm as we have written it out on the earlier slide."
  },
  {
    "index": "F16995",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのアルゴリズムの最初のステップはクラスター割り振りのステップで、そこで各点をクラスター重心に割り振る。",
    "output": "And what this first step of this algorithm is, this was the cluster assignment step where we assigned each point to the closest centroid."
  },
  {
    "index": "F16996",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クラスター割り振りのステップは実際に変数c1,c2、、、とc(m)までの観点からJを最小化している、という事を数学的に証明する事が出来る。この間、もっとも近い重心である、ミュー1からミューkまでは固定しておく。",
    "output": "And it's possible to show mathematically that what the cluster assignment step is doing is exactly Minimizing J, with respect to the variables c1, c2 and so on, up to cm, while holding the cluster centroids mu 1 up to mu K, fixed."
  },
  {
    "index": "F16997",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、最初の割り振りのステップで何をやるかというと、そのステップではクラスタ重心は変更しない、その代わりにコスト関数、またはディストーション関数であるJを最小化するc1,c2,...cmの値を選ぶ、という事をする。",
    "output": "So what the cluster assignment step does is it doesn't change the cluster centroids, but what it's doing is this is exactly picking the values of c1, c2, up to cm. That minimizes the cost function, or the distortion function J."
  },
  {
    "index": "F16998",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして数学的にもやろうと思えば証明出来るが、ここではやらんけど、直感的にも自然だと思うけどこれらの点に対しもっとも近いクラスタ重心を割り振っていく。",
    "output": "And it's possible to prove that mathematically, but I won't do so here."
  },
  {
    "index": "F16999",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "というのはそれが点と対応するクラスタ重心の間の二乗距離の和を最小化する割り振り方だから。",
    "output": "But it has a pretty intuitive meaning of just well, let's assign each point to a cluster centroid that is closest to it, because that's what minimizes the square of distance between the points in the cluster centroid."
  }
]