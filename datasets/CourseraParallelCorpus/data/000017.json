[
  {
    "index": "F17000",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして残りのK-meansの部分、2番目のステップは、この二番目のステップとなるが、この二番目のステップは重心を移動するステップで、ここでも、証明はしないが、数学的にも証明出来る事として、重心移動のステップでは、Jを最小化するミューを選ぶ。つまりコスト関数Jを以下の観点から最小化する、ここでwrtはwithrespectto(以下の観点から)の省略だ。",
    "output": "And once again I won't prove it, but it can be shown mathematically that what the move centroid step does is it chooses the values of mu that minimizes J, so it minimizes the cost function J with respect to, wrt is my abbreviation for, with respect to, when it minimizes J with respect to the locations of the cluster centroids mu 1 through mu K."
  },
  {
    "index": "F17001",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、K-meansが実際にやっているのは、二つの種類の変数群をとり、そしてそれら二つを二つに分けて、まずcの変数群、次にミューの変数群として、そしてやるのは、まず、Jを変数cに関して最小化する、変数ミューに関して最小化する、そしてそれを繰り返し続ける。",
    "output": "So if is really is doing is this taking the two sets of variables and partitioning them into two halves right here."
  },
  {
    "index": "F17002",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がK-meansのやる事の全てだ。そして今やK-meansを理解したので、コスト関数Jを最小化しよう。",
    "output": "And what it does is it first minimizes J with respect to the variable c and then it minimizes J with respect to the variables mu and then it keeps on."
  },
  {
    "index": "F17003",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また我らのK-meansの実装が正しく走っているかを確認するのにも使える。",
    "output": "And, so all that's all that k-means does."
  },
  {
    "index": "F17004",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、いまや我らはK-meansアルゴリズムを、コスト関数Jを最小化する物として理解した。",
    "output": "And now that we understand k-means as trying to minimize this cost function J, we can also use this to try to debug other any algorithm and just kind of make sure that our implementation of k-means is running correctly."
  },
  {
    "index": "F17005",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コスト関数はディストーション関数とも呼ばれるんだった。",
    "output": "So, we now understand the k-means algorithm as trying to optimize this cost function J, which is also called the distortion function."
  },
  {
    "index": "F17006",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その事実を用いて、K-meansをデバッグしたり、K-meansが収束しているのを見たり出来る。そしてそれが正しく実行されているかも。",
    "output": "We can use that to debug k-means and help make sure that k-means is converging and is running properly."
  },
  {
    "index": "F17007",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、どうこれを用いてK-meansがより良いクラスタを見つける助けと出来るか、そしてどうK-meansが局所最適を避ける助けと出来るかを話していく。",
    "output": "And in the next video we'll also see how we can use this to help k-means find better clusters and to help k-means to avoid"
  },
  {
    "index": "F17008",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんにちは、競争戦略上級クラスへようこそ。",
    "output": "Hello and welcome to advanced competitive strategy."
  },
  {
    "index": "F17009",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私は、ミュンヘンのルドウィグ-マキシミリアン大学の戦略、テクノロジー、組織の教授をしているトビアス・クレチメルです。",
    "output": "My name is Tobias Kretschmer and I'm a professor of , strategy technology and organization at Ludwig- Maximilians-University or LMU in Munich."
  },
  {
    "index": "F17010",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私たちはあたらしいコースという冒険を始めようとしています。",
    "output": "We are about to start a new course and a new adventure."
  },
  {
    "index": "F17011",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "競争戦略上級クラスは7つの異なるモジュールをもっており、ビジネス戦略を競争的状況でどのように応用するかを学びます。",
    "output": "Advanced competitive strategy consists of seven different modules in which you will learn how you can apply business strategy in competitive situations."
  },
  {
    "index": "F17012",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それぞれの異なる7つのモジュールは約10のビデオからなっており、それぞれに、一つか二つのクイズがついています。そこで皆さんが新しく学んだ知識をすぐにテストすることができます。",
    "output": "Each one of our seven modules consists of a variety of around 10 different videos all of which include one or two individual quizzes, that are going to let you test your newly acquired knowledge immediately."
  },
  {
    "index": "F17013",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、これらは楽しくなくてはいけません。",
    "output": "And of course, they should also be fun."
  },
  {
    "index": "F17014",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それぞれのモジュールの最後に長い小テストがあります。",
    "output": "There will also be a longer quiz at the end of each module."
  },
  {
    "index": "F17015",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、皆さんが7つのすべてのモジュールを終えると自動的に最終試験を受けているのです。",
    "output": "So, once you've completed all seven modules, you can also eventually take the final exam."
  },
  {
    "index": "F17016",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、私たちのフォーラムを確認してください。",
    "output": "And please, do check out our forum."
  },
  {
    "index": "F17017",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "なぜなら、クラスメイトとつながって、スタディグループなどを作る素晴らしい機会だからです。そこで、疑問について議論したり、素晴らしくて知識豊富なティーチングアシスタントとコミュニケーションをとる機会でもあります。",
    "output": "Because it's a brilliant opportunity to connect to fellow students, to build study groups, to openly discuss questions, and to communicate with our wonderful and knowledgeable taching assistants."
  },
  {
    "index": "F17018",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、K-meansをどう初期化するのかと、その延長としてより重要な、K-meansでどう局所最適の問題を回避するのかについての2つを話す。",
    "output": "In this video, I'd like to talk about how to initialize K-means and more importantly, this will lead into a discussion of how to make K-means avoid local optima as well."
  },
  {
    "index": "F17019",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに、以前に話したK-meansのクラスタリングアルゴリズムがある。",
    "output": "Here's the K-means clustering algorithm that we talked about earlier."
  },
  {
    "index": "F17020",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでちゃんと話してなかったステップの一つにクラスタ重心を、どうランダムに初期化するか、というのがある。",
    "output": "One step that we never really talked much about was this step of how you randomly initialize the cluster centroids."
  },
  {
    "index": "F17021",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ランダムにクラスタ重心を初期化する、と言われた時には、いくつかの方法が考えられるが、その中の一つが、それ以外の思いつく選択肢の多くよりも優れている、というものがある。",
    "output": "But, it turns out that there is one method that is much more recommended than most of the other options one might think about."
  },
  {
    "index": "F17022",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからその方法について議論しようと思う、何故ならそれがしばしば一番良く機能する選択肢だからだ。",
    "output": "So, let me tell you about that option since it's what often seems to work best."
  },
  {
    "index": "F17023",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが私が普段クラスタ重心を初期化するやり方だ。",
    "output": "Here's how I usually initialize my cluster centroids."
  },
  {
    "index": "F17024",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-meansを実行する時にはクラスタ重心の数、Kをトレーニング手本の数mよりも小さい数に設定しなくてはならない。",
    "output": "When running K-means, you should have the number of cluster centroids, K, set to be less than the number of training examples M."
  },
  {
    "index": "F17025",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-meansを、手本の数と同じかそれより多い数のクラスタ重心に対して実行する、というのはヘンテコなことだってのは分かるでしょ?",
    "output": "It would be really weird to run K-means with a number of cluster centroids that's, you know, equal or greater than the number of examples you have, right?"
  },
  {
    "index": "F17026",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、私が普段K-meansを初期化するのに使う方法はK個のトレーニング手本をランダムに取り出す。",
    "output": "So the way I usually initialize K-means is, I would randomly pick k training examples."
  },
  {
    "index": "F17027",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にやるのは、ミュー1からミューKに、これらをセットする、という事。具体例を見てみよう。",
    "output": "So, and, what I do is then set Mu1 of MuK equal to these k examples."
  },
  {
    "index": "F17028",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Kをイコール2だとしてみよう。つまりこの右側の手本に対し、2つのクラスタを見つけたい、とする。",
    "output": "Lets say that k is equal to 2 and so on this example on the right let's say I want to find two clusters."
  },
  {
    "index": "F17029",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、クラスタ重心を初期化する為に私がやるのは2つの手本をランダムに選ぶ。",
    "output": "So, what I'm going to do in order to initialize my cluster centroids is, I'm going to randomly pick a couple examples."
  },
  {
    "index": "F17030",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、例えばこれとそれを選んだとしよう。",
    "output": "And let's say, I pick this one and I pick that one."
  },
  {
    "index": "F17031",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合私がクラスタ重心を初期化するやり方はそれらの手本の真上にクラスタ重心を置く、という方法。",
    "output": "And the way I'm going to initialize my cluster centroids is, I'm just going to initialize my cluster centroids to be right on top of those examples."
  },
  {
    "index": "F17032",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが最初のクラスタ重心となり、これが二番目のクラスタ重心となる。これがK-meansをランダムに初期化する方法だ。",
    "output": "So that's my first cluster centroid and that's my second cluster centroid, and that's one random initialization of K-means."
  },
  {
    "index": "F17033",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに描いたのは極めて良い物っぽいけど、たまにはもっとついてなくて、結局最初の奴としてこんなのをランダムな最初の手本として、そして二番目としてこんなのを選ぶはめになるかもしれない。",
    "output": "And sometimes I might get less lucky and maybe I'll end up picking that as my first random initial example, and that as my second one."
  },
  {
    "index": "F17034",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで2つの手本を選んだのはK=2としたから。",
    "output": "And here I'm picking two examples because k equals 2."
  },
  {
    "index": "F17035",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "適当に2つのトレーニング手本をランダムに2つ選んだ、それをこれら2つとすると、その時はこれを最初のクラスタ重心にこれを2つ目のクラスタ重心の初期位置とする。",
    "output": "Some we have randomly picked two training examples and if I chose those two then I'll end up with, may be this as my first cluster centroid and that as my second initial location of the cluster centroid."
  },
  {
    "index": "F17036",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がランダムにクラスタ重心を初期化するやり方だ。",
    "output": "So, that's how you can randomly initialize the cluster centroids."
  },
  {
    "index": "F17037",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり初期化の時にはクラスタ重心ミュー1は適当に選んだ値iの元にx(i)と等しくなる。ミュー2は、iとは別の異なるランダムに選ばれた数jを使ってx(j)と表せる物と等しくなる、などなどと、もっと多くのクラスタとクラスタ重心があるなら続けていく。",
    "output": "And so at initialization, your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on, if you have more clusters and more cluster centroid."
  },
  {
    "index": "F17038",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでちょっと補足しておく。以前の動画で最初にK-meansをアニメーションで例示した時の話だ。",
    "output": "I should say that in the earlier video where I first illustrated K-means with the animation."
  },
  {
    "index": "F17039",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでのスライドは、例示の為だけの目的の物だった。",
    "output": "In that set of slides. Only for the purpose of illustration."
  },
  {
    "index": "F17040",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私は実際には、そこで書いていたのとは違うクラスタ重心の初期化の仕方をする。",
    "output": "I actually used a different method of initialization for my cluster centroids."
  },
  {
    "index": "F17041",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このスライドで説明している方法こそが本当に推奨する方法だ。",
    "output": "But the method described on this slide, this is really the recommended way."
  },
  {
    "index": "F17042",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして恐らく、あなたがK-meansを実装する時にも使うべき方法と言える。",
    "output": "And the way that you should probably use, when you implement K-means."
  },
  {
    "index": "F17043",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、この右側の二つの図で例示出来ているかもしれない事として、K-meansは実際にどう初期化されるかに応じてつまりどうランダム初期化されるかに応じて異なる解に収束しうるという事が想像出来るかもしれない。",
    "output": "You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized, and so, depending on the random initialization."
  },
  {
    "index": "F17044",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-meansは異なる解に収束しうる。",
    "output": "K-means can end up at different solutions."
  },
  {
    "index": "F17045",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際、K-meansは異なる局所最適解に落ち着きうる。",
    "output": "And, in particular, K-means can actually end up at local optima."
  },
  {
    "index": "F17046",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこんなデータセットを与えられたとするとうーん、見た所、ここには三つのクラスタがあるように見える。",
    "output": "If you're given the data sale like this."
  },
  {
    "index": "F17047",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしK-meansを実行したらもし良い局所最適解に落ち着けば、これが極めて良い局所最適解に見える、その時には、このクラスタの輪となるだろう。",
    "output": "Well, it looks like, you know, there are three clusters, and so, if you run K-means and if it ends up at a good local optima this might be really the global optima, you might end up with that cluster ring."
  },
  {
    "index": "F17048",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがあなたが特にアンラッキーでランダム初期化によっては、K-meansは異なる局所最適にスタックしてしまうかもしれない。",
    "output": "But if you had a particularly unlucky, random initialization, K-means can also get stuck at different local optima."
  },
  {
    "index": "F17049",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、この左側の例では、青いクラスタがたくさんの点を捕捉してしまい、緑と赤のクラスタはそれぞれ相対的にはちょっとの点しか捕捉出来ていない。",
    "output": "So, in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points."
  },
  {
    "index": "F17050",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、悪い局所最適解に対応している。何故ならそれは、これら二つのクラスタを一個に取り出してしまっていて、さらに、二番目のクラスタを二つのサブクラスタに分割してしまっている。",
    "output": "And so, this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore, has split the second cluster into two separate sub-clusters like so, and it has also taken the second cluster and split it into two separate sub-clusters like so, and so, both of these examples on the lower right correspond to different local optima of K-means and in fact, in this example here, the cluster, the red cluster has captured only a single optima example."
  },
  {
    "index": "F17051",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで局所最適という用語は、このディストーション関数Jの局所最適を示している、そしてこれらの左下の解は、これらの局所最適な解はK-meansが局所最適にスタックしてしまい、このディストーション関数Jを最小化するのにあんまり良い仕事はしていない事に対応する。",
    "output": "And the term local optima, by the way, refers to local optima of this distortion function J, and what these solutions on the lower left, what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it's not doing a very good job minimizing this distortion function J."
  },
  {
    "index": "F17052",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、もしK-meansが局所最適にスタックしてしまいそうと心配だったら、もしK-meansが可能な中でベストなクラスタリングを見つけるオッズを高めたければ、この上に見せたように、我らに取れる手段としては複数のランダム初期化を試みる事だ。",
    "output": "So, if you're worried about K-means getting stuck in local optima, if you want to increase the odds of K-means finding the best possible clustering, like that shown on top here, what we can do, is try multiple, random initializations."
  },
  {
    "index": "F17053",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりK-meansを一回だけ初期化してうまく行く事を祈る代わりにやれる事として、K-meansをたくさん初期化してそしてK-meansをたくさん実行する事だ。そしてそれを用いて、可能な限り一番良い局所最適、つまりはグローバル最適を得ている事を確認するのだ。",
    "output": "So, instead of just initializing K-means once and hopping that that works, what we can do is, initialize K-means lots of times and run K-means lots of times, and use that to try to make sure we get as good a solution, as good a local or global optima as possible."
  },
  {
    "index": "F17054",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、これが、そのやり方だ。",
    "output": "Concretely, here's how you could go about doing that."
  },
  {
    "index": "F17055",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのループを100回実行する。そしてそれはかなりありがちな数で、だいたい50から1000の間のあたりの回数だと思う。",
    "output": "Let's say, I decide to run K-meanss a hundred times so I'll execute this loop a hundred times and it's fairly typical a number of times when came to will be something from 50 up to may be 1000."
  },
  {
    "index": "F17056",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、K-meansを100回実行しよう、と決めたとしよう。",
    "output": "So, let's say you decide to say K-means one hundred times."
  },
  {
    "index": "F17057",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはどういう事かというと、K-meansをランダム初期化するという事でそしてこれらの100回のランダム初期化の時に、毎回K-meansを走らせる。",
    "output": "So what that means is that we would randomnly initialize K-means."
  },
  {
    "index": "F17058",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとクラスタとクラスタ重心の集合が得られて、そしてそれを用いてディストーション関数Jを計算する。つまりこの得られたクラスタ割り振りとクラスタ重心の集合に対して、コスト関数をそれぞれ計算していく。",
    "output": "And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings, and a set of cluster centroids, and then we would then compute the distortion J, that is compute this cause function on the set of cluster assignments and cluster centroids that we got."
  },
  {
    "index": "F17059",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、これらのプロセス全てを100回行ったら、100個の異なる、データをクラスタリングする方法が得られる事になる。",
    "output": "You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data, just pick one, that gives us the lowest cost."
  },
  {
    "index": "F17060",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこから最後に行うのは、これら全ての100通りの見つけたクラスタリングのデータから、単純に一番コストが低い物を選ぶだけ。",
    "output": "And it turns out that if you are running K-means with a fairly small number of clusters , so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often, can sometimes make sure that you find a better local optima."
  },
  {
    "index": "F17061",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしかなり少ない数のクラスター数に対してK-meansを走らせれば、つまり、そうだなぁ、クラスタの数がだいたい2から10とかその位の範囲なら、複数回のランダム初期化を行う事はかなりしばしばより良い局所最適を見つけた事を確認出来る、より良いクラスタリングのデータを見つけた事を。",
    "output": "Make sure you find the better clustering data."
  },
  {
    "index": "F17062",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもKがとても大きい時は、Kが10よりもずっと大きく、そうだなぁ、だいたいもしあなたが何百ものクラスタを見つけようとしている時には、その時には複数回のランダム初期化を行っても、そんなに大きな違いは無いだろう。そして最初のランダム初期化ですでにかなり良い解が得られている公算が高い。",
    "output": "But if K is very large, so, if K is much greater than 10, certainly if K were, you know, if you were trying to find hundreds of clusters, then, having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing, doing multiple random initializations will probably give you a slightly better solution but, but maybe not that much."
  },
  {
    "index": "F17063",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして複数回、ランダム初期化を行ったら、たぶん、ちょっとはマシな解は得られるだろうが、でもそんなには変わらない。だが、相対的に小さな数のクラスタ数の範囲に居る時は、とくに2とか3とか4個のクラスタの時には、複数回ランダム初期化は、ディストーション関数をちゃんと最小化し、ひいては良いクラスタリングを与えてくれている、と確認するのに、やるとやらないとでは大違いとなる。",
    "output": "But it's really in the regime of where you have a relatively small number of clusters, especially if you have, maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering."
  },
  {
    "index": "F17064",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以上がK-meansのランダム初期化だ。",
    "output": "So, that's K-means with random initialization."
  },
  {
    "index": "F17065",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし相対的に小さな数のクラスタ、たとえば2とか3とか4とか5とか、うーん、6とか7でも、それらの数のクラスタで学習させたい時には複数回ランダム初期化を用いる事で、より良いデータのクラスタリングが得られる助けとなる事がある。",
    "output": "If you're trying to learn a clustering with a relatively small number of clusters, 2, 3, 4, 5, maybe, 6, 7, using multiple random initializations can sometimes, help you find much better clustering of the data."
  },
  {
    "index": "F17066",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、大きな数のクラスタの時だって、ここで説明したランダム初期化の方法はK-meansに良いクラスタを探させるリーズナブルなスタート地点となるだろう。",
    "output": "But, even if you are learning a large number of clusters, the initialization, the random initialization method that I describe here. That should give K-means a reasonable starting point to start from for finding a good set of clusters."
  },
  {
    "index": "F17067",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、最後に残った一つのK-meansについての詳細であるどうやってクラスタの数を選ぶのか、またはパラメータの大文字Kをどうやって選ぶのか、という事を議論する。",
    "output": "In this video I'd like to talk about one last detail of K-means clustering which is how to choose the number of clusters, or how to choose the value of the parameter capsule K."
  },
  {
    "index": "F17068",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを自動で解決するような方法も。現在の所、もっとも一般的なクラスタ数の選び方は可視化した結果を見てか、クラスタアルゴリズムの結果を見てか、その他何かを見て、手動で選ぶという方法だ。",
    "output": "To be honest, there actually isn't a great way of answering this or doing this automatically and by far the most common way of choosing the number of clusters, is still choosing it manually by looking at visualizations or by looking at the output of the clustering algorithm or something else."
  },
  {
    "index": "F17069",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがどうやってクラスタ数を選ぶかという質問をほんとに良く受けるので、現在のところの人々のこの件に関する考えをお話したい。",
    "output": "But I do get asked this question quite a lot of how do you choose the number of clusters, and so I just want to tell you know what are peoples' current thinking on it although, the most common thing is actually to choose the number of clusters by hand."
  },
  {
    "index": "F17070",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故クラスタの総数を選ぶのがいつも簡単にはいかないかというと、その理由の大部分はそもそもにデータに幾つのクラスタがあるのか、というのは曖昧な事があるからだ。",
    "output": "A large part of why it might not always be easy to choose the number of clusters is that it is often generally ambiguous how many clusters there are in the data."
  },
  {
    "index": "F17071",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このデータセットを見てくれ。何人かは4つのクラスタに見えるかもしれない、その場合はK=4を提案する事になる。",
    "output": "Looking at this data set some of you may see four clusters and that would suggest using K equals 4."
  },
  {
    "index": "F17072",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "またある人々には二つのクラスタに見えるかもしれない。その場合はK=2を提案する事になる。",
    "output": "Or some of you may see two clusters and that will suggest K equals 2 and now this may see three clusters."
  },
  {
    "index": "F17073",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、こんなデータセットを見てみる場合、クラスタの総数は、これは本質的に曖昧に見える。そして一つの正しい正解があるようには思えない。",
    "output": "And so, looking at the data set like this, the true number of clusters, it actually seems genuinely ambiguous to me, and I don't think there is one right answer."
  },
  {
    "index": "F17074",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは教師無し学習という物の一部だ。",
    "output": "And this is part of our supervised learning."
  },
  {
    "index": "F17075",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ラベルが与えられていないので、明確な答えがいつもあるとは限らない。",
    "output": "We are aren't given labels, and so there isn't always a clear cut answer."
  },
  {
    "index": "F17076",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれが、幾つのクラスタが望ましいかを自動で決めるアルゴリズムを作るのがより難しい理由の一つである。",
    "output": "And this is one of the things that makes it more difficult to say, have an automatic algorithm for choosing how many clusters to have."
  },
  {
    "index": "F17077",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "人々がクラスタの総数を選ぶ方法を議論している時に、ちょくちょく出てくる方法にエルボー(肘)法というのがある。",
    "output": "When people talk about ways of choosing the number of clusters, one method that people sometimes talk about is something called the Elbow Method."
  },
  {
    "index": "F17078",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それについてちょっと説明しよう。そしてその後にその利点と欠点についても言及する。",
    "output": "Let me just tell you a little bit about that, and then mention some of its advantages but also shortcomings."
  },
  {
    "index": "F17079",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、エルボー法。我らがやる事は、Kを変化させていって、、、ここでKはクラスタの総数だ。",
    "output": "So the Elbow Method, what we're going to do is vary K, which is the total number of clusters."
  },
  {
    "index": "F17080",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてまず、K-meansを、1個のクラスタで実行する、それはつまり、全てを一つのクラスタにグルーピングして、そしてコスト関数またはディストーション関数であるJを計算してここにプロットする、という事。",
    "output": "So, we're going to run K-means with one cluster, that means really, everything gets grouped into a single cluster and compute the cost function or compute the distortion J and plot that here."
  },
  {
    "index": "F17081",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にK-meansを二つのクラスタに対して実行する、ランダムの初期化に対してかもしれないし、そうで無いかもしれない。",
    "output": "And then we're going to run K means with two clusters, maybe with multiple random initial agents, maybe not."
  },
  {
    "index": "F17082",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "なんにせよ、二つのクラスタに対しての方がディストーションは小さくなる事が期待される、だからこんな感じにプロットしておく。",
    "output": "But then, you know, with two clusters we should get, hopefully, a smaller distortion, and so plot that there."
  },
  {
    "index": "F17083",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にK-meansを3つのクラスタに対して実行すると、期待される事としてはより小さなディストーションとなる。",
    "output": "And then run K-means with three clusters, hopefully, you get even smaller distortion and plot that there."
  },
  {
    "index": "F17084",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらにK-meansを4、5などに対して走らせていく。",
    "output": "I'm gonna run K-means with four, five and so on."
  },
  {
    "index": "F17085",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最終的には、クラスタ数を増加させていくと、ディストーションがどう現象していくかを示す曲線となる。",
    "output": "And so we end up with a curve showing how the distortion, you know, goes down as we increase the number of clusters."
  },
  {
    "index": "F17086",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、こんな曲線が得られる。",
    "output": "And so we get a curve that maybe looks like this."
  },
  {
    "index": "F17087",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのカーブを見てみると、エルボー法がやる事は、「あー、このプロットを見て見てくれ、ここにエルボー(肘)っぽい形がはっきり見えるだろ?",
    "output": "And if you look at this curve, what the Elbow Method does it says \"Well, let's look at this plot."
  },
  {
    "index": "F17088",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これは人間の手みたいだ、という事だ。手を伸ばした所を想像してみると、これが肩の関節、これがエルボーの関節、そして手はここで終わる。",
    "output": "Right, this is, would be by analogy to the human arm where, you know, if you imagine that you reach out your arm, then, this is your shoulder joint, this is your elbow joint and I guess, your hand is at the end over here."
  },
  {
    "index": "F17089",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれはエルボー法と呼ばれる。",
    "output": "And so this is the Elbow Method."
  },
  {
    "index": "F17090",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの手のパターンが見つかったら、つまりディストーションが1から2、2から3へと急激に変化したら、そして3で肘に到達したら、そしてそこからディストーションがとてもゆっくりに低下したら、その場合はどうやら、三つのクラスタを使うのがクラスタの正しい総数かもしれない。",
    "output": "Then you find this sort of pattern where the distortion goes down rapidly from 1 to 2, and 2 to 3, and then you reach an elbow at 3, and then the distortion goes down very slowly after that."
  },
  {
    "index": "F17091",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならそれがこの曲線の肘(エルボー)だからだ。でしょ?",
    "output": "And then it looks like, you know what, maybe using three clusters is the right number of clusters, because that's the elbow of this curve, right?"
  },
  {
    "index": "F17092",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこではディストーションはKが3になるまで急激に減少していて、そこからはとても緩慢にしか減少しなくなる。",
    "output": "That it goes down, distortion goes down rapidly until K equals 3, really goes down very slowly after that."
  },
  {
    "index": "F17093",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからK=3を選ぶ事としよう。",
    "output": "So let's pick K equals 3."
  },
  {
    "index": "F17094",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしエルボー法を適用して実際にこんな見た目のプロットが得られたなら、それはとてもグッド!これは合理的なクラスタの総数の決め方となる。",
    "output": "If you apply the Elbow Method, and if you get a plot that actually looks like this, then, that's pretty good, and this would be a reasonable way of choosing the number of clusters."
  },
  {
    "index": "F17095",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実のところ、エルボー法は、そんなに良くは使われていない。その理由の一つには、実際にこれをクラスタの問題に使ってみると、かなりしばしば、もっと曖昧な、こんな感じの曲線が得られる事がある。",
    "output": "It turns out the Elbow Method isn't used that often, and one reason is that, if you actually use this on a clustering problem, it turns out that fairly often, you know, you end up with a curve that looks much more ambiguous, maybe something like this."
  },
  {
    "index": "F17096",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうも、明確なエルボーは無く見える。",
    "output": "And if you look at this, I don't know, maybe there's no clear elbow, but it looks like distortion continuously goes down, maybe 3 is a good number, maybe 4 is a good number, maybe 5 is also not bad."
  },
  {
    "index": "F17097",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれを実際にやってみると、もしプロットした結果が左側みたいだったら、それは素晴らしい。",
    "output": "And so, if you actually do this in a practice, you know, if your plot looks like the one on the left and that's great."
  },
  {
    "index": "F17098",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "割と明確な答えを提供してくれている。でも同じくらいしばしば、右側みたいなプロットを得るはめにもなり、そこではどこがエルボーの場所なのか、あまりはっきりしない。",
    "output": "It gives you a clear answer, but just as often, you end up with a plot that looks like the one on the right and is not clear where the ready location of the elbow is."
  },
  {
    "index": "F17099",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合はこの手法を使ってクラスタの数を決めるのはより難しい。",
    "output": "It makes it harder to choose a number of clusters using this method."
  },
  {
    "index": "F17100",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり簡単にエルボー法について要約すると、一発試してみる価値はある、だがどんな問題にもとてもうまくいく、などと期待はしない方がいい。",
    "output": "So maybe the quick summary of the Elbow Method is that is worth the shot but I wouldn't necessarily, you know, have a very high expectation of it working for any particular problem."
  },
  {
    "index": "F17101",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だいたいは、人々はK-meansを、のちの問題で使う為のクラスタを得る為に用いる。言い換えるとある種の下流の目的の為に。",
    "output": "Finally, here's one other way of how, thinking about how you choose the value of K, very often people are running K-means in order you get clusters for some later purpose, or for some sort of downstream purpose."
  },
  {
    "index": "F17102",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-meansを、マーケットのセグメンテーションの為に使いたいのかもしれない。ここまで話してきたTシャツの例のように。",
    "output": "Maybe you want to use K-means in order to do market segmentation, like in the T-shirt sizing example that we talked about."
  },
  {
    "index": "F17103",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "K-meansをコンピュータのクラスタをより良く構成する為に使いたいのかもしれない。または別の目的の為にクラスタを学習させたいのかもしれない。",
    "output": "Maybe you want K-means to organize a computer cluster better, or maybe a learning cluster for some different purpose, and so, if that later, downstream purpose, such as market segmentation."
  },
  {
    "index": "F17104",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしもあとに続く、下流の目的、マーケットのセグメンテーションのような物、もしそれが評価指標を与えてくれるなら、その場合はより良いクラスタ総数の決定方法はそれぞれのクラスタ数がどれだけ下流の目的にうまく貢献出来るかを見てみる事だ。",
    "output": "If that gives you an evaluation metric, then often, a better way to determine the number of clusters, is to see how well different numbers of clusters serve that later downstream purpose."
  },
  {
    "index": "F17105",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体例を見ていこう。",
    "output": "Let me step through a specific example."
  },
  {
    "index": "F17106",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでもTシャツのサイズの例を見ていく事にする。三つのTシャツのサイズにしたいのか?",
    "output": "Let me go through the T-shirt size example again, and I'm trying to decide, do I want three T-shirt sizes?"
  },
  {
    "index": "F17107",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうなら、K=3を選ぶ、たぶんsmall、medium、largeのTシャツとなるだろう。",
    "output": "So, I choose K equals 3, then I might have small, medium and large T-shirts."
  },
  {
    "index": "F17108",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "またはK=5を選ぶかもしれない、その時は、extrasmall、small、medium、largeとextralargeのTシャツサイズとなるだろう。",
    "output": "Or maybe, I want to choose K equals 5, and then I might have, you know, extra small, small, medium, large and extra large T-shirt sizes."
  },
  {
    "index": "F17109",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、3つとか4つとか5つのTシャツサイズがあり得る。",
    "output": "So, you can have like 3 T-shirt sizes or four or five T-shirt sizes."
  },
  {
    "index": "F17110",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "4つのTシャツのサイズはあり得るのだが、しかし便宜上、だが簡単の為、ここでは3と5をスライドをシンプルにする為見せておく。",
    "output": "We could also have four T-shirt sizes, but I'm just showing three and five here, just to simplify this slide for now."
  },
  {
    "index": "F17111",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、K-MeansをK=3で走らせると、結果としては例えばここがsmallで、ここがmedium、ここがlargeという感じになる。",
    "output": "So, if I run K-means with K equals 3, maybe I end up with, that's my small and that's my medium and that's my large."
  },
  {
    "index": "F17112",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、5クラスタでK-meansを実行すると、結局得られるのはたとえば、これらがextrasmallのTシャツ、これらがsmall、これらがmedium、これらがlarge、これらがextralargeだ。",
    "output": "Whereas, if I run K-means with 5 clusters, maybe I end up with, those are my extra small T-shirts, these are my small, these are my medium, these are my large and these are my extra large."
  },
  {
    "index": "F17113",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの例で良い事として、これはまた、3か4か5個のクラスターを選ぶ、もう一つの方法を与える。",
    "output": "And the nice thing about this example is that, this then maybe gives us another way to choose whether we want 3 or 4 or 5 clusters, and in particular, what you can do is, you know, think about this from the perspective of the T-shirt business and ask: \"Well if I have five segments, then how well will my T-shirts fit my customers and so, how many T-shirts can I sell?"
  },
  {
    "index": "F17114",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、行える事は、Tシャツビジネスの観点からこの事を考えてみて、こう問うてみる事だ:「うーん、もし私が5つの顧客セグメントを持ったとしたら、どれだけの顧客に私のTシャツはフィットして、どれだけの数私のTシャツは売れるのか?私の顧客は、どれだけ幸せだろうか?",
    "output": "How happy will my customers be?\" What really makes sense, from the perspective of the T-shirt business, in terms of whether, I want to have Goer T-shirt sizes so that my T-shirts fit my customers better."
  },
  {
    "index": "F17115",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Tシャツビジネスの観点から何が最も説得力があるか、私は自分たちのTシャツが顧客にフィットするようにより多くのサイズがある事を望んでいるのか、それともTシャツのサイズを少なくして作らなくてはいけないTシャツのサイズを減らしたいのか。",
    "output": "Or do I want to have fewer T-shirt sizes so that I make fewer sizes of T-shirts."
  },
  {
    "index": "F17116",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、Tシャツを売るビジネスという観点が3つのクラスタと5つのクラスタのどちらを選ぶのかを決める方法を、提供してくれるかもしれない。",
    "output": "And so, the t-shirt selling business, that might give you a way to decide, between three clusters versus five clusters."
  },
  {
    "index": "F17117",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、下流でどう使うかの目的が例えばどんなTシャツを作るか、とかが、クラスタ数を選ぶ為にクラスタの数を評価する指標を与えてくれる事がどんな風にありうるかの例だ。",
    "output": "So, that gives you an example of how a later downstream purpose like the problem of deciding what T-shirts to manufacture, how that can give you an evaluation metric for choosing the number of clusters."
  },
  {
    "index": "F17118",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "プログラムの課題をやってる人なら、もし今週のK-meansに関するプログラムの課題を見てみたら、そこにはK-meansを画像圧縮に使う例がある。",
    "output": "For those of you that are doing the program exercises, if you look at this week's program exercise associative K-means, that's an example there of using K-means for image compression."
  },
  {
    "index": "F17119",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでは、どれだけの数のクラスタをその問題に使うべきかを選ぼうとするなら、そこでもまた、画像圧縮の評価指標を使って、クラスターの総数Kを選ぶ事になる。",
    "output": "And so if you were trying to choose how many clusters to use for that problem, you could also, again use the evaluation metric of image compression to choose the number of clusters, K?"
  },
  {
    "index": "F17120",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、どれだけ画像を良く保ちたいかと、どれだけ画像のファイルサイズを圧縮したいのか。実際にプログラムの課題をやったら、今私の言った事がもっと良く分かるだろう。",
    "output": "So, how good do you want the image to look versus, how much do you want to compress the file size of the image, and, you know, if you do the programming exercise, what I've just said will make more sense at that time."
  },
  {
    "index": "F17121",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではまとめだ。大多数のケースで、クラスター数Kを選ぶのは未だに手動で、人間による入力または洞察によって為されている。",
    "output": "So, just summarize, for the most part, the number of customers K is still chosen by hand by human input or human insight."
  },
  {
    "index": "F17122",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがそれはいつもうまくいく、と期待出来るほどの物じゃない。それよりも、クラスターの数を選ぶより良い方法と考えられるのは、どういう目的であなたがK-meansを実行しているのか、という事を問うてみる事だ。",
    "output": "One way to try to do so is to use the Elbow Method, but I wouldn't always expect that to work well, but I think the better way to think about how to choose the number of clusters is to ask, for what purpose are you running K-means?"
  },
  {
    "index": "F17123",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてどんなクラスターの数Kが、K-meansを走らせる目的となっているあとに続く問題にーーそれがなんであれーーうまく寄与するかを。",
    "output": "And then to think, what is the number of clusters K that serves that, you know, whatever later purpose that you actually run the K-means for."
  },
  {
    "index": "F17124",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、二番目の種類の教師なし学習の問題である、次元削減について始めたいと思う。",
    "output": "In this video, I'd like to start talking about a second type of unsupervised learning problem called dimensionality reduction."
  },
  {
    "index": "F17125",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次元を削減したい、と思う幾つかの異なる理由が存在する。",
    "output": "There are a couple of different reasons why one might want to do dimensionality reduction."
  },
  {
    "index": "F17126",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つはデータを圧縮したい時、そして後で見るように、2,3後のビデオで見るように、データ圧縮はデータを圧縮してより少ないメモリやディスク容量を占有するようになるだけでなく、また学習アルゴリズムのスピードアップにもなる。",
    "output": "One is data compression, and as we'll see later, a few videos later, data compression not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms."
  },
  {
    "index": "F17127",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがまずは、次元削減とは何かを話す事から始めよう。",
    "output": "But first, let's start by talking about what is dimensionality reduction."
  },
  {
    "index": "F17128",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "動機を理解する例として、たくさん、たくさん、たーくさんのフィーチャーのデータセットを集めたとしよう。そしてそれらのうちの二つだけをここにプロットした。",
    "output": "As a motivating example, let's say that we've collected a data set with many, many, many features, and I've plotted just two of them here."
  },
  {
    "index": "F17129",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして例えばそれら二つのフィーチャーは実際は何かのセンチメートルによる長さと、それとは別のフィーチャーx2はインチによる長さだとしよう。",
    "output": "And let's say that unknown to us two of the features were actually the length of something in centimeters, and a different feature, x2, is the length of the same thing in inches."
  },
  {
    "index": "F17130",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、極めて冗長な表現となっていて、x1とx2の別々のフィーチャーを保持し続けるよりは、それら両方が基本的には同じ長さを測っているのだから、我らはデータを削減して一次元にしたい、と思うだろう。そしてこの長さを測る数字一つだけを保持すれば良い。",
    "output": "So, this gives us a highly redundant representation and maybe instead of having two separate features x1 then x2, both of which basically measure the length, maybe what we want to do is reduce the data to one-dimensional and just have one number measuring this length."
  },
  {
    "index": "F17131",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例はちょっとうさんくさいように思うかもしれない。でも実の所、このセンチメーターとインチの例は、そんなに非現実的でも無い。",
    "output": "In case this example seems a bit contrived, this centimeter and inches example is actually not that unrealistic, and not that different from things that I see happening in industry."
  },
  {
    "index": "F17132",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし何百、何千ものフィーチャーがあるのなら、すぐに、簡単に何のフィーチャーを持っていたのか、トラック出来なくなるものだ。",
    "output": "If you have hundreds or thousands of features, it is often this easy to lose track of exactly what features you have."
  },
  {
    "index": "F17133",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして時には、別々のエンジニアのチームが、例えばあるエンジニアのチームが200のフィーチャーをあなたに与え、二番目のエンジニアチームがまた別の300フィーチャーを与え、そして三番目のエンジニアチームが500フィーチャーをあなたに提供したとする。すると全部で1000フィーチャーとなる訳だが、それは実際にどのフィーチャーがどこのチームから来た物かなどを正確にトラックするのは難しくなる。",
    "output": "And sometimes may have a few different engineering teams, maybe one engineering team gives you two hundred features, a second engineering team gives you another three hundred features, and a third engineering team gives you five hundred features so you have a thousand features all together, and it actually becomes hard to keep track of you know, exactly which features you got from which team, and it's actually not that want to have highly redundant features like these."
  },
  {
    "index": "F17134",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしセンチメーターの長さが近傍のセンチメーターに丸めてしまって、インチの長さは近傍のインチに丸めてしまえば、それが理由でこの手本は直線に完全には乗らない事になる、何故なら、近傍のセンチメーターなり近傍のインチなりへの丸め誤差の為に。",
    "output": "And so if the length in centimeters were rounded off to the nearest centimeter and lengthened inches was rounded off to the nearest inch. Then, that's why these examples don't lie perfectly on a straight line, because of, you know, round-off error to the nearest centimeter or the nearest inch."
  },
  {
    "index": "F17135",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしデータを2次元の代わりに1次元に削減出来たら、冗長性も削減出来る。",
    "output": "And if we can reduce the data to one dimension instead of two dimensions, that reduces the redundancy."
  },
  {
    "index": "F17136",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "別の例としては、これもまた嘘っぽいかもしれないが、自律的なヘリコプターパイロットと何年も仕事をしてきた。",
    "output": "For may years I've been working with autonomous helicopter pilots."
  },
  {
    "index": "F17137",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると私はヘリコプターを飛ばすパイロットと仕事をしてきた。",
    "output": "Or I've been working with pilots that fly helicopters."
  },
  {
    "index": "F17138",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、だ。",
    "output": "And so."
  },
  {
    "index": "F17139",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし仮に、、、もし仮に調査なり試験なりをこれらの別々のパイロットに実施したとして、そこでは一つのフィーチャーx1として例えばこれらのヘリコプターパイロットのスキルだとして、そしてx2は例えばパイロットの楽しみ度合いだとする。",
    "output": "If you were to measure--if you were to, you know, do a survey or do a test of these different pilots--you might have one feature, x1, which is maybe the skill of these helicopter pilots, and maybe \"x2\" could be the pilot enjoyment."
  },
  {
    "index": "F17140",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、彼らがどれだけ飛行を楽しんだのか、だ。これら二つのフィーチャーは高く相関しているだろう。",
    "output": "That is, you know, how much they enjoy flying, and maybe these two features will be highly correlated."
  },
  {
    "index": "F17141",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして本当に気になっているのは、この種の、この種の、この方向、本当にパイロットの適性を測っている、別のフィーチャーかもしれない。",
    "output": "And what you really care about might be this sort of this sort of, this direction, a different feature that really measures pilot aptitude."
  },
  {
    "index": "F17142",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私は適性、という名前を創り上げた。だがふたたび、もし高く相関したフィーチャーがあれば、実際に次元を削減したくなるだろう。",
    "output": "And I'm making up the name aptitude of course, but again, if you highly correlated features, maybe you really want to reduce the dimension."
  },
  {
    "index": "F17143",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、もうちょっと詳しく、データの次元を2次元から、2Dから1次元、1Dへと削減する、という事が本当は何を意味しているかを説明しよう。",
    "output": "So, let me say a little bit more about what it really means to reduce the dimension of the data from 2 dimensions down from 2D to 1 dimensional or to 1D."
  },
  {
    "index": "F17144",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの手本を別々の色で、色付けしよう。",
    "output": "Let me color in these examples by using different colors."
  },
  {
    "index": "F17145",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのケースでは、次元を削減する、という時に意味している事は、例えばこの直線を探して、もっとも多くのデータが載るような直線の方向を探して、そこに全てのデータを射影する、そうする事で、この直線上の各手本の位置を測る事が出来る。",
    "output": "And in this case by reducing the dimension what I mean is that I would like to find maybe this line, this, you know, direction on which most of the data seems to lie and project all the data onto that line which is true, and by doing so, what I can do is just measure the position of each of the examples on that line."
  },
  {
    "index": "F17146",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして新しい一つのフィーチャーz1という考えに行き着く。直線上の位置を示すのに一つの数だけしか必要としなくなったので、つまりz1はこれらの点のこの緑の直線上の位置を示す、新しいフィーチャーなのだ。",
    "output": "And what I can do is come up with a new feature, z1, and to specify the position on the line I need only one number, so it says z1 is a new feature that specifies the location of each of those points on this green line."
  },
  {
    "index": "F17147",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これの意味する所は、前と同じに手本x1があったとすると、例えばこれは最初の手本x1だとする。",
    "output": "And what this means, is that where as previously if i had an example x1, maybe this was my first example, x1."
  },
  {
    "index": "F17148",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、x1を表す為にはもともとのx1には二次元の数が必要だった、または二次元のフィーチャーベクトルが必要だった。",
    "output": "So in order to represent x1 originally x1. I needed a two dimensional number, or a two dimensional feature vector."
  },
  {
    "index": "F17149",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その代わりにここでは、z1で表す事が出来る。",
    "output": "Instead now I can represent z1."
  },
  {
    "index": "F17150",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私は最初の手本を表すのに、z1だけで表す事が出来る、ここでz1は実数。",
    "output": "I could use just z1 to represent my first example, and that's going to be a real number."
  },
  {
    "index": "F17151",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様にx2は、x2をここの二番目の手本とすると、以前はこれを表すのに二つの数が必要だったが、もし代わりに直線上の黒い線に射影した物を計算すれば、そうすれば今や、私はこのz2の線上の位置を示すには、たった一つの実数しか必要としない。",
    "output": "And similarly x2 you know, if x2 is my second example there, then previously, whereas this required two numbers to represent if I instead compute the projection of that black cross onto the line."
  },
  {
    "index": "F17152",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし元のデータセットを、全ての手本を緑の直線に射影する、という近似を許容するなら、その時は一つの数しか必要としない、直線の上の点の場所を示すのに、たった一つの実数しか必要としない。",
    "output": "And now I only need one real number which is z2 to represent the location of this point z2 on the line. And so on through my M examples."
  },
  {
    "index": "F17153",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ただ一つの数を使って、各手本の位置を表す事が出来る、各手本を、緑の直線に射影した後では。",
    "output": "So, just to summarize, if we allow ourselves to approximate the original data set by projecting all of my original examples onto this green line over here, then I need only one number, I need only real number to specify the position of a point on the line, and so what I can do is therefore use just one number to represent the location of each of my training examples after they've been projected onto that green line."
  },
  {
    "index": "F17154",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、元のトレーニングセットを近似した物となっている、何故ならトレーニング手本を直線に射影しているから。",
    "output": "So this is an approximation to the original training self because I have projected all of my training examples onto a line."
  },
  {
    "index": "F17155",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、いまや私は各手本に対してたった一つの数を保持するだけで良くなっている。",
    "output": "But now, I need to keep around only one number for each of my examples."
  },
  {
    "index": "F17156",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれは必要なメモリ量、または必要なスペース、またはデータを保存する方法がなんであれ、それを半減させる。",
    "output": "And so this halves the memory requirement, or a space requirement, or what have you, for how to store my data."
  },
  {
    "index": "F17157",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもっと興味深い事には、もっと重要な事としては、後で観る事になるが、あとのビデオで、それは、これが我らの学習アルゴリズムをもっと早く走らせてくれる、という事だ。",
    "output": "And perhaps more interestingly, more importantly, what we'll see later, in the later video as well is that this will allow us to make our learning algorithms run more quickly as well."
  },
  {
    "index": "F17158",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれは実際、たぶん、データを保持するディスクスペース要件やメモリ要件を減らす、という事よりも、より興味深いデータ圧縮の適用例と言えるだろう。",
    "output": "And that is actually, perhaps, even the more interesting application of this data compression rather than reducing the memory or disk space requirement for storing the data."
  },
  {
    "index": "F17159",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前のスライドでは、データを2Dから1Dへと削減する例を見た。",
    "output": "On the previous slide we showed an example of reducing data from 2D to 1D."
  },
  {
    "index": "F17160",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このスライドでは、別のデータ削減の例である、3次元の3Dから二次元の2Dへの削減をお見せする。",
    "output": "On this slide, I'm going to show another example of reducing data from three dimensional 3D to two dimensional 2D."
  },
  {
    "index": "F17161",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、より典型的な次元削減の例としては、1000次元、つまり1000Dとかのデータとかもあり、それを例えば100次元または100Dに削減したい、とかいう事がある。",
    "output": "By the way, in the more typical example of dimensionality reduction we might have a thousand dimensional data or 1000D data that we might want to reduce to let's say a hundred dimensional or 100D, but because of the limitations of what I can plot on the slide."
  },
  {
    "index": "F17162",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがスライドにプロット可能な限界という制限の為に、3Dから2Dと、2Dから1Dの例を使っていく。",
    "output": "I'm going to use examples of 3D to 2D, or 2D to 1D."
  },
  {
    "index": "F17163",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、ここに見せたようなデータセットがあるとする。",
    "output": "So, let's have a data set like that shown here."
  },
  {
    "index": "F17164",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、手本の集合x(i)があり、それはR3の点の集まりだ。",
    "output": "And so, I would have a set of examples x(i) which are points in r3."
  },
  {
    "index": "F17165",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、三次元の手本がある。",
    "output": "So, I have three dimension examples."
  },
  {
    "index": "F17166",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはスライド上では見づらいとは思うが、3次元プロットの雲を一応見せておく。",
    "output": "I know it might be a little bit hard to see this on the slide, but I'll show a 3D point cloud in a little bit."
  },
  {
    "index": "F17167",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ちょっと見づらいけど、このデータはだいたい全部平面に乗っている。こんな感じ。",
    "output": "And it might be hard to see here, but all of this data maybe lies roughly on the plane, like so."
  },
  {
    "index": "F17168",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで次元削減でやる事としては、このデータを全部持ってきて二次元平面に射影する事。",
    "output": "And so what we can do with dimensionality reduction, is take all of this data and project the data down onto a two dimensional plane."
  },
  {
    "index": "F17169",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで私がやった事は、データを全部持ってきて、全てのデータを平面上に乗るように射影した、という事。",
    "output": "So, here what I've done is, I've taken all the data and I've projected all of the data, so that it all lies on the plane."
  },
  {
    "index": "F17170",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、最終的に、平面上の位置を示す為に、二つの数字が必要だ。でしょ?",
    "output": "Now, finally, in order to specify the location of a point within a plane, we need two numbers, right?"
  },
  {
    "index": "F17171",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この軸に沿った点の位置を、示す必要があり、そしてまたこの軸に沿った場所も示す必要がある。つまり我らは二つの数字が要る。",
    "output": "We need to, maybe, specify the location of a point along this axis, and then also specify it's location along that axis."
  },
  {
    "index": "F17172",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "z1とz2と呼ぼうか。平面上の点の場所を示す為に。",
    "output": "So, we need two numbers, maybe called z1 and z2 to specify the location of a point within a plane."
  },
  {
    "index": "F17173",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれの意味する所は、今や我らは各手本を各トレーニング手本を、ここに書いた二つの数z1とz2で、表す事が出来る。",
    "output": "And so, what that means, is that we can now represent each example, each training example, using two numbers that I've drawn here, z1, and z2."
  },
  {
    "index": "F17174",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり我らはデータを、R2のベクトルzを用いて表す事が出来る、という事だ。",
    "output": "So, our data can be represented using vector z which are in r2."
  },
  {
    "index": "F17175",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの下付き添字、z下付き添字1、z下付き添字2は、それが意味するのは、このベクトルzは、二次元のベクトルz1とz2だ、という事だ。",
    "output": "And these subscript, z subscript 1, z subscript 2, what I just mean by that is that my vectors here, z, you know, are two dimensional vectors, z1, z2."
  },
  {
    "index": "F17176",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもし私はある特定の手本、z(i)があるとする、またはそれは二次元ベクトルz(i)1とz(i)2だ。",
    "output": "And so if I have some particular examples, z(i), or that's the two dimensional vector, z(i)1, z(i)2."
  },
  {
    "index": "F17177",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして前のスライドで、データを一次元に削減した時、z1しか無かった。",
    "output": "And on the previous slide when I was reducing data to one dimensional data then I had only z1, right?"
  },
  {
    "index": "F17178",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれこそが前のスライドでのz(1)下付き添字1だった。だがここでは二次元のデータなので、z1とz2を持ってる。",
    "output": "And that is what a z1 subscript 1 on the previous slide was, but here I have two dimensional data, so I have z1 and z2 as the two components of the data."
  },
  {
    "index": "F17179",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さてこれらの図の意味を確認しよう。",
    "output": "Now, let me just make sure that these figures make sense."
  },
  {
    "index": "F17180",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その為に、これらの三つの図を全く同じ物を、けど3Dプロットで再掲する。",
    "output": "So let me just reshow these exact three figures again but with 3D plots."
  },
  {
    "index": "F17181",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らがやってきたプロセスは、まず左にあるのがもともとのデータセットで、真ん中が2Dに投影したデータセット。そして右側が、z1とz2としての、2Dのデータセット。",
    "output": "So the process we went through was that shown in the lab is the optimal data set, in the middle the data set projects on the 2D, and on the right the 2D data sets with z1 and z2 as the axis."
  },
  {
    "index": "F17182",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらをもうちょっと詳しく見てみよう。",
    "output": "Let's look at them a little bit further."
  },
  {
    "index": "F17183",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり私は、3Dの点の雲から始めた。そこでは、軸はx1,x2,x3などとラベルづけされていた。",
    "output": "Here's my original data set, shown on the left, and so I had started off with a 3D point cloud like so, where the axis are labeled x1, x2, x3, and so there's a 3D point but most of the data, maybe roughly lies on some, you know, not too far from some 2D plain."
  },
  {
    "index": "F17184",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでこんな事が出来る、このデータを持ってきて、これが真ん中の図だ。",
    "output": "So, what we can do is take this data and here's my middle figure."
  },
  {
    "index": "F17185",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを2Dに射影する。",
    "output": "I'm going to project it onto 2D."
  },
  {
    "index": "F17186",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これら全てのデータが2Dの表面に乗るように射影した。見ての通り、全てのデータは平面上にある。",
    "output": "So, I've projected this data so that all of it now lies on this 2D surface."
  },
  {
    "index": "F17187",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら、全てを平面に射影したから。つまりこれが意味する事は、今や平面上の点の位置を表すには、たった二つの数、z1とz2しか、必要としない。",
    "output": "As you can see all the data lies on a plane, 'cause we've projected everything onto a plane, and so what this means is that now I need only two numbers, z1 and z2, to represent the location of point on the plane."
  },
  {
    "index": "F17188",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、データを3次元から2次元に削減する時に行う手続きとなる。",
    "output": "And so that's the process that we can go through to reduce our data from three dimensional to two dimensional."
  },
  {
    "index": "F17189",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が次元削減で、そしてそれを用いてデータを圧縮する方法だ。",
    "output": "So that's dimensionality reduction and how we can use it to compress our data."
  },
  {
    "index": "F17190",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして後で見るように、これは我らの学習アルゴリズムをもっと早く走らせる時にもまた用いる事が出来るが、それはあとのビデオでやろう。",
    "output": "And as we'll see later this will allow us to make some of our learning algorithms run much later as well, but we'll get to that only in a later video."
  },
  {
    "index": "F17191",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオで、次元圧縮の話をしてきた。",
    "output": "In the last video, we talked about dimensionality reduction for the purpose of compressing the data."
  },
  {
    "index": "F17192",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは二番目の適用例である、データの可視化についてお話する。",
    "output": "In this video, I'd like to tell you about a second application of dimensionality reduction and that is to visualize the data."
  },
  {
    "index": "F17193",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たくさんの機械学習の応用に対してそれは本当に効率的な学習アルゴリズムを構築するのを助けてくれる。",
    "output": "For a lot of machine learning applications, it really helps us to develop effective learning algorithms, if we can understand our data better."
  },
  {
    "index": "F17194",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らがデータをより良く理解出来るか、データを可視化する、より良い方法があるかは。次元削減は我らにそれを行う新たな手段を与えてくれる。",
    "output": "If there is some way of visualizing the data better, and so, dimensionality reduction offers us, often, another useful tool to do so."
  },
  {
    "index": "F17195",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例を見る事から始めよう。",
    "output": "Let's start with an example."
  },
  {
    "index": "F17196",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "世界中の様々な国に対する、たくさんの統計量や事実に関する大量のデータを収集したとしよう。",
    "output": "Let's say we've collected a large data set of many statistics and facts about different countries around the world."
  },
  {
    "index": "F17197",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば最初のフィーチャーx1は国のGDP、つまりGrossDomesticProduct(国民総生産)かもしれない。x2はpercapita、つまり一人あたりGDPかもしれないし、x3は人間開発指数(HumanDevelopmentIndex)とか平均余命とか、x5、x6などなど。",
    "output": "So, maybe the first feature, X1 is the country's GDP, or the Gross Domestic Product, and X2 is a per capita, meaning the per person GDP, X3 human development index, life expectancy, X5, X6 and so on."
  },
  {
    "index": "F17198",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこんな風に膨大なデータセットを持ってるとする。例えば国ごとに50フィーチャーを、そして国の数も膨大とする。",
    "output": "And we may have a huge data set like this, where, you know, maybe 50 features for every country, and we have a huge set of countries."
  },
  {
    "index": "F17199",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、我らがデータをより良く理解する為に何かやれる事は無いか?",
    "output": "So is there something we can do to try to understand our data better?"
  },
  {
    "index": "F17200",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この膨大な数のテーブルを与えられたとする。",
    "output": "I've given this huge table of numbers."
  },
  {
    "index": "F17201",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このデータをどう可視化する?",
    "output": "How do you visualize this data?"
  },
  {
    "index": "F17202",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし50のフィーチャーがあると、50次元のデータをプロットするのは計算する。",
    "output": "If you have 50 features, it's very difficult to plot 50-dimensional data."
  },
  {
    "index": "F17203",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このデータを調べる、良い方法は無いものか?",
    "output": "What is a good way to examine this data?"
  },
  {
    "index": "F17204",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次元削減を用いると出来る事としては、各国をこのフィーチャーベクトルxiで表す代わりに、それは50次元な訳だが、つまり例えばカナダのような国をカナダを表す50個のフィーチャーを持つ代わりに、別のフィーチャーベクトルで表現して、それはこれらのR2のベクトルzのような物を得られるとしよう。",
    "output": "Using dimensionality reduction, what we can do is, instead of having each country represented by this featured vector, xi, which is 50-dimensional, so instead of, say, having a country like Canada, instead of having 50 numbers to represent the features of Canada, let's say we can come up with a different feature representation that is these z vectors, that is in R2."
  },
  {
    "index": "F17205",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしそれが可能なら、もし我らは50個の数字を要約するような、2つの数字のペアz1とz2が得られるなら、我らに取れる手段としては、これらの国々をR2にプロットする、そしてそれを用いて、別々の国のフィーチャー空間をより良く理解する事を試みる、という手が考えられる。",
    "output": "If that's the case, if we can have just a pair of numbers, z1 and z2 that somehow, summarizes my 50 numbers, maybe what we can do to."
  },
  {
    "index": "F17206",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうする事で、これを2次元プロットしてプロット出来るようになる。",
    "output": "It's often up to us to figure out you know, roughly what these features means."
  },
  {
    "index": "F17207",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、もしこれらのフィーチャーをプロットすると、例えばこんな結果が得られる。",
    "output": "But, And if you plot those features, here is what you might find."
  },
  {
    "index": "F17208",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、各国は、点z(i)で表される。",
    "output": "So, here, every country is represented by a point ZI, which is an R2 and so each of those."
  },
  {
    "index": "F17209",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこの図で、各点が、一つの国を表している。つまりこれがz1でこれがz2。",
    "output": "Dots, and this figure represents a country, and so, here's Z1 and here's Z2, and of these."
  },
  {
    "index": "F17210",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、例えば横軸のz1軸はだいたいの国のサイズ、または国の全体的な経済活動のサイズに、対応している、と気づくかもしれない。",
    "output": "So, you might find, for example, That the horizontial axis the Z1 axis corresponds roughly to the overall country size, or the overall economic activity of a country."
  },
  {
    "index": "F17211",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、全体のGDP、国全体の経済活動のサイズ。",
    "output": "So the overall GDP, overall economic size of a country."
  },
  {
    "index": "F17212",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方このデータの縦軸は、一人当たりGDPなどに対応しているかもしれない。",
    "output": "Whereas the vertical axis in our data might correspond to the per person GDP."
  },
  {
    "index": "F17213",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または個人ごとの暮らし向き、または一人当たりの経済活動。そしてこれら50個のフィーチャーが与えられた時に、これら二つの次元が、もっとも主要な分散となっている、と気づくかもしれない。",
    "output": "Or the per person well being, or the per person economic activity, and, you might find that, given these 50 features, you know, these are really the 2 main dimensions of the deviation, and so, out here you may have a country like the U.S.A., which is a relatively large GDP, you know, is a very large GDP and a relatively high per-person GDP as well."
  },
  {
    "index": "F17214",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、ここには、シンガポールのような国、一人当たりのGDPがとても高くて、でもシンガポールはもっとずっと小さい国なので、シンガポールの経済全体のサイズはUSよりもずっと小さい。",
    "output": "Whereas here you might have a country like Singapore, which actually has a very high per person GDP as well, but because Singapore is a much smaller country the overall economy size of Singapore is much smaller than the US."
  },
  {
    "index": "F17215",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、こちらは、不幸にもより厚生が低く、平均余命が短かったりだとか、ヘルスケアがいまいちだったりとか、より経済的に成熟していないだとかで、かつ小さな国で、一方でこの点などは、かなりの経済活動を行っていながら、しかし個人はあまり良い暮らし向きじゃない傾向にある国に対応している。",
    "output": "And, over here, you would have countries where individuals are unfortunately some are less well off, maybe shorter life expectancy, less health care, less economic maturity that's why smaller countries, whereas a point like this will correspond to a country that has a fair, has a substantial amount of economic activity, but where individuals tend to be somewhat less well off."
  },
  {
    "index": "F17216",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、z1軸とz2軸は様々な国の間の中でもっとも違う二つの次元は何なのか、を完結に捕捉する事を助けてくれる事に、気づいただろうか。",
    "output": "So you might find that the axes Z1 and Z2 can help you to most succinctly capture really what are the two main dimensions of the variations amongst different countries."
  },
  {
    "index": "F17217",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "国の経済活動全体だとか、国のサイズを反映した経済活動全体だとか、そういった物や、各個人の健康状態や厚生、一人あたりで測ったGDP、一人あたり健康保険、などなど。",
    "output": "Such as the overall economic activity of the country projected by the size of the country's overall economy as well as the per-person individual well-being, measured by per-person GDP, per-person healthcare, and things like that."
  },
  {
    "index": "F17218",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、どうやって次元削減を用いて、データを50次元からであれ何からであれ、そこから二次元とか、もしくは三次元に削減する方法だ、それをプロットして、よりデータを理解出来るように。",
    "output": "So that's how you can use dimensionality reduction, in order to reduce data from 50 dimensions or whatever, down to two dimensions, or maybe down to three dimensions, so that you can plot it and understand your data better."
  },
  {
    "index": "F17219",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、PCAとかPrincipalComponentAnalysis(主成分分析)などと呼ばれるアルゴリズムを開発する。それは我らがここで述べた事や以前に述べたデータの圧縮のような応用に用いる事が出来る。",
    "output": "In the next video, we'll start to develop a specific algorithm, called PCA, or Principal Component Analysis, which will allow us to do this and also do the earlier application I talked about of compressing the data."
  },
  {
    "index": "F17220",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次元削減の問題において、現在の所一番人気で、一番良く使われているアルゴリズムは、主成分分析(PrincipalComponentsAnalysis)、またはPCAと呼ばれる物だ。",
    "output": "For the problem of dimensionality reduction, by far the most popular, by far the most commonly used algorithm is something called principle components analysis, or PCA."
  },
  {
    "index": "F17221",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、PCAの問題の定式化について議論を開始する。",
    "output": "In this video, I'd like to start talking about the problem formulation for PCA."
  },
  {
    "index": "F17222",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、PCAに何をして欲しいのか、を詳細かつ厳密に定式化しよう。",
    "output": "In other words, let's try to formulate, precisely, exactly what we would like PCA to do."
  },
  {
    "index": "F17223",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんなデータセットがあるとしよう。",
    "output": "Let's say we have a data set like this."
  },
  {
    "index": "F17224",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり手本xがR2にあるようなデータセット。そしてデータの次元を2次元から1次元に削減したいとしよう。",
    "output": "So, this is a data set of examples x and R2 and let's say I want to reduce the dimension of the data from two-dimensional to one-dimensional."
  },
  {
    "index": "F17225",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、私は、データを射影する先の直線を探したい。",
    "output": "In other words, I would like to find a line onto which to project the data."
  },
  {
    "index": "F17226",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、データを射影するのに良さそうな直線とは、どんな物か?",
    "output": "So what seems like a good line onto which to project the data, it's a line like this, might be a pretty good choice."
  },
  {
    "index": "F17227",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな感じの直線はかなり良い選択だと言えそうだ。",
    "output": "And the reason we think this might be a good choice is that if you look at where the projected versions of the point scales, so I take this point and project it down here."
  },
  {
    "index": "F17228",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれが良い選択だと思う理由は、もし射影された点がどこに行くのかを見てみると、つまりこの点に対して、ここに射影するとこれを得る。",
    "output": "Get that, this point gets projected here, to here, to here, to here."
  },
  {
    "index": "F17229",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この点はここに射影される、ここ、ここ、ここ、ここ、各点と射影された点との距離がとても小さくなっている事に気付くだろう。",
    "output": "What we find is that the distance between each point and the projected version is pretty small."
  },
  {
    "index": "F17230",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この青い線はきわめて短い。",
    "output": "That is, these blue line segments are pretty short."
  },
  {
    "index": "F17231",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、PCAがやる事はより低次元の平面、この場合は実際は直線になるが、そういう平面で射影する先として、これらの青い線分の二乗和を最小化する物を探そうと試みる事だ。",
    "output": "So what PCA does formally is it tries to find a lower dimensional surface, really a line in this case, onto which to project the data so that the sum of squares of these little blue line segments is minimized."
  },
  {
    "index": "F17232",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの青い線分の長さは射影誤差と呼ばれる事もある。",
    "output": "The length of those blue line segments, that's sometimes also called the projection error."
  },
  {
    "index": "F17233",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとPCAがやる事はそれを最小化するような射影先の平面を探す事と言える。",
    "output": "And so what PCA does is it tries to find a surface onto which to project the data so as to minimize that."
  },
  {
    "index": "F17234",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ちよっと脇にそれるが、PCAを適用する前にはます平均標準化を、そしてフィーチャースケーリングをかけておく、そうする事でフィーチャーx1とフィーチャーx2が平均0で、比較可能な範囲の値を持つようにしておく。",
    "output": "As an aside, before applying PCA, it's standard practice to first perform mean normalization at feature scaling so that the features x1 and x2 should have zero mean, and should have comparable ranges of values."
  },
  {
    "index": "F17235",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例に関しては既に私がやった。だがこの件については、PCAという文脈でのフィーチャースケーリングと平均標準化については、後でもっと詳しく議論する事にする。",
    "output": "I've already done this for this example, but I'll come back to this later and talk more about feature scaling and the normalization in the context of PCA later."
  },
  {
    "index": "F17236",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例に戻ると、このさっき引いた赤の直線とは別のデータを射影する直線もあり得る。このマゼンタの直線とか。",
    "output": "But coming back to this example, in contrast to the red line that I just drew, here's a different line onto which I could project my data, which is this magenta line."
  },
  {
    "index": "F17237",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "見て分かるように、このマゼンタの直線はデータの射影先としては、よりまずい方向だ。でしょ?",
    "output": "And, as we'll see, this magenta line is a much worse direction onto which to project my data, right?"
  },
  {
    "index": "F17238",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりもしこのマゼンタの直線にデータを射影すると、他の点もこんな感じで。",
    "output": "So if I were to project my data onto the magenta line, we'd get a set of points like that."
  },
  {
    "index": "F17239",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして射影誤差、つまり青い線分は巨大になる。",
    "output": "And the projection errors, that is these blue line segments, will be huge."
  },
  {
    "index": "F17240",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらの点はマゼンタの直線の上に移動するには、つまり射影するには、大きな距離を移動しなくてはならない。",
    "output": "So these points have to move a huge distance in order to get projected onto the magenta line."
  },
  {
    "index": "F17241",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりPCA、PrincipalComponentAnalysis(主成分分析)は、ここのマゼンタの直線のような物じゃなく赤い線みたいな物を選ぶ物だ。",
    "output": "And so that's why PCA, principal components analysis, will choose something like the red line rather than the magenta line down here."
  },
  {
    "index": "F17242",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PCAの問題をよりフォーマルに書こう。",
    "output": "Let's write out the PCA problem a little more formally."
  },
  {
    "index": "F17243",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PCAのゴールは仮にデータを2次元から1次元に削減したいとすると、以下のようなベクトルを探す、と言える。",
    "output": "The goal of PCA, if we want to reduce data from two-dimensional to one-dimensional is, we're going to try find a vector that is a vector u1, which is going to be an Rn, so that would be an R2 in this case."
  },
  {
    "index": "F17244",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのベクトルはuiと呼ぶ事にしよう、それはRnのベクトルで、この場合はR2だ。それは射影誤差を最小化するようなデータの射影先の方向を持つようなベクトルだ。",
    "output": "I'm gonna find the direction onto which to project the data, so it's to minimize the projection error."
  },
  {
    "index": "F17245",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの例では、PCAにはこのベクトルを探してくれる事を期待している、それをu1と呼ぼう、それの持つ性質は、そのベクトルを延長して定義した直線にデータを射影すると、とても小さな射影誤差となるようなベクトルだ。",
    "output": "So, in this example I'm hoping that PCA will find this vector, which l wanna call u(1), so that when I project the data onto the line that I define by extending out this vector, I end up with pretty small reconstruction errors."
  },
  {
    "index": "F17246",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "データの例はこんな感じだ。",
    "output": "And that reference of data that looks like this."
  },
  {
    "index": "F17247",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、PCAはu1を与えるか-u1を与えるかは、重要では無い、という事は言っておこう。",
    "output": "And by the way, I should mention that where the PCA gives me u(1) or -u(1), doesn't matter."
  },
  {
    "index": "F17248",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからPCAが正のベクトルでこの向きの物を与えたら、それはそれで良い。",
    "output": "So if it gives me a positive vector in this direction, that's fine."
  },
  {
    "index": "F17249",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "またもし、反対方向のベクトル、反対の向きを向いたベクトルを与えたら、つまり-u1だったとして、代わりに青で描くと、正のu1を与えようが-u1を与えようが、それはどっちでも良い。",
    "output": "If it gives me the opposite vector facing in the opposite direction, so that would be like minus u(1). Let's draw that in blue instead, right?"
  },
  {
    "index": "F17250",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならこれらのベクトルはどちらも、同じ赤い直線を定義する物で、そこに私はデータを射影する訳だから。",
    "output": "But it gives a positive u(1) or negative u(1), it doesn't matter because each of these vectors defines the same red line onto which I'm projecting my data."
  },
  {
    "index": "F17251",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が、データを2次元から1次元に削減するケースだ。",
    "output": "So this is a case of reducing data from two-dimensional to one-dimensional."
  },
  {
    "index": "F17252",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的には、N次元のデータセットを持っていて、そしてそれをK次元へと削減したい。",
    "output": "In the more general case we have n-dimensional data and we'll want to reduce it to k-dimensions."
  },
  {
    "index": "F17253",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合は、データを射影する先の単独のベクトルを探したいのでは無く、データを射影するK次元を探したい。",
    "output": "In that case we want to find not just a single vector onto which to project the data but we want to find k-dimensions onto which to project the data."
  },
  {
    "index": "F17254",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "射影誤差を最小化するように。",
    "output": "So as to minimize this projection error."
  },
  {
    "index": "F17255",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばこんなだ。",
    "output": "So here's the example."
  },
  {
    "index": "F17256",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "仮に、こんな感じの3D点の雲があったとしよう、そこて私が見つけたいのは、ベクトル、、、じゃなかった、ベクトルのペアを探したい。",
    "output": "If I have a 3D point cloud like this, then maybe what I want to do is find vectors. So find a pair of vectors."
  },
  {
    "index": "F17257",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私はベクトルのペアを探したい、ここを原点として、これがu1、これが二番目のベクトルu2と呼ぼう。",
    "output": "I'm going to find a pair of vectors, sustained from the origin."
  },
  {
    "index": "F17258",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらを合わせて、これら二つのベクトルが、平面を定義する。言い換えるとそれらが2D平面を定義する。",
    "output": "And together, these two vectors define a plane, or they define a 2D surface, right?"
  },
  {
    "index": "F17259",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たとえばこんな感じの2D平面、データを射影する先の。",
    "output": "Like this with a 2D surface onto which I am going to project my data."
  },
  {
    "index": "F17260",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "線形代数に通じた視聴者の方々には、真の線形代数マスターの方々にとっては、これの正式な定義は、我らはu1,u2,...,ukまでのベクトルを探す、そして我らがやる事は、このk本のベクトルが張る線形部分空間に、データを射影する、という事だ。",
    "output": "And what we're going to do is project the data onto the linear subspace spanned by this set of k vectors."
  },
  {
    "index": "F17261",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが線形代数にあんまり慣れていないなら、データを射影する先として、1方向の代わりにk方向を探す、と考えておけばよろしい。",
    "output": "But if you're not familiar with linear algebra, just think of it as finding k directions instead of just one direction onto which to project the data."
  },
  {
    "index": "F17262",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、k次元の平面を探す為に、このケースでは実際は2D平面だが、この図に示したように、ここでは平面上の点の位置を、k本の方向で定義出来る。",
    "output": "So finding a k-dimensional surface is really finding a 2D plane in this case, shown in this figure, where we can define the position of the points in a plane using k directions."
  },
  {
    "index": "F17263",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんな訳で、PCAにおいては、我らはデータを射影する先となるk本のベクトルを見つけたい。",
    "output": "And that's why for PCA we want to find k vectors onto which to project the data."
  },
  {
    "index": "F17264",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、よりフォーマルに言うと、PCAにおいては、我らがやりたい事は、ある種の射影距離、つまり射影先の点と射影元の点との距離を最小化するような、射影方法を見つけたい。",
    "output": "And so more formally in PCA, what we want to do is find this way to project the data so as to minimize the sort of projection distance, which is the distance between the points and the projections."
  },
  {
    "index": "F17265",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの3Dの例でも、所与の点に対し、この各点を2Dの平面に射影する。",
    "output": "And so in this 3D example too."
  },
  {
    "index": "F17266",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを行う時には、射影誤差は、元の点と2D平面へ射影した先の点との距離となる。",
    "output": "Given a point we would take the point and project it onto this 2D surface. We are done with that."
  },
  {
    "index": "F17267",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりPCAがやる事とは、データを射影する、直線なり平面なりそれ以外なり、とにかく射影先で、射影の二乗を最小化するような物を探すという事だ、射影とは90度、または直行する射影の誤差だ。",
    "output": "And so the projection error would be, the distance between the point and where it gets projected down to my 2D surface."
  },
  {
    "index": "F17268",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、たまに質問される事の一つに、PCAと線形回帰の関係について、というのがある。",
    "output": "And so what PCA does is I try to find the line, or a plane, or whatever, onto which to project the data, to try to minimize that square projection, that 90 degree or that orthogonal projection error."
  },
  {
    "index": "F17269",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら私がPCAを説明する時に、たまにこんな図を描くので、これが線形回帰みたいだからだろう。",
    "output": "Finally, one question I sometimes get asked is how does PCA relate to linear regression? Because when explaining PCA, I sometimes end up drawing diagrams like these and that looks a little bit like linear regression."
  },
  {
    "index": "F17270",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実はPCAは、線形回帰では無い。表面上はある程度似ているにもかかわらず、これらは実は全く異なるアルゴリズムなのだ。",
    "output": "It turns out PCA is not linear regression, and despite some cosmetic similarity, these are actually totally different algorithms."
  },
  {
    "index": "F17271",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "線形回帰をやる時というのは、我らがやるのは、この左側で、あるフィーチャーxを入力としてある変数yを予想しようと試みる、という事だ。",
    "output": "If we were doing linear regression, what we would do would be, on the left we would be trying to predict the value of some variable y given some info features x."
  },
  {
    "index": "F17272",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり線形回帰においては、我らがやっている事は、点と直線との距離による二乗誤差を最小化するような、直線をフィッティングしているのだ。",
    "output": "And so linear regression, what we're doing is we're fitting a straight line so as to minimize the square error between point and this straight line."
  },
  {
    "index": "F17273",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから我らが最小化するのは、この青い線の二乗だ。",
    "output": "And so what we're minimizing would be the squared magnitude of these blue lines."
  },
  {
    "index": "F17274",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして私はこれらの青い線を垂直に書いた事に気づいただろうか。",
    "output": "And notice that I'm drawing these blue lines vertically."
  },
  {
    "index": "F17275",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらは点と仮説が予測した値との垂直な距離となっている、一方で対称的にPCAにおいては、これらの青い線の大きさを最小化したい、これは角度をつけて描いてあり、実際は直行した最短距離で、点とこの赤い直線との最短距離で、そしてこれは、データセットによっては、大きく異なった結果となる。",
    "output": "That these blue lines are the vertical distance between the point and the value predicted by the hypothesis. Whereas in contrast, in PCA, what it does is it tries to minimize the magnitude of these blue lines, which are drawn at an angle."
  },
  {
    "index": "F17276",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的には、、、より一般的には、線形回帰をする時には、特別な変数yという物が予測しようとする変数として存在する。",
    "output": "And more generally, when you're doing linear regression, there is this distinguished variable y they we're trying to predict."
  },
  {
    "index": "F17277",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "線形回帰とは、Xを全て使ってYを予測しようとする物だ。",
    "output": "All that linear regression as well as taking all the values of x and try to use that to predict y."
  },
  {
    "index": "F17278",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方でPCAは、特別な役割の変数Y、予測しようとする変数Yは存在しない。",
    "output": "Whereas in PCA, there is no distinguish, or there is no special variable y that we're trying to predict."
  },
  {
    "index": "F17279",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その代わりに、フィーチャーのリストx1,x2...とxnまでのフィーチャーがあって、これらのフィーチャーは全て等しく扱われる。つまりどれも特別では無い。",
    "output": "And instead, we have a list of features, x1, x2, and so on, up to xn, and all of these features are treated equally, so no one of them is special."
  },
  {
    "index": "F17280",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後の例として、三次元のデータの場合、そしてこれを3Dから2Dに削減したい場合、つまり例えば二つの方向、u1とu2を探して、そこにデータを射影したい。",
    "output": "As one last example, if I have three-dimensional data and I want to reduce data from 3D to 2D, so maybe I wanna find two directions, u(1) and u(2), onto which to project my data."
  },
  {
    "index": "F17281",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、最初にあるのは、3つのフィーチャーx1,x2,x3で、これらは全て同じように扱われる。",
    "output": "Then what I have is I have three features, x1, x2, x3, and all of these are treated alike."
  },
  {
    "index": "F17282",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら三つは全て対称的に扱われて、予測をしたいと思う特別な変数yは存在していない。",
    "output": "All of these are treated symmetrically and there's no special variable y that I'm trying to predict."
  },
  {
    "index": "F17283",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、PCAは線形回帰じゃないって事だ。表面上はある程度関係しているように見えるかもしれないが、これらは実際のところ、とても異なったアルゴリズムだ。",
    "output": "And so PCA is not a linear regression, and even though at some cosmetic level they might look related, these are actually very different algorithms."
  },
  {
    "index": "F17284",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、PCAが何をしているか、わかったかな。",
    "output": "So hopefully you now understand what PCA is doing."
  },
  {
    "index": "F17285",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PCAは、以下の条件を満たすような、より低い次元の平面を探す事を試みる、その条件とはこの二乗射影誤差を最小化するような物、各点と射影先の点の位置との距離の二乗を最小化する、という条件。",
    "output": "It's trying to find a lower dimensional surface onto which to project the data, so as to minimize this squared projection error. To minimize the square distance between each point and the location of where it gets projected."
  },
  {
    "index": "F17286",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、データを射影する先の低次元の平面を実際にどうやって探すのかについて、議論を開始する。",
    "output": "In the next video, we'll start to talk about how to actually find this lower dimensional surface onto which to project the data."
  },
  {
    "index": "F17287",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、主成分分析のアルゴリズムについてお話ししたい。",
    "output": "In this video I'd like to tell you about the principle components analysis algorithm."
  },
  {
    "index": "F17288",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオが終わる頃には、あなたはPCAを自分自身で実装する方法を知る事になる。",
    "output": "And by the end of this video you know to implement PCA for yourself."
  },
  {
    "index": "F17289",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれを用いてあなたのデータの次元を削減する。",
    "output": "And use it reduce the dimension of your data."
  },
  {
    "index": "F17290",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PCAを適用する前に、必ずやらなくてはいけないデータの前処理のステップがある。",
    "output": "Before applying PCA, there is a data pre-processing step which you should always do."
  },
  {
    "index": "F17291",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニングセットの手本が与えられたとして、必ずやらなくてはいけない事は、平均標準化だ。そして次にデータによっては、フィーチャースケーリングも実行する場合がある。",
    "output": "Given the trading sets of the examples is important to always perform mean normalization, and then depending on your data, maybe perform feature scaling as well."
  },
  {
    "index": "F17292",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは教師あり学習での平均標準化とフィーチャースケーリングのプロセスととても似た物だ。",
    "output": "this is very similar to the mean normalization and feature scaling process that we have for supervised learning."
  },
  {
    "index": "F17293",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実のところ、それはラベル無しデータのx1からxmに対して行う、という事以外は完全に同じ物だ。",
    "output": "In fact it's exactly the same procedure except that we're doing it now to our unlabeled data, X1 through Xm."
  },
  {
    "index": "F17294",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "平均標準化の為に、各フィーチャーの平均をまず計算する必要がある。そして各フィーチャーxをx-平均で置き換える。",
    "output": "So for mean normalization we first compute the mean of each feature and then we replace each feature, X, with X minus its mean, and so this makes each feature now have exactly zero mean The different features have very different scales."
  },
  {
    "index": "F17295",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしそれぞれのフィーチャーがとても異なるスケールを持つなら、例えばx1が家の大きさ、x2が寝室の数なら、以前の例を引っ張り出してみたが、そうすると各フィーチャーが比較可能な範囲になるように、スケールする必要もある。",
    "output": "So for example, if x1 is the size of a house, and x2 is the number of bedrooms, to use our earlier example, we then also scale each feature to have a comparable range of values."
  },
  {
    "index": "F17296",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから教師あり学習でやったのと同様に、x(i)の下付き添字jをこれはjのフィーチャーだが、そこから平均を引いて、これが分子となる、そしてそれをsjで割る。",
    "output": "And so, similar to what we had with supervised learning, we would take x, i substitute j, that's the j feature and so we would subtract of the mean, now that's what we have on top, and then divide by sj."
  },
  {
    "index": "F17297",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでsjはフィーチャーjの値の範囲に関する、なんらかの指標だ。",
    "output": "Here, sj is some measure of the beta values of feature j."
  },
  {
    "index": "F17298",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、max-minでも良いし、もっと良く使われている物としては、フィーチャーjの標準偏差がある。",
    "output": "So, it could be the max minus min value, or more commonly, it is the standard deviation of feature j."
  },
  {
    "index": "F17299",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この種のデータの前処理を行った上で、PCAはこんな事をする。",
    "output": "Having done this sort of data pre-processing, here's what the PCA algorithm does."
  },
  {
    "index": "F17300",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または2Dのケースでは二つのベクトル、u1とu2を見つけたい、データを射影するこの平面を定義するような。",
    "output": "We saw from the previous video that what PCA does is, it tries to find a lower dimensional sub-space onto which to project the data, so as to minimize the squared projection errors, sum of the squared projection errors, as the square of the length of those blue lines that and so what we wanted to do specifically is find a vector, u1, which specifies that direction or in the 2D case we want to find two vectors, u1 and u2, to define this surface onto which to project the data."
  },
  {
    "index": "F17301",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではちょっと簡単に、データの次元を削減する、というのが何を意味するのかを振り返ってみよう。この例では、左側には手本xiが与えられていて、それはR2に存在する。",
    "output": "So, just as a quick reminder of what reducing the dimension of the data means, for this example on the left we were given the examples xI, which are in r2."
  },
  {
    "index": "F17302",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして我らがやりたい事は、我らのデータを表すRの数字ziを、探す事だ。",
    "output": "And what we like to do is find a set of numbers zI in r push to represent our data."
  },
  {
    "index": "F17303",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が2Dから1Dへの削減、の意味する所だ。",
    "output": "So that's what from reduction from 2D to 1D means."
  },
  {
    "index": "F17304",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、この赤い直線へデータを射影する事で、この直線の上の点の位置を示すのは、数字一つで十分だ。",
    "output": "So specifically by projecting data onto this red line there. We need only one number to specify the position of the points on the line."
  },
  {
    "index": "F17305",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その数字を、zとかz1と呼ぶ事にしよう。",
    "output": "So i'm going to call that number z or z1."
  },
  {
    "index": "F17306",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでzは実数だ。つまりこれは一次元ベクトルみたいな物だ。",
    "output": "Z here real number, so that's like a one dimensional vector."
  },
  {
    "index": "F17307",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "z1は単にこの1x1行列、または1次元ベクトルの最初の要素を表す。",
    "output": "So z1 just refers to the first component of this, you know, one by one matrix, or this one dimensional vector."
  },
  {
    "index": "F17308",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり点の位置を示すのに一つの数しか必要としない。",
    "output": "And so we need only one number to specify the position of a point."
  },
  {
    "index": "F17309",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの手本がx2なら、それはここにマップされるかな。",
    "output": "So if this example here was my example X1, then maybe that gets mapped here."
  },
  {
    "index": "F17310",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのここの点はz1となり、ここのこの点はz2となる。同様に、これら他の点も、x3,x4,x5とすると、これらはz3,z4,z5にマップされるなど。",
    "output": "And if this example was X2 maybe that example gets mapped And so this point here will be Z1 and this point here will be Z2, and similarly we would have those other points for These, maybe X3, X4, X5 get mapped to Z1, Z2, Z3."
  },
  {
    "index": "F17311",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりPCAの任務は、二つの事を計算する方法を考え出す必要がある、という事だ。",
    "output": "So What PCA has to do is we need to come up with a way to compute two things."
  },
  {
    "index": "F17312",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ目はこれらのベクトル、u1と、こちらのケースではu1とu2。",
    "output": "One is to compute these vectors, u1, and in this case u1 and u2."
  },
  {
    "index": "F17313",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもう一つは、これらの数、Zを計算する方法。",
    "output": "And the other is how do we compute these numbers, Z."
  },
  {
    "index": "F17314",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり左側の例では、データを2Dから1Dへと削減した。",
    "output": "So on the example on the left we're reducing the data from 2D to 1D."
  },
  {
    "index": "F17315",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "右側の例では、データを3次元、R3から、zi、これはここでは2次元へと削減している。",
    "output": "In the example on the right, we would be reducing data from 3 dimensional as in r3, to zi, which is now two dimensional."
  },
  {
    "index": "F17316",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらのzベクトルは、いまは2次元だ。",
    "output": "So these z vectors would now be two dimensional."
  },
  {
    "index": "F17317",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれはz1とz2、みたいな形。だから我らはこれらの新しい表現z1とz2を計算する方法を同様に知らなくてはならない。",
    "output": "So it would be z1 z2 like so, and so we need to give away to compute these new representations, the z1 and z2 of the data as well."
  },
  {
    "index": "F17318",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこれら全ての量を、どうやって計算したら良いだろうか?",
    "output": "So how do you compute all of these quantities?"
  },
  {
    "index": "F17319",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "数学的な導出は、数学的な証明も、u1,u2,z1,z2などの正しい値は何か、を証明する事は、その数学的証明はとても複雑で、このコースの範囲を越える。",
    "output": "It turns out that a mathematical derivation, also the mathematical proof, for what is the right value U1, U2, Z1, Z2, and so on."
  },
  {
    "index": "F17320",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが一度その手順をマスターしてしばえば、実際に望みのu1の値を探す手続きというのは、そんなに大変でも無い事が分かる。この値が正しい値だ、と数学的に証明するのは、よりしっかりとやらないと分からない事で、それは私がこのコースでやりたいレベルを越えてしまう。",
    "output": "But once you've done it turns out that the procedure to actually find the value of u1 that you want is not that hard, even though so that the mathematical proof that this value is the correct value is someone more involved and more than i want to get into."
  },
  {
    "index": "F17321",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、具体的な手順は、記述してみよう、これら全てのベクトル、u1,u2,ベクトルzを計算する為にやる必要のある事を。",
    "output": "But let me just describe the specific procedure that you have to implement in order to compute all of these things, the vectors, u1, u2, the vector z."
  },
  {
    "index": "F17322",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがその手順だ。",
    "output": "Here's the procedure."
  },
  {
    "index": "F17323",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで我らがやるのは、まず、共分散行列と呼ばれる物を計算する。共分散行列は普通ギリシャ文字の大文字のギリシャ文字のシグマで表す慣例となっている。",
    "output": "Let's say we want to reduce the data to n dimensions to k dimension What we're going to do is first compute something called the covariance matrix, and the covariance matrix is commonly denoted by this Greek alphabet which is the capital Greek alphabet sigma."
  },
  {
    "index": "F17324",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "和のシグマとギリシャ文字のシグマがまったく同じなのは、ちょっとした不幸だ。",
    "output": "It's a bit unfortunate that the Greek alphabet sigma looks exactly like the summation symbols."
  },
  {
    "index": "F17325",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのギリシャ文字シグマは行列を示すのに使われ、このこれは和の記号だ。",
    "output": "So this is the Greek alphabet Sigma is used to denote a matrix and this here is a summation symbol."
  },
  {
    "index": "F17326",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのスライドにおいては、どれがSigma行列、行列か、それが和の記号か、どれがどちらか、その区別は、文脈から私がどっちを使ってるかははっきり分かると思う。",
    "output": "So hopefully in these slides there won't be ambiguity about which is Sigma Matrix, the matrix, which is a summation symbol, and hopefully it will be clear from context when I'm using each one."
  },
  {
    "index": "F17327",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この行列をどう計算するか?Octaveの変数Sigmaにそれが入っているとしよう。",
    "output": "How do you compute this matrix let's say we want to store it in an octave variable called sigma."
  },
  {
    "index": "F17328",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らがやるべき事は、行列Sigmaの固有ベクトルと呼ばれる物を計算する事だ。",
    "output": "What we need to do is compute something called the eigenvectors of the matrix sigma."
  },
  {
    "index": "F17329",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Octaveでは、それをやる方法は、以下のコマンドを使う事だ:USVイコールのsvdのSigma。",
    "output": "And an octave, the way you do that is you use this command, u s v equals s v d of sigma."
  },
  {
    "index": "F17330",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、svdは、SingularValueDecomposition(特異値分解)の略。",
    "output": "SVD, by the way, stands for singular value decomposition."
  },
  {
    "index": "F17331",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしあなたが線形代数のエキスパートで、前から固有ベクトルを知ってたなら、ひょっとしたらoctaveのeigと呼ばれる関数を知ってるかもしれない。これもまた同じ物を計算するのに使う事が出来る。",
    "output": "It is much more advanced linear algebra than you actually need to know but now It turns out that when sigma is equal to matrix there is a few ways to compute these are high in vectors and If you are an expert in linear algebra and if you've heard of high in vectors before you may know that there is another octet function called I, which can also be used to compute the same thing."
  },
  {
    "index": "F17332",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "結局は、svd関数とeig関数は、同じベクトルを返す。でもsvdの方がちょっとだけ数値計算的に安定だが。",
    "output": "and It turns out that the SVD function and the I function it will give you the same vectors, although SVD is a little more numerically stable."
  },
  {
    "index": "F17333",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから私はsvdを使う事にしている、でも私には、同じ事をするのにeig関数を使う友人が何人かいるが。だが、これを共分散行列Sigmaに適用する場合、どちらも同じ結果を返す。",
    "output": "So I tend to use SVD, although I have a few friends that use the I function to do this as wellbut when you apply this to a covariance matrix sigma it gives you the same thing."
  },
  {
    "index": "F17334",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それの意味する所は知る必要は無いが、svdとeig関数は、異なる関数だが、共分散行列に適用する時には、いつもこの数学的性質を満たす事が証明されていて、いつも同じ結果を返す、という事だ。",
    "output": "This is because the covariance matrix always satisfies a mathematical Property called symmetric positive definite You really don't need to know what that means, but the SVD and I-functions are different functions but when they are applied to a covariance matrix which can be proved to always satisfy this mathematical property; they'll always give you the same thing."
  },
  {
    "index": "F17335",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まあいいや。以上はたぶん知る必要があるよりも大分たくさんの線形代数の詳細だろう。",
    "output": "Okay, that was probably much more linear algebra than you needed to know."
  },
  {
    "index": "F17336",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら全部が何言ってるかさっぱり分からなくても、気にしなくてよろしい。",
    "output": "In case none of that made sense, don't worry about it."
  },
  {
    "index": "F17337",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "知らなきゃいけない事の全ては、Octaveで、このシステムコマンドを実装しなくてはいけない、って事だけ。",
    "output": "All you need to know is that this system command you should implement in Octave."
  },
  {
    "index": "F17338",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしOctaveやMATLAB以外の別の言語でこれを実装しなくてはいけない、としたら、その時はあなたは、数値計算の線形代数ライブラリで、svd、つまり特異値分解が計算出来る物を探すべきだ。そんなライブラリは、主要なプログラミング言語ならどれにもたくさんあるはず。",
    "output": "And if you're implementing this in a different language than Octave or MATLAB, what you should do is find the numerical linear algebra library that can compute the SVD or singular value decomposition, and there are many such libraries for probably all of the major programming languages."
  },
  {
    "index": "F17339",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "人々はそれを用いて共分散行列シグマの行列U,S,Dを計算するのに使う事が出来る。",
    "output": "People can use that to compute the matrices u, s, and d of the covariance matrix sigma."
  },
  {
    "index": "F17340",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではより詳細を埋めていこう。この共分散行列シグマは、n掛けるnの行列だ。",
    "output": "So just to fill in some more details, this covariance matrix sigma will be an n by n matrix."
  },
  {
    "index": "F17341",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを確認する一つの方法としては、定義を見ると、これはn掛ける1ベクトルで、そしてこれはxiの転置は1掛けるn。だからこれら二つの積は、n掛けるnの行列となる。",
    "output": "And one way to see that is if you look at the definition this is an n by 1 vector and this here I transpose is 1 by N so the product of these two things is going to be an N by N matrix."
  },
  {
    "index": "F17342",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1xNの転置と1xNだから、NxN行列となり、これらを全て足しあわせても、NxN行列のまま。",
    "output": "1xN transfers, 1xN, so there's an NxN matrix and when we add up all of these you still have an NxN matrix."
  },
  {
    "index": "F17343",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてsvdの出力は、3つの行列U,S,Vだ。",
    "output": "And what the SVD outputs three matrices, u, s, and v."
  },
  {
    "index": "F17344",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "svdの結果でこの場合本当に必要とするのは、U行列のみだ。",
    "output": "The thing you really need out of the SVD is the u matrix."
  },
  {
    "index": "F17345",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "U行列もまたNxN行列だ。",
    "output": "The u matrix will also be a NxN matrix."
  },
  {
    "index": "F17346",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしU行列の列を見てみると、U行列の列は、ベクトルu1,u2,などなどとまったく同じ事が分かる。",
    "output": "And if we look at the columns of the U matrix it turns out that the columns of the U matrix will be exactly those vectors, u1, u2 and so on."
  },
  {
    "index": "F17347",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "分かる。つまりUは行列。",
    "output": "So u, will be matrix."
  },
  {
    "index": "F17348",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてN次元からk次元へとデータを削減したい場合、やるべき事はまず最初のk本のベクトルを取りそれによって我らがデータを射影したいと思う、u1からukまでの、k本の方向が得られる。",
    "output": "And if we want to reduce the data from n dimensions down to k dimensions, then what we need to do is take the first k vectors."
  },
  {
    "index": "F17349",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこからの手順としては、SVD数値計算線形代数ライブラリのルーチンからこの行列Uが得られる。",
    "output": "the rest of the procedure from this SVD numerical linear algebra routine we get this matrix u."
  },
  {
    "index": "F17350",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、残りの手順をまとめて記述してみよう。SVD数値計算線形代数ルーチンから、これらの行列、U,S,Dを得る。",
    "output": "So, just to wrap up the description of the rest of the procedure, from the SVD numerical linear algebra routine we get these matrices u, s, and d."
  },
  {
    "index": "F17351",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、次にやるべき事は、元になるデータセットのXを持ってきて、このXはRnだが、そしてより低じ次元の表現であるzを見つけてくる、それはRkだ。",
    "output": "we're going to use the first K columns of this matrix to get u1-uK. Now the other thing we need to is take my original data set, X which is an RN And find a lower dimensional representation Z, which is a R K for this data."
  },
  {
    "index": "F17352",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを行う方法としては、U行列の最初のk列を取り出して、この行列を構築する。",
    "output": "So the way we're going to do that is take the first K Columns of the U matrix. Construct this matrix."
  },
  {
    "index": "F17353",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "u1,u2,...とukまで列に積み上げる。",
    "output": "Stack up U1, U2 and so on up to U K in columns."
  },
  {
    "index": "F17354",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは基本的には、行列のこの部分を取り出す事だ、この行列の最初のk列を。",
    "output": "It's really basically taking, you know, this part of the matrix, the first K columns of this matrix."
  },
  {
    "index": "F17355",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれは、n掛けるk行列となる。",
    "output": "And so this is going to be an N by K matrix."
  },
  {
    "index": "F17356",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この行列に名前を与えておこう。",
    "output": "I'm going to give this matrix a name."
  },
  {
    "index": "F17357",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この行列を、Uの下付き添字reduce、と呼ぶ事にする、reduce(削減)されたバージョンのU、みたいな意味だ。",
    "output": "I'm going to call this matrix U, subscript \"reduce,\" sort of a reduced version of the U matrix maybe."
  },
  {
    "index": "F17358",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを私のデータの次元を削減するのに使っていく。",
    "output": "I'm going to use it to reduce the dimension of my data."
  },
  {
    "index": "F17359",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてzを計算する方法は、zイコールの、このUreduce行列の転置に、掛ける事のx。",
    "output": "And the way I'm going to compute Z is going to let Z be equal to this U reduce matrix transpose times X."
  },
  {
    "index": "F17360",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または別の書き方をすると、この転置が意味する所を書きくだして、このU行列の転置を取ると、最終的に得られるのは、今や列に並んでいるこれらのベクトルだ。",
    "output": "When I take this transpose of this U matrix, what I'm going to end up with is these vectors now in rows."
  },
  {
    "index": "F17361",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "u1転置からuk転置まで。",
    "output": "I have U1 transpose down to UK transpose."
  },
  {
    "index": "F17362",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれに、掛けることx。これがベクトルzを得る方法だ。",
    "output": "Then take that times X, and that's how I get my vector Z."
  },
  {
    "index": "F17363",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの次元を納得する為に確認してみよう。このここの行列は、k掛けるnとなり、ここのxは、n掛ける1となる。",
    "output": "Just to make sure that these dimensions make sense, this matrix here is going to be k by n and x here is going to be n by 1 and so the product here will be k by 1."
  },
  {
    "index": "F17364",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりzはk次元の、、、k次元ベクトルで、それはまさに我らの求める物だ。",
    "output": "And so z is k dimensional, is a k dimensional vector, which is exactly what we wanted."
  },
  {
    "index": "F17365",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもちろんこれらのxは、トレーニングセットの手本でも良いし、クロスバリデーションセットの手本でも良いし、テストセットの手本でも良い。例えば、トレーニング手本のxiをとりたいとする。",
    "output": "And of course these x's here right, can be Examples in our training set can be examples in our cross validation set, can be examples in our test set, and for example if you know, I wanted to take training example i, I can write this as xi XI and that's what will give me ZI over there."
  },
  {
    "index": "F17366",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではまとめだ。これはPCAアルゴリズムを一つのスライドに納めた。",
    "output": "So, to summarize, here's the PCA algorithm on one slide."
  },
  {
    "index": "F17367",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "平均標準化の後で、各フィーチャーの平均は0なのを保証した後で、さらにオプションでフィーチャースケーリングも、もし本当にフィーチャースケーリングが必要なら、もしあなたのフィーチャーがそれぞれとても異なるレンジの値を取るなら、行う。",
    "output": "After mean normalization, to ensure that every feature is zero mean and optional feature scaling whichYou really should do feature scaling if your features take on very different ranges of values."
  },
  {
    "index": "F17368",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、こんな形で行列としてデータがある時に、こんな風にデータが行で与えられている時には、もし行列Xとしてトレーニングセットが行の形で書かれていて、x1の転置から、xnの転置までが並んでいるとしたら、これの共分散行列Sigmaは実は、ナイスなベクトル化された実装が存在する。",
    "output": "After this pre-processing we compute the carrier matrix Sigma like so by the way if your data is given as a matrix like hits if you have your data Given in rows like this. If you have a matrix X which is your time trading sets written in rows where x1 transpose down to x1 transpose, this covariance matrix sigma actually has a nice vectorizing implementation."
  },
  {
    "index": "F17369",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Sigmaイコール、1/m掛ける事の、ここにあるXに転置をとって、さらにXを掛ける。このシンプルな式が、行列Sigmaを計算する為のベクトル化した実装だ。",
    "output": "You can implement in octave, you can even run sigma equals 1 over m, times x, which is this matrix up here, transpose times x and this simple expression, that's the vectorize implementation of how to compute the matrix sigma."
  },
  {
    "index": "F17370",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今日のところはこれを証明したりはしない。",
    "output": "I'm not going to prove that today."
  },
  {
    "index": "F17371",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしやりたければ、簡単に自分でOctaveで試す事で、数値的に確認出来るし、つまりこれと、この実装が同じ結果を返す、と。または自分で数学的に証明する事も、やれば出来る。",
    "output": "This is the correct vectorization whether you want, you can either numerically test this on yourself by trying out an octave and making sure that both this and this implementations give the same answers or you Can try to prove it yourself mathematically."
  },
  {
    "index": "F17372",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どちらにしろ、これは正しいベクトル化した実装だ。",
    "output": "Either way but this is the correct vectorizing implementation, without compusingnext we can apply the SVD routine to get u, s, and d."
  },
  {
    "index": "F17373",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "計算量がより少ない。次にsvdルーチンを適用し、U、S、Dを取得する。",
    "output": "And then we grab the first k columns of the u matrix you reduce and finally this defines how we go from a feature vector x to this reduce dimension representation z."
  },
  {
    "index": "F17374",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、U行列の最初のk列をつかみとって、Ureduceとし、そして最後に、これがフィーチャーベクトルxから、この削減された次元の表現zへと変換する方法だ。",
    "output": "And similar to k Means if you're apply PCA, they way you'd apply this is with vectors X and RN."
  },
  {
    "index": "F17375",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりx0=1となるx0無しで行う。",
    "output": "So, this is not done with X-0 1."
  },
  {
    "index": "F17376",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がPCAアルゴリズムだ。",
    "output": "So that was the PCA algorithm."
  },
  {
    "index": "F17377",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはk次元部分空間への、k次元平面へのデータの射影が、実際に二乗射影誤差を最小化する、という事。それの証明はこのコースのスコープを越える。",
    "output": "One thing I didn't do is give a mathematical proof that this There it actually give the projection of the data onto the K dimensional subspace onto the K dimensional surface that actually minimizes the square projection error Proof of that is beyond the scope of this course."
  },
  {
    "index": "F17378",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "幸運な事に、PCAのアルゴリズムはそんなたくさんのコードにならずに実装する事が可能だ。",
    "output": "Fortunately the PCA algorithm can be implemented in not too many lines of code."
  },
  {
    "index": "F17379",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのアルゴリズムをoctaveで実装してしまえば、あなたはとても効率的な次元削減のアルゴリズムを得る事が出来る。",
    "output": "and if you implement this in octave or algorithm, you actually get a very effective dimensionality reduction algorithm."
  },
  {
    "index": "F17380",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がPCAアルゴリズムだ。",
    "output": "So, that was the PCA algorithm."
  },
  {
    "index": "F17381",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つまだやってない事としては、u1とかu2などや、zなどが、ここまでの手続きで得られるこれらのベクトルがこれらの二乗射影誤差を本当に最小化している、という数学的な証明を与える事だ。",
    "output": "One thing I didn't do was give a mathematical proof that the U1 and U2 and so on and the Z and so on you get out of this procedure is really the choices that would minimize these squared projection error."
  },
  {
    "index": "F17382",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "思い出してみよう。PCAがやろうとしているのは、データを射影する平面とか直線で、二乗射影誤差を最小化する物を探す、という事だった。",
    "output": "Right, remember we said What PCA tries to do is try to find a surface or line onto which to project the data so as to minimize to square projection error."
  },
  {
    "index": "F17383",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれが、その条件をみたしている、とは証明していない。そしてその数学的証明はこのコースの範囲を超えちゃう。",
    "output": "So I didn't prove that this that, and the mathematical proof of that is beyond the scope of this course."
  },
  {
    "index": "F17384",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが幸運な事に、PCAのアルゴリズムはOctaveのコードではそんなにたくさんの行をかけずに実装出来る。",
    "output": "But fortunately the PCA algorithm can be implemented in not too many lines of octave code."
  },
  {
    "index": "F17385",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしこれを実装すれば、これは実際にうまく機能する。そしてこのアルゴリズムを実装する事で、とても有効な次元削減のアルゴリズムを得る事が出来るのだ。",
    "output": "And if you implement this, this is actually what will work, or this will work well, and if you implement this algorithm, you get a very effective dimensionality reduction algorithm."
  },
  {
    "index": "F17386",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは射影二乗誤差を最小化するという仕事を正しくこなす。",
    "output": "That does do the right thing of minimizing this square projection error."
  },
  {
    "index": "F17387",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以前の動画で、私はPCAは時には、学習アルゴリズムの実行時間をスピードアップする為に使う事が出来る、という事に言及した。",
    "output": "In an earlier video, I had said that PCA can be sometimes used to speed up the running time of a learning algorithm."
  },
  {
    "index": "F17388",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオで私はそれを実際にどうやるのかを説明する。そしてまた、PCAをどう適用するのかについて、幾つか助言も与えておきたい。",
    "output": "In this video, I'd like to explain how to actually do that, and also say some, just try to give some advice about how to apply PCA."
  },
  {
    "index": "F17389",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがPCAを学習アルゴリズムのスピードアップに使う方法だ。そしてこの教師あり学習のスピードアップが私が個人的にPCAを使うもっとも一般的な用途だ。",
    "output": "Here's how you can use PCA to speed up a learning algorithm, and this supervised learning algorithm speed up is actually the most common use that I personally make of PCA."
  },
  {
    "index": "F17390",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "入力xとラベルyの教師あり学習。そしてあなたの手本xiが凄い高次元だとしよう。",
    "output": "Let's say you have a supervised learning problem, note this is a supervised learning problem with inputs X and labels Y, and let's say that your examples xi are very high dimensional."
  },
  {
    "index": "F17391",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、あなたの手本xiは、1万次元のフィーチャーベクトルだとする。",
    "output": "So, lets say that your examples, xi are 10,000 dimensional feature vectors."
  },
  {
    "index": "F17392",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでは100x100の画像だとすると、つまり100x100は、1万ピクセルだ。だから例えば、xiが1万ピクセルの明度の値のフィーチャーベクトルだとすれば、1万次元のフィーチャーベクトルを持つ事になる。",
    "output": "One example of that, would be, if you were doing some computer vision problem, where you have a 100x100 images, and so if you have 100x100, that's 10000 pixels, and so if xi are, you know, feature vectors that contain your 10000 pixel intensity values, then you have 10000 dimensional feature vectors."
  },
  {
    "index": "F17393",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのようなとても高い次元のフィーチャーベクトルの場合、学習アルゴリズムを走らせると、遅くなりがちだ。",
    "output": "So with very high-dimensional feature vectors like this, running a learning algorithm can be slow, right?"
  },
  {
    "index": "F17394",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1万次元のフィーチャーベクトルを食わせようとすれば、それがロジスティック回帰でもニューラルネットワークでもサポートベクタマシンでも、その他何でも、単にデータが多量だ、という理由なだけで、それは1万個の数字なので、学習アルゴリズムの実行を遅くしうる。",
    "output": "Just, if you feed 10,000 dimensional feature vectors into logistic regression, or a new network, or support vector machine or what have you, just because that's a lot of data, that's 10,000 numbers, it can make your learning algorithm run more slowly."
  },
  {
    "index": "F17395",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "幸運にも、PCAでもって、我らはこのデータの次元を減らす事が出来る。だからアルゴリズムをもっと効率的に走らせる事が出来る。",
    "output": "Fortunately with PCA we'll be able to reduce the dimension of this data and so make our algorithms run more efficiently."
  },
  {
    "index": "F17396",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのやり方はこうだ。",
    "output": "Here's how you do that."
  },
  {
    "index": "F17397",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず最初にラベル付けされたトレーニングセットをチェックして、入力のみを引っ張り出す。x達を引きぬいて、一時的にyの事は脇にどけておく。",
    "output": "We are going first check our labeled training set and extract just the inputs, we're just going to extract the X's and temporarily put aside the Y's."
  },
  {
    "index": "F17398",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうすると、ここには、ラベル無しのトレーニングセットx1からxmが、得られる。それらは、例えば1万次元とかのデータだ。",
    "output": "So this will now give us an unlabelled training set x1 through xm which are maybe there's a ten thousand dimensional data, ten thousand dimensional examples we have."
  },
  {
    "index": "F17399",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり単に入力ベクトルである所のx1からxmまでを取り出しただけ。",
    "output": "So just extract the input vectors x1 through xm."
  },
  {
    "index": "F17400",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にPCAを適用して、その結果我らは、データの削減された次元の表現を得る、つまり1万次元のフィーチャーベクトルの代わりに、いまや例えば1000次元のフィーチャーベクトルを得る事になる。つまりこれは、10倍の節約になる。",
    "output": "Then we're going to apply PCA and this will give me a reduced dimension representation of the data, so instead of 10,000 dimensional feature vectors I now have maybe one thousand dimensional feature vectors."
  },
  {
    "index": "F17401",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは我らに、新しいトレーニングセットを与えるのだ。",
    "output": "So this gives me, if you will, a new training set."
  },
  {
    "index": "F17402",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり以前には、私は手本としてx1,y1を持っていた訳だが、それが今や最初のトレーニングの入力はz1となり、つまり我らは、ある種の新しいトレーニング手本を得る訳だ、それはz1とy1がペアになって、同様にz2,y2,点点点と続きzm,ymまで。",
    "output": "So whereas previously I might have had an example x1, y1, my first training input, is now represented by z1. And so we'll have a new sort of training example, which is Z1 paired with y1."
  },
  {
    "index": "F17403",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら、トレーニング手本はいまやこのより低い次元の表現、z1,z2,...,zmで表されるから。",
    "output": "Because my training examples are now represented with this much lower dimensional representation Z1, Z2, up to ZM."
  },
  {
    "index": "F17404",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、この削減された次元のトレーニングセットに対して、これらを学習アルゴリズム、例えばニューラルネットワークとか、ロジスティック回帰とかに食わせて、そして仮説hを学習する事が出来る、この仮説はこれらの低次元の表現zを入力として受け取り、予測を試みる物だ。",
    "output": "Finally, I can take this reduced dimension training set and feed it to a learning algorithm maybe a neural network, maybe logistic regression, and I can learn the hypothesis H, that takes this input, these low-dimensional representations Z and tries to make predictions."
  },
  {
    "index": "F17405",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばロジスティック回帰を使ってるとするなら、1割ることの1足すeのマイナスシータ転置zを出力する仮説を訓練する、これは入力としてこれらのzベクトルの一つを受け取り、予測を試みる。",
    "output": "So if I were using logistic regression for example, I would train a hypothesis that outputs, you know, one over one plus E to the negative-theta transpose Z, that takes this input to one these z vectors, and tries to make a prediction."
  },
  {
    "index": "F17406",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、新しい手本を得たら、それは新しいテスト手本のxかもしれないが、そこであなたがすべきは、テストの手本xをPCAで見つけられた同じマッピングを用いて対応するzにマッピングする。",
    "output": "And finally, if you have a new example, maybe a new test example X."
  },
  {
    "index": "F17407",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にそのzをこの仮説に食わせる、そしてこの仮説が入力xに対応する予測を行う。最後に一つ注意を。",
    "output": "What you do is you would take your test example x, map it through the same mapping that was found by PCA to get you your corresponding z."
  },
  {
    "index": "F17408",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PCAがやっている事はxからzへのマッピングを定義する、という事だ。",
    "output": "And that z then gets fed to this hypothesis, and this hypothesis then makes a prediction on your input x."
  },
  {
    "index": "F17409",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのxからzへのマッピングはPCAをトレーニングセットだけに対して走らせる事で定義するべきだ。",
    "output": "One final note, what PCA does is it defines a mapping from x to z and this mapping from x to z should be defined by running PCA only on the training sets."
  },
  {
    "index": "F17410",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に言うと、このマッピング、PCAが学習するこのマッピングは、パラメータの集合を計算する訳だ。",
    "output": "And in particular, this mapping that PCA is learning, right, this mapping, what that does is it computes the set of parameters."
  },
  {
    "index": "F17411",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フィーチャースケーリングして平均標準化して、そしてこのUreduce行列を計算する。",
    "output": "That's the feature scaling and mean normalization. And there's also computing this matrix U reduced."
  },
  {
    "index": "F17412",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これもまたPCAで学習されたパラメータのような物で、我らはパラメータのフィッティングはトレーニングセットだけに対して行うべきだ。そしてクロスバリデーションセットやテストセットに対してフィッティングしてはいけない。",
    "output": "But all of these things that U reduce, that's like a parameter that is learned by PCA and we should be fitting our parameters only to our training sets and not to our cross validation or test sets and so these things the U reduced so on, that should be obtained by running PCA only on your training set."
  },
  {
    "index": "F17413",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれらの事、Ureduceとかは、トレーニングセットだけにPCAを適用して取得するべきである。そしてUreduceを見出したら、またフィーチャースケーリングのパラメータを見出したら、平均標準化して、スケールでフィーチャーを割って比較可能なスケールにするのだった。",
    "output": "And then having found U reduced, or having found the parameters for feature scaling where the mean normalization and scaling the scale that you divide the features by to get them on to comparable scales."
  },
  {
    "index": "F17414",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら全てのパラメータをトレーニングセットに対して見出したら、その次には、同じマッピングをその他の手本、クロスバリデーションセットとかテストセットにある手本に対して適用出来るのだ。まとめておこう。",
    "output": "Having found all those parameters on the training set, you can then apply the same mapping to other examples that may be In your cross-validation sets or in your test sets, OK?"
  },
  {
    "index": "F17415",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PCAを走らせる時は、手持ちのデータのうちトレーニングセットにだけ走らせないといけない。クロスバリデーションセットとテストセットの部分には実行してはいけない。",
    "output": "Just to summarize, when you're running PCA, run your PCA only on the training set portion of the data not the cross-validation set or the test set portion of your data."
  },
  {
    "index": "F17416",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそうする事で、xからzへのマッピングの定義が得られる。そこで次にそのマッピングをクロスバリデーションセットやテストセットに適用する。",
    "output": "And that defines the mapping from x to z and you can then apply that mapping to your cross-validation set and your test set and by the way in this example I talked about reducing the data from ten thousand dimensional to one thousand dimensional, this is actually not that unrealistic."
  },
  {
    "index": "F17417",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、私はデータの削減として、1万次元から1000次元にする、という話をしているが、これはそんなに非現実的な数字では無い。",
    "output": "For many problems we actually reduce the dimensional data."
  },
  {
    "index": "F17418",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多くの問題で、我らは実際には高次元データを、5倍とか10倍とか削減して、しかもほとんどの分散を保持したままに出来て、だからパフォーマンスにほとんど影響を与えずに行える、パフォーマンスというのは分類の正確さとかの観点という事だが、学習アルゴリズムの正確さにほとんど影響を与えずに行う事が出来る。",
    "output": "You know by 5x maybe by 10x and still retain most of the variance and we can do this barely effecting the performance, in terms of classification accuracy, let's say, barely affecting the classification accuracy of the learning algorithm."
  },
  {
    "index": "F17419",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてより低い次元のデータで作業を行う事で、学習アルゴリズムは、しばしばずっと早く走る。",
    "output": "And by working with lower dimensional data our learning algorithm can often run much much faster."
  },
  {
    "index": "F17420",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まとめると、ここまでに我らは以下のようなPCAの応用例を話してきた。",
    "output": "To summarize, we've so far talked about the following applications of PCA."
  },
  {
    "index": "F17421",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "圧縮したいのは、データを保存するのに必要なメモリやディスク容量を減らす為かもしれないし、今話したように、学習アルゴリズムをスピードアップする為かもしれない。",
    "output": "First is the compression application where we might do so to reduce the memory or the disk space needed to store data and we just talked about how to use this to speed up a learning algorithm."
  },
  {
    "index": "F17422",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの応用では、kを選ぶ為にはしばしば、保持される分散のパーセンテージを調べる。つまり、この学習アルゴリズムのスピードアップという応用例の場合、よく使われるのは99%の分散を保持する、というライン。",
    "output": "In these applications, in order to choose K, often we'll do so according to, figuring out what is the percentage of variance retained, and so for this learning algorithm, speed up application often will retain 99% of the variance."
  },
  {
    "index": "F17423",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはとても典型的なkを選ぶ為の選択と言える。",
    "output": "That would be a very typical choice for how to choose k."
  },
  {
    "index": "F17424",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がこれらの圧縮の応用に際し、kを選ぶ方法だ。",
    "output": "So that's how you choose k for these compression applications."
  },
  {
    "index": "F17425",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり可視化の応用では、普通はkを2か3と選ぶ。何故なら我らは2Dか3Dのデータセットしかプロット出来ないから。",
    "output": "Whereas for visualization applications while usually we know how to plot only two dimensional data or three dimensional data, and so for visualization applications, we'll usually choose k equals 2 or k equals 3, because we can plot only 2D and 3D data sets."
  },
  {
    "index": "F17426",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がPCAの主な応用の要約だ。それとそれぞれの応用に際してのkの選び方だ。",
    "output": "So that summarizes the main applications of PCA, as well as how to choose the value of k for these different applications."
  },
  {
    "index": "F17427",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたも時には、他の人がこれをやってしまっている、という事を耳にする事があるだろう。そんなに多くは無いとは思いたいが。",
    "output": "I should mention that there is often one frequent misuse of PCA and you sometimes hear about others doing this hopefully not too often."
  },
  {
    "index": "F17428",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私がこれを言及したいのは、あなたにやって欲しくないからだ。",
    "output": "I just want to mention this so that you know not to do it."
  },
  {
    "index": "F17429",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんな良く無いPCAの誤用としては、オーバーフィッティングを防ぐ為に使ってしまう、という物。",
    "output": "And there is one bad use of PCA, which iss to try to use it to prevent over-fitting."
  },
  {
    "index": "F17430",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこういう理由だ。",
    "output": "Here's the reasoning."
  },
  {
    "index": "F17431",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはPCAの良い使い方では無い、だが、これがこの手法を用いる背後にある理由だ、それは、えーと、xiがあるとして、それがnフィーチャーだったとする。そしてそのデータを圧縮する、ziを代わりに使う、するとフィーチャーの数をk個に減らせられる、それはnよりもっと低い次元のはずだ。",
    "output": "This is not a great way to use PCA, but here's the reasoning behind this method, which is,you know if we have Xi, then maybe we'll have n features, but if we compress the data, and use Zi instead and that reduces the number of features to k, which could be much lower dimensional."
  },
  {
    "index": "F17432",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "kが1000でnが1万なら、我らは1000次元のデータしか持たなくなるのだから、オーバーフィットもしにくくなるだろう、1万次元のデータを使うよりは、1000個のフィーチャーを使う方が。",
    "output": "And so if we have a much smaller number of features, if k is 1,000 and n is 10,000, then if we have only 1,000 dimensional data, maybe we're less likely to over-fit than if we were using 10,000-dimensional data with like a thousand features."
  },
  {
    "index": "F17433",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、PCAをオーバーフィットを防止する方法と考える人が居る。",
    "output": "So some people think of PCA as a way to prevent over-fitting."
  },
  {
    "index": "F17434",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、強調しておきたいが、これはPCAの悪い使い方で、これをやるのを、私は推奨しない。",
    "output": "But just to emphasize this is a bad application of PCA and I do not recommend doing this."
  },
  {
    "index": "F17435",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこの手法が悪い振る舞いをするって訳じゃない。",
    "output": "And it's not that this method works badly."
  },
  {
    "index": "F17436",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたがオーバーフィットを防止する為にデータの次元を減らす為にこの手法を使ったとする、たぶんそれはうまく行くと思う。",
    "output": "If you want to use this method to reduce the dimensional data, to try to prevent over-fitting, it might actually work OK."
  },
  {
    "index": "F17437",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、これは単にオーバーフィッティングに対応するのに良い方法じゃない、とういだけ。その代わりに、もしオーバーフィットに悩んでいるなら、それに対処するもっと良い方法は、PCAを使ってデータの次元を削減する代わりに正規化を使うという事だ。",
    "output": "But this just is not a good way to address over-fitting and instead, if you're worried about over-fitting, there is a much better way to address it, to use regularization instead of using PCA to reduce the dimension of the data."
  },
  {
    "index": "F17438",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その理由は、PCAがどう機能するかを考えてみると、それはラベルyを使わない。",
    "output": "And the reason is, if you think about how PCA works, it does not use the labels y."
  },
  {
    "index": "F17439",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単に入力のxiだけを見ていって、そしてそれを用いてデータの低次元の近似を探していく。",
    "output": "You are just looking at your inputs xi, and you're using that to find a lower-dimensional approximation to your data."
  },
  {
    "index": "F17440",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりPCAが行うのは、なんらかの情報を捨て去るって事だが、PCAはyの値が何かを知らずにデータの次元を捨てる、あるいは削減する。",
    "output": "So what PCA does, is it throws away some information."
  },
  {
    "index": "F17441",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、これはたぶんOKなんだが、こういう風にPCAを使うのはたぶんOKなんだが、、、99%の分散とかを保持している限りは。分散のほとんどを維持しているのだから。",
    "output": "It throws away or reduces the dimension of your data without knowing what the values of y is, so this is probably okay using PCA this way is probably okay if, say 99 percent of the variance is retained, if you're keeping most of the variance, but it might also throw away some valuable information."
  },
  {
    "index": "F17442",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "分散の99%を保持していようが分散の95%を保持していようが、はたまたどれだけの分散を保持していようが、単に正規化を使うだけの方がどんなに悪くても同程度に良いオーバーフィッティングを防ぐ手法だという事が分かっている。そして多くの場合には、正規化の方が単によりうまく機能する。",
    "output": "And it turns out that if you're retaining 99% of the variance or 95% of the variance or whatever, it turns out that just using regularization will often give you at least as good a method for preventing over-fitting and regularization will often just work better, because when you are applying linear regression or logistic regression or some other method with regularization, well, this minimization problem actually knows what the values of y are, and so is less likely to throw away some valuable information, whereas PCA doesn't make use of the labels and is more likely to throw away valuable information."
  },
  {
    "index": "F17443",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたの主なモチベーションが学習アルゴリズムのスピードアップなら、それはPCAの良い使い方だ。だがオーバーフィッティングを防止する為にPCAを使うなら、それは良く無いPCAの使い方だ、その場合は正規化を代わりに使うべきだ、それこそが多くの人々が代わりに推奨している事でもある。",
    "output": "So, to summarize, it is a good use of PCA, if your main motivation to speed up your learning algorithm, but using PCA to prevent over-fitting, that is not a good use of PCA, and using regularization instead is really what many people would recommend doing instead."
  },
  {
    "index": "F17444",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、PCAの誤った使い方を一つ。",
    "output": "Finally, one last misuse of PCA."
  },
  {
    "index": "F17445",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PCAはとても便利なアルゴリズムだと言うべきだろう、私は圧縮とか可視化の目的で、良くPCAを使う。",
    "output": "And so I should say PCA is a very useful algorithm, I often use it for the compression on the visualization purposes."
  },
  {
    "index": "F17446",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、私が時々目にするのは、人々がPCAを、使うべきでは無い所でもまた使ってしまっている事がある。",
    "output": "But, what I sometimes see, is also people sometimes use PCA where it shouldn't be."
  },
  {
    "index": "F17447",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな事を私は良く見かける:誰かが機械学習のシステムを設計している時に、こんなプランを書きだすとする。",
    "output": "So, here's a pretty common thing that I see, which is if someone is designing a machine-learning system, they may write down the plan like this: let's design a learning system."
  },
  {
    "index": "F17448",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニングセットを集めて、そして次に、PCAを走らせる、そしてロジスティック回帰を訓練し、そしてテストデータでテストする、っと。",
    "output": "Get a training set and then, you know, what I'm going to do is run PCA, then train logistic regression and then test on my test data."
  },
  {
    "index": "F17449",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、しばしばプロジェクトの本当にしょっぱなの所で、PCAが組み込まれた、これら四つのステップをやろう、という、プロジェクトのプランを書く人が居る。",
    "output": "So often at the very start of a project, someone will just write out a project plan than says lets do these four steps with PCA inside."
  },
  {
    "index": "F17450",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PCAを用いたこういうプロジェクトのプランを書く前に、自身に問うてみるのがとても有益な問いは、PCA無しでこれら全部を行ったらどうだろうか?",
    "output": "Before writing down a project plan the incorporates PCA like this, one very good question to ask is, well, what if we were to just do the whole without using PCA."
  },
  {
    "index": "F17451",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてしばしば人々は、このような複雑なプロジェクトプランを作り出してPCAを実装したりする前に、このステップを検討していない。",
    "output": "And often people do not consider this step before coming up with a complicated project plan and implementing PCA and so on."
  },
  {
    "index": "F17452",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから具体的に言うと、私が良く人々にアドバイスする事としては、PCAを実装する前に、私が最初に提案するのは、あなたがやっている事がなんであれ、何をやりたいのであれ、まずはオリジナルの生のデータxiでやってみる事を検討せよ、という事だ。それが望む結果を生まなかったら、その時になって初めて、PCAを実装し、ziを使う事を検討すべきだ。",
    "output": "And sometime, and so specifically, what I often advise people is, before you implement PCA, I would first suggest that, you know, do whatever it is, take whatever it is you want to do and first consider doing it with your original raw data xi, and only if that doesn't do what you want, then implement PCA before using Zi."
  },
  {
    "index": "F17453",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりPCAを使う前に、データの次元を減らす代わりに、私が代わりに検討するのは、このPCAのステップをさぼってみよう。そして検討する事は、オリジナルのデータに対して単純に学習アルゴリズムを訓練してみよう、って事だ。",
    "output": "So, before using PCA you know, instead of reducing the dimension of the data, I would consider well, let's ditch this PCA step, and I would consider, let's just train my learning algorithm on my original data."
  },
  {
    "index": "F17454",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "オリジナルの生の入力xiを使ってみよう、そして私が推奨するのは、PCAをアルゴリズムに組み込む代わりに、あなたがやってる事がなんであれ、最初のxiでやってみる事を推奨する。",
    "output": "Let's just use my original raw inputs xi, and I would recommend, instead of putting PCA into the algorithm, just try doing whatever it is you're doing with the xi first."
  },
  {
    "index": "F17455",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれがうまくいかない、と信じる理由を得て初めて、つまりあなたの学習アルゴリズムがあまりにも実行速度が遅いという結果になって初めて、または必要メモリの要求量や必要ディスクの要求量があまりにも大きくなり過ぎて、だから表現を圧縮したい、と思った時に、、、それはxiを使ってみてうまく行かなかった時になって、、、xiではうまく行かない、という証拠がある時か、またはそう信じる強い理由があるようになって、、、その時初めて、PCAを実装して表現を圧縮する事に使えば良い。",
    "output": "And only if you have a reason to believe that doesn't work, so that only if your learning algorithm ends up running too slowly, or only if the memory requirement or the disk space requirement is too large, so you want to compress your representation, but if only using the xi doesn't work, only if you have evidence or strong reason to believe that using the xi won't work, then implement PCA and consider using the compressed representation."
  },
  {
    "index": "F17456",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな事を言うのは、人々が最初からプロジェクトのプランにPCAを用いる事を計画していて、そして時々、彼らが何をするにせよ、PCA無しでもうまく行く、という事態を私はちょくちょく目撃して来たからだ。",
    "output": "Because what I do see, is sometimes people start off with a project plan that incorporates PCA inside, and sometimes they, whatever they're doing will work just fine, even with out using PCA instead."
  },
  {
    "index": "F17457",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからPCAを使うのは、代替案も検討した後にすべきだ、PCAを得たり、どのkを使うべきかとかを見出したりにたくさん時間を費やしたりする前に。",
    "output": "So, just consider that as an alternative as well, before you go to spend a lot of time to get PCA in, figure out what k is and so on."
  },
  {
    "index": "F17458",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以上がPCAだ。",
    "output": "So, that's it for PCA."
  },
  {
    "index": "F17459",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして現実に、私はPCAをしょっちゅう使う。私の場合は、PCAを使うのは、学習アルゴリズムの実行時間をスピードアップするのに使う事が一番多い。",
    "output": "Despite these last sets of comments, PCA is an incredibly useful algorithm, when you use it for the appropriate applications and I've actually used PCA pretty often and for me, I use it mostly to speed up the running time of my learning algorithms."
  },
  {
    "index": "F17460",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが私が思うに、PCAはその他の用途も同じくらい一般的だと思う。データを圧縮するのに使う、これはメモリやディスク容量の要件を減らす事が出来るし、またデータの可視化にも良く使われる。",
    "output": "But I think, just as common an application of PCA, is to use it to compress data, to reduce the memory or disk space requirements, or to use it to visualize data."
  },
  {
    "index": "F17461",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてPCAは教師なし学習のアルゴリズムのうち、もっとも良く使われていて、もっともパワフルな物の一つと言えるだろう。",
    "output": "And PCA is one of the most commonly used and one of the most powerful unsupervised learning algorithms."
  },
  {
    "index": "F17462",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらのビデオで学んだ事を用いて、あなたは、きっとPCAを実装する事が出来て、これらの目的全てに対しても使っていく事が出来るだろう。",
    "output": "And with what you've learned in these videos, I think hopefully you'll be able to implement PCA and use them through all of these purposes as well."
  },
  {
    "index": "F17463",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今後の一連のビデオで、アノマリー検出、と言われる問題を扱いたい。",
    "output": "In this next set of videos, I'd like to tell you about a problem called Anomaly Detection."
  },
  {
    "index": "F17464",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは割と良く使われる種類の機械学習で、そして興味深い側面の一つに、これはだいたい教師なし学習の問題でありながら、またある側面では教師有り学習の問題にとても似ている部分もある。",
    "output": "This is a reasonably commonly use you type machine learning. And one of the interesting aspects is that it's mainly for unsupervised problem, that there's some aspects of it that are also very similar to sort of the supervised learning problem."
  },
  {
    "index": "F17465",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、アノマリー検出とは何か?",
    "output": "So, what is anomaly detection?"
  },
  {
    "index": "F17466",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてあなたの航空機のエンジンが組み立てラインからロールオフして、そしてQAをしている、つまり品質保証テストをしているとしよう。そしてそのテストの一貫として、航空機のエンジンのある機能ーーそうだなぁ、生成される熱とか振動とかを測っているとしよう。",
    "output": "Let me use the motivating example of: Imagine that you're a manufacturer of aircraft engines, and let's say that as your aircraft engines roll off the assembly line, you're doing, you know, QA or quality assurance testing, and as part of that testing you measure features of your aircraft engine, like maybe, you measure the heat generated, things like the vibrations and so on."
  },
  {
    "index": "F17467",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ずっと昔に私の友人達がこの問題に挑んでいてこれらのフィーチャーは本当に彼らが実際の航空機エンジンから集めた物だ。あなたは今、X1からXmのデータセットを持っている。",
    "output": "I share some friends that worked on this problem a long time ago, and these were actually the sorts of features that they were collecting off actual aircraft engines so you now have a data set of X1 through Xm, if you have manufactured m aircraft engines, and if you plot your data, maybe it looks like this."
  },
  {
    "index": "F17468",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この各点、各バッテンはあなたのラベル無し手本だ。",
    "output": "So, each point here, each cross here as one of your unlabeled examples."
  },
  {
    "index": "F17469",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてアノマリー検出の問題は以下のような感じ。",
    "output": "So, the anomaly detection problem is the following."
  },
  {
    "index": "F17470",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "翌日、新しい航空機のエンジンが組立ラインからロールオフしたとしてみよう。あなたの新しい航空エンジンは幾つかのフィーチャーの集合、x-testを持つ。",
    "output": "Let's say that on, you know, the next day, you have a new aircraft engine that rolls off the assembly line and your new aircraft engine has some set of features x-test."
  },
  {
    "index": "F17471",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アノマリー検出問題とはこの航空機エンジンがとにかく何かしら普通でないかを知りたい。",
    "output": "What the anomaly detection problem is, we want to know if this aircraft engine is anomalous in any way, in other words, we want to know if, maybe, this engine should undergo further testing because, or if it looks like an okay engine, and so it's okay to just ship it to a customer without further testing."
  },
  {
    "index": "F17472",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、たとえばこのエンジンをさらなるテストに回さなきゃいけないかを知りたい、またはこのエンジンは問題なさそうなのかを。問題なさそうで、追加のテスト無しで客に出荷して良さそうかを。",
    "output": "So, if your new aircraft engine looks like a point over there, well, you know, that looks a lot like the aircraft engines we've seen before, and so maybe we'll say that it looks okay."
  },
  {
    "index": "F17473",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、その新しい航空機エンジンがそこの点ならうーむ、それ以前のたくさんの航空機エンジンと似てるので、たぶんOKそうでしょう。",
    "output": "Whereas, if your new aircraft engine, if x-test, you know, were a point that were out here, so that if X1 and X2 are the features of this new example."
  },
  {
    "index": "F17474",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、新しい航空機エンジンがx-testが、ここの点なら、つまりx1とx2がこの新しい手本のフィーチャーなら、x-testがはるかこの外にあるなら、それはアノマリーと呼んで良かろう。",
    "output": "If x-tests were all the way out there, then we would call that an anomaly."
  },
  {
    "index": "F17475",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからたぶん、顧客に出荷する前に追加のテストに送り出しても良いといえよう。だってこのエンジンは、それ以外に見た物と大きく異なっているから。",
    "output": "and maybe send that aircraft engine for further testing before we ship it to a customer, since it looks very different than the rest of the aircraft engines we've seen before."
  },
  {
    "index": "F17476",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より正式には、アノマリー検出の問題ではなんらかのデータセットが与えられて、それはx1からxmまでの手本としておく、そして、普通はこれらのm個の手本をノーマル、またはアノマリーでは無い、と想定する。",
    "output": "More formally in the anomaly detection problem, we're give some data sets, x1 through Xm of examples, and we usually assume that these end examples are normal or non-anomalous examples, and we want an algorithm to tell us if some new example x-test is anomalous."
  },
  {
    "index": "F17477",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてある新しいサンプル、x-testが来た時にに、それがアノマリーっぽいかをアルゴリズムに教えて欲しい。その為に我らがとるアプローチは与えられたデータセットに対しラベル無しのトレーニングセットが与えられた時にモデルp(x)を構築する。",
    "output": "The approach that we're going to take is that given this training set, given the unlabeled training set, we're going to build a model for p of x."
  },
  {
    "index": "F17478",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、xの時の確率のモデルを構築するということ、ここでxはこれらのフィーチャー、例えば航空機のエンジンとかの。",
    "output": "In other words, we're going to build a model for the probability of x, where x are these features of, say, aircraft engines."
  },
  {
    "index": "F17479",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてxの時の確率のモデルを構築して、新しい航空機エンジンに対し、p(x-test)が、あるエプシロンより小さいかを見る。そしてこれがアノマリーかどうかのフラグをつける。",
    "output": "And so, having built a model of the probability of x we're then going to say that for the new aircraft engine, if p of x-test is less than some epsilon then we flag this as an anomaly."
  },
  {
    "index": "F17480",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり新しいエンジンでデータから推計したモデル、p(x)による確率がとても小さいのを見かけたら、これをアノマリーとフラグをつける。もしp(x-test)が、例えばある小さな閾値より大きければそれはオーケーっぽいと言うわけ。",
    "output": "So we see a new engine that, you know, has very low probability under a model p of x that we estimate from the data, then we flag this anomaly, whereas if p of x-test is, say, greater than or equal to some small threshold."
  },
  {
    "index": "F17481",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして与えられたトレーニングセットがここにプロットしたような物だとして、以下のようなモデルを構築して、航空機エンジンの、、、いや、モデルp(x)に、どこかこの中のあたりにある点に対しては、とても高い確率だと言って欲しく、他方、ちょっと離れた所にある点には、低い確率だと言って欲しい。",
    "output": "And so, given the training set, like that plotted here, if you build a model, hopefully you will find that aircraft engines, or hopefully the model p of x will say that points that lie, you know, somewhere in the middle, that's pretty high probability, whereas points a little bit further out have lower probability."
  },
  {
    "index": "F17482",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらに遠く離れた点に対してはなんらかの、より低い確率になって欲しい。そしてこの離れた点やこの離れた点はアノマリーだろう。",
    "output": "Points that are even further out have somewhat lower probability, and the point that's way out here, the point that's way out there, would be an anomaly."
  },
  {
    "index": "F17483",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方ここにある点、ちょうどなかほどにある点、これはOKだろう、だってp(x)はなかほどの点に対してはとても高くなるだろうから、だってその辺にはたくさんの点が見られているから。",
    "output": "Whereas the point that's way in there, right in the middle, this would be okay because p of x right in the middle of that would be very high cause we've seen a lot of points in that region."
  },
  {
    "index": "F17484",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たぷん一番一般的なアノマリー検出の応用例は、たくさんのユーザーが居て、各ユーザーが異なるアクティビティを行なっている時に、たとえばwebサイト上とか物理的な工場とかそういうので、各ユーザーごとのアクティビティのフィーチャーを計算する事が出来る時に、モデルを構築して、いわば、異なるユーザーが別々の行動をとる確率を言わせる事が出来る。",
    "output": "Perhaps the most common application of anomaly detection is actually for detection if you have many users, and if each of your users take different activities, you know maybe on your website or in the physical plant or something, you can compute features of the different users activities."
  },
  {
    "index": "F17485",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ユーザー行動を表すフィーチャーのあるベクトルがどの位の確率となるのか、たとえばユーザーアクティビティのフィーチャーの例としては、webサイトの場合なら、x1がユーザーのログインの頻度で、x2は、うーん、訪問したページの総数とか、取引の総数で、x3は、うーん、そのユーザーがフォーラムにポストした投稿の総数で、フィーチャーx4はユーザーのタイピング速度とかの可能性だってありえる。",
    "output": "And what you can do is build a model to say, you know, what is the probability of different users behaving different ways."
  },
  {
    "index": "F17486",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実際一秒あたりのユーザーのタイプした文字速度をトラックしているwebサイトもある。",
    "output": "What is the probability of a particular vector of features of a users behavior so you know examples of features of a users activity may be on the website it'd be things like, maybe x1 is how often does this user log in, x2, you know, maybe the number of what pages visited, or the number of transactions, maybe x3 is, you know, the number of posts of the users on the forum, feature x4 could be what is the typing speed of the user and some websites can actually track that was the typing speed of this user in characters per second."
  },
  {
    "index": "F17487",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこんな類のデータに対して、p(x)をモデリング出来るわけだ。",
    "output": "And so you can model p of x based on this sort of data."
  },
  {
    "index": "F17488",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、その得られたモデルp(x)を使って、あなたのwebサイトで凄く奇妙な行動をとっているユーザーを特定する事が出来る、どのユーザーが確率的にエプシロン以下なのかをチェックする事によって。",
    "output": "And finally having your model p of x, you can try to identify users that are behaving very strangely on your website by checking which ones have probably effects less than epsilon and maybe send the profiles of those users for further review."
  },
  {
    "index": "F17489",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれらのユーザーのプロファイルをさらなるレビューに送り出すとか、または、それらのユーザーからは追加の身分証明を提出させるとか、そういう、あなたのwebサイトを奇妙な行動や詐欺っぽい行動からガードする何らかの措置を講ずるのだ。",
    "output": "Or demand additional identification from those users, or some such to guard against you know, strange behavior or fraudulent behavior on your website."
  },
  {
    "index": "F17490",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この種の技術は、普通でない行動をしているユーザーをフラグ付けしてしまい、それは必ずしも不正をしているユーザーだけとは限らない。",
    "output": "This sort of technique will tend of flag the users that are behaving unusually, not just users that maybe behaving fraudulently."
  },
  {
    "index": "F17491",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりいっつも盗みを働いているユーザーだけじゃなくたんにふざけてるだけのユーザーも。",
    "output": "So not just constantly having stolen or users that are trying to do funny things, or just find unusual users."
  },
  {
    "index": "F17492",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこれは実際にたくさんの商品を販売しているオンラインwebサイトにおいて、詐欺行為をしているか不正にのっとったアカウントを使っている事を示す事を期待すべく、奇妙な行動をとっているユーザーを見つける為に使われているテクニックだ。",
    "output": "But this is actually the technique that is used by many online websites that sell things to try identify users behaving strangely that might be indicative of either fraudulent behavior or of computer accounts that have been stolen."
  },
  {
    "index": "F17493",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アノマリー検出のもう一つの応用例は製造業だ。",
    "output": "Another example of anomaly detection is manufacturing."
  },
  {
    "index": "F17494",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "航空機エンジンのケースを既に話したが、そこでは普通とは異なる航空機エンジンを見つけ出して、それらをさらなるレビューへと送り出すのだった。",
    "output": "So, already talked about the aircraft engine thing where you can find unusual, say, aircraft engines and send those for further review."
  },
  {
    "index": "F17495",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "三番目の応用例はデータセンターのコンピュータをモニタリングするという事。",
    "output": "A third application would be monitoring computers in a data center."
  },
  {
    "index": "F17496",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これに実際に従事してる友達が何人かいるよ。",
    "output": "I actually have some friends who work on this too."
  },
  {
    "index": "F17497",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コンピュータのクラスタなりデータセンターなりにたくさんのコンピュータがあったとして、各マシンのフィーチャーを計算出来る。",
    "output": "So if you have a lot of machines in a computer cluster or in a data center, we can do things like compute features at each machine."
  },
  {
    "index": "F17498",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばどれだけのメモリを使ってるかとかディスクアクセスの総数だとかCPU負荷だとかもっと複雑なフィーチャーでも良い、このマシンのCPUロードをこのマシンのネットワークトラフィックの量で割ったりだとか、そういう物を捉えたようなフィーチャー。",
    "output": "As well as more complex features like what is the CPU load on this machine divided by the amount of network traffic on this machine?"
  },
  {
    "index": "F17499",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてデータセンターの通常時の振る舞いのデータを与えられた時に、xとなる確率をモデリング出来る。つまりこれらのマシンが様々なメモリ使用量となる確率、またはこれらのマシンが様々なディスクアクセスの回数となる確率、様々なCPU負荷となる確率などをモデリング出来る。",
    "output": "Then given the dataset of how your computers in your data center usually behave, you can model the probability of x, so you can model the probability of these machines having different amounts of memory use or probability of these machines having different numbers of disc accesses or different CPU loads and so on."
  },
  {
    "index": "F17500",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもし確率xが、p(x)がとても小さいマシンがあったら、そのマシンは普通でなく振舞ってるという事が分かり、そのマシンは落ちる所かもしれないのでそれをフラグ付けしてシステム管理者にレビューさせたり出来る。",
    "output": "And if you ever have a machine whose probability of x, p of x, is very small then you know that machine is behaving unusually and maybe that machine is about to go down, and you can flag that for review by a system administrator."
  },
  {
    "index": "F17501",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれは実際に、こんにち様々なデータセンターで自分たちのマシンに普通でない事が起きていないか監視するのに使われている。",
    "output": "And this is actually being used today by various data centers to watch out for unusual things happening on their machines."
  },
  {
    "index": "F17502",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がアノマリー検出。",
    "output": "So, that's anomaly detection."
  },
  {
    "index": "F17503",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオではガウス分布とガウス分布の性質をちょこっと議論し、そしてその後のビデオでアノマリー検出のアルゴリズムを開発するのにそれを使っていく。",
    "output": "In the next video, I'll talk a bit about the Gaussian distribution and review properties of the Gaussian probability distribution, and in videos after that, we will apply it to develop an anomaly detection algorithm."
  },
  {
    "index": "F17504",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、ガウス分布について議論したい、それは正規分布とも呼ばれる。",
    "output": "In this video, I'd like to talk about the Gaussian distribution which is also called the normal distribution."
  },
  {
    "index": "F17505",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしガウス分布に既に十分に慣れ親しんでいるのなら、たぶんこのビデオはスキップしてOKだ。だがもしあんま自信無かったりガウス分布または正規分布を使っていた頃から随分と時間が経っているのなら、このビデオを終わりまで見てみてください。",
    "output": "In case you're already intimately familiar with the Gaussian distribution, it's probably okay to skip this video, but if you're not sure or if it has been a while since you've worked with the Gaussian distribution or normal distribution then please do watch this video all the way to the end."
  },
  {
    "index": "F17506",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのビデオの後には、アノマリー検出のアルゴリズムを開発する為にガウス分布を用いていきます。",
    "output": "And in the video after this we'll start applying the Gaussian distribution to developing an anomaly detection algorithm."
  },
  {
    "index": "F17507",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "xは実数のランダムな数とします。つまりxは実数です。",
    "output": "Let's say x is a row value's random variable, so x is a row number."
  },
  {
    "index": "F17508",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしxの確率分布がガウス分布で、平均ミューと分散シグマ二乗である事をこう書ける:ランダム変数xチルダここでこの小さなチルダ記号の意味する所は分布が等しいという事で、その後にガウス分布を記述する訳だが、それには、記号のNにかっこでミュー、シグマ二乗と書く事になっている。",
    "output": "If the probability distribution of x is Gaussian with mean mu and variance sigma squared."
  },
  {
    "index": "F17509",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの記号NはノーマルのN、だってガウス分布は正規(ノーマル)分布だから。",
    "output": "So this script N stands for normal since Gaussian and normal they mean the thing are synonyms."
  },
  {
    "index": "F17510",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは同じ意味で、Normalの省略系でガウス分布は2つのパラメータでパラメトライズされてる、という事を表す。2つのパラメータとは平均を表すミューとシグマ二乗で示される分散のパラメータ。",
    "output": "And the Gaussian distribution is parametarized by two parameters, by a mean parameter which we denote mu and a variance parameter which we denote via sigma squared."
  },
  {
    "index": "F17511",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ガウス分布またはガウス確率密度をプロットすると、ベル型のカーブとなる、見たことあるかもしれないね。",
    "output": "If we plot the Gaussian distribution or Gaussian probability density. It'll look like the bell shaped curve which you may have seen before."
  },
  {
    "index": "F17512",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてつまり、このベル型のカーブは2つのパラメータ、ミューとシグマでパラメトライズされてる。",
    "output": "And so this bell shaped curve is paramafied by those two parameters, mu and sequel."
  },
  {
    "index": "F17513",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このベル型のカーブの中心は平均のミューで、そしてこのベル型のカーブの幅がだいたい、このパラメータシグマであり、1標準偏差とも呼ばれている。",
    "output": "And the location of the center of this bell shaped curve is the mean mu."
  },
  {
    "index": "F17514",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはxが様々な値を取る確率を示している、つまりxがこの真ん中の値を取ると、極めて高くなる。",
    "output": "And the width of this bell shaped curve, roughly that, is this parameter, sigma, is also called one standard deviation, and so this specifies the probability of x taking on different values."
  },
  {
    "index": "F17515",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "なぜならガウス分布のここはとても高い一方でxがこの遥か離れた所の値を取る場合は確率は減衰するだろう。",
    "output": "So, x taking on values here in the middle here it's pretty high, since the Gaussian density here is pretty high, whereas x taking on values further, and further away will be diminishing in probability."
  },
  {
    "index": "F17516",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、完璧を期するという目的の為だけに、ガウス分布の式を書き下しておこう。",
    "output": "Finally just for completeness let me write out the formula for the Gaussian distribution."
  },
  {
    "index": "F17517",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "xの確率は、、、ところでたまに、p(x)と書く代わりに、これをpのxにセミコロンでミュー、シグマ二乗とつなげて書くことがある。これはxの確率は2つのパラメータミューとシグマ二乗でパラメトライズされている事を示す。",
    "output": "So the probability of x, and I'll sometimes write this as the p (x) when we write this as P ( x ; mu, sigma squared), and so this denotes that the probability of X is parameterized by the two parameters mu and sigma squared."
  },
  {
    "index": "F17518",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてガウス密度の式はこれだ。2パイのシグマ分の一のeのマイナスx引くミューの二乗割ることの2シグマ二乗。",
    "output": "And the formula for the Gaussian density is this 1/ root 2 pi, sigma e (-(x-mu/g) squared/2 sigma squared."
  },
  {
    "index": "F17519",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この式を暗記する必要は無い。",
    "output": "So there's no need to memorize this formula."
  },
  {
    "index": "F17520",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは単にここの左にあるベル型のカーブの式ってだけに過ぎない。",
    "output": "This is just the formula for the bell-shaped curve over here on the left."
  },
  {
    "index": "F17521",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを暗記する必要は無いし、もしこれを使う必要が出てきても、その時調べれば良い。",
    "output": "There's no need to memorize it, and if you ever need to use this, you can always look this up."
  },
  {
    "index": "F17522",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がこの左にある図でこれがミューとシグマを固定してp(x)をプロットしたら得る物だ。",
    "output": "And so that figure on the left, that is what you get if you take a fixed value of mu and take a fixed value of sigma, and you plot P(x) so this curve here."
  },
  {
    "index": "F17523",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このここのカーブ、これがミューとシグマ二乗、つまり分散を固定してp(x)をxの関数としてプロットした物だ。",
    "output": "This is really p(x) plotted as a function of X for a fixed value of Mu and of sigma squared."
  },
  {
    "index": "F17524",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "時には(シグマ二乗より)シグマで考えた方が楽な事もある。",
    "output": "And sometimes is easier to think in terms of sigma."
  },
  {
    "index": "F17525",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シグマは標準偏差と呼ばれる物でそれはガウスの確率分布の幅を規定するもので、一方シグマ二乗、シグマの二乗は分散と呼ばれる物。",
    "output": "So sigma is called the standard deviation, and so it specifies the width of this Gaussian probability density, where as the square sigma, or sigma squared, is called the variance."
  },
  {
    "index": "F17526",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ガウス分布が実際にどんな感じか、例を見てみよう。",
    "output": "Let's look at some examples of what the Gaussian distribution looks like."
  },
  {
    "index": "F17527",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ミューが0でシグマが1だとすると、ゼロを中心に持つガウス分布となる、何故ならそれはミューの事で、ガウス分布の幅はつまり1標準偏差はここのシグマとなる。",
    "output": "Then we have a Gaussian distribution that's centered around zero, because that's mu and the width of this Gaussian, so that's one standard deviation is sigma over there."
  },
  {
    "index": "F17528",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ガウス分布の例を見てみよう。",
    "output": "Let's look at some examples of Gaussians."
  },
  {
    "index": "F17529",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ミューが0でシグマが1の時はゼロの所を中心とするガウス分布に対応する、何故ならミューが0だから。そしてガウス分布の幅は、、、ガウス分布では幅はシグマにより、つまり分散のパラメータ、シグマにより制御されている。",
    "output": "If mu is equal to zero and sigma equals one, then that corresponds to a Gaussian distribution that is centered at zero, since mu is zero, and the width of this Gaussian is is controlled by sigma by that variance parameter sigma."
  },
  {
    "index": "F17530",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もう一つの例はこんな感じ。",
    "output": "Here's another example."
  },
  {
    "index": "F17531",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ミューが0でシグマが1/2としよう。つまり標準偏差が1/2で分散、シグマ二乗は0.5の二乗、つまり0.25となる。",
    "output": "That same mu is equal to 0 and sigma is equal to .5 so the standard deviation is .5 and the variance sigma squared would therefore be the square of 0.5 would be 0.25 and in that case the Gaussian distribution, the Gaussian probability density goes like this."
  },
  {
    "index": "F17532",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、ガウス分布、ガウス確率密度はこんな感じとなる、今回もゼロを中心としているが、だが今回は幅がより狭くなっている、何故なら分散が小さくなったから。ガウス密度はだいたい半分の幅となっている。",
    "output": "But now the width of this is much smaller because the smaller the area is, the width of this Gaussian density is roughly half as wide."
  },
  {
    "index": "F17533",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがこれは確率分布なのだから、カーブの下の面積、つまり影をつけた部分は、積分すると必ず1となる。",
    "output": "But because this is a probability distribution, the area under the curve, that's the shaded area there."
  },
  {
    "index": "F17534",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは確率分布の性質だ。",
    "output": "That area must integrate to one this is a property of probability distributing."
  },
  {
    "index": "F17535",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また、そうであるから、これはより背の高いガウス密度となる、だって標準偏差が半分だから幅も半分になるので、高さは二倍となる訳だ。もう一つ例。",
    "output": "So this is a much taller Gaussian density because this half is Y but half the standard deviation but it twice as tall."
  },
  {
    "index": "F17536",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シグマが2だとより太った、またはより幅の広いガウス密度となる。つまりここでは、パラメータのシグマはガウス密度がどれだけの幅を持つかを制御している。",
    "output": "Another example is sigma is equal to 2 then you get a much fatter a much wider Gaussian density and so here the sigma parameter controls that Gaussian distribution has a wider width."
  },
  {
    "index": "F17537",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして今回も、曲線の下の面積はこの影をつけた領域だが、それはいつでも積分すると1となる。",
    "output": "And once again, the area under the curve, that is the shaded area, will always integrate to one, that's the property of probability distributions and because it's wider it's also half as tall in order to still integrate to the same thing."
  },
  {
    "index": "F17538",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは確率分布の性質で今回はより幅が広くなったのだから、高さは半分になっている、積分した結果が同じになるように。",
    "output": "And finally one last example would be if we now change the mu parameters as well."
  },
  {
    "index": "F17539",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、最後の例は、ミューも同じように変更していった場合、ゼロを真ん中に分布する代わりに、ここでは3の回りに分布するガウス分布を得る。何故ならこれはガウス分布全体をシフトさせるから。",
    "output": "Then instead of being centered at 0 we now have a Gaussian distribution that's centered at 3 because this shifts over the entire Gaussian distribution."
  },
  {
    "index": "F17540",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にパラメータの推計の話題にうつろう。",
    "output": "Next, let's talk about the Parameter estimation problem."
  },
  {
    "index": "F17541",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "パラメータの推計問題とはなんだろう?",
    "output": "So what's the parameter estimation problem?"
  },
  {
    "index": "F17542",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "m個の手本データセットがあるとしよう。つまりx1からxmまで。",
    "output": "Let's say we have a dataset of m examples so exponents x m and lets say each of this example is a row number."
  },
  {
    "index": "F17543",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの手本の個々は実数だとしよう。この図で、私は手本のデータセットをプロットした。",
    "output": "Here in the figure I've plotted an example of the dataset so the horizontal axis is the x axis and either will have a range of examples of x, and I've just plotted them on this figure here."
  },
  {
    "index": "F17544",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "横軸はx軸で手本のデータは、xがある範囲に広がっている。それをここに単純にプロットしてみた。",
    "output": "And the parameter estimation problem is, let's say I suspect that these examples came from a Gaussian distribution."
  },
  {
    "index": "F17545",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてパラメータ推計の問題はこれらの手本がガウス分布だったと思っているとして、つまりこの各手本x(i)が以下のように分布、、、それがこのチルダの意味だったね。",
    "output": "So let's say I suspect that each of my examples, x i, was distributed. That's what this tilde thing means."
  },
  {
    "index": "F17546",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、これらの各手本が、正規分布またの名をガウス分布に従って分布していると思ってるとする、あるパラメータ、ミューとシグマ二乗の。",
    "output": "Let's not suspect that each of these examples were distributed according to a normal distribution, or Gaussian distribution, with some parameter mu and some parameter sigma square."
  },
  {
    "index": "F17547",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、これらの値が幾つかは知らない。",
    "output": "But I don't know what the values of these parameters are."
  },
  {
    "index": "F17548",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "パラメータ推計の問題とは、与えられたデータセットに対して、ミューやシグマ二乗の値が何になるのかを推計したい、という事。",
    "output": "The problem of parameter estimation is, given my data set, I want to try to figure out, well I want to estimate what are the values of mu and sigma squared."
  },
  {
    "index": "F17549",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこんなデータセットを与えられたら、どんなガウス分布からこのデータが生成されているかを推計したら、多分だいたいミューを中心としてシグマ、つまり標準偏差がこのガウス分布の幅をコントロールしている。",
    "output": "So if you're given a data set like this, it looks like maybe if I estimate what Gaussian distribution the data came from, maybe that might be roughly the Gaussian distribution it came from."
  },
  {
    "index": "F17550",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはだいたいデータにリーズナブルにフィットしている。",
    "output": "Seems like a reasonable fit to the data."
  },
  {
    "index": "F17551",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならデータは見た感じ中心のあたりにとても高い確率を持っていて、外に離れれば離れる程低い確率となっている。",
    "output": "Because, you know, looks like the data has a very high probability of being in the central region, and a low probability of being further out, even though probability of being further out, and so on."
  },
  {
    "index": "F17552",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれはたぶんミューとシグマ二乗のリーズナブルな推計となっている。",
    "output": "So maybe this is a reasonable estimate of mu and sigma squared."
  },
  {
    "index": "F17553",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、そのデータがガウス分布に従っているのなら、それはこんな見た目のはずだ。",
    "output": "That is, if it corresponds to a Gaussian distribution function that looks like this."
  },
  {
    "index": "F17554",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで私は、こうする:式、つまり正規分布の式を書き下して、パラメータのミューとシグマ二乗を推計する。",
    "output": "So what I'm going to do is just write out the formula the standard formulas for estimating the parameters Mu and sigma squared."
  },
  {
    "index": "F17555",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ミューを推計する方法は単に平均をとるだけ、手本の全体にわたって。",
    "output": "Our estimate or the way we're going to estimate mu is going to be just the average of my example."
  },
  {
    "index": "F17556",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ミューは平均のパラメータだから。",
    "output": "So mu is the mean parameter."
  },
  {
    "index": "F17557",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりトレーニングセットから、m個の手本から、それらの平均をとる。",
    "output": "Just take my training set, take my m examples and average them."
  },
  {
    "index": "F17558",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは単にこの分布の、中心を与える。",
    "output": "And that just means the center of this distribution."
  },
  {
    "index": "F17559",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではシグマ二乗はどうか?",
    "output": "How about sigma squared?"
  },
  {
    "index": "F17560",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "分散に関しては、また通常の式を書き下すと、1からmまでに渡り、x(i)からミューを引いた物の二乗の和を計算する。",
    "output": "Well, the variance, I'll just write out the standard formula again, I'm going to estimate as sum over one through m of x i minus mu squared."
  },
  {
    "index": "F17561",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでこのミューはこの式を使ってここで計算したものだ。",
    "output": "And so this mu here is actually the mu that I compute over here using this formula."
  },
  {
    "index": "F17562",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして分散とは何か、というと一つの解釈としては、この項を見ると分かるように、手本の値と平均との差の二乗となっている。",
    "output": "And what the variance is, or one interpretation of the variance is that if you look at this term, that's the square difference between the value I got in my example minus the mean."
  },
  {
    "index": "F17563",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり中心との差、分布の平均との差の。",
    "output": "Minus the center, minus the mean of the distribution."
  },
  {
    "index": "F17564",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして分散を、差分、つまり手本と平均との間の差分の二乗の平均として推計する訳だ。",
    "output": "And so in the variance I'm gonna estimate as just the average of the square differences between my examples, minus the mean."
  },
  {
    "index": "F17565",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでちょっと補足しておくと、あなたが統計のエキスパートだったら統計のエキスパートだったら最尤法という物について聞いたことがあるかもしれない。そうであるなら、これらの推計は実際には最尤法によるパラメータ、ミューとシグマ二乗の推計である。",
    "output": "If you're an expert in statistics, and if you've heard of maximum likelihood estimation, then these parameters, these estimates, are actually the maximum likelihood estimates of the primes of mu and sigma squared but if you haven't heard of that before don't worry about it, all you need to know is that these are the two standard formulas for how to figure out what are mu and Sigma squared given the data set."
  },
  {
    "index": "F17566",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これもまた、以前に統計のクラスを取った事のある人向けの話だ。",
    "output": "Finally one last side comment again only for those of you that have maybe taken the statistics class before but if you've taken statistics This class before."
  },
  {
    "index": "F17567",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以前に統計のクラスを取った事があるなら、ここにある式を以前に見た時はmではなくてm-1だったかもしれない。",
    "output": "Some of you may have seen the formula here where this is M-1 instead of M so this first term becomes 1/M-1 instead of 1/M."
  },
  {
    "index": "F17568",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりの最初の項が、1/mではなくて、1/(m-1)になる。機械学習では1/mの式が使われる事が多い。",
    "output": "In machine learning people tend to learn 1/M formula but in practice whether it is 1/M or 1/M-1 it makes essentially no difference assuming M is reasonably large."
  },
  {
    "index": "F17569",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが現実問題としては、1/mだろうが1/(m-1)だろうが、本質的には大差無い、mが普通に考えられる程度に大きい、つまりトレーニングセットのサイズが普通に大きければ。",
    "output": "a reasonably large training set size."
  },
  {
    "index": "F17570",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ようするに、以前に別のバージョンの方を見た事があった場合の為に補足しておくと、どっちのバージョンでもちゃんと機能する。",
    "output": "So just in case you've seen this other version before."
  },
  {
    "index": "F17571",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが機械学習では多くの人々はこの公式の1/mの方を使う傾向にある。2つのバージョンは理論的にはわずかに異なった特徴があり、わずかに異なった数学的な特徴があるが、現実の場面ではほとんど違いは無い。",
    "output": "In either version it works just about equally well but in machine learning most people tend to use 1/M in this formula.And the two versions have slightly different theoretical properties like these are different math properties."
  },
  {
    "index": "F17572",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、ガウス分布がどんな感じか、感覚的に分かるようになってくれただろうか。",
    "output": "Bit of practice it really makes makes very little difference, if any."
  },
  {
    "index": "F17573",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニングセットを与えられた時、ガウス分布に従ったデータだと思われるがそのパラメータ、ミューとシグマ二乗が不明な時。",
    "output": "So, hopefully you now have a good sense of what the Gaussian distribution looks like, as well as how to estimate the parameters mu and sigma squared of Gaussian distribution if you're given a training set, that is if you're given a set of data that you suspect comes from a Gaussian distribution with unknown parameters, mu and sigma squared."
  },
  {
    "index": "F17574",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、これを用いて、アノマリー検出のアルゴリズムを開発するのに適用していく。",
    "output": "In the next video, we'll start to take this and apply it to develop an anomaly detection algorithm."
  },
  {
    "index": "F17575",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではガウス分布について話した。",
    "output": "In the last video, we talked about the Gaussian distribution."
  },
  {
    "index": "F17576",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今回のビデオではそれを用いてアノマリー検出のアルゴリズムを開発する。",
    "output": "In this video lets apply that to develop an anomaly detection algorithm."
  },
  {
    "index": "F17577",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの手本の個々はRnに属するフィーチャーとなっている。つまりトレーニングセットは例えば製造した直近m個の航空機エンジンのフィーチャーベクトルでも良し、m人のユーザーのフィーチャーでもそれ以外の何かでも良い。",
    "output": "Let's say that we have an unlabeled training set of M examples, and each of these examples is going to be a feature in Rn so your training set could be, feature vectors from the last M aircraft engines being manufactured."
  },
  {
    "index": "F17578",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アノマリー検出にどうアプローチしていくか、というと、データセットからp(x)をモデリングしていきます。",
    "output": "The way we are going to address anomaly detection, is we are going to model p of x from the data sets."
  },
  {
    "index": "F17579",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どんなフィーチャーの組みの確率が高くてどんなフィーチャーの組みが低い確率なのかを見つけ出したい。",
    "output": "We're going to try to figure out what are high probability features, what are lower probability types of features."
  },
  {
    "index": "F17580",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、xはベクトルなので、p(x)をモデリングするとはx1の確率、、、ここでx1はxの最初の成分ですが、x1の確率に、掛けることのx2となる確率、これは二番目のフィーチャーの確率で、それに掛けることの三番目のフィーチャーの確率、などなどと、最後のフィーチャーxnの確率まで掛け合わせます。",
    "output": "So, x is a vector and what we are going to do is model p of x, as probability of x1, that is of the first component of x, times the probability of x2, that is the probability of the second feature, times the probability of the third feature, and so on up to the probability of the final feature of Xn."
  },
  {
    "index": "F17581",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここにスペースを空けたのは、あとでここに書きたい事があるからです。",
    "output": "Now I'm leaving space here cause I'll fill in something in a minute."
  },
  {
    "index": "F17582",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、これらの個々の項、p(x1)、p(x2)などをどうモデリングするのでしょうか?",
    "output": "So, how do we model each of these terms, p of X1, p of X2, and so on."
  },
  {
    "index": "F17583",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうやるかというと、x1がガウス分布に従って分布していると想定します、なんらかの平均、それをミュー1と書きましょう、そしてなんらかの分散、それをシグマ二乗の1と書くことにしましょう。そしてp(x1)は平均がミュー1で分散シグマ二乗1のガウスの確率分布に従うとする。",
    "output": "What we're going to do, is assume that the feature, X1, is distributed according to a Gaussian distribution, with some mean, which you want to write as mu1 and some variance, which I'm going to write as sigma squared 1, and so p of X1 is going to be a Gaussian probability distribution, with mean mu1 and variance sigma squared 1."
  },
  {
    "index": "F17584",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様にx2はガウス分布に従って分布していると、、、ところで、この小さなチルダ記号は(右辺に従い)分布している、という事を意味する。で、平均がミュー2で分散がシグマ二乗の2のガウス分布に従っている、つまり異なるガウス分布に従って分布している、と想定する。",
    "output": "And similarly I'm going to assume that X2 is distributed, Gaussian, that's what this little tilda stands for, that means distributed Gaussian with mean mu2 and Sigma squared 2, so it's distributed according to a different Gaussian, which has a different set of parameters, mu2 sigma square 2."
  },
  {
    "index": "F17585",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に、x3もまた別のガウス分布で、これもまた他のフィーチャーとは異なる平均と標準偏差を持ちえる。などなどと、xnまで続く。",
    "output": "And similarly, you know, X3 is yet another Gaussian, so this can have a different mean and a different standard deviation than the other features, and so on, up to XN."
  },
  {
    "index": "F17586",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が私のモデルとなる。",
    "output": "And so that's my model."
  },
  {
    "index": "F17587",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたたちの中のうち、統計のスペシャリストの人向けの余談ですが、私の書き下した方程式の実際にはフィーチャーの値、x1からxnまでの値が独立である事を仮定している。",
    "output": "Just as a side comment for those of you that are experts in statistics, it turns out that this equation that I just wrote out actually corresponds to an independence assumption on the values of the features x1 through xn."
  },
  {
    "index": "F17588",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが実際には、ここに書いたアルゴリズム片は、これらのフィーチャーが独立に近いかどうかに関わらず機能する、実は独立の仮定が成り立たない時ですらこのアルゴリズムはうまく機能する。",
    "output": "But in practice it turns out that the algorithm of this fragment, it works just fine, whether or not these features are anywhere close to independent and even if independence assumption doesn't hold true this algorithm works just fine."
  },
  {
    "index": "F17589",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だけどもしあなたが、たった今、私が使った独立の仮定だとかの用語の意味が分からなければ気にしないでよろしい。",
    "output": "But in case you don't know those terms I just used independence assumptions and so on, don't worry about it."
  },
  {
    "index": "F17590",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのうち分かるようになるだろうし、このアルゴリズムは正しく実装出来るだろうから。さきのコメントは単に統計の専門家向けのコメントに過ぎない。",
    "output": "You'll be able to understand it and implement this algorithm just fine and that comment was really meant only for the experts in statistics."
  },
  {
    "index": "F17591",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、これのまとめとして、この式をもうちょっとだけコンパクトに書く。",
    "output": "Finally, in order to wrap this up, let me take this expression and write it a little bit more compactly."
  },
  {
    "index": "F17592",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを、以下のようにjが1からnまでの以下のpの積として書くことにする。pのxjで、xjはミューjとシグマ二乗jでパラメトライズされている。",
    "output": "So, we're going to write this is a product from J equals one through N, of P of XJ parameterized by mu j comma sigma squared j."
  },
  {
    "index": "F17593",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのファニーな記号、大文字のギリシャ文字のアルファベット、パイだが、そのファニーな記号は、値の集合に対して積をとる事に対応する。",
    "output": "So this funny symbol here, there is capital Greek alphabet pi, that funny symbol there corresponds to taking the product of a set of values."
  },
  {
    "index": "F17594",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、和の記号に慣れているなら、iを1からnまでのiの和を取ると、この意味は1+2+3+...とnまでの和という意味。",
    "output": "And so, you're familiar with the summation notation, so the sum from i equals one through n, of i. This means 1 + 2 + 3 plus dot dot dot, up to n."
  },
  {
    "index": "F17595",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方このファニーな記号は、この掛け算記号は、iが1からnまでのiの掛け算。",
    "output": "Where as this funny symbol here, this product symbol, right product from i equals 1 through n of i."
  },
  {
    "index": "F17596",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これの意味する所は和の記号とほとんど同じだけど、今回は足す代わりに掛け算する、という意味。",
    "output": "Then this means that, it's just like summation except that we're now multiplying."
  },
  {
    "index": "F17597",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、この積の記法を使うと、このjが1からnまでこの式を掛け合わせる、というのを使うともっとコンパクトになる。",
    "output": "And so using this product notation, this product from j equals 1 through n of this expression."
  },
  {
    "index": "F17598",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの項全部を書く、より短い方法となるワケ。",
    "output": "It's just more compact, it's just shorter way for writing out this product of of all of these terms up there."
  },
  {
    "index": "F17599",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だってこれらのpの、ミューjとシグマjが与えられた時のxjの値をとって、それらを掛け合わせているのだから。",
    "output": "Since we're are taking these p of x j given mu j comma sigma squared j terms and multiplying them together."
  },
  {
    "index": "F17600",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、このpのxという分布を推計する問題は密度推計の問題、と呼ばれる事がある。",
    "output": "And, by the way the problem of estimating this distribution p of x, they're sometimes called the problem of density estimation."
  },
  {
    "index": "F17601",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがこのスライドのタイトルでもある。",
    "output": "Hence the title of the slide."
  },
  {
    "index": "F17602",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、全部合わせると、我らのアノマリー検出のアルゴリズムはこうなる。",
    "output": "So putting everything together, here is our anomaly detection algorithm."
  },
  {
    "index": "F17603",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初のステップはフィーチャーを選ぶ事、または見つけ出すことだ、アノマリーであるサンプルを示してくれると思われるようなフィーチャーxiを。",
    "output": "The first step is to choose features, or come up with features xi that we think might be indicative of anomalous examples."
  },
  {
    "index": "F17604",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、どういう意味かというと、以下のようなフィーチャーを探し出すという事です:あなたのシステムに詐欺行為を働いているかもしれない普通でないユーザーがいた時に、または航空機エンジンの例では、何かおかしな事、何か奇妙な事が航空機エンジンに起こっている時には普通でないほど大きな値、または普通でないほど小さな値をとる、とあなたが思うようなフィーチャーxiを選ぶ、アノマリーのサンプルがどんな感じかを知る為に。",
    "output": "So what I mean by that, is, try to come up with features, so that when there's an unusual user in your system that may be doing fraudulent things, or when the aircraft engine examples, you know there's something funny, something strange about one of the aircraft engines."
  },
  {
    "index": "F17605",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "またはより一般的に、集めているデータの対象の一般的な性質を良くとらえているフィーチャーを集めても良い。",
    "output": "Choose features X I, that you think might take on unusually large values, or unusually small values, for what an anomalous example might look like."
  },
  {
    "index": "F17606",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの式は前回のビデオでやった式と似ている。これらのパラメータの推計にこの式を使っていく。",
    "output": "But more generally, just try to choose features that describe general properties of the things that you're collecting data on."
  },
  {
    "index": "F17607",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いくつか解釈を与えておくと、ミューj、これはフィーチャーjの平均だ。",
    "output": "Next, given a training set, of M, unlabled examples, X1 through X M, we then fit the parameters, mu 1 through mu n, and sigma squared 1 through sigma squared n, and so these were the formulas similar to the formulas we have in the previous video, that we're going to use the estimate each of these parameters, and just to give some interpretation, mu J, that's my average value of the j feature."
  },
  {
    "index": "F17608",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのミューjはpのxjの式にパラメータとして入る、それはミューjとシグマ二乗jでパラメトライズされていたのだった。",
    "output": "Mu j goes in this term p of xj. which is parametrized by mu J and sigma squared J."
  },
  {
    "index": "F17609",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれの言ってる事はミューjは単にトレーニングセットに渡ってフィーチャーjの平均を取った物、という事。",
    "output": "And so this says for the mu J just take the mean over my training set of the values of the j feature."
  },
  {
    "index": "F17610",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして補足しておくと、あなたは、これらの式をjが1からnまでに渡って計算することになる。",
    "output": "And, just to mention, that you do this, you compute these formulas for j equals one through n."
  },
  {
    "index": "F17611",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらの式を使ってミュー1、ミュー2、とミューnまで、推計する。シグマ二乗も同様だ。",
    "output": "So use these formulas to estimate mu 1, to estimate mu 2, and so on up to mu n, and similarly for sigma squared, and it's also possible to come up with vectorized versions of these."
  },
  {
    "index": "F17612",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ミューをベクトルとして考えると、つまりミュー1があって、ミュー2があって、、、とミューnまで。するとベクトル化したバージョンのパラメータのセットは1からnまでのxiの和と書ける。",
    "output": "So if you think of mu as a vector, so mu if is a vector there's mu 1, mu 2, down to mu n, then a vectorized version of that set of parameters can be written like so sum from 1 equals one through n xi."
  },
  {
    "index": "F17613",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり今書いたこの式でこのxiをフィーチャーベクトルとしてn個全てのミューの値を同時に推計する。",
    "output": "So, this formula that I just wrote out estimates this xi as the feature vectors that estimates mu for all the values of n simultaneously."
  },
  {
    "index": "F17614",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてまた、シグマ二乗jの推計についてもベクトル化した式を作れる。",
    "output": "And it's also possible to come up with a vectorized formula for estimating sigma squared j."
  },
  {
    "index": "F17615",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、新しいサンプルを与えられたら、つまり、新しい航空エンジンが来て、この航空機エンジンがアノマリーなのかどうかを知りたいとすると、やらなくてはならない事はp(x)を、この新しいエンジンの確率を計算する事だ。",
    "output": "Finally, when you're given a new example, so when you have a new aircraft engine and you want to know is this aircraft engine anomalous."
  },
  {
    "index": "F17616",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、p(x)はこの積と等しくて、実装としては、計算するのは、この式で、ここにある、これはガウス確率の式だ。",
    "output": "What we need to do is then compute p of x, what's the probability of this new example?"
  },
  {
    "index": "F17617",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれを計算し、最終的に、この確率がとても小さければ、これをアノマリーとフラグづけする。",
    "output": "So, p of x is equal to this product, and what you implement, what you compute, is this formula and where over here, this thing here this is just the formula for the Gaussian probability, so you compute this thing, and finally if this probability is very small, then you flag this thing as an anomaly."
  },
  {
    "index": "F17618",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはこの手法の適用例だ。",
    "output": "Here's an example of an application of this method."
  },
  {
    "index": "F17619",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このスライドの左上にプロットしたようなデータがあるとしよう。",
    "output": "Let's say we have this data set plotted on the upper left of this slide."
  },
  {
    "index": "F17620",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこの、フィーチャーx1を見てみると、もしこのデータセットを見てみると、見た感じだいたいフィーチャーx1の平均は5のあたりで、標準偏差はこのデータセットのx1の値だけを見ると、標準偏差はだいたい2くらい。",
    "output": "if you look at this, well, lets look the feature of x1. If you look at this data set, it looks like on average, the features x1 has a mean of about 5 and the standard deviation, if you only look at just the x1 values of this data set has the standard deviation of maybe 2."
  },
  {
    "index": "F17621",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてフィーチャーx2の値を見ると、それは縦軸で測れてその平均は見たところだいたい3くらいで、標準偏差はだいたい1。",
    "output": "So that sigma 1 and looks like x2 the values of the features as measured on the vertical axis, looks like it has an average value of about 3, and a standard deviation of about 1."
  },
  {
    "index": "F17622",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこのデータセットに対してミュー1、ミュー2と、シグマ1、シグマ2を推計すると、これが得られる物だ。",
    "output": "So if you take this data set and if you estimate mu1, mu2, sigma1, sigma2, this is what you get."
  },
  {
    "index": "F17623",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "標準偏差について考えているけれど、前のスライドの式はこれらの二乗の推計を与える物だった。つまりシグマ二乗の1、シグマ二乗の2。",
    "output": "And again, I'm writing sigma here, I'm think about standard deviations, but the formula on the previous 5 actually gave the estimates of the squares of theses things, so sigma squared 1 and sigma squared 2."
  },
  {
    "index": "F17624",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、シグマ1とシグマ2を使ってるのかそれともシグマ二乗1とシグマ二乗2を使ってるのかに、注意しなさい。",
    "output": "So, just be careful whether you are using sigma 1, sigma 2, or sigma squared 1 or sigma squared 2."
  },
  {
    "index": "F17625",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてシグマ二乗1はもちろん、イコール4となる。",
    "output": "So, sigma squared 1 of course would be equal to 4, for example, as the square of 2."
  },
  {
    "index": "F17626",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "図では、ミュー1とシグマ二乗1でパラメトライズされているp(x1)と、ミュー2とシグマ二乗2でパラメトライズされたp(x2)はここにあるこれら2つの分布のような見た目となるだろう。",
    "output": "And in pictures what p of x1 parametrized by mu1 and sigma squared 1 and p of x2, parametrized by mu 2 and sigma squared 2, that would look like these two distributions over here."
  },
  {
    "index": "F17627",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして結局、p(x)をプロットするとそれはこれら2つの積だったのだから実際にはこんな感じの平面プロットが得られる。",
    "output": "And, turns out that if were to plot of p of x, right, which is the product of these two things, you can actually get a surface plot that looks like this."
  },
  {
    "index": "F17628",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはp(x)のプロットでその高さ、この上の高さ、平面上のある特定の点の高さは、つまりあるx1、x2が与えられた時にx2の値が2でx1の値が2なら、この点だ。",
    "output": "This is a plot of p of x, where the height above of this, where the height of this surface at a particular point, so given a particular x1 x2 values of x2 if x1 equals 2, x equal 2, that's this point."
  },
  {
    "index": "F17629",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの三次元平面の高さがそれこそがp(x)だ。",
    "output": "And the height of this 3-D surface here, that's p of x."
  },
  {
    "index": "F17630",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりp(x)、それがこのプロットの高さだ。それは文字通りミュー1とシグマ二乗1でパラメトライズされたp(x1)掛けることのミュー2とシグマ二乗2でパラメトライズされたp(x2)である。",
    "output": "So p of x, that is the height of this plot, is literally just p of x1 parametrized by mu 1 sigma squared 1, times p of x2 parametrized by mu 2 sigma squared 2."
  },
  {
    "index": "F17631",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これがパラメータをこのデータにフィットさせる方法だ。",
    "output": "Now, so this is how we fit the parameters to this data."
  },
  {
    "index": "F17632",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "新しいサンプルが幾つか来た場合を考えてみよう。",
    "output": "Let's see if we have a couple of new examples."
  },
  {
    "index": "F17633",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "新しいサンプルはここかもしれない。",
    "output": "Maybe I have a new example there."
  },
  {
    "index": "F17634",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはアノマリーか?",
    "output": "Is this an anomaly or not?"
  },
  {
    "index": "F17635",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはアノマリーか?",
    "output": "So, is that an anomaly or not?"
  },
  {
    "index": "F17636",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうでないか?それを区別する方法はある値エプシロンをセットする、ここではエプシロンを0.02としたとしよう。",
    "output": "They way we do that is, we would set some value for Epsilon, let's say I've chosen Epsilon equals 0.02."
  },
  {
    "index": "F17637",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうやってエプシロンを選ぶかは後で話す。",
    "output": "I'll say later how we choose Epsilon."
  },
  {
    "index": "F17638",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まずは最初のサンプルを取ってみよう。このサンプルをx1testと呼ぶ事にする。",
    "output": "But let's take this first example, let me call this example X1 test."
  },
  {
    "index": "F17639",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして二番目のサンプルをx2testと呼ぶことにする。",
    "output": "And let me call the second example X2 test."
  },
  {
    "index": "F17640",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その為に、この式を使ってそれを計算する。これは見たところかなり大きな値に見える。",
    "output": "What we do is, we then compute p of X1 test, so we use this formula to compute it and this looks like a pretty large value."
  },
  {
    "index": "F17641",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、これはエプシロン以上となっている。",
    "output": "In particular, this is greater than, or greater than or equal to epsilon."
  },
  {
    "index": "F17642",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれは、きわめて大きい確率で少なくともエプシロンより大きいとは言える。だからx1testはアノマリーでは無い、と言って良かろう。",
    "output": "And so this is a pretty high probability at least bigger than epsilon, so we'll say that X1 test is not an anomaly."
  },
  {
    "index": "F17643",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方p(x2test)を計算してみると、これはもっとずっと小さい値となる。",
    "output": "Whereas, if you compute p of X2 test, well that is just a much smaller value."
  },
  {
    "index": "F17644",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、これは確かにアノマリーだと言える。何故ならそれは、我らが選んだエプシロンより小さいから。",
    "output": "So this is less than epsilon and so we'll say that that is indeed an anomaly, because it is much smaller than that epsilon that we then chose."
  },
  {
    "index": "F17645",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際、ここで言った事は、これの本当に意味している事は、三次元での表面プロットを見ると、それの言わんとしている事は、その点の上の表面までの高さが高い点となるx1,x2は、全て非アノマリーのサンプルに、つまりOKというかノーマルなサンプルに対応しているという事。",
    "output": "What this is really saying is that, you look through the 3d surface plot."
  },
  {
    "index": "F17646",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、遥か離れたこの辺の点は全て遥か離れたこの辺の点は全て、これらの点は全てとても低い確率となっている。つまり、我らとしては、これらの点をアノマリーとフラグづけする事になる。",
    "output": "It's saying that all the values of x1 and x2 that have a high height above the surface, corresponds to an a non-anomalous example of an OK or normal example."
  },
  {
    "index": "F17647",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりある領域をそれはこんな感じとなるだろうが、その外の全ての点をアノマリーとフラグづけする事となる。一方で、このエプシロンの内部にある物たちは、ここに書いたように、これはOK、または非アノマリーなサンプルとみなす。",
    "output": "Whereas all the points far out here, all the points out here, all of those points have very low probability, so we are going to flag those points as anomalous, and so it's gonna define some region, that maybe looks like this, so that everything outside this, it flags as anomalous, whereas the things inside this ellipse I just drew, if it considers okay, or non-anomalous, not anomalous examples."
  },
  {
    "index": "F17648",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのサンプルx2testはその領域の外に位置しているのでつまりそれはとても低い確率なので、それはアノマリーのサンプルとみなす訳だ。",
    "output": "And so this example x2 test lies outside that region, and so it has very small probability, and so we consider it an anomalous example."
  },
  {
    "index": "F17649",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、どうやってp(x)を推計するかを議論してきた。xとなる確率を。",
    "output": "In this video we talked about how to estimate p of x, the probability of x, for the purpose of developing an anomaly detection algorithm."
  },
  {
    "index": "F17650",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのビデオではまた、所与のデータセットに対してパラメータをフィッティングする、パラメータを推計する全体の手順も見ていった。",
    "output": "And in this video, we also stepped through an entire process of giving data set, we have, fitting the parameters, doing parameter estimations."
  },
  {
    "index": "F17651",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "パラメータであるミューとシグマを取得して、次に新しいサンプルに対してそれがアノマリーかどうかを判断した。",
    "output": "We get mu and sigma parameters, and then taking new examples and deciding if the new examples are anomalous or not."
  },
  {
    "index": "F17652",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、このアルゴリズムに対してより深く見ていく。そしてこれを実際にうまく運用する為にちょっとした補足の話もしていく。",
    "output": "In the next few videos we will delve deeper into this algorithm, and talk a bit more about how to actually get this to work well."
  },
  {
    "index": "F17653",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではアノマリー検出のアルゴリズムを開発した。",
    "output": "In the last video, we developed an anomaly detection algorithm."
  },
  {
    "index": "F17654",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、具体的な問題に対してどのようにアノマリー検出を適用するか、そのプロセスを議論していく。特に、今回はアノマリー検出アルゴリズムの評価をどう行うかについてフォーカスしていきたい。",
    "output": "In this video, I like to talk about the process of how to go about developing a specific application of anomaly detection to a problem and in particular this will focus on the problem of how to evaluate an anomaly detection algorithm."
  },
  {
    "index": "F17655",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオでは、実数による評価の重要性について話してきた。これは、特定の応用に関する学習アルゴリズムを開発しようとしている時に、通常たくさんの選択を行う必要がある、たとえばどのフィーチャーを使うか、などを選択する必要がある、という発想を捉えている。",
    "output": "In previous videos, we've already talked about the importance of real number evaluation and this captures the idea that when you're trying to develop a learning algorithm for a specific application, you need to often make a lot of choices like, you know, choosing what features to use and then so on."
  },
  {
    "index": "F17656",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの選択の全てについて、決断を下すには、あなたの学習アルゴリズムを評価する数字が得られる方が、より簡単になる事がしばしばある。",
    "output": "And making decisions about all of these choices is often much easier, and if you have a way to evaluate your learning algorithm that just gives you back a number."
  },
  {
    "index": "F17657",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たとえば、もう一つ追加のフィーチャーを加えるかについて、何か追加のフィーチャーについて心当たりがある時、もしそのフィーチャーを加えてアルゴリズムを走らせて、さらにフィーチャー無しでアルゴリズムを走らせてそこから数字を得たら、それがこのフィーチャーを加えた事でパフォーマンスが改善したか悪化したかを教えてくれる。",
    "output": "If you can run the algorithm with the feature, and run the algorithm without the feature, and just get back a number that tells you, you know, did it improve or worsen performance to add this feature?"
  },
  {
    "index": "F17658",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはつまり、そのフィーチャーを含めるべきかどうかを決定する為のより良い、よりシンプルな方法を提供してくれる。",
    "output": "Then it gives you a much better way, a much simpler way, with which to decide whether or not to include that feature."
  },
  {
    "index": "F17659",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、アノマリー検出のシステムを手早く開発する為には、そのアノマリー検出のシステムを評価する方法がある事は、とても有用だ。",
    "output": "So in order to be able to develop an anomaly detection system quickly, it would be a really helpful to have a way of evaluating an anomaly detection system."
  },
  {
    "index": "F17660",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを行う為に、アノマリー検出のシステムを評価する為に、実際にはあるラベル付けされたデータを仮定する。",
    "output": "In order to do this, in order to evaluate an anomaly detection system, we're actually going to assume have some labeled data."
  },
  {
    "index": "F17661",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでの所、アノマリー検出を教師なし学習として扱ってきた、ラベル付けされていないデータを扱って。",
    "output": "So, so far, we'll be treating anomaly detection as an unsupervised learning problem, using unlabeled data."
  },
  {
    "index": "F17662",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも、もしどれがアノマリーのサンプルかを示すラベルのついたデータがあれば、そしてどれが非アノマリーのサンプルかを示すデータがあれば、これは普通の、アノマリー検出のアルゴリズムを評価する方法と考える事が出来る。",
    "output": "But if you have some labeled data that specifies what are some anomalous examples, and what are some non-anomalous examples, then this is how we actually think of as the standard way of evaluating an anomaly detection algorithm."
  },
  {
    "index": "F17663",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "航空機野エンジンの例をふたたび考えよう。",
    "output": "So taking the aircraft engine example again."
  },
  {
    "index": "F17664",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ラベル付けされたデータで、そのうちのちょっとだけがアノマリーの航空機エンジンのサンプル、つまり過去に製造されていてアノマリーだと、欠陥品か、とにかく何らかの意味で異常な物だと判明しているとする。",
    "output": "Let's say that, you know, we have some label data of just a few anomalous examples of some aircraft engines that were manufactured in the past that turns out to be anomalous."
  },
  {
    "index": "F17665",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また、非アノマリーの完璧にオーケーなサンプルも幾つかあるとしよう。",
    "output": "Let's say we use we also have some non-anomalous examples, so some perfectly okay examples."
  },
  {
    "index": "F17666",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "y=0をノーマル、または非アノマリーのサンプルを表すのに使い、そしてy=1を、アノマリーのサンプルを表すのに使う。",
    "output": "I'm going to use y equals 0 to denote the normal or the non-anomalous example and y equals 1 to denote the anomalous examples."
  },
  {
    "index": "F17667",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アノマリー検出のアルゴリズムを開発し、評価していくプロセスは以下のようになる。",
    "output": "The process of developing and evaluating an anomaly detection algorithm is as follows."
  },
  {
    "index": "F17668",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まずはトレーニングセットについて、ところでクロスバリデーションセットとテストセットについては後で話すが、まずはトレーニングセットについて。これは普通、ラベルづけされてないトレーニングセットと考える。",
    "output": "We're going to think of it as a training set and talk about the cross validation in test sets later, but the training set we usually think of this as still the unlabeled training set."
  },
  {
    "index": "F17669",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは普通の、アノマリーで無いデータが大量に集まっている、と考える。",
    "output": "And so this is our large collection of normal, non-anomalous or not anomalous examples."
  },
  {
    "index": "F17670",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "普通はこれを非アノマリーと考えるが、だが実際はちょっとアノマリーなのが紛れ込むくらいはオーケーだ。それがラベル無しトレーニングセットに入っちゃってても。",
    "output": "And usually we think of this as being as non-anomalous, but it's actually okay even if a few anomalies slip into your unlabeled training set."
  },
  {
    "index": "F17671",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、クロスバリデーションセットとテストセットを定義する。それで個々のアノマリー検出アルゴリズムの評価を行う。",
    "output": "And next we are going to define a cross validation set and a test set, with which to evaluate a particular anomaly detection algorithm."
  },
  {
    "index": "F17672",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、具体的には、クロスバリデーションとテストセットではどちらも、アノマリーだと分かっているサンプルが含まれているという前提だ。",
    "output": "So, specifically, for both the cross validation test sets we're going to assume that, you know, we can include a few examples in the cross validation set and the test set that contain examples that are known to be anomalous."
  },
  {
    "index": "F17673",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりテストセットにはy=1となるサンプルが幾つか含まれている。それはアノマリーな航空機エンジンに対応している。",
    "output": "So the test sets say we have a few examples with y equals 1 that correspond to anomalous aircraft engines."
  },
  {
    "index": "F17674",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的にはこうだ。",
    "output": "So here's a specific example."
  },
  {
    "index": "F17675",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらを合わせて、我らの持ってるデータだとしよう。",
    "output": "Let's say that, altogether, this is the data that we have."
  },
  {
    "index": "F17676",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1万機の、分かってる範囲では完全に正常な航空機エンジンを製造したとしよう。",
    "output": "We have manufactured 10,000 examples of engines that, as far as we know we're perfectly normal, perfectly good aircraft engines."
  },
  {
    "index": "F17677",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここでも、ちょっとの欠陥品がこの1万の中に紛れ込んでたとしても、実際はオーケーだ。だがこれらの中の大多数は良い、普通の、アノマリーでないエンジンである、と仮定する。",
    "output": "And again, it turns out to be okay even if a few flawed engine slips into the set of 10,000 is actually okay, but we kind of assumed that the vast majority of these 10,000 examples are, you know, good and normal non-anomalous engines."
  },
  {
    "index": "F17678",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれまでに長い期間工場を運営してきて、フィーチャーを計測していてだいたい20機の欠陥エンジン、アノマリーのエンジンを得ているとする。",
    "output": "And let's say that, you know, historically, however long we've been running on manufacturing plant, let's say that we end up getting features, getting 24 to 28 anomalous engines as well."
  },
  {
    "index": "F17679",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところでアノマリー検出のきわめて典型的な適用ケースでは、アノマリーのサンプルの数は、つまりy=1となるサンプルの数はだいたい20から50ってあたりだ。",
    "output": "And for a pretty typical application of anomaly detection, you know, the number non-anomalous examples, that is with y equals 1, we may have anywhere from, you know, 20 to 50."
  },
  {
    "index": "F17680",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この辺が典型的なy=1となる数の範囲。",
    "output": "It would be a pretty typical range of examples, number of examples that we have with y equals 1."
  },
  {
    "index": "F17681",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして通常は、もっとずっと多くの正常なサンプルがある。",
    "output": "And usually we will have a much larger number of good examples."
  },
  {
    "index": "F17682",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからデータセットが与えられた時の極めて標準的なトレーニングセット、クロスバリデーションセット、テストセットの分割方法は以下のようになる。",
    "output": "So, given this data set, a fairly typical way to split it into the training set, cross validation set and test set would be as follows."
  },
  {
    "index": "F17683",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1万の良品の航空機エンジンから6000をラベル無しのトレーニングセットとして取り分ける。",
    "output": "Let's take 10,000 good aircraft engines and put 6,000 of that into the unlabeled training set."
  },
  {
    "index": "F17684",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをラベル無しと言ったが、これらは全てy=0に対応したサンプルだ、我らの知る限りは。",
    "output": "So, I'm calling this an unlabeled training set but all of these examples are really ones that correspond to y equals 0, as far as we know."
  },
  {
    "index": "F17685",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれをp(x)をフィットするのに使う。",
    "output": "And so, we will use this to fit p of x, right."
  },
  {
    "index": "F17686",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり我らはこれら6000のエンジンをp(x)をフィットするのに使う、ここでp(x1)はミュー1とシグマ二乗1でパラメトライズされていて、これはp(xn)がミューnとシグマ二乗nでパラメトライズされている所まで続く。",
    "output": "So, we will use these 6000 engines to fit p of x, which is that p of x one parametrized by Mu 1, sigma squared 1, up to p of Xn parametrized by Mu N sigma squared n."
  },
  {
    "index": "F17687",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれら6000の手本を使ってパラメータのミュー1、シグマ二乗1からミューn、シグマ二乗nまでを推計する。",
    "output": "And so it would be these 6,000 examples that we would use to estimate the parameters Mu 1, sigma squared 1, up to Mu N, sigma squared N."
  },
  {
    "index": "F17688",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が我らのトレーニングセットでそれらは全て正常な、または少なくとも大部分が正常なサンプルだ。",
    "output": "And so that's our training set of all, you know, good, or the vast majority of good examples."
  },
  {
    "index": "F17689",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、正常な航空機エンジンの中からいくらかをクロスバリデーションセットに、さらにまた幾らかをテストセットに入れます。",
    "output": "Next we will take our good aircraft engines and put some number of them in a cross validation set plus some number of them in the test sets."
  },
  {
    "index": "F17690",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、20機の不良エンジンがあるとして、それを、例えば10個ずつに分けて、クロスバリデーションセットとテストセットにそれぞれ入れます。",
    "output": "And then we also have 20 flawed aircraft engines, and we'll take that and maybe split it up, you know, put ten of them in the cross validation set and put ten of them in the test sets."
  },
  {
    "index": "F17691",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次のスライドでこれらを実際に、どのように使ってアノマリー検出のアルゴリズムを評価するかを議論していく。",
    "output": "And in the next slide we will talk about how to actually use this to evaluate the anomaly detection algorithm."
  },
  {
    "index": "F17692",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでで私が書いてきた事は、ラベル付けされたデータとラベル付けされていないデータの分割する望ましい方法だ。",
    "output": "So what I have just described here is a you know probably the recommend a good way of splitting the labeled and unlabeled example."
  },
  {
    "index": "F17693",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "航空機エンジンのうち、良品と欠陥品の。",
    "output": "The good and the flawed aircraft engines."
  },
  {
    "index": "F17694",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは良品に関しては60、20,20%にそれぞれ分割し、欠陥品に関してはクロスバリデーションセットとテストセットだけに入れた。次のスライドでどうしてそうしたかが分かる。",
    "output": "Where we use like a 60, 20, 20% split for the good engines and we take the flawed engines, and we put them just in the cross validation set, and just in the test set, then we'll see in the next slide why that's the case."
  },
  {
    "index": "F17695",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "補足になるが、たまに世の中の人々がアノマリー検出のアルゴリズムを適用するやり方として、たまにデータを異なったやり方で分割してるのを見かける事もあるかもしれない。",
    "output": "Just as an aside, if you look at how people apply anomaly detection algorithms, sometimes you see other peoples' split the data differently as well."
  },
  {
    "index": "F17696",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはあまりオススメしない代替案だが代替案としては、ある人々は1万個の良品を6000個をトレーニングセットに、残りの4000をクロスバリデーションセットとテストセットの両方に入れる、という事をやるかもしれない。",
    "output": "So, another alternative, this is really not a recommended alternative, but some people want to take off your 10,000 good engines, maybe put 6000 of them in your training set and then put the same 4000 in the cross validation set and the test set."
  },
  {
    "index": "F17697",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもご存知の通り、クロスバリデーションセットとテストセットを完全に異なるデータと我々は思いたいのだった。",
    "output": "And so, you know, we like to think of the cross validation set and the test set as being completely different data sets to each other."
  },
  {
    "index": "F17698",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがアノマリー検出では、たまにテストセットとクロスバリデーションセットの両方に同じ良品のエンジンを使うのを見かける場合があり、時には欠陥品の方のエンジンまで全く同じセットをクロスバリデーションセットとテストセットに使ってるのを見る事すらある。",
    "output": "But you know, in anomaly detection, you know, for sometimes you see people, sort of, use the same set of good engines in the cross validation sets, and the test sets, and sometimes you see people use exactly the same sets of anomalous engines in the cross validation set and the test set."
  },
  {
    "index": "F17699",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらは皆、より悪いやり方だとみなす事が出来るので、決してオススメしない。",
    "output": "And so, all of these are considered, you know, less good practices and definitely less recommended."
  },
  {
    "index": "F17700",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "確実に、クロスバリデーションセットとテストセットで同じデータを使うのは機械学習における良い習慣では無い。",
    "output": "Certainly using the same data in the cross validation set and the test set, that is not considered a good machine learning practice."
  },
  {
    "index": "F17701",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもたまにこれをやってる人を見かけるだろう。",
    "output": "But, sometimes you see people do this too."
  },
  {
    "index": "F17702",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、トレーニング、クロスバリデーション、そしてテストセットが与えられたとして、これが評価方法、、、いや、これがアルゴリズムの開発方法と評価方法だ。",
    "output": "So, given the training cross validation and test sets, here's how you evaluate or here is how you develop and evaluate an algorithm."
  },
  {
    "index": "F17703",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず、トレーニングセットに対し、モデルp(x)をフィッティングする。",
    "output": "First, we take the training sets and we fit the model p of x."
  },
  {
    "index": "F17704",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "m個のラベル無し航空機エンジンのサンプルを。ところでこれらをラベル無しサンプルと呼んだが、実のところこれらは、良品、正常な航空機エンジンであると想定している。",
    "output": "So, we fit, you know, all these Gaussians to my m unlabeled examples of aircraft engines, and these, I am calling them unlabeled examples, but these are really examples that we're assuming our goods are the normal aircraft engines."
  },
  {
    "index": "F17705",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、アノマリー検出のアルゴリズムが実際に予測をしているのを想像してみよう。",
    "output": "Then imagine that your anomaly detection algorithm is actually making prediction."
  },
  {
    "index": "F17706",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりクロスバリデーションとテストのセットに対し、サンプルxを検定する。p(x)がエプシロンより小さければ、アルゴリズムはy=1を予測したと考え、もしp(x)がエプシロン以上ならy=0に違いない、と考える。",
    "output": "So, on the cross validation of the test set, given that, say, test example X, think of the algorithm as predicting that y is equal to 1, p of x is less than epsilon, we must be taking zero, if p of x is greater than or equal to epsilon."
  },
  {
    "index": "F17707",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりxが与えられた時に、ラベルがどちらかを予測しようとする。y=1はアノマリーに対応し、y=0は通常のサンプルに対応する。",
    "output": "So, given x, it's trying to predict, what is the label, given y equals 1 corresponding to an anomaly or is it y equals 0 corresponding to a normal example?"
  },
  {
    "index": "F17708",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではトレーニング、クロスバリデーション、テストセットがそれぞれ与えられたとして、どうアルゴリズムを開発していくか?",
    "output": "So given the training, cross validation, and test sets."
  },
  {
    "index": "F17709",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より詳細に言うと、アノマリー検出のアルゴリズムをどうやって評価するか?",
    "output": "And more specifically, how do you evaluate an anomaly detection algorithm?"
  },
  {
    "index": "F17710",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらを元に、最初のステップはラベル無しのトレーニングセットに対しモデルp(x)をフィッティングする。",
    "output": "Well, to this whole, the first step is to take the unlabeled training set, and to fit the model p of x lead training data."
  },
  {
    "index": "F17711",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれを取るのだが、これはラベル無しトレーニングセットと言っているが、実際は、それらの大多数は通常の航空機エンジンだと想定している物だった。それらはアノマリーでは無さそうだからそれにモデルp(x)をフィッティングする訳だ。",
    "output": "So you take this, you know on I'm coming, unlabeled training set, but really, these are examples that we are assuming, vast majority of which are normal aircraft engines, not because they're not anomalies and it will fit the model p of x."
  },
  {
    "index": "F17712",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このパラメータ全部で、、、これらガウス分布のデータ全てでフィッティングする。",
    "output": "It will fit all those parameters for all the Gaussians on this data."
  },
  {
    "index": "F17713",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にクロスバリデーションとテストセットに対して、アノマリー検出のアルゴリズムを、yの値を予測する物とみなす。",
    "output": "Next on the cross validation of the test set, we're going to think of the anomaly detention algorithm as trying to predict the value of y."
  },
  {
    "index": "F17714",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり各テストのサンプルに対して、これらxitestとyitestがある訳だが、ここでyは、これがアノマリーかどうかに応じて1か0のどちらかの値を取る。",
    "output": "So in each of like say test examples."
  },
  {
    "index": "F17715",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "テストセットから入力xが与えられた時、このアノマリー検出のアルゴリズムをp(x)がイプシロンより小さかったらyが1と予測している、とみなす。",
    "output": "So given input x in my test set, my anomaly detection algorithm think of it as predicting the y as 1 if p of x is less than epsilon."
  },
  {
    "index": "F17716",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれがアノマリーと予測するという事は、そんなサンプルとなる確率がとても低いという事だ。",
    "output": "So predicting that it is an anomaly, it is probably is very low."
  },
  {
    "index": "F17717",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてアルゴリズムは、p(x)がイプシロンより大きいか等しい場合にy=0を予測しているとみなす。",
    "output": "And we think of the algorithm is predicting that y is equal to 0. If p of x is greater then or equals epsilon."
  },
  {
    "index": "F17718",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりp(x)が普通の感覚で十分に大きければそれらを正常なサンプルとみなす。",
    "output": "So predicting those normal example if the p of x is reasonably large."
  },
  {
    "index": "F17719",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これでアノマリー検出のアルゴリズムをこれらのテストセット、クロスバリデーションセットのyラベルの値を予測するものとみなす事が出来た訳だ。",
    "output": "And so we can now think of the anomaly detection algorithm as making predictions for what are the values of these y labels in the test sets or on the cross validation set."
  },
  {
    "index": "F17720",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはようするに、教師有り学習に似たようなセッティングになる、でしょ?",
    "output": "And this puts us somewhat more similar to the supervised learning setting, right?"
  },
  {
    "index": "F17721",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこではラベル付きのテストセットがあって、我らのアルゴリズムはこれらのラベルに対して予測を行う。つまりこれらのラベルをどれだけ当てられるかで、それを評価する訳だ。",
    "output": "Where we have label test set and our algorithm is making predictions on these labels and so we can evaluate it you know by seeing how often it gets these labels right."
  },
  {
    "index": "F17722",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろんこれらのラベルはとても歪んだ割合となっている、何故ならy=0、それは正常なサンプルだが、このケースの方が通常はy=1、つまりアノマリーのサンプルとなるよりもずっと一般的だからだ。",
    "output": "Of course these labels are will be very skewed because y equals zero, that is normal examples, usually be much more common than y equals 1 than anomalous examples."
  },
  {
    "index": "F17723",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがそれも、教師有り学習の評価で使った評価指標と似た事情だ。",
    "output": "But, you know, this is much closer to the source of evaluation metrics we can use in supervised learning."
  },
  {
    "index": "F17724",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "データはとても歪んでいるのだからy=0の方がずっと一般的なのだから、分類の精度はきっと良い指標では無いだろう。",
    "output": "Well, because the data is very skewed, because y equals 0 is much more common, classification accuracy would not be a good the evaluation metrics."
  },
  {
    "index": "F17725",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしとても歪んだデータセットに対するなら、いつもy=0と予測するだけで、とても高い分類精度が得られてしまう。",
    "output": "So, if you have a very skewed data set, then predicting y equals 0 all the time, will have very high classification accuracy."
  },
  {
    "index": "F17726",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その代わりに、以下のような評価指標を使うべきだ:真陽性(truepositive)、偽陽性(falsepositive)、偽陰性(falsenegative)、真陰性(truenagative)の比率の計算結果とか、適合率(precision)と再現率(recall)を計算するとか、F1スコアなど、、、これは適合率と再現率を一つの実数で要約するような物だったが、それを計算するなどする。",
    "output": "Instead, we should use evaluation metrics, like computing the fraction of true positives, false positives, false negatives, true negatives or compute the position of the v curve of this algorithm or do things like compute the f1 score, right, which is a single real number way of summarizing the position and the recall numbers."
  },
  {
    "index": "F17727",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらがクロスバリデーションセットとテストセットでアノマリー検出のアルゴリズムを評価する方法となる。",
    "output": "And so these would be ways to evaluate an anomaly detection algorithm on your cross validation set or on your test set."
  },
  {
    "index": "F17728",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、アノマリー検出のアルゴリズムにはこのイプシロン、というバラメータもあった。",
    "output": "Finally, earlier in the anomaly detection algorithm, we also had this parameter epsilon, right?"
  },
  {
    "index": "F17729",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "イプシロンはある物をアノマリーとフラグ付けするかを決定する閾値となる。",
    "output": "So, epsilon is this threshold that we would use to decide when to flag something as an anomaly."
  },
  {
    "index": "F17730",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからクロスバリデーションセットがある時に、このパラメータイプシロンを選ぶ、もう一つの方法は、たくさんの異なるイプシロンの値を、ほんとにたくさん試して、そして例えばf1スコアを最大化するイプシロンを選ぶという方法が考えられる。そうすれば、クロスバリデーションセットでは良い結果が得られるから。",
    "output": "And so, if you have a cross validation set, another way to and to choose this parameter epsilon, would be to try a different, try many different values of epsilon, and then pick the value of epsilon that, let's say, maximizes f1 score, or that otherwise does well on your cross validation set."
  },
  {
    "index": "F17731",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的には、トレーニング、テスト、そしてクロスバリデーションセットを減らす方法としては、意思決定をしようとする時には、例えばどのフィーチャーを含めるべきかとか、またはパラメータのイプシロンをチューンしたい時には、クロスバリデーションセットに対して継続的にアルゴリズムを評価して、それらの決定は全て、、、例えばどのフィーチャーを含めるかとかイプシロンを幾つに設定するかとか、そういう時はクロスバリデーションセットに対してアルゴリズムを評価し、そしてフィーチャーを選んだり、これでいい!",
    "output": "And more generally, the way to reduce the training, testing, and cross validation sets, is that when we are trying to make decisions, like what features to include, or trying to, you know, tune the parameter epsilon, we would then continually evaluate the algorithm on the cross validation sets and make all those decisions like what features did you use, you know, how to set epsilon, use that, evaluate the algorithm on the cross validation set, and then when we've picked the set of features, when we've found the value of epsilon that we're happy with, we can then take the final model and evaluate it, you know, do the final evaluation of the algorithm on the test sets."
  },
  {
    "index": "F17732",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、アノマリー検出のアルゴリズムの評価をどうやるか、の手順を議論した。ここでも、アルゴリズムを評価する時には、単一の実数による評価、例えばF1スコアみたいな物は、しばしばあなたの時間をより効率的に使わせてくれる、アノマリー検出のシステムを開発しようとしている時には。",
    "output": "So, in this video, we talked about the process of how to evaluate an anomaly detection algorithm, and again, having being able to evaluate an algorithm, you know, with a single real number evaluation, with a number like an F1 score that often allows you to much more efficient use of your time when you are trying to develop an anomaly detection system."
  },
  {
    "index": "F17733",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの種の意思決定、たとえばイプシロンを選ぶとか、どのフィーチャーを含めるかとか、そういう意思決定をしたい時には。",
    "output": "And we try to make these sorts of decisions. I have to chose epsilon, what features to include, and so on."
  },
  {
    "index": "F17734",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、アノマリー検出のアルゴリズムを評価する為に、ラベル付けされたデータを少量使う。これはちょっとだけ教師有り学習に近くなる。",
    "output": "In this video, we started to use a bit of labeled data in order to evaluate the anomaly detection algorithm and this takes us a little bit closer to a supervised learning setting."
  },
  {
    "index": "F17735",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、その事ついてもうすこし説明します。",
    "output": "In the next video, I'm going to say a bit more about that."
  },
  {
    "index": "F17736",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、どういう時はアノマリー検出アルゴリズムを使うべきで、どういう時はその代わりに教師有り学習を採用すべきか、そしてこれら2つの形式化の違いについても扱います。",
    "output": "And in particular we'll talk about when should you be using an anomaly detection algorithm and when should we be thinking about using supervised learning instead, and what are the differences between these two formalisms."
  },
  {
    "index": "F17737",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではアノマリー検出のアルゴリズムを評価する手順について話した。",
    "output": "In the last video we talked about the process of evaluating an anomaly detection algorithm."
  },
  {
    "index": "F17738",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこではラベル付きのデータをアノマリーかそうでないか分かっているサンプルに対して、y=1か0かで対応させる事でラベル付けしたデータを用いた。",
    "output": "And there we started to use some label data with examples that we knew were either anomalous or not anomalous with Y equals one, or Y equals 0."
  },
  {
    "index": "F17739",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで浮かぶ疑問としては、このラベル付きデータがあるならどれがアノマリーでどれがアノマリーじゃない、という事が分かっているデータがあるのなら、何故単純に教師有り学習アルゴリズムを使わないのか?",
    "output": "And so, the question then arises of, and if we have the label data, that we have some examples and know the anomalies, and some of them will not be anomalies. Why don't we just use a supervisor on half of them?"
  },
  {
    "index": "F17740",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりなんでただロジスティック回帰やニューラルネットワークを使ってラベル付けされたデータから直接学習してしまわないのか?そうしてyが1か0かを予測してしまえばいいのでは?",
    "output": "So why don't we just use logistic regression, or a neuro network to try to learn directly from our labeled data to predict whether Y equals one or Y equals 0."
  },
  {
    "index": "F17741",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、どういう時にアノマリー検出アルゴリズムを使いどういう時には教師有り学習アルゴリズムの使用を検討したほうが実りが多いのかについて、いくつかの考え方とガイドラインをお話したい。",
    "output": "In this video, I'll try to share with you some of the thinking and some guidelines for when you should probably use an anomaly detection algorithm, and whether it might be more fruitful instead of using a supervisor in the algorithm."
  },
  {
    "index": "F17742",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このスライドでは、どういう状況ではアノマリー検出を使った方が良くてどういう時は教師有り学習の方が実りが多いのかを示している。",
    "output": "This slide shows what are the settings under which you should maybe use anomaly detection versus when supervised learning might be more fruitful."
  },
  {
    "index": "F17743",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし陽性のサンプルの数がとても少い場合は、ここでy=1がアノマリーのサンプルだったかだが、その場合はアノマリー検出のアルゴリズムの使用を検討すべきだ。",
    "output": "If you have a problem with a very small number of positive examples, and remember the examples of y equals one are the anomaly examples. Then you might consider using an anomaly detection algorithm instead."
  },
  {
    "index": "F17744",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり0から20個とか、まぁ50個くらいまでの陽性のサンプルはとても典型的な範囲で、通常はそんなに陽性のサンプル数が少ない時は、それらの陽性のサンプルをクロスバリデーションセットとテストセットの為に取っておくのが良かろう。",
    "output": "So, having 0 to 20, it may be up to 50 positive examples, might be pretty typical."
  },
  {
    "index": "F17745",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "逆に、典型的なアノマリー検出の状況としては、相対的にとても大量の陰性のサンプルがある。",
    "output": "And usually we have such a small positive, set of positive examples, we're going to save the positive examples just for the cross validation set in the test set."
  },
  {
    "index": "F17746",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら正常なサンプル、これら正常は航空機エンジンなど。",
    "output": "And in contrast, in a typical normal anomaly detection setting, we will often have a relatively large number of negative examples of the normal examples of normal aircraft engines."
  },
  {
    "index": "F17747",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらのとても大量のサンプルを使ってモデルp(x)のフィッティングを行う事が出来る。",
    "output": "And we can then use this very large number of negative examples With which to fit the model p(x)."
  },
  {
    "index": "F17748",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、だいたいアノマリー検出の適用のケースでは陽性のサンプルがほんのちょっとしか無くて陰性のサンプルは大量にある場合だ。",
    "output": "And so there's this idea that in many anomaly detection applications, you have very few positive examples and lots of negative examples."
  },
  {
    "index": "F17749",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてp(x)を推計している時、ガウス分布のパラメータをフィッティングしている時は陰性のサンプルしか必要としない。",
    "output": "And when we're doing the process of estimating p(x), affecting all those Gaussian parameters, we need only negative examples to do that."
  },
  {
    "index": "F17750",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからたくさんの陰性のデータがあれば、p(x)をフィッティングするのは極めてうまくやれる。",
    "output": "So if you have a lot negative data, we can still fit p(x) pretty well."
  },
  {
    "index": "F17751",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "逆に教師有り学習の時は、十分にたくさんの、陽性と陰性のサンプル両方があるのがより典型的だ。",
    "output": "In contrast, for supervised learning, more typically we would have a reasonably large number of both positive and negative examples."
  },
  {
    "index": "F17752",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがあなたの問題に対してアノマリー検出アルゴリズムを使うべきか教師有り学習のアルゴリズムを使うべきかを決めるのに着目すべきポイントの一つだ。",
    "output": "And so this is one way to look at your problem and decide if you should use an anomaly detection algorithm or a supervised."
  },
  {
    "index": "F17753",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、アノマリー検出のアルゴリズムについて皆が良く思う、もう一つの考え方はこうだ。",
    "output": "Here's another way that people often think about anomaly detection."
  },
  {
    "index": "F17754",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アノマリー検出の適用時は、しばしばたくさんの異なる種類のアノマリーがあるものだ。",
    "output": "So for anomaly detection applications, often there are very different types of anomalies."
  },
  {
    "index": "F17755",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "様々な物が不良品となり得て、それぞれが航空機エンジンの故障の原因たりえる。",
    "output": "There are so many things that could go wrong that could the aircraft engine."
  },
  {
    "index": "F17756",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてさらに、もし陽性のサンプルがとても少ししか得られていなければそのちょっとの陽性のサンプルからアノマリーとはどんな物かをそこから学習するのは、難しいかもしれない。",
    "output": "And so if that's the case, and if you have a pretty small set of positive examples, then it can be hard for an algorithm, difficult for an algorithm to learn from your small set of positive examples what the anomalies look like."
  },
  {
    "index": "F17757",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてさらに、将来起こりうるアノマリーがここまで見た物と全く似てないかもしれない。",
    "output": "And in particular, you know future anomalies may look nothing like the ones you've seen so far."
  },
  {
    "index": "F17758",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりあなたの集めた陽性のサンプルは、5とか10とか20種類の相異なる航空機のエンジンが、どう壊れるかのパターンを示しているかもしれないが、だが明日にはひょっとすると、全く新しい、新種のアノマリーを、全く新しい航空機エンジンの故障の仕方で、まったく見たこと無いような物を検出する必要がある。",
    "output": "So maybe in your set of positive examples, maybe you've seen 5 or 10 or 20 different ways that an aircraft engine could go wrong."
  },
  {
    "index": "F17759",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合は単純に陰性のサンプルをモデリングする方が、ガウス分布のモデルp(x)でモデリングする方が、より筋が良いだろう。陽性のサンプルを必死にモデリングするよりも。",
    "output": "And if that's the case, it might be more promising to just model the negative examples with this sort of calcium model p of x instead of try to hard to model the positive examples."
  },
  {
    "index": "F17760",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら明日のアノマリーはここまで見た物とはまったく似てないかも知れないのだから。",
    "output": "Because tomorrow's anomaly may be nothing like the ones you've seen so far."
  },
  {
    "index": "F17761",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "逆に、他の問題で、十分な陽性のサンプルを持っていて、アルゴリズムに陽性のサンプルとはどんな感じか、という感じが掴めそうならそしてとりわけ、もし将来現れるであろう陽性のサンプルがトレーニングセットにある物と似たような物だろうと思われるなら、その場合には教師有り学習のアルゴリズムを用いる方がより合理的かもしれない。",
    "output": "In contrast, in some other problems, you have enough positive examples for an algorithm to get a sense of what the positive examples are like."
  },
  {
    "index": "F17762",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはたくさんの陽性のサンプルを見てたくさんの陰性のサンプルを見て、そしてそれらを用いて陽性と陰性を見分けようとする物だ。",
    "output": "In particular, if you think that future positive examples are likely to be similar to ones in the training set; then in that setting, it might be more reasonable to have a supervisor in the algorithm that looks at all of the positive examples, looks at all of the negative examples, and uses that to try to distinguish between positives and negatives."
  },
  {
    "index": "F17763",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、もし特定の問題が目の前にあった時にアノマリー検出のアルゴリズムの使用を検討すべきかまたは教師有り学習のアルゴリズムを検討すべきか、だいたいの感じはつかめたかな。",
    "output": "Hopefully, this gives you a sense of if you have a specific problem, should you think about using an anomaly detection algorithm, or a supervised learning algorithm."
  },
  {
    "index": "F17764",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "キーとなる重要な違いはアノマリー検出では、陽性のサンプルがちょっとしか無いからそんなちょっとの物から学習アルゴリズムが多くを学ぶなんてそもそも不可能なのだ。",
    "output": "And a key difference really is that in anomaly detection, often we have such a small number of positive examples that it is not possible for a learning algorithm to learn that much from the positive examples."
  },
  {
    "index": "F17765",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから代わりにやる事としては、たくさんの陰性のサンプルをとってきて、それについてたくさん学習し、p(x)を陰性のサンプルだけから、例えば正常な航空機エンジンだけから学習する。",
    "output": "And so what we do instead is take a large set of negative examples and have it just learn a lot, learn p(x) from just the negative examples."
  },
  {
    "index": "F17766",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして少ししかない陽性のサンプルを我らのアルゴリズムを評価する為にクロスバリデーションセットかテストセットにとっておく。",
    "output": "Of the normal and we've reserved the small number of positive examples for evaluating our algorithms to use in the either the transvalidation set or the test set."
  },
  {
    "index": "F17767",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらたくさんの異なるアノマリーについて補足しておくと、以前のビデオではeメールのスパムについて議論した。",
    "output": "And just as a side comment about this many different types of easier. In some earlier videos we talked about the email spam examples."
  },
  {
    "index": "F17768",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらの例でも、たくさんの異なる種類のスパムメールが実際にある。",
    "output": "In those examples, there are actually many different types of spam email, right?"
  },
  {
    "index": "F17769",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スパムメールは物をあなたに売りつけようとする物もあるし、スパムメールはあなたのパスワードを盗もうとする場合もある。",
    "output": "There's spam email that's trying to sell you things."
  },
  {
    "index": "F17770",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他にも様々なスパムメールがある。だがスパムの問題では、普通は調べる事が出来る十分な数のスパムが手に入る。",
    "output": "Spam email trying to steal your passwords, this is called phishing emails and many different types of spam emails."
  },
  {
    "index": "F17771",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら異なる種類のほとんどのスパムメールを入手出来る。何故なら我らは大量のスパムメールを持ってるからだ。",
    "output": "But for the spam problem we usually have enough examples of spam email to see most of these different types of spam email because we have a large set of examples of spam."
  },
  {
    "index": "F17772",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてだからこそ普通スパムの問題を教師有り学習の問題とみなすのだ。たくさんの種類のスパムがあるにも関わらず。",
    "output": "And that's why we usually think of spam as a supervised learning setting even though there are many different types of."
  },
  {
    "index": "F17773",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、実際の応用においてアノマリー検出と教師有り学習を比較してみると、欠陥の検出において、もし様々な種類の欠陥とみなしたい物がありそうなら、そして相対的に少しのトレーニングセットしか、詐欺行為をしているユーザーがあなたのwebサイトにはちょっとしかいなければその時は私はアノマリー検出のアルゴリズムを使うだろう。",
    "output": "If we look at some applications of anomaly detection versus supervised learning we'll find fraud detection."
  },
  {
    "index": "F17774",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこんな時、たとえばあなたがとても大きなオンラインの小売商だったとして、実際にたくさんの人が詐欺行為を働こうとしているとする。",
    "output": "If you have many different types of ways for people to try to commit fraud and a relatively small number of fraudulent users on your website, then I use an anomaly detection algorithm."
  },
  {
    "index": "F17775",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり実際たくさんのy=1となるサンプルがあるとすると、そういう場合など、時には詐欺の検出は現実には教師有り学習へと変化する場合がありうる。",
    "output": "I should say, if you have, if you're a very major online retailer and if you actually have had a lot of people commit fraud on your website, so you actually have a lot of examples of y=1, then sometimes fraud detection could actually shift over to the supervised learning column."
  },
  {
    "index": "F17776",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもしあなたがあなたのwebサイトにおいて奇妙な行動に出るユーザーのサンプルがそんなには見られないのなら、こちらの方がありがちだが、この場合は詐欺検出はアノマリー検出として扱う事になる、教師有り学習では無く。",
    "output": "But, if you haven't seen that many examples of users doing strange things on your website, then more frequently fraud detection is actually treated as an anomaly detection algorithm rather than a supervised learning algorithm."
  },
  {
    "index": "F17777",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他の例でも、製造業については既に話したが、望ましいのはたくさんの正常なサンプルとそんなに多くないアノマリーを見る、という状態だが、ある製造工程ではもし大量の製造を行なっていて、既にたくさんの不良品のサンプルが見られているなら、その製造も教師有り学習アルゴリズムに変化しうる。",
    "output": "Other examples, we've talked about manufacturing already."
  },
  {
    "index": "F17778",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも過去の製造結果にそんなにたくさんの不良品のサンプルが見られていないなら、私だったらこれをアノマリー検出で扱うね。",
    "output": "Hopefully, you see more and more examples are not that many anomalies but if again for some manufacturing processes, if you manufacture in very large volumes and you see a lot of bad examples, maybe manufacturing can shift to the supervised learning column as well."
  },
  {
    "index": "F17779",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "データセンターでのマシーンのモニタリングでも同種の議論が適用出来る。",
    "output": "But if you haven't seen that many bad examples of so to do the anomaly detection monitoring machines in a data center similar source of apply."
  },
  {
    "index": "F17780",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方eメールのスパム分類、天気予報、ガンの分類などでは、だいたい同等の数の陽性と陰性のサンプルがあるなら、陽性と陰性のサンプルをたくさん得ているならこれら全てを教師有り学習の問題とみなす傾向にある。",
    "output": "Whereas, you must have classification, weather prediction, and classifying cancers. If you have equal numbers of positive and negative examples."
  },
  {
    "index": "F17781",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で問題のどんな性質がそれをあなたがアノマリー検出の問題と扱うか、または教師有り学習の問題と扱うかを決めているかについて、感じがつかめただろうか。",
    "output": "So hopefully, that gives you a sense of one of the properties of a learning problem that would cause you to treat it as an anomaly detection problem versus a supervisory problem."
  },
  {
    "index": "F17782",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "様々な技術の会社において、直面する問題の多くでは、本当にちょっとの陽性のサンプルしか無かったり、時にはゼロ個の陽性のサンプルしかなかったり、あまりにもたくさんの種類のまだ見ぬアノマリーが存在したりする。",
    "output": "And for many other problems that are faced by various technology companies and so on, we actually are in the settings where we have very few or sometimes zero positive training examples. There's just so many different types of anomalies that we've never seen them before."
  },
  {
    "index": "F17783",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そういう種類の問題ではとてもしばしば用いられる学習アルゴリズムはアノマリー検出のアルゴリズムだ。",
    "output": "And for those sorts of problems, very often the algorithm that is used is an anomaly detection algorithm."
  },
  {
    "index": "F17784",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでで我々はアノマリー検出のアルゴリズムを見てきた。また、アノマリー検出のアルゴリズムをどう評価するのかも見てきた。",
    "output": "By now you've seen the anomaly detection algorithm and we've also talked about how to evaluate an anomaly detection algorithm."
  },
  {
    "index": "F17785",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実は、実際にアノマリー検出を適用してみると、それがどれだけうまく機能するかに巨大な影響を与えているのはなんのフィーチャーを使うのか、何のフィーチャーを選んでアノマリー検出のアルゴリズムに与えるか、という部分だ。",
    "output": "It turns out, that when you're applying anomaly detection, one of the things that has a huge effect on how well it does, is what features you use, and what features you choose, to give the anomaly detection algorithm."
  },
  {
    "index": "F17786",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこのビデオでは、アノマリー検出のアルゴリズムに食わせるフィーチャーをどうデザインするか、どう選ぶかについて二、三の助言、提案をしたいと思う。",
    "output": "So in this video, what I'd like to do is say a few words, give some suggestions and guidelines for how to go about designing or selecting features give to an anomaly detection algorithm."
  },
  {
    "index": "F17787",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らのアノマリー検出のアルゴリズムにおいてはこんな種類のガウス分布を用いてフィーチャー達をモデリングするという過程があります。",
    "output": "In our anomaly detection algorithm, one of the things we did was model the features using this sort of Gaussian distribution."
  },
  {
    "index": "F17788",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "xiを、ミューiとシグマ二乗iで。",
    "output": "With xi to mu i, sigma squared i, lets say."
  },
  {
    "index": "F17789",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうであるから、私が良くやる事としては、私のアノマリー検出のアルゴリズムにデータを食わせる前にこのデータのヒストグラムをプロットしてみてなんとなくガウス分布っぽいかを見てみる、という事があります。",
    "output": "And so one thing that I often do would be to plot the data or the histogram of the data, to make sure that the data looks vaguely Gaussian before feeding it to my anomaly detection algorithm."
  },
  {
    "index": "F17790",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたのデータがガウス分布でなくても普通はオーケーです。だけどこれは、実行してみるに値する、良いサニティチェックです。",
    "output": "And, it'll usually work okay, even if your data isn't Gaussian, but this is sort of a nice sanitary check to run."
  },
  {
    "index": "F17791",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、あなたのデータが非ガウス分布をとっていても、アルゴリズムは普通に正しく機能する事が多い。",
    "output": "And by the way, in case your data looks non-Gaussian, the algorithms will often work just find."
  },
  {
    "index": "F17792",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に見るとデータをこんな風にプロットしてみて、ヒストグラムがこんな感じに見えたらところでヒストグラムのプロットの仕方はhist関数をOctaveでは使いますがそこでこんな見た目なら、これはだいたいガウス分布っぽい。つまりフィーチャーがこんな感じならアルゴリズムに、とても幸せな気持ちで食わせられる。",
    "output": "But, concretely if I plot the data like this, and if it looks like a histogram like this, and the way to plot a histogram is to use the HIST, or the HIST command in Octave, but it looks like this, this looks vaguely Gaussian, so if my features look like this, I would be pretty happy feeding into my algorithm."
  },
  {
    "index": "F17793",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、もしもデータのヒストグラムをプロットしてみたらそれがもしもこんな感じなら、うーん、こいつはベル型のカーブにはまったく見えないなぁ。これはとても非対称な分布だ。",
    "output": "But if i were to plot a histogram of my data, and it were to look like this well, this doesn't look at all like a bell shaped curve, this is a very asymmetric distribution, it has a peak way off to one side."
  },
  {
    "index": "F17794",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこんな風に私のデータが見えたら私が良くやる手段は様々なデータ変換を試してもっとガウス分布っぽく見えるようにする。",
    "output": "If this is what my data looks like, what I'll often do is play with different transformations of the data in order to make it look more Gaussian."
  },
  {
    "index": "F17795",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "繰り返しになるが、別にそんな事しなくても、だいたいはアルゴリズムはちゃんと機能する。",
    "output": "And again the algorithm will usually work okay, even if you don't."
  },
  {
    "index": "F17796",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でももしこれらの変換でデータをよりガウス分布っぽく見えるように出来たら、そっちの方がアルゴリズムはちょっとだけ改善される。",
    "output": "But if you use these transformations to make your data more gaussian, it might work a bit better."
  },
  {
    "index": "F17797",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこんな見た目のデータセットが与えられた時、私が試すだろう事はlogをとってデータを変換しその後にデータをヒストグラムとして再プロットしてこの例の場合は例えば結果としてこんなヒストグラムを得る。",
    "output": "So given the data set that looks like this, what I might do is take a log transformation of the data and if i do that and re-plot the histogram, what I end up with in this particular example, is a histogram that looks like this."
  },
  {
    "index": "F17798",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こっちの方がガウス分布っぽいでしょう?",
    "output": "And this looks much more Gaussian, right?"
  },
  {
    "index": "F17799",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こっちの方がより古典的なベル型のカーブに見える。それはある平均と分散のパラメータでフィッティング出来る。",
    "output": "This looks much more like the classic bell shaped curve, that we can fit with some mean and variance paramater sigma."
  },
  {
    "index": "F17800",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ログ変換を取る、と言っているのは具体的にはあるフィーチャーx1があったとして、そのx1のヒストグラムがこんな見た目だとするとフィーチャーx1をlogx1で置き換える。そしてこれを新しいx1とみなし、それのヒストグラムを右にプロットすると、よりガウス分布っぽくなっている。",
    "output": "So what I mean by taking a log transform, is really that if I have some feature x1 and then the histogram of x1 looks like this then I might take my feature x1 and replace it with log of x1 and this is my new x1 that I'll plot to the histogram over on the right, and this looks much more Guassian."
  },
  {
    "index": "F17801",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "log変換の他に、選択肢として考えられる物は異なるフィーチャーx2があったとして、それをlog(x+1)で置き換える、またはより一般にlogxのxをx2とある定数cで置き換えた物で置き換える、というのが考えられる。この定数cはいろいろ調整してなるべくガウス分布っぽく見えるようにする。",
    "output": "Rather than just a log transform some other things you can do, might be, let's say I have a different feature x2, maybe I'll replace that will log x plus 1, or more generally with log x with x2 and some constant c and this constant could be something that I play with, to try to make it look as Gaussian as possible."
  },
  {
    "index": "F17802",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他には別のフィーチャーx3に対しx3をルートを取った物で置き換えても良いかもしれない。",
    "output": "Or for a different feature x3, maybe I'll replace it with x3, I might take the square root."
  },
  {
    "index": "F17803",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ルートってのはx3の、単なる1/2乗に過ぎない、でしょ?",
    "output": "The square root is just x3 to the power of one half, right?"
  },
  {
    "index": "F17804",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの1/2というのもいろいろ調整する事が出来るパラメータとなる。",
    "output": "And this one half is another example of a parameter I can play with."
  },
  {
    "index": "F17805",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "x4があったとして、そのx4を代わりにx4の違う指数乗、例えば1/3乗とかで置き換えても良い。",
    "output": "So, I might have x4 and maybe I might instead replace that with x4 to the power of something else, maybe to the power of 1/3."
  },
  {
    "index": "F17806",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれら全て、これ、この指数乗のパラメータ、またはパラメータCも、これら全てがあなたのデータをちょっとでもガウス分布っぽく見せる為にいじれるパラメータの例となる。",
    "output": "And these, all of these, this one, this exponent parameter, or the C parameter, all of these are examples of parameters that you can play with in order to make your data look a little bit more Gaussian."
  },
  {
    "index": "F17807",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではここで、私のデータを実際にいろいろいじってよりガウス分布っぽくする生のデモをお見せしよう。",
    "output": "So, let me show you a live demo of how I actually go about playing with my data to make it look more Gaussian."
  },
  {
    "index": "F17808",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その為、ここに既にOctaveにロードしておいた、幾つかのフィーチャーxを。",
    "output": "So, I have already loaded in to octave here a set of features x I have a thousand examples loaded over there."
  },
  {
    "index": "F17809",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では私のデータのヒストグラムを出しておこう、このhistxのコマンドで。",
    "output": "So let's pull up the histogram of my data."
  },
  {
    "index": "F17810",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが私のヒストグラムだ。",
    "output": "So there's my histogram."
  },
  {
    "index": "F17811",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "デフォルトだと確かこの10個のビンをヒストグラムは使うと思うが私はもっと粒度の細かいグリッドのヒストグラムを見たい。",
    "output": "By default, I think this uses 10 bins of histograms, but I want to see a more fine grid histogram."
  },
  {
    "index": "F17812",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからhistにx,50と渡すこれは50個のことなるビンでプロットする。",
    "output": "So we do hist to the x, 50, so, this plots it in 50 different bins."
  },
  {
    "index": "F17813",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "オーケー。良くなった。",
    "output": "Okay, that looks better."
  },
  {
    "index": "F17814",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "現在のところ、これはそんなにガウス分布っぽくは無い。でしょ?",
    "output": "Now, this doesn't look very Gaussian, does it?"
  },
  {
    "index": "F17815",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まずhistのxの0.5乗を試してみよう。",
    "output": "Lets try a hist of x to the 0.5."
  },
  {
    "index": "F17816",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりデータのルートを取ってそのヒストグラムをプロットする。",
    "output": "So we take the square root of the data, and plot that histogram."
  },
  {
    "index": "F17817",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがまだまだだね。ではこの0.5というパラメータをいじってみよう。",
    "output": "And, okay, it looks a little bit more Gaussian, but not quite there, so let's play at the 0.5 parameter."
  },
  {
    "index": "F17818",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを0.2にセットする。",
    "output": "Set this to 0.2."
  },
  {
    "index": "F17819",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もうちょっとガウス分布っぽくなった。",
    "output": "Looks a little bit more Gaussian."
  },
  {
    "index": "F17820",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もうちょっと減らして0.1にしてみよう。",
    "output": "Let's reduce a little bit more 0.1."
  },
  {
    "index": "F17821",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらに減らして0.05にしてみよう。",
    "output": "Well, let's reduce it to 0.05."
  },
  {
    "index": "F17822",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから新しいフィーチャーとしてxミューイコールxの0.05乗と定義する。するとこの新しいフィーチャーxミューは、以前のよりもよりガウス分布っぽくなって、これを代わりにアノマリー検出のアルゴリズムに食わせても良い。",
    "output": "Okay, this looks pretty Gaussian, so I can define a new feature which is x mu equals x to the 0.05, and now my new feature x Mu looks more Gaussian than my previous one and then I might instead use this new feature to feed into my anomaly detection algorithm."
  },
  {
    "index": "F17823",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、これを行う方法は一つだけでは無い。",
    "output": "And of course, there is more than one way to do this."
  },
  {
    "index": "F17824",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "histのlogxというのも試しても良い。それはもう一つの試してみる価値のある変換の例だ。",
    "output": "You could also have hist of log of x, that's another example of a transformation you can use."
  },
  {
    "index": "F17825",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれも、見ての通りなかなかガウスっぽい。",
    "output": "And, you know, that also look pretty Gaussian."
  },
  {
    "index": "F17826",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからxミューをlogxと定義しても良い。",
    "output": "So, I can also define x mu equals log of x."
  },
  {
    "index": "F17827",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これもまたなかなか良いフィーチャーのチョイスといえる。",
    "output": "and that would be another pretty good choice of a feature to use."
  },
  {
    "index": "F17828",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まとめると、データのヒストグラムをプロットしてみて、それがだいぶガウス分布っぽくなかったら今回試したような変換でちょっとデータをつついてみるのは試してみる価値がある。もうちょっとガウス分布っぽくならないかなぁ、と学習アルゴリズムに食わしてみる前に。",
    "output": "So to summarize, if you plot a histogram with the data, and find that it looks pretty non-Gaussian, it's worth playing around a little bit with different transformations like these, to see if you can make your data look a little bit more Gaussian, before you feed it to your learning algorithm, although even if you don't, it might work okay."
  },
  {
    "index": "F17829",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも普段、私はこのステップを踏む。",
    "output": "But I usually do take this step."
  },
  {
    "index": "F17830",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで二番目に話したい事として、アノマリー検出に使うフィーチャーをどう見つけるか、というのがある。",
    "output": "Now, the second thing I want to talk about is, how do you come up with features for an anomaly detection algorithm."
  },
  {
    "index": "F17831",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私が良くやるのは誤差分析の手順だ。",
    "output": "And the way I often do so, is via an error analysis procedure."
  },
  {
    "index": "F17832",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それの意味する所は教師有り学習での誤差分析の手順と本当に似た物で、そこでは完全にアルゴリズムを学習させて、それをクロスバリデーションセットにかける、そして間違いのサンプルを見る、そしてフィーチャーを追加してそれがクロスバリデーションセットで誤りだった物を正しく扱う助けとなるかを見てみる。",
    "output": "So what I mean by that, is that this is really similar to the error analysis procedure that we have for supervised learning, where we would train a complete algorithm, and run the algorithm on a cross validation set, and look at the examples it gets wrong, and see if we can come up with extra features to help the algorithm do better on the examples that it got wrong in the cross-validation set."
  },
  {
    "index": "F17833",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこのプロセスを例を通して見てみよう。",
    "output": "So lets try to reason through an example of this process."
  },
  {
    "index": "F17834",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アノマリー検出のアルゴリズムにおいては通常のサンプルについてはpのxが大きく、アノマリーなサンプルでは小さくなる事を期待している。",
    "output": "In anomaly detection, we are hoping that p of x will be large for the normal examples and it will be small for the anomalous examples."
  },
  {
    "index": "F17835",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、良くある問題としてはpのxが同じような値という場合、例えば普通の物とアノマリーな物が両方とも大きい場合などだ。",
    "output": "And so a pretty common problem would be if p of x is comparable, maybe both are large for both the normal and the anomalous examples."
  },
  {
    "index": "F17836",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その具体例を見てみよう。",
    "output": "Lets look at a specific example of that."
  },
  {
    "index": "F17837",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが私のラベル無しデータとする。",
    "output": "Let's say that this is my unlabeled data."
  },
  {
    "index": "F17838",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここではたった一つのフィーチャーx1しか無いのでこれをガウス分布でフィッティングする。",
    "output": "So, here I have just one feature, x1 and so I'm gonna fit a Gaussian to this."
  },
  {
    "index": "F17839",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてフィッティングしたガウス曲線がこんな感じだったとしよう。",
    "output": "And maybe my Gaussian that I fit to my data looks like that."
  },
  {
    "index": "F17840",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてアノマラスなサンプルがあったとしよう。",
    "output": "And now let's say I have an anomalous example, and let's say that my anomalous example takes on an x value of 2.5."
  },
  {
    "index": "F17841",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのアノマラスなサンプルは緑で描いた物だが、それはとても高い確率となる。それは青い曲線の高さで表される訳だ。",
    "output": "And you know, it's kind of buried in the middle of a bunch of normal examples, and so, just this anomalous example that I've drawn in green, it gets a pretty high probability, where it's the height of the blue curve, and the algorithm fails to flag this as an anomalous example."
  },
  {
    "index": "F17842",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、これは航空機エンジンの製造とかだとしよう。",
    "output": "Now, if this were maybe aircraft engine manufacturing or something, what I would do is, I would actually look at my training examples and look at what went wrong with that particular aircraft engine, and see, if looking at that example can inspire me to come up with a new feature x2, that helps to distinguish between this bad example, compared to the rest of my red examples, compared to all of my normal aircraft engines."
  },
  {
    "index": "F17843",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで実際に取れる対策としてはトレーニングサンプルを実際に見て、うまく行っていない特定の不良エンジンについてどこが悪くなっているかを調べる。そしてもしそのエンジンを調べていてそれにインスパイアされて新たなフィーチャーx2を思いついたとする。",
    "output": "And if I managed to do so, the hope would be then, that, if I can create a new feature, X2, so that when I re-plot my data, if I take all my normal examples of my training set, hopefully I find that all my training examples are these red crosses here."
  },
  {
    "index": "F17844",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがこの不良エンジンを識別するには有用な訳だ。残りの赤いサンプル、私の全ての正常な航空機エンジンと比較して。",
    "output": "And hopefully, if I find that for my anomalous example, the feature x2 takes on the the unusual value."
  },
  {
    "index": "F17845",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてなんとかそこまで行ったら以下のような事を期待する訳だ:新しいフィーチャーx2を作って、私のデータを再プロットしたらトレーニングセットの正常なサンプルはプロットしてみたら正常なサンプルは赤いバツのサンプルはこんな感じで見えたとして、アノマラスなサンプルはフィーチャーx2が異常な値を取るという風に出来る事を期待する訳だ。",
    "output": "So for my green example here, this anomaly, right, my X1 value, is still 2.5."
  },
  {
    "index": "F17846",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば私のこの緑のサンプルこのアノマリーはx1の値は2.5のままだけど、x2の値は例えば、とても大きな値とか、こっちの3.5とかそういう値か、またらとても小さな値を期待する訳だ。",
    "output": "Then maybe my X2 value, hopefully it takes on a very large value like 3.5 over there, or a very small value."
  },
  {
    "index": "F17847",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでこのデータをモデリングすると、アノマリー検出のアルゴリズムはまんなかのあたりの領域のデータに高い確率を与え、そのちょっと低い確率がこの辺に、それよりさらにちょっと低い確率がこの辺に、となる。",
    "output": "But now, if I model my data, I'll find that my anomaly detection algorithm gives high probability to data in the central regions, slightly lower probability to that, sightly lower probability to that."
  },
  {
    "index": "F17848",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの遥か遠くのサンプルに対しては私のアルゴリズムはここではとても低い確率を与えるだろう。",
    "output": "An example that's all the way out there, my algorithm will now give very low probability to."
  },
  {
    "index": "F17849",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのプロセスは実際に間違いだった物、アルゴリズムがフラグ付けに失敗したアノマリーを直接調べて、それが新しいフィーチャーを作るようインスパイアしてくれるかどうかを見てみる。",
    "output": "And so, the process of this is, really look at the mistakes that it is making."
  },
  {
    "index": "F17850",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その航空機エンジンの何か普通で無い所を探してそれを用いて新しいフィーチャーを作る。",
    "output": "Look at the anomaly that the algorithm is failing to flag, and see if that inspires you to create some new feature."
  },
  {
    "index": "F17851",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この新しいフィーチャーで正常なサンプルとアノマリー達をより簡単に区別出来るようになるように。",
    "output": "So find something unusual about that aircraft engine and use that to create a new feature, so that with this new feature it becomes easier to distinguish the anomalies from your good examples."
  },
  {
    "index": "F17852",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が誤差解析の手順であり、それをアノマリー検出に使う新しいフィーチャーを作るのに使うやり方だ。",
    "output": "And so that's the process of error analysis and using that to create new features for anomaly detection."
  },
  {
    "index": "F17853",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、私が普段アノマリー検出のフィーチャーを選ぶのにどうやってるのかについて、私の考えを共有しておきたい。",
    "output": "Finally, let me share with you my thinking on how I usually go about choosing features for anomaly detection."
  },
  {
    "index": "F17854",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "普段私がフィーチャーの選択についてどう考えているかといえば、アノマリーのサンプルだと思う時にはなるべく凄く凄く小さくなるか凄く凄く大きくなるようなフィーチャーを探したい、と考えている。",
    "output": "So, usually, the way I think about choosing features is I want to choose features that will take on either very, very large values, or very, very small values, for examples that I think might turn out to be anomalies."
  },
  {
    "index": "F17855",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでも再びコンピューターのデータセンターのモニタリングの例を考えてみよう。",
    "output": "So let's use our example again of monitoring the computers in a data center."
  },
  {
    "index": "F17856",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "たくさんのマシンがあってたとえば何千とか何万とかのマシンがデータセンターにあるかもしれない。",
    "output": "And so you have lots of machines, maybe thousands, or tens of thousands of machines in a data center."
  },
  {
    "index": "F17857",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてマシンの一つがコンピュータの一つがいかれているか、つまり何か妙な事をしているかを知りたい。",
    "output": "And we want to know if one of the machines, one of our computers is acting up, so doing something strange."
  },
  {
    "index": "F17858",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでこんなフィーチャーを選ぶかもしれない。メモリー使用量とかディスクアクセスの回数とかCPUロードとかネットワークトラフィックとか。",
    "output": "So here are examples of features you may choose, maybe memory used, number of disc accesses, CPU load, network traffic."
  },
  {
    "index": "F17859",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがここで、失敗してるケースの一つに、私のデータセットの失敗してるケースの一つが、CPUロードとネットワークトラフィックが共にリニアに増えていくと疑っているとしよう。",
    "output": "But now, lets say that I suspect one of the failure cases, let's say that in my data set I think that CPU load the network traffic tend to grow linearly with each other."
  },
  {
    "index": "F17860",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばたくさんのwebサーバを走らせていてそのサービスの一つがたくさんのユーザに対してサービスを提供していたらとても高いCPUロードと高いネットワークトラフィックを得るだろう。",
    "output": "Maybe I'm running a bunch of web servers, and so, here if one of my servers is serving a lot of users, I have a very high CPU load, and have a very high network traffic."
  },
  {
    "index": "F17861",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが例えば、失敗のケースの一つはもしコンピュータの一つが無限ループで固まってるジョブを持っていると疑ってるとしよう。",
    "output": "But let's say, I think, let's say I have a suspicion, that one of the failure cases is if one of my computers has a job that gets stuck in some infinite loop."
  },
  {
    "index": "F17862",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり私はある失敗のケースはあるマシンのあるwebserver、もといサーバーのcodeが無限ループで詰まってる、と思ってるとすると、その時はCPUロードは上がるだろうがネットワークトラフィックは上昇しないだろう。何故ならそれは単に糸車がくるくると回っていてたくさんのCPUの仕事をしている、つまり無限ループに詰まってる。",
    "output": "So if I think one of the failure cases, is one of my machines, one of my web servers--server code-- gets stuck in some infinite loop, and so the CPU load grows, but the network traffic doesn't because it's just spinning it's wheels and doing a lot of CPU work, you know, stuck in some infinite loop."
  },
  {
    "index": "F17863",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、その種のアノマリーを検出する為には、新たなフィーチャーX5として、CPUロードをネットワークトラフィックで割ったような新たなフィーチャーを作るかもしれない。",
    "output": "In that case, to detect that type of anomaly, I might create a new feature, X5, which might be CPU load divided by network traffic."
  },
  {
    "index": "F17864",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとx5は、もしあるマシンがとても大きなCPUロードでありながらそんなに大きくないネットワークトラフィックの時には異常に大きな値をとるだろう。だからこのフィーチャーはあなたのアノマリー検出がある種のアノマリーを検出するのを助ける事になる。",
    "output": "And so here X5 will take on a unusually large value if one of the machines has a very large CPU load but not that much network traffic and so this will be a feature that will help your anomaly detection capture, a certain type of anomaly."
  },
  {
    "index": "F17865",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな風にさらに別のフィーチャーも思いつくかもしれない。",
    "output": "And you can also get creative and come up with other features as well."
  },
  {
    "index": "F17866",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばx6としてCPUロードを二乗した物をネットワークトラフィックで割った物を使うかもしれない。",
    "output": "Like maybe I have a feature x6 thats CPU load squared divided by network traffic."
  },
  {
    "index": "F17867",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはマシンの一つがとても高いCPUロードにありながらそれに見合うネットワークトラフィックが無い物を見分けようとしているフィーチャーだ。",
    "output": "And this would be another variant of a feature like x5 to try to capture anomalies where one of your machines has a very high CPU load, that maybe doesn't have a commensurately large network traffic."
  },
  {
    "index": "F17868",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのようなフィーチャーを作る事でフィーチャーの異常な組み合わせを捉えていく事が出来る。",
    "output": "And by creating features like these, you can start to capture anomalies that correspond to unusual combinations of values of the features."
  },
  {
    "index": "F17869",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオではどうフィーチャーを選び必要ならアルゴリズムに食わせる前にちょっと変換してよりガウス分布っぽくするかを議論した。",
    "output": "So in this video we talked about how to and take a feature, and maybe transform it a little bit, so that it becomes a bit more Gaussian, before feeding into an anomaly detection algorithm."
  },
  {
    "index": "F17870",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また、新しいフィーチャーを作って異なる種類のアノマリーを捕捉する為のエラー分析の手順も議論した。",
    "output": "And also the error analysis in this process of creating features to try to capture different types of anomalies."
  },
  {
    "index": "F17871",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのガイドラインがあなたが良いフィーチャーを選びすべてのアノマリーをあなたの検出アルゴリズムが捕捉出来る一助にならんことを!",
    "output": "And with these sorts of guidelines hopefully that will help you to choose good features, to give to your anomaly detection algorithm, to help it capture all sorts of anomalies."
  },
  {
    "index": "F17872",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオとこの次のビデオで、ここまでに開発してきたアノマリー検出のアルゴリズムの考えられる一つの拡張をお話しよう。",
    "output": "In this and the next video, I'd like to tell you about one possible extension to the anomaly detection algorithm that we've developed so far."
  },
  {
    "index": "F17873",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この拡張は、多変量ガウス分布と呼ばれる物を使う。そしてそれはある長所があり、そしてある短所もある。",
    "output": "This extension uses something called the multivariate Gaussian distribution, and it has some advantages, and some disadvantages, and it can sometimes catch some anomalies that the earlier algorithm didn't."
  },
  {
    "index": "F17874",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この動機付けを理解する為に、具体例から始めよう。",
    "output": "To motivate this, let's start with an example."
  },
  {
    "index": "F17875",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここにプロットしたようなラベル無しデータがあるとする。",
    "output": "Let's say that so our unlabeled data looks like what I have plotted here."
  },
  {
    "index": "F17876",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてデータセンターのマシンのモニタリングの例を使っていく事にする。データセンターのコンピュータをモニタリングする。",
    "output": "And I'm going to use the example of monitoring machines in the data center, monitoring computers in the data center."
  },
  {
    "index": "F17877",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり私の二つのフィーチャーはx1がCPUロードで、x2が例えばメモリの使用量。",
    "output": "So my two features are x1 which is the CPU load and x2 which is maybe the memory use."
  },
  {
    "index": "F17878",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで二つのフィーチャー、x1とx2に対して、ガウス分布でモデリングすると、これがx1フィーチャーによるプロットで、これがx2フィーチャーによるプロットだ。ここにガウス分布をフィッティングすると、こんな感じのガウス分布が得られるだろう。",
    "output": "So if I take my two features, x1 and x2, and I model them as Gaussians then here's a plot of my X1 features, here's a plot of my X2 features, and so if I fit a Gaussian to that, maybe I'll get a Gaussian like this, so here's P of X 1, which depends on the parameters mu 1, and sigma squared 1, and here's my memory used, and, you know, maybe I'll get a Gaussian that looks like this, and this is my P of X 2, which depends on mu 2 and sigma squared 2."
  },
  {
    "index": "F17879",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以上がアノマリー検出のアルゴリズムがx1とx2をモデリングする方法だ。",
    "output": "And so this is how the anomaly detection algorithm models X1 and X2."
  },
  {
    "index": "F17880",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、テストセットにこんな手本があったとする。",
    "output": "Now let's say that in the test sets I have an example that looks like this."
  },
  {
    "index": "F17881",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この緑のバッテンの位置、つまりx1の値がだいたい0.4で、x2の値がだいたい1.5くらいの位置。",
    "output": "The location of that green cross, so the value of X 1 is about 0.4, and the value of X 2 is about 1.5."
  },
  {
    "index": "F17882",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、データを見てみると、ぱっと見た感じ多くのデータはこの範囲に存在しているので、この緑のバッテンは観測されているデータのどれとも、かなり離れている。",
    "output": "Now, if you look at the data, it looks like, yeah, most of the data data lies in this region, and so that green cross is pretty far away from any of the data I've seen."
  },
  {
    "index": "F17883",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはアノマリーとして提起されるべきに見える。",
    "output": "It looks like that should be raised as an anomaly."
  },
  {
    "index": "F17884",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、私のデータにおいては、私の正常な手本のデータでは、CPUロードとメモリ使用量は、お互いに線形に上昇する傾向にあるように見える。",
    "output": "So, in my data, in my, in the data of my good examples, it looks like, you know, the CPU load, and the memory use, they sort of grow linearly with each other."
  },
  {
    "index": "F17885",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方でこの例、この緑の例では、CPUロードはとても低いが、メモリ使用量はとても高い。こんな物はトレーニングセットでは見られなかった物だ。",
    "output": "So if I have a machine using lots of CPU, you know memory use will also be high, whereas this example, this green example it looks like here, the CPU load is very low, but the memory use is very high, and I just have not seen that before in my training set."
  },
  {
    "index": "F17886",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはアノマリーだとみなすべきだろう。",
    "output": "It looks like that should be an anomaly."
  },
  {
    "index": "F17887",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、アノマリー検出のアルゴリズムが何をするか、見てみよう。",
    "output": "But let's see what the anomaly detection algorithm will do."
  },
  {
    "index": "F17888",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはリーズナブルに高い確率の範囲で、これまで見た手本の集団からそれ程離れた場所では無い。一方、メモリ使用量に関しては、ここの点は0.5のあたりだが、一方でメモリ使用量は、1.5くらいで、それはここ。",
    "output": "Well, for the CPU load, it puts it at around there 0.5 and this reasonably high probability is not that far from other examples we've seen, maybe, whereas, for the memory use, this appointment, 0.5, whereas for the memory use, it's about 1.5, which is there."
  },
  {
    "index": "F17889",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これもまた、ガウス分布の尻尾の方ではあるが、でもこの値とこの値は、観測されているその他のたくさんの手本と比べても、そんなに違う物では無い。つまりp(x1)はかなり大きくなる、リーズナブルな程度には大きくなるだろうし、p(x2)もリーズナブルな程度には大きい。",
    "output": "Again, you know, it's all to us, it's not terribly Gaussian, but the value here and the value here is not that different from many other examples we've seen, and so P of X 1, will be pretty high, reasonably high."
  },
  {
    "index": "F17890",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この右側のプロットを見ると、この、ここの点は、そんなに悪くは見えないし、そしてこのプロットを見るとここのバッテンを見ると、そんなに悪くも見えない。",
    "output": "I mean, if you look at this plot right, this point here, it doesn't look that bad, and if you look at this plot, you know across here, doesn't look that bad."
  },
  {
    "index": "F17891",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、もっとメモリ使用量が多い手本もある訳だし、また、もっと少ないCPU使用量の手本もある。つまり、この手本はそんなにアノマリーっぽくは見えない。",
    "output": "I mean, I have had examples with even greater memory used, or with even less CPU use, and so this example doesn't look that anomalous."
  },
  {
    "index": "F17892",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、アノマリー検出のアルゴリズムはこの点をアノマリーだとフラグ付けするのに失敗するだろう。",
    "output": "And so, an anomaly detection algorithm will fail to flag this point as an anomaly."
  },
  {
    "index": "F17893",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの隣の円は、一段低い確率となり、そしてここの手本は、さらに低い確率となり、そして、ここの緑のバッテンは、かなり高い確率となってしまう。そして具体的には、この領域は全て、私が円でくくってる直線上の点は全て、だいたい同じ確率だ、と考える。",
    "output": "And it turns out what our anomaly detection algorithm is doing is that it is not realizing that this blue ellipse shows the high probability region, is that, one of the thing is that, examples here, a high probability, and the examples, the next circle of from a lower probably, and examples here are even lower probability, and somehow, here are things that are, green cross there, it's pretty high probability, and in particular, it tends to think that, you know, everything in this region, everything on the line that I'm circling over, has, you know, about equal probability, and it doesn't realize that something out here actually has much lower probability than something over there."
  },
  {
    "index": "F17894",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで、これを修正する為に、修正版のアノマリー検出のアルゴリズムを開発していこう、多変量ガウス分布とか、多変量正規分布と呼ばれる物を用いて。",
    "output": "So, in order to fix this, we can, we're going to develop a modified version of the anomaly detection algorithm, using something called the multivariate Gaussian distribution also called the multivariate normal distribution."
  },
  {
    "index": "F17895",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが、我らがやる事だ。",
    "output": "So here's what we're going to do."
  },
  {
    "index": "F17896",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らはフィーチャーとしてRnのxを持つ。p(x1),p(x2)と別々にする代わりに、p(x)、という風に全部をいっぺんにモデリングする。",
    "output": "We have features x which are in Rn and instead of P of X 1, P of X 2, separately, we're going to model P of X, all in one go, so model P of X, you know, all at the same time."
  },
  {
    "index": "F17897",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、多変量ガウス分布のパラメータとしては、まずミュー、これはベクトル。そしてシグマ、これはn掛けるn行列で、共分散行列とも呼ばれる。",
    "output": "So the parameters of the multivariate Gaussian distribution are mu, which is a vector, and sigma, which is an n by n matrix, called a covariance matrix, and this is similar to the covariance matrix that we saw when we were working with the PCA, with the principal components analysis algorithm."
  },
  {
    "index": "F17898",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが完全を期して、多変量ガウス分布の式を書いてみよう。",
    "output": "For the second complete is, let me just write out the formula for the multivariate Gaussian distribution."
  },
  {
    "index": "F17899",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここのこれ、シグマの絶対値、ここのこれ、この記号を書くと、これはシグマの行列式(determinant)と呼ばれる物で、これは行列に対する、数学的な関数だ。",
    "output": "So we say that probability of X, and this is parameterized by my parameters mu and sigma that the probability of x is equal to once again there's absolutely no need to memorize this formula."
  },
  {
    "index": "F17900",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてあなたは、行列の行列式が何かを実際にしっている必要は全くない。",
    "output": "You know, you can look it up whenever you need to use it, but this is what the probability of X looks like."
  },
  {
    "index": "F17901",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたが実際に知る必要があるのは、それをOctaveで計算するにはコマンドdet(Sigma)を使う、という事だけだ。",
    "output": "And this thing here, the absolute value of sigma, this thing here when you write this symbol, this is called the determent of sigma and this is a mathematical function of a matrix and you really don't need to know what the determinant of a matrix is, but really all you need to know is that you can compute it in octave by using the octave command DET of sigma."
  },
  {
    "index": "F17902",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "よし。",
    "output": "Okay, and again, just be clear, alright?"
  },
  {
    "index": "F17903",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この式において、これらのシグマ、これらは単なるn掛けるn行列だ。",
    "output": "In this expression, these sigmas here, these are just n by n matrix."
  },
  {
    "index": "F17904",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは和の記号じゃない。そしてこのシグマは、n掛けるn行列。",
    "output": "This is not a summation and you know, the sigma there is an n by n matrix."
  },
  {
    "index": "F17905",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以上がp(x)の式だ。だが、もっと興味深い事としては、あるいはもっと重要な事としては、p(x)は実際に、どんな感じになるのだろうか?",
    "output": "So that's the formula for P of X, but it's more interestingly, or more importantly, what does P of X actually looks like?"
  },
  {
    "index": "F17906",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多変量のガウス分布の例を幾つか見ていこう。",
    "output": "Lets look at some examples of multivariate Gaussian distributions."
  },
  {
    "index": "F17907",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、二次元の例を考えよう。n=2なら、二つのフィーチャー、x1とx2を持つ。",
    "output": "So let's take a two dimensional example, say if I have N equals 2, I have two features, X 1 and X 2."
  },
  {
    "index": "F17908",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ミューをイコール0にセットして、シグマをこの行列としよう。",
    "output": "Lets say I set MU to be equal to 0 and sigma to be equal to this matrix here."
  },
  {
    "index": "F17909",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "対角成分が1で、非対角成分が0。この行列はまた、単位行列と呼ばれる事もある。",
    "output": "With 1s on the diagonals and 0s on the off-diagonals, this matrix is sometimes also called the identity matrix."
  },
  {
    "index": "F17910",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合、pのxはこんな見た目となる。そしてこの図で私が示しているのは、ある特定のx1の値とある特定のx2の値の時に、この表面の高さ、これがpのxの値だ。",
    "output": "In that case, p of x will look like this, and what I'm showing in this figure is, you know, for a specific value of X1 and for a specific value of X2, the height of this surface the value of p of x."
  },
  {
    "index": "F17911",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのパラメータの設定では、x1とx2がイコール0の時もっとも高くなる、つまりそこがガウス分布のピークとなり、そして確率分布は、こんな感じの二次元ガウス分布として、あるいはこの二次元のベル型の平面として減衰していく。",
    "output": "And so with this setting the parameters p of x is highest when X1 and X2 equal zero 0, so that's the peak of this Gaussian distribution, and the probability falls off with this sort of two dimensional Gaussian or this bell shaped two dimensional bell-shaped surface."
  },
  {
    "index": "F17912",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この下には、同じ事を代わりに等高線プロットを用いてプロットしてみた、等高線は様々な色を使う事で表現される。つまりこの真中の、赤が激しく集中している所は、もっとも高い値に対応している。",
    "output": "Down below is the same thing but plotted using a contour plot instead, or using different colors, and so this heavy intense red in the middle, corresponds to the highest values, and then the values decrease with the yellow being slightly lower values the cyan being lower values and this deep blue being the lowest values so this is really the same figure but plotted viewed from the top instead, using colors instead."
  },
  {
    "index": "F17913",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、この分布では、確率の大半は0,0のそばにあり、そして0,0から外に出て行くと、x1とx2の確率は下がっていく。",
    "output": "And so, with this distribution, you see that it faces most of the probability near 0,0 and then as you go out from 0,0 the probability of X1 and X2 goes down."
  },
  {
    "index": "F17914",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでパラメータの幾つかを変更していき、何が起こるか見ていこう。",
    "output": "Now lets try varying some of the parameters and see what happens."
  },
  {
    "index": "F17915",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シグマを変えてみよう。シグマをちょっと縮めてみよう。",
    "output": "So let's take sigma and change it so let's say sigma shrinks a little bit."
  },
  {
    "index": "F17916",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シグマは共分散行列なので、フィーチャーx1,x2の分散、または変わりやすさを測った物だ。",
    "output": "Sigma is a covariance matrix and so it measures the variance or the variability of the features X1 X2."
  },
  {
    "index": "F17917",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからシグマを縮めると、得られるのは、、、得られるのは、このコブの幅が減少し、高さもちょっとだけ増加する。何故なら、平面の下の体積は1だから。",
    "output": "So if the shrink sigma then what you get is what you get is that the width of this bump diminishes and the height also increases a bit, because the area under the surface is equal to 1."
  },
  {
    "index": "F17918",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり平面の下の体積の積分結果は、イコール1だ。何故なら確率分布の積分は1にならなくてはいけないから。",
    "output": "So the integral of the volume under the surface is equal to 1, because probability distribution must integrate to one."
  },
  {
    "index": "F17919",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもし分散を縮めると、それはシグマ二乗を縮める事に相当し、より狭い分布が得られる、そしてその背の高さはちょっとだけ高くなる。",
    "output": "But, if you shrink the variance, it's kinda like shrinking sigma squared, you end up with a narrower distribution, and one that's a little bit taller."
  },
  {
    "index": "F17920",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして見て分かるように、同心の楕円もちょっとだけ縮む。",
    "output": "And so you see here also the concentric ellipsis has shrunk a little bit."
  },
  {
    "index": "F17921",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方で、対照的に、もしシグマの対角成分を2,2に増加させると、するとこれは単位行列の2倍となり、もっと幅広くて、もっと平坦なガウス分布が得られる。",
    "output": "Whereas in contrast if you were to increase sigma to 2 2 on the diagonals, so it is now two times the identity then you end up with a much wider and much flatter Gaussian."
  },
  {
    "index": "F17922",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この幅はもっと広くなる。",
    "output": "And so the width of this is much wider."
  },
  {
    "index": "F17923",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは見づらいが、これもまだベル型のコブになっていて、ただ凄い平坦になっているだけだ、それはより幅広くなった、つまり、x1とx2の分散、あるいは変わりやすさは単により広くなった。",
    "output": "This is hard to see but this is still a bell shaped bump, it's just flattened down a lot, it has become much wider and so the variance or the variability of X1 and X2 just becomes wider."
  },
  {
    "index": "F17924",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今度は、一度にシグマの要素の一方だけを変更していこう。",
    "output": "Now lets try varying one of the elements of sigma at the time."
  },
  {
    "index": "F17925",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シグマのこっちを0.6にこっちを1にしてみよう。こうすると、最初のフィーチャーx1の分散が減少し、同時にフィーチャーx2の分散は同一に保たれている。",
    "output": "What this does, is this reduces the variance of the first feature, X 1, while keeping the variance of the second feature X 2, the same."
  },
  {
    "index": "F17926",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、このパラメータの設定では、こんな物をモデリング出来る。",
    "output": "And so with this setting of parameters, you can model things like that."
  },
  {
    "index": "F17927",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方、もし私がこの行列、2,1をセットすると、今度は以下のような手本をモデリングする事が出来る。それはx1が広い範囲の値をとり、一方x2は相対的に狭い範囲の値を、取る、というような。",
    "output": "Whereas if I do this, if I set this matrix to 2, 1 then you can also model examples where you know here we'll say X1 can have take on a large range of values whereas X2 takes on a relatively narrower range of values."
  },
  {
    "index": "F17928",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその事は、この図にも反映されている。ここでは、分布はx1から離れるに連れてよりゆっくりと減衰していて、そしてx2が0から離れると急激に減衰している。",
    "output": "And that's reflected in this figure as well, you know where, the distribution falls off more slowly as X 1 moves away from 0, and falls off very rapidly as X 2 moves away from 0."
  },
  {
    "index": "F17929",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして同様に、代わりに行列のこの要素を変更すると、前のスライドと似ているが、違いとしては、ここでいじっている所は、x2がとても小さい範囲の値をとるようになるという事で、つまりここでは、これが0.6だと、x2がオリジナルの例よりもより小さな範囲を取るようになるという事に気づく。一方で、もしシグマをイコール2にセットしたら、それはx2がもっと幅広い範囲の値を取る、という事だ。",
    "output": "And similarly if we were to modify this element of the matrix instead, then similar to the previous slide, except that here where you know playing around here saying that X2 can take on a very small range of values and so here if this is 0.6, we notice now X2 tends to take on a much smaller range of values than the original example, whereas if we were to set sigma to be equal to 2 then that's like saying X2 you know, has a much larger range of values."
  },
  {
    "index": "F17930",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、多変量ガウス分布のクールな事の一つには、データ同士の相関をモデリングするのに使う事が出来る、という事がある。",
    "output": "Now, one of the cool things about the multivariate Gaussian distribution is that you can also use it to model correlations between the data."
  },
  {
    "index": "F17931",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、多変量ガウス分布を用いて、例えばx1とx2が互いに高く相関する傾向にある、という事実をモデリング出来る。",
    "output": "That is we can use it to model the fact that X1 and X2 tend to be highly correlated with each other for example."
  },
  {
    "index": "F17932",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、この共分散行列の非対角成分を変更する事で、異なる種類のガウス分布を得る事が出来る。",
    "output": "So specifically if you start to change the off diagonal entries of this covariance matrix you can get a different type of Gaussian distribution."
  },
  {
    "index": "F17933",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、非対角成分の要素を0.5から0.8に増加させると、この分布が得られる。これはこの、x=yの直線に沿ったより狭い範囲に山があるような分布。",
    "output": "And so as I increase the off-diagonal entries from .5 to .8, what I get is this distribution that is more and more thinly peaked along this sort of x equals y line."
  },
  {
    "index": "F17934",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ここの等高線は、xとyがともに増加する傾向にある、と主張していて、x1が大きくてx2も大きい、という時とx1が小さくてx2も小さい、という時は大きな確率となっている。あるいはその両者の間も高い確率となっている。",
    "output": "And so here the contour says that x and y tend to grow together and the things that are with large probability are if either X1 is large and Y2 is large or X1 is small and Y2 is small."
  },
  {
    "index": "F17935",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの要素、0.8が大きくなると、この種の狭い領域に全ての確率が存在するようなガウス分布が得られ、xはだいたいイコールyとなる。",
    "output": "And as this entry, 0.8 gets large, you get a Gaussian distribution, that's sort of where all the probability lies on this sort of narrow region, where x is approximately equal to y."
  },
  {
    "index": "F17936",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはとても背が高く、薄い分布で、だいたいこの直線に沿った、xがyと近い領域を中心とした直線に沿った。",
    "output": "This is a very tall, thin distribution you know line mostly along this line central region where x is close to y."
  },
  {
    "index": "F17937",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "逆に、もしこれらの値を負の値にセットすると、これを-0.5から-0.8まで減少させていくと、以下のようなモデルが得られる:確率の多くをいわゆるx1とx2が負の相関となっている範囲に置くようなモデル。つまり、確率のほとんどが、今度はこの範囲に存在し、そこはx1がだいたいイコール-x2となっている、x1=x2の代わりに。",
    "output": "In contrast if we set these to negative values, as I decreases it to -.5 down to -.8, then what we get is a model where we put most of the probability in this sort of negative X one in the next 2 correlation region, and so, most of the probability now lies in this region, where X 1 is about equal to -X 2, rather than X 1 equals X 2."
  },
  {
    "index": "F17938",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これは、いわゆるx1とx2の間の負の相関を捉えている。",
    "output": "And so this captures a sort of negative correlation between x1 and x2."
  },
  {
    "index": "F17939",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、あなたも多変量ガウス分布が捉える事が出来る、様々な分布が、感覚的につかめただろうか。",
    "output": "And so this is a hopefully this gives you a sense of the different distributions that the multivariate Gaussian distribution can capture."
  },
  {
    "index": "F17940",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、ミューの値を変えていくと、分布のピークが変わっていって、もしミューがイコール、0,0.5なら、ピークはちょうどx1=0でx2=0.5の所にあり、つまり分布のピーク、あるいは中心は、シフトする。そしてもしミューが1.5,-0.5なら、ふたたび同様に、分布のピークは今度は別の場所にシフトする。",
    "output": "So follow up in varying, the covariance matrix sigma, the other thing you can do is also, vary the mean parameter mu, and so operationally, we have mu equal 0 0, and so the distribution was centered around X 1 equals 0, X2 equals 0, so the peak of the distribution is here, whereas, if we vary the values of mu, then that varies the peak of the distribution and so, if mu equals 0, 0.5, the peak is at, you know, X1 equals zero, and X2 equals 0.5, and so the peak or the center of this distribution has shifted, and if mu was 1.5 minus 0.5 then OK, and similarly the peak of the distribution has now shifted to a different location, corresponding to where, you know, X1 is 1.5 and X2 is -0.5, and so varying the mu parameter, just shifts around the center of this whole distribution."
  },
  {
    "index": "F17941",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら全ての別々の図を見る事で、多変量のガウス分布で捉える事の出来る、確率分布の感じがつかめたかな。",
    "output": "So, hopefully, looking at all these different pictures gives you a sense of the sort of probability distributions that the Multivariate Gaussian Distribution allows you to capture."
  },
  {
    "index": "F17942",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそのキーとなる利点としては、それを用いると二つの別々のフィーチャーが正の相関を持っている、とか負の相関を持っている、という事が期待される時にそれらを捕捉出来るようになる、という事だ。",
    "output": "And the key advantage of it is it allows you to capture, when you'd expect two different features to be positively correlated, or maybe negatively correlated."
  },
  {
    "index": "F17943",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオでは、多変量ガウス分布を、アノマリー検出に適用する。",
    "output": "In the next video, we'll take this multivariate Gaussian distribution and apply it to anomaly detection."
  },
  {
    "index": "F17944",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオで多変量ガウス分布を議論した。そしてパラメータのミューとシグマを変えていく事でモデリング出来る分布の例を幾つか見た。",
    "output": "In the last video we talked about the Multivariate Gaussian Distribution and saw some examples of the sorts of distributions you can model, as you vary the parameters, mu and sigma."
  },
  {
    "index": "F17945",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、それらのアイデアを用いて、別のアノマリー検出のアルゴリズムを開発していこう。",
    "output": "In this video, let's take those ideas, and apply them to develop a different anomaly detection algorithm."
  },
  {
    "index": "F17946",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "復習しておくと、多変量ガウス分布、またの名を多変量正規分は、二つのパラメータ、ミューとシグマを持っている。",
    "output": "To recap the multivariate Gaussian distribution and the multivariate normal distribution has two parameters, mu and sigma."
  },
  {
    "index": "F17947",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでミューはn次元ベクトルで、シグマ、共分散行列は、n掛けるn行列。",
    "output": "Where mu this an n dimensional vector and sigma, the covariance matrix, is an n by n matrix."
  },
  {
    "index": "F17948",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれがxの確率の式である、ミューとシグマでパラメトライズされた。そしてミューとシグマを変更していく事で、あなたは様々な範囲の分布を得る事が出来る、例えばこれら三つの例は我らが前回のビデオで見た、そんな物の中の一例だ。",
    "output": "And here's the formula for the probability of X, as parameterized by mu and sigma, and as you vary mu and sigma, you can get a range of different distributions, like, you know, these are three examples of the ones that we saw in the previous video."
  },
  {
    "index": "F17949",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではパラメータのフィッティングについて議論しよう、言い換えるとパラメータ推計の問題について。",
    "output": "So let's talk about the parameter fitting or the parameter estimation problem."
  },
  {
    "index": "F17950",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "問いは、いつも通り、x1からxmまでの手本の集合があったとして、そしてここではこれらの手本はおのおのn次元ベクトルで、そしてこの手本は多変量ガウス分布の分布に従っている、と思ったとする。",
    "output": "The question, as usual, is if I have a set of examples X1 through XM and here each of these examples is an n dimensional vector and I think my examples come from a multivariate Gaussian distribution."
  },
  {
    "index": "F17951",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうやってパラメータ、ミューとシグマを推計したらいいだろうか?",
    "output": "How do I try to estimate my parameters mu and sigma?"
  },
  {
    "index": "F17952",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらを推計する標準的な公式は、ミューにトレーニング手本の平均をセットすれば良い。",
    "output": "Well the standard formulas for estimating them is you set mu to be just the average of your training examples."
  },
  {
    "index": "F17953",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは実際に、PCA、あるいは主成分分析を使った時に我らが書き下した物と同様の物だ。",
    "output": "And this is actually just like the sigma that we had written out, when we were using the PCA or the Principal Components Analysis algorithm."
  },
  {
    "index": "F17954",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、これら二つの式に代入するだけで、推計されたパラメータ、ミューと、推計されたパラメータのシグマが得られる。",
    "output": "So you just plug in these two formulas and this would give you your estimated parameter mu and your estimated parameter sigma."
  },
  {
    "index": "F17955",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりデータセットが与えられた時に、ミューとシグマを推計する方法だ。",
    "output": "So given the data set here is how you estimate mu and sigma."
  },
  {
    "index": "F17956",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこの手法をアノマリー検出のアルゴリズムに組み込んでみよう。",
    "output": "Let's take this method and just plug it into an anomaly detection algorithm."
  },
  {
    "index": "F17957",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、どうやったら我らはこれらを全て組み合わせてアノマリー検出のアルゴリズムを開発出来るだろうか?",
    "output": "So how do we put all of this together to develop an anomaly detection algorithm?"
  },
  {
    "index": "F17958",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らがやるべきは、こうだ。",
    "output": "Here 's what we do."
  },
  {
    "index": "F17959",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず、トレーニングセットに対し、モデルをフィッティングする、p(x)をフィッティングする、前のスライドで記述したミューとシグマを設定する事で。",
    "output": "First we take our training set, and we fit the model, we fit P of X, by, you know, setting mu and sigma as described on the previous slide."
  },
  {
    "index": "F17960",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、新しい手本xが与えられた時に、テストの手本が与えられたとすると、、、前の例から、新しい手本をここに得たとしよう。",
    "output": "Next when you are given a new example X. So if you are given a test example, lets take an earlier example to have a new example out here."
  },
  {
    "index": "F17961",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが私のテスト手本。",
    "output": "And that is my test example."
  },
  {
    "index": "F17962",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "新しい手本xが与えられた時に、我らがやる事は、p(x)を計算する事、この多変量ガウス分布の式を用いる事で。",
    "output": "Given the new example X, what we are going to do is compute P of X, using this formula for the multivariate Gaussian distribution."
  },
  {
    "index": "F17963",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、p(x)がとても小さかったら、それをアノマリーとフラグ立てする。他方、p(x)がパラメータのイプシロンより大きければ、それにはアノマリーのフラグは立てない。",
    "output": "And then, if P of X is very small, then we flagged it as an anomaly, whereas, if P of X is greater than that parameter epsilon, then we don't flag it as an anomaly."
  },
  {
    "index": "F17964",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり結局、もしこのデータセットに多変量ガウス分布をフィッティングするとすると、このデータセットというのは赤のクロスで、緑の手本は含まないが、そうすると結局、以下のようなガウス分布が得られる:中心の領域にたくさんの確率を、ここにはちょっとだけ低い確率を、ここにはさらにちょっとだけ低い確率を、ここにはさらにちょっとだけ低い確率を、そしてここより外はとても低い確率となるような物だ。",
    "output": "So it turns out, if we were to fit a multivariate Gaussian distribution to this data set, so just the red crosses, not the green example, you end up with a Gaussian distribution that places lots of probability in the central region, slightly less probability here, slightly less probability here, slightly less probability here, and very low probability at the point that is way out here."
  },
  {
    "index": "F17965",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この例で多変量ガウス分布を適用すると、これは実際にただしくこの手本をフラグ立てする、アノマリーだと。",
    "output": "And so, if you apply the multivariate Gaussian distribution to this example, it will actually correctly flag that example. as an anomaly."
  },
  {
    "index": "F17966",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、多変量ガウス分布モデルと元のモデルとの間の関係について2,3言っておく価値があるだろう、ここで元のモデルとは、p(x)をp(x1),p(x2),...,p(xn)までの積でモデリングしたもの。",
    "output": "Finally it's worth saying a few words about what is the relationship between the multivariate Gaussian distribution model, and the original model, where we were modeling P of X as a product of this P of X1, P of X2, up to P of Xn."
  },
  {
    "index": "F17967",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで証明はしないが。だが、多変量ガウス分布のモデルとオリジナルのモデルの間のこの関係を数学的に証明する事が出来る。",
    "output": "It turns out that you can prove mathematically, I'm not going to do the proof here, but you can prove mathematically that this relationship, between the multivariate Gaussian model and this original one."
  },
  {
    "index": "F17968",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より詳細には、オリジナルのモデルは、多変量ガウス分布の、ガウス分布の等高線がいつも軸に沿っている場合に対応している事が知られている。",
    "output": "And in particular, it turns out that the original model corresponds to multivariate Gaussians, where the contours of the Gaussian are always axis aligned."
  },
  {
    "index": "F17969",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これら三つは全てオリジナルのモデルを使ってフィッティングする事が可能なガウス分布の例だ。",
    "output": "So all three of these are examples of Gaussian distributions that you can fit using the original model."
  },
  {
    "index": "F17970",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは結局、多変量ガウス分布のうち、この楕円、分布の等高線が、、、このモデルは実際に多変量ガウス分布の特別なケースに対応している。",
    "output": "It turns out that that corresponds to multivariate Gaussian, where, you know, the ellipsis here, the contours of this distribution--it turns out that this model actually corresponds to a special case of a multivariate Gaussian distribution."
  },
  {
    "index": "F17971",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、この特別なケースとは、p(x)の分布、多変量ガウス分布としてのp(x)の分布を、確率密度関数の、、、確率分布関数の等高線を軸に沿った物に制限した物、として定義する事が出来る。",
    "output": "And in particular, this special case is defined by constraining the distribution of p of x, the multivariate a Gaussian distribution of p of x, so that the contours of the probability density function, of the probability distribution function, are axis aligned."
  },
  {
    "index": "F17972",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり多変量ガウス分布のp(x)として、このような物や、このような物、またはこのような物を、得る事が出来る。",
    "output": "And so you can get a p of x with a multivariate Gaussian that looks like this, or like this, or like this."
  },
  {
    "index": "F17973",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして見て分かるように、これら三つは全て、これらの楕円は、または私が今描いた楕円は、その軸がx1x2軸に沿っている。",
    "output": "And you notice, that in all 3 of these examples, these ellipses, or these ovals that I'm drawing, have their axes aligned with the X1 X2 axes."
  },
  {
    "index": "F17974",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして我らが持っていない物として、角度のある等高線の集合だ。でしょ?",
    "output": "And what we do not have, is a set of contours that are at an angle, right?"
  },
  {
    "index": "F17975",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの事は、シグマがイコール1,1とか0.8,0.8などの例に対応した物で、非対角成分に非0成分が存在していないという事。",
    "output": "And this corresponded to examples where sigma is equal to 1 1, 0.8, 0.8."
  },
  {
    "index": "F17976",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、このモデルは実際に多変量ガウス分布に制約を加えた物である、という事を数学的に証明出来る事が知られている。",
    "output": "So, it turns out that it's possible to show mathematically that this model actually is the same as a multivariate Gaussian distribution but with a constraint."
  },
  {
    "index": "F17977",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその制約とは、共分散行列のシグマは非対角成分が0でなくてはならない、とい事だ。",
    "output": "And the constraint is that the covariance matrix sigma must have 0's on the off diagonal elements."
  },
  {
    "index": "F17978",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に言うと、共分散行列シグマはここのこれは、シグマ二乗1、シグマ二乗2,、、、とシグマ二乗nまでと、そしてそれ以外の全ての非対角成分の要素は、行列の、対角線より上側のこれら全てと、下側のこれら全て、それら全てが、ゼロとなる。",
    "output": "In particular, the covariance matrix sigma, this thing here, it would be sigma squared 1, sigma squared 2, down to sigma squared n, and then everything on the off diagonal entries, all of these elements above and below the diagonal of the matrix, all of those are going to be zero."
  },
  {
    "index": "F17979",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際、もしこれらのシグマの値、シグマ二乗の1,シグマ二乗の2,...とシグマ二乗のnまでの値を、ここに代入して、この共分散行列に代入すると、二つのモデルは、実際に同一となる。",
    "output": "And in fact if you take these values of sigma, sigma squared 1, sigma squared 2, down to sigma squared n, and plug them into here, and you know, plug them into this covariance matrix, then the two models are actually identical."
  },
  {
    "index": "F17980",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この新しいモデル、多変量ガウス分布を用いたこのモデルは、古いモデルと厳密に一致する、もし非対角成分が全てゼロとなるような共分散行列のシグマの場合には。それは具体的には、ガウス分布のうち、この分布関数の等高線が軸に沿っている物に対応している。",
    "output": "That is, this new model, using a multivariate Gaussian distribution, corresponds exactly to the old model, if the covariance matrix sigma, has only 0 elements off the diagonals, and in pictures that corresponds to having Gaussian distributions, where the contours of this distribution function are axis aligned."
  },
  {
    "index": "F17981",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから異なるフィーチャー間の相関をモデリングする事は許されていない。",
    "output": "So you aren't allowed to model the correlations between the diffrent features."
  },
  {
    "index": "F17982",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりその意味で、オリジナルのモデルは、実際に多変量ガウス分布のモデルの特別な場合となっている。",
    "output": "So in that sense the original model is actually a special case of this multivariate Gaussian model."
  },
  {
    "index": "F17983",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、どういう場合に、これら二つのモデルのどちらを使ったらいいか?",
    "output": "So when would you use each of these two models?"
  },
  {
    "index": "F17984",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりあなたはいつ、オリジナルのモデルを使うべきで、そしてあなたはいつ、多変量ガウス分布のモデルを使うべきなのだろうか?",
    "output": "So when would you the original model and when would you use the multivariate Gaussian model?"
  },
  {
    "index": "F17985",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "オリジナルのモデルは、おそらくより頻繁に用いられるだろう、一方で多変量ガウス分布はおそらくより少ししか用いられないだろう、だがそちらの方には、フィーチャー間の相関を捉える事が出来る、という利点がある。",
    "output": "The original model is probably used somewhat more often, and whereas the multivariate Gaussian distribution is used somewhat less but it has the advantage of being able to capture correlations between features."
  },
  {
    "index": "F17986",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、例えば異なるフィーチャー同士、ここではフィーチャーx1,x2としよう、これらのフィーチャー同士の異常な組み合わせの値となるアノマリーを検出したいとしよう、つまり前の例で言うと、CPUロードとメモリ使用量が異常な値の組み合わせとなっているアノマリーの手本があった。もしオリジナルのモデルを使ってこれを捉えたいと思えば、あなたはこんな追加のフィーチャーを作る必要がある、それは例えばx3=x1/x2みたいな。",
    "output": "So suppose you want to capture anomalies where you have different features say where features x1, x2 take on unusual combinations of values so in the earlier example, we had that example where the anomaly was with the CPU load and the memory use taking on unusual combinations of values, if you want to use the original model to capture that, then what you need to do is create an extra feature, such as X3 equals X1/X2, you know equals maybe the CPU load divided by the memory used, or something, and you need to create extra features if there's unusual combinations of values where X1 and X2 take on an unusual combination of values even though X1 by itself and X2 by itself looks like it's taking a perfectly normal value."
  },
  {
    "index": "F17987",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、もしこのような追加のフィーチャーを作る事に喜んで時間を差し出せるなら、オリジナルなモデルはちゃんと機能するだろう。",
    "output": "But if you're willing to spend the time to manually create an extra feature like this, then the original model will work fine."
  },
  {
    "index": "F17988",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、対照的に、多変量ガウス分布のモデルは、自動的にフィーチャー同士の相関を捉える事が出来る。",
    "output": "Whereas in contrast, the multivariate Gaussian model can automatically capture correlations between different features."
  },
  {
    "index": "F17989",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんな利点のうち大きな物としては、オリジナルのモデルの方が計算量的に安上がりだ、という事。これを別の見方をすると、これは非常に大きなnの値に、つまり非常に大きなフィーチャーの数により良くスケールする。",
    "output": "But the original model has some other more significant advantages, too, and one huge advantage of the original model is that it is computationally cheaper, and another view on this is that is scales better to very large values of n and very large numbers of features, and so even if n were ten thousand, or even if n were equal to a hundred thousand, the original model will usually work just fine."
  },
  {
    "index": "F17990",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方、対照的に多変量ガウス分布のモデルでは、例えばここを見ると、行列シグマの逆行列を計算する必要があり、このシグマはn掛けるn行列だ。つまり、シグマで計算する時には、シグマが10万掛ける10万の行列の時には、計算量的にとても高価になってしまう。",
    "output": "Whereas in contrast for the multivariate Gaussian model notice here, for example, that we need to compute the inverse of the matrix sigma where sigma is an n by n matrix and so computing sigma if sigma is a hundred thousand by a hundred thousand matrix that is going to be very computationally expensive."
  },
  {
    "index": "F17991",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、多変量ガウス分布のモデルは、大きなnの値に対しては、相対的には、よりいまいちスケールしない。",
    "output": "And so the multivariate Gaussian model scales less well to large values of N."
  },
  {
    "index": "F17992",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、オリジナルの方のモデルでは、たとえトレーニングセットが相対的に小さくても、うまく機能する事が分かっている。これはp(x)をモデリングする時に用いるラベル無しのトレーニング手本が小さくても、うまく機能する、たとえmが、50とか100でも、たぶんうまく機能する。",
    "output": "And finally for the original model, it turns out to work out ok even if you have a relatively small training set this is the small unlabeled examples that we use to model p of x of course, and this works fine, even if M is, you know, maybe 50, 100, works fine."
  },
  {
    "index": "F17993",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方、多変量ガウシアンでは、アルゴリズムにある種の数学的な性質があって、mはnより大きくないといけない。つまり、手本の数は、フィーチャーの数よりは大きくないといけない。",
    "output": "Whereas for the multivariate Gaussian, it is sort of a mathematical property of the algorithm that you must have m greater than n, so that the number of examples is greater than the number of features you have."
  },
  {
    "index": "F17994",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこれが真で無いと、つまりmがn以下だと、この行列が可逆じゃなくなる、つまりこの行列が特異行列となり、何かしら変更しないと多変量ガウス分布のモデルは、使う事すら出来ない。だけど典型的なルールとしての経験則として私が採用しているのは、私はmがnよりもずっと大きい時しか多変量ガウス分布は使わない、という物。",
    "output": "And there's a mathematical property of the way we estimate the parameters that if this is not true, so if m is less than or equal to n, then this matrix isn't even invertible, that is this matrix is singular, and so you can't even use the multivariate Gaussian model unless you make some changes to it."
  },
  {
    "index": "F17995",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、実際的な物よりちょっと狭い数学的な要求と言える。実際には、私は多変量ガウス分布のモデルは、mがかなりnより大きい時しか使わない。",
    "output": "But a typical rule of thumb that I use is, I will use the multivariate Gaussian model only if m is much greater than n, so this is sort of the narrow mathematical requirement, but in practice, I would use the multivariate Gaussian model, only if m were quite a bit bigger than n."
  },
  {
    "index": "F17996",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばmが10掛けるn以上なら、とかは、リーズナブルな経験則と言えそう。そしてこれを満たしていなければ、、、多変量ガウス分布のモデルがたくさんのパラメータを持っていると、この共分散行列シグマはn掛けるn行列なので、これはだいたいn二乗のパラメータを持つ、これは対称行列だから、実際はn二乗割る2に近いくらいのパラメータだが。",
    "output": "So if m were greater than or equal to 10 times n, let's say, might be a reasonable rule of thumb, and if it doesn't satisfy this, then the multivariate Gaussian model has a lot of parameters, right, so this covariance matrix sigma is an n by n matrix, so it has, you know, roughly n squared parameters, because it's a symmetric matrix, it's actually closer to n squared over 2 parameters, but this is a lot of parameters, so you need make sure you have a fairly large value for m, make sure you have enough data to fit all these parameters."
  },
  {
    "index": "F17997",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでmが10n以上、というのは、この共分散行列を推計出来る為の、リーズナブルな経験則と言えるだろう。",
    "output": "And m greater than or equal to 10 n would be a reasonable rule of thumb to make sure that you can estimate this covariance matrix sigma reasonably well."
  },
  {
    "index": "F17998",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、実践の場では、左に示したオリジナルのモデルの方が、より頻繁に使われる。",
    "output": "So in practice the original model shown on the left that is used more often."
  },
  {
    "index": "F17999",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしあなたが、フィーチャー同士の相関を捉える必要があるんじゃないか、と思っているなら、そういう時は人々はよく、単に人力でそれらの異常な値の組み合わせを捉える追加のフィーチャーをデザインする、という事をする。",
    "output": "And if you suspect that you need to capture correlations between features what people will often do is just manually design extra features like these to capture specific unusual combinations of values."
  }
]