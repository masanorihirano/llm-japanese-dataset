[
  {
    "index": "F18000",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、とても大量のトレーニングセットを持っていて、つまりmがとても大きくて、そしてnがそんなに大きく無いような問題に直面している時は、多変量ガウス分布のモデルは、検討してみる価値があり、よりうまく機能する事もあり、異常なフィーチャーの値の組を捉える結果となるような追加のフィーチャーを人力で作るのに時間を費やさずに済ます事が出来るかもしれない。",
    "output": "But in problems where you have a very large training set or m is very large and n is not too large, then the multivariate Gaussian model is well worth considering and may work better as well, and can save you from having to spend your time to manually create extra features in case the anomalies turn out to be captured by unusual combinations of values of the features."
  },
  {
    "index": "F18001",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、いくらかテクニカルな性質だが、ちょっと簡単に触れておきたい事がある。多変量ガウス分布のモデルをフィッティングする時に、もし共分散行列のシグマが特異行列だと見出したら、あるいはそれが非可逆だと見出したら、それには普通、二つの場合がある。",
    "output": "Finally I just want to briefly mention one somewhat technical property, but if you're fitting multivariate Gaussian model, and if you find that the covariance matrix sigma is singular, or you find it's non-invertible, they're usually 2 cases for this."
  },
  {
    "index": "F18002",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つ目は、このmがnより大きい、という条件を満たしていない場合。",
    "output": "One is if it's failing to satisfy this m greater than n condition, and the second case is if you have redundant features."
  },
  {
    "index": "F18003",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二番目は、冗長なフィーチャーがある場合。",
    "output": "So by redundant features, I mean, if you have 2 features that are the same."
  },
  {
    "index": "F18004",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "冗長なフィーチャーという言葉で私が意味している事は、同一のフィーチャーが二つある、という事。",
    "output": "Somehow you accidentally made two copies of the feature, so your x1 is just equal to x2."
  },
  {
    "index": "F18005",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どうにかして、偶然二つのコピーのフィーチャーを作ってしまった、つまりx1は単に、イコールx2であるようなフィーチャーを作ってしまった、だとか、あるいは例えばフィーチャーx3=フィーチャーx4+フィーチャーx5のような冗長なフィーチャーがあるという場合。",
    "output": "Or if you have redundant features like maybe your features X3 is equal to feature X4, plus feature X5."
  },
  {
    "index": "F18006",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "オーケー。で、このようなとても冗長なフィーチャーがある場合、もしx3=x4+x5なら、x3は何も追加の情報を含んでいない。",
    "output": "Okay, so if you have highly redundant features like these, you know, where if X3 is equal to X4 plus X5, well X3 doesn't contain any extra information, right?"
  },
  {
    "index": "F18007",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単にこれら二つの別のフィーチャーを、足し合わせるだけだ。",
    "output": "You just take these 2 other features, and add them together."
  },
  {
    "index": "F18008",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしあなたがこの手の冗長な、重複したフィーチャーとかこの種のフィーチャーを保持している時には、シグマは非可逆になってしまう。",
    "output": "And if you have this sort of redundant features, duplicated features, or this sort of features, than sigma may be non-invertible."
  },
  {
    "index": "F18009",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上はデバッギングの時の豆知識だ。これは本当に稀にしか起こらないはずで、だからあなたが鉢合わせする事も多分無いだろう。",
    "output": "And so there's a debugging set-- this should very rarely happen, so you probably won't run into this, it is very unlikely that you have to worry about this-- but in case you implement a multivariate Gaussian model you find that sigma is non-invertible."
  },
  {
    "index": "F18010",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがあなたが多変量ガウス分布のモデルを実装してみて、シグマが非可逆だという事を発見したら、私だったらまず、mがnよりもずっと大きい事を確認し、もしそうであるなら、次に二番目に私がやる事は、冗長なフィーチャーをチェックする事だ。",
    "output": "What I would do is first make sure that M is quite a bit bigger than N, and if it is then, the second thing I do, is just check for redundant features."
  },
  {
    "index": "F18011",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、もし単純に等しい二つのフィーチャーがある時には、一方を取り除けば良い。そしてもしx3=x4+x5のような冗長なフィーチャーがある時には、その冗長なフィーチャーを取り除けば良い。",
    "output": "And so if there are 2 features that are equal, just get rid of one of them, or if you have redundant if these , X3 equals X4 plus X5, just get rid of the redundant feature, and then it should work fine again."
  },
  {
    "index": "F18012",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "線形代数のエキスパートの聴衆の為にちょっと脇道に逸れると、冗長なフィーチャー、という言葉で私が意味している事の正式な用語は、フィーチャーが線形に従属している、という事。",
    "output": "As an aside for those of you who are experts in linear algebra, by redundant features, what I mean is the formal term is features that are linearly dependent."
  },
  {
    "index": "F18013",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが実践に際しては、それが実際に意味している事は、これらの問題の一つがアルゴリズムをつまづかせていたら、あなたはフィーチャーを冗長では無くすだけで、シグマが非可逆、という問題を解決出来るはずだ。",
    "output": "But in practice what that really means is one of these problems tripping up the algorithm if you just make you features non-redundant., that should solve the problem of sigma being non-invertable."
  },
  {
    "index": "F18014",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもう一度繰り返しておくとあなたが生涯でこの問題に遭遇するという事象に対するオッズは極めて低い。だから、おそらくあなたは、mがnより大きい、という事さえ注意しておけば、多変量ガウス分布のモデルをシグマが非可逆である、という事を心配する事無く適用する事が出来るだろう。",
    "output": "But once again the odds of your running into this at all are pretty low so chances are, you can just apply the multivariate Gaussian model, without having to worry about sigma being non-invertible, so long as m is greater than or equal to n."
  },
  {
    "index": "F18015",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以上が多変量ガウス分布によるアノマリー検出だ。",
    "output": "So that's it for anomaly detection, with the multivariate Gaussian distribution."
  },
  {
    "index": "F18016",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの手法を適用すれば、あなたのフィーチャー間での正や負の相関を自動で捉えるようなアノマリー検出のアルゴリズムを得る事が出来、そしてフィーチャーの値の組み合わせが異常なのを目撃したら、アノマリーだとフラグを立てる事が出来る。",
    "output": "And if you apply this method you would be able to have an anomaly detection algorithm that automatically captures positive and negative correlations between your different features and flags an anomaly if it sees is unusual combination of the values of the features."
  },
  {
    "index": "F18017",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオでは、リコメンダーシステムの問題について議論した。そこでは例えば、映画の集合があって、ユーザーの集合があって、それぞれのユーザーが映画の部分集合をレーティングする。",
    "output": "In the last video, we talked about the recommender systems problem where for example you might have a set of movies and you may have a set of users, each who have rated some subset of the movies."
  },
  {
    "index": "F18018",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "映画を星一つから星5までとか、または星0から星5までのように。",
    "output": "They've rated the movies one to five stars or zero to five stars."
  },
  {
    "index": "F18019",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてやりたい事はこれらのユーザーを見る事で、彼らがまだレーティングしていない映画についてどうレーティングするかを予測したい。",
    "output": "And what we would like to do is look at these users and predict how they would have rated other movies that they have not yet rated."
  },
  {
    "index": "F18020",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、リコメンダーシステムを作り上げる最初のアプローチについて議論する。",
    "output": "In this video I'd like to talk about our first approach to building a recommender system."
  },
  {
    "index": "F18021",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このアプローチはコンテントベースのリコメンデーションと言われる物だ。",
    "output": "This approach is called content based recommendations."
  },
  {
    "index": "F18022",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回から引き続き、これがデータセットで、ちょっと記法を再度説明しておくと、私はnuで、ユーザーの数を表す事にしていた。だからそれはイコール4だ。",
    "output": "Here's our data set from before and just to remind you of a bit of notation, I was using nu to denote the number of users and so that's equal to 4, and nm to denote the number of movies, I have 5 movies."
  },
  {
    "index": "F18023",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、どうやってこの欠けた値を予測出来るか?",
    "output": "So, how do I predict what these missing values would be?"
  },
  {
    "index": "F18024",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの映画に、それぞれフィーチャーの集合があると仮定しよう。",
    "output": "Let's suppose that for each of these movies I have a set of features for them."
  },
  {
    "index": "F18025",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に、今回は各映画に二つのフィーチャーがあるとしよう。それぞれx1とx2で表す事にする。",
    "output": "In particular, let's say that for each of the movies have two features which I'm going to denote x1 and x2."
  },
  {
    "index": "F18026",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "x1はその映画がどの位ロマンティックな映画かの指標として。",
    "output": "Where x1 measures the degree to which a movie is a romantic movie and x2 measures the degree to which a movie is an action movie."
  },
  {
    "index": "F18027",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしLoveatlastの映画を選ぶとすると、ロマンスのスケールは0.9レーティングとなっているので、とてもロマンティックな映画だが、アクションのスケールは0なので、映画の中にはほとんどアクションシーンが無い。",
    "output": "So, if you take a movie, Love at last, you know it's 0.9 rating on the romance scale. This is a highly romantic movie, but zero on the action scale."
  },
  {
    "index": "F18028",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Romanceforeverは1.0で、たくさんのロマンスがあって、アクションは0.01。",
    "output": "Romance forever is a 1.0, lot of romance and 0.01 action."
  },
  {
    "index": "F18029",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "良く知らんが、映画の中にちょっとだけ車の衝突とかそういうのがあるのかもね。",
    "output": "I don't know, maybe there's a minor car crash in that movie or something."
  },
  {
    "index": "F18030",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからわずかにアクションがあるのみ。",
    "output": "So there's a little bit of action."
  },
  {
    "index": "F18031",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "飛ばしてswordsvskarate(ソードvs空手)を見てみると、それはロマンスが0のレーティングとなっているので、ロマンスはまったく無いが、たくさんのアクションがある。",
    "output": "Skipping one, let's do Swords vs karate, maybe that has a 0 romance rating and no romance at all in that but plenty of action."
  },
  {
    "index": "F18032",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてnon-stopcarcrashesもほんのちょっとだけロマンスが映画の中にあるみたいだが、ただだいたいはアクションだ。",
    "output": "And Nonstop car chases, maybe again there's a tiny bit of romance in that movie but mainly action."
  },
  {
    "index": "F18033",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてCutepuppiesofloveもだいたいロマンスの映画でアクションは全く無し。",
    "output": "And Cute puppies of love mainly a romance movie with no action at all."
  },
  {
    "index": "F18034",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらのようなフィーチャーがあれば、各映画をフィーチャーのベクトルで表現出来る。",
    "output": "So if we have features like these, then each movie can be represented with a feature vector."
  },
  {
    "index": "F18035",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "映画1を見てみよう。",
    "output": "Let's take movie one."
  },
  {
    "index": "F18036",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの映画を単に映画1,2,3,4そして5とだけ呼ぶ事にする。",
    "output": "So let's call these movies 1, 2, 3, 4, and 5."
  },
  {
    "index": "F18037",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の映画は、Loveatlastだが、そこには二つのフィーチャー、0.9と0がある。",
    "output": "But my first movie, Love at last, I have my two features, 0.9 and 0."
  },
  {
    "index": "F18038",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらがフィーチャーx1とx2で、そしてさらにいつも通り追加のフィーチャーを足そう、切片項であるフィーチャーx0だ。",
    "output": "And let's add an extra feature as usual, which is my interceptor feature x0 = 1."
  },
  {
    "index": "F18039",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上をあわせると、フィーチャーx1が得られる、ここでこの上付き添字の1は、それが最初の映画のフィーチャーである事を表していて、このフィーチャーベクトルはイコール、1と、、、ここでこの最初の1は切片項だが、そして二つのフィーチャー、0.9と0、となる。",
    "output": "And so putting these together I would then have a feature x1."
  },
  {
    "index": "F18040",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、Loveatlastに関しては、フィーチャーベクトルx1があり、映画、RomanceForeverに関しては別個のフィーチャーベクトルであるx2がある、などなど。",
    "output": "The superscript 1 denotes it's the feature vector for my first movie, and this feature vector is equal to 1."
  },
  {
    "index": "F18041",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてSwordsvs.karateではまた、別のフィーチャーベクトルであるxに上付き添字5という物が対応する。",
    "output": "So for Love at last I would have a feature vector x1, for the movie Romance forever I may have a software feature of vector x2, and so on, and for Swords vs karate I would have a different feature vector x superscript 5."
  },
  {
    "index": "F18042",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "また、以前の記法と一貫させる為にnをフィーチャーの数とする、そしてこれはx0、つまり切片項はカウントしない。",
    "output": "Also, consistence with our earlier node notation that we were using, we're going to set n to be the number of features not counting this x0 interceptor."
  },
  {
    "index": "F18043",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、n=2だ。何故なら、フィーチャーは二つ、x1とx2で、それが各映画のロマンス度合いとアクション度合いを捕捉しているのだから。",
    "output": "So n is equal to 2 because it's we have two features x1 and x2 capturing the degree of romance and the degree of action in each movie."
  },
  {
    "index": "F18044",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、予測を行う為に出来る事として、こんな事が考えられる。それは各ユーザーのレーティングを予測する事を、独立した線形回帰の問題と扱う事だ。",
    "output": "Now in order to make predictions here's one thing that we do which is that we could treat predicting the ratings of each user as a separate linear regression problem."
  },
  {
    "index": "F18045",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、各ユーザーjに対しパラメータベクトルであるシータjを学習する、これはこの場合、Rの3だ。",
    "output": "So specifically, let's say that for each user j, we're going to learn the parameter vector theta j, which would be an R3 in this case."
  },
  {
    "index": "F18046",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的には、シータのjはRのn+1で、ここでnはフィーチャーの数のうち、切片項を考慮に入れない物だ。",
    "output": "More generally, theta (j) would be an R (n+1), where n is the number of features not counting the set term."
  },
  {
    "index": "F18047",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてユーザーjがムービーiをレーティングする事を、単にパラメータベクトルであるシータとフィーチャーx(i)の内積で予測する事とする。",
    "output": "And we're going to predict user j as rating movie i with just the inner product between parameters vectors theta and the features xi."
  },
  {
    "index": "F18048",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的に例を見てみよう。",
    "output": "So let's take a specific example."
  },
  {
    "index": "F18049",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてAliceはなんらかのパラメーターのベクトル、シータ1に関連づけられている。",
    "output": "And associated with Alice would be some parameter vector theta 1."
  },
  {
    "index": "F18050",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして二番目のユーザー、Bobは別のパラメータベクトル、シータ2に関連づけられている。",
    "output": "And our second user, Bob, will be associated a different parameter vector theta 2."
  },
  {
    "index": "F18051",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Carolはさらに別のパラメータベクトル、シータ3に、そしてDaveも別のパラメータベクトルシータ4に関連づけられている。",
    "output": "Carol will be associated with a different parameter vector theta 3 and Dave a different parameter vector theta 4."
  },
  {
    "index": "F18052",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、我らは例えばAliceが映画、CutePuppiesofloveをどう思うかなどを予測したい。",
    "output": "So let's say you want to make a prediction for what Alice will think of the movie Cute puppies of love."
  },
  {
    "index": "F18053",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その映画は何らかのパラメータベクトルx3を持つ事になる。ここでこのx3はイコール、切片項の1と0.99と0と等しい。",
    "output": "Well that movie is going to have some parameter vector x3 where we have that x3 is going to be equal to 1, which is my intercept term and then 0.99 and then 0."
  },
  {
    "index": "F18054",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらにこの例で、どうにかして既に、Aliceに関するパラメータシータ1を得ているとしよう。",
    "output": "And let's say, for this example, let's say that we've somehow already gotten a parameter vector theta 1 for Alice."
  },
  {
    "index": "F18055",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでは今は単に、何らかの学習アルゴリズムでパラメータベクトルのシータ1を学習したとしよう。そしてその結果は0,5,0だったとする。",
    "output": "But let's just say for now that some unspecified learning algorithm has learned the parameter vector theta 1 and is equal to this 0,5,0."
  },
  {
    "index": "F18056",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとこのエントリの予測はシータ1--これはAliceのパラメータベクトル--これに、x3の転置--これはCutePuppiesofLove、つまり映画3のフィーチャーベクトル--と等しくなる。",
    "output": "So our prediction for this entry is going to be equal to theta 1, that is Alice's parameter vector, transpose x3, that is the feature vector for the Cute puppies of love movie, number 3."
  },
  {
    "index": "F18057",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これら二つのベクトルの内積は、5掛ける0.99となる。それはイコール、4.95。",
    "output": "And so the inner product between these two vectors is gonna be 5 times 0.99, which is equal to 4.95."
  },
  {
    "index": "F18058",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからここの値の予測値は4.95となる。それはなかなか良さそうな値に見える。",
    "output": "And so my prediction for this value over here is going to be 4.95."
  },
  {
    "index": "F18059",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが本当にパラメータベクトルのシータ1ならね。",
    "output": "And maybe that seems like a reasonable value if indeed this is my parameter vector theta 1."
  },
  {
    "index": "F18060",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでやってきた事は、ようするに別々の、本質的には線形回帰を、各ユーザーに適用してきた、という事だ。そしてAliceはあるパラメータベクトルシータ1という物を持っている事にして、それを使って、彼女のレーティングをどれだけロマンティックかどれだけアクションか--その映画が--の関数として、予測した。",
    "output": "So, all we're doing here is we're applying a different copy of this linear regression for each user, and we're saying that what Alice does is Alice has some parameter vector theta 1 that she uses, that we use to predict her ratings as a function of how romantic and how action packed a movie is."
  },
  {
    "index": "F18061",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてBob、Carol、そしてDaveはそれぞれ別の映画に関しての、ロマンティック度合いとアクション度合いに関する別々の線形関数を持つ事になる。そしてそれをもって、彼らの星のレーティングを予測する。",
    "output": "And Bob and Carol and Dave, each of them have a different linear function of the romanticness and actionness, or degree of romance and degree of action in a movie and that that's how we're gonna predict that their star ratings."
  },
  {
    "index": "F18062",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より正式には、これが我らの問題を書き下した物だ。",
    "output": "More formally, here's how we can write down the problem."
  },
  {
    "index": "F18063",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らの記法では、rijはもしユーザーjが映画iをレーティングしていたら1とする。そしてyijはその映画のレーティングがもし存在すればその値とする。",
    "output": "Our notation is that r(i,j) is equal to 1 if user j has rated movie i and y(i,j) is the rating of that movie, if that rating exists."
  },
  {
    "index": "F18064",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、そのユーザーが、もし実際にその映画をレーティングしていたら。",
    "output": "That is, if that user has actually rated that movie."
  },
  {
    "index": "F18065",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして前のスライドで、また我らはシータjを定義した。それは各ユーザーのパラメータだった。",
    "output": "And, on the previous slide we also defined these, theta j, which is a parameter for the user xi, which is a feature vector for a specific movie."
  },
  {
    "index": "F18066",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてxiは個々の映画のフィーチャーベクトルで、そして各ユーザーの各映画ごとにレーティングを、以下のように予測する事が出来る。",
    "output": "And for each user and each movie, we predict that rating as follows."
  },
  {
    "index": "F18067",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一時的にもう一つ、追加の記法mjを導入しよう。",
    "output": "So let me introduce just temporarily introduce one extra bit of notation mj."
  },
  {
    "index": "F18068",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "mjを、映画jをレートしたユーザーの数を示すのに用いる事にする。",
    "output": "We're gonna use mj to denote the number of users rated by movie j. We don't need this notation only for this line."
  },
  {
    "index": "F18069",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、パラメータベクトルのシータjを学習する為にうーん、どうやったらいいかね?",
    "output": "Now in order to learn the parameter vector for theta j, well how do we do so."
  },
  {
    "index": "F18070",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは基本的には線形回帰の問題だ。",
    "output": "This is basically a linear regression problem."
  },
  {
    "index": "F18071",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから可能な手段としては、パラメータベクトルのシータjを、予測された値がトレーニングセットで観測されている値にデータとして観測されている値になるべく近くなるように選ぶ、という事だ。",
    "output": "So what we can do is just choose a parameter vector theta j so that the predicted values here are as close as possible to the values that we observed in our training sets and the values we observed in our data."
  },
  {
    "index": "F18072",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを書き下してみよう。",
    "output": "So let's write that down."
  },
  {
    "index": "F18073",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "パラメータベクトルのシータjを学習する為には、パラメータベクトルのシータjに関して、最小化する事の、和で--ところで和は、ユーザーjがレーティングした映画全てに渡って取る--つまり、和は、以下を満たす全てのiで、そのiとはコロン、rij=1となるi全てだ。",
    "output": "In order to learn the parameter vector theta j, let's minimize over the parameter vector theta j of sum, and I want to sum over all movies that user j has rated."
  },
  {
    "index": "F18074",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの和のインデックスの読み方は、これはrij=1となるような全てのiに渡って和を取る、という事。",
    "output": "So the way to read this summation syntax is this is summation over all the values of i, so the r(i.j) is equal to 1."
  },
  {
    "index": "F18075",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、ユーザーjがレーティングした全ての映画に渡って和を取る事になる。",
    "output": "So you'll be summing over all the movies that user j has rated."
  },
  {
    "index": "F18076",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、シータj転置xiを計算する、これは、ユーザーjが映画iをどうレーティングするかの予測だ。",
    "output": "And then I'm going to compute theta j, transpose x i. So that's the prediction of using j's rating on movie i,- y (i,j)."
  },
  {
    "index": "F18077",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは実際に観測されたレーティング。そして二乗。",
    "output": "So that's the actual observed rating squared."
  },
  {
    "index": "F18078",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、ユーザーjが実際にレーティングした映画の数で割っておく。",
    "output": "And then, let me just divide by the number of movies that user j has actually rated."
  },
  {
    "index": "F18079",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、これは単なる最小二乗法の回帰となった。",
    "output": "And so this is just like the least squares regressions."
  },
  {
    "index": "F18080",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単なる線形回帰だよね。パラメータベクトルのシータjを、この種の二乗誤差項を最小化するように選ぶ、という。",
    "output": "It's just like linear regression, where we want to choose the parameter vector theta j to minimize this type of squared error term."
  },
  {
    "index": "F18081",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしお望みなら、正規化項を足しても良い。つまり足す事の、ラムダ/2m。",
    "output": "And if you want, you can also add in irregularization terms so plus lambda over 2m and this is really 2mj because we have mj examples."
  },
  {
    "index": "F18082",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だってm(j)個の手本があるみたいな物だから。何故ならユーザーjが複数の映画をレーティングしたら、それはパラメータシータjをフィットするデータポイントがそれだけあるみたいな物だから。",
    "output": "User j has rated that many movies, it's not like we have that many data points with which to fit the parameters of theta j."
  },
  {
    "index": "F18083",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここに、通常の正規化項である、シータjkの二乗を足そう。",
    "output": "And then let me add in my usual regularization term here of theta j k squared."
  },
  {
    "index": "F18084",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いつも通り、この和はk=1からnまでに渡って取る。",
    "output": "As usual, this sum is from k equals 1 through n, so here, theta j is going to be an n plus 1 dimensional vector, where in our early example n was equal to 2."
  },
  {
    "index": "F18085",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこのシータjはn+1次元のベクトルとなり、前の例だとnはイコール2だったが、より一般的にはnは各映画の持つフィーチャーの数だ。",
    "output": "But more broadly, more generally n is the number of features we have per movie."
  },
  {
    "index": "F18086",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしていつも通り、シータ0は正規化しない。",
    "output": "And so as usual we don't regularize over theta 0."
  },
  {
    "index": "F18087",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バイアス項は正規化しない。",
    "output": "We don't regularize over the bias terms."
  },
  {
    "index": "F18088",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら和はk=1からnに渡ってとっているから。",
    "output": "The sum is from k equals 1 through n."
  },
  {
    "index": "F18089",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこれをシータjの関数として最小化すると、良い解が得られる、パラメータベクトルのシータjについてとても良い推計が得られて、それを用いてユーザーjの映画のレーティングを予測出来る。",
    "output": "So if you minimize this as a function of theta j you get a good solution, you get a pretty good estimate of a parameter vector theta j with which to make predictions for user j's movie ratings."
  },
  {
    "index": "F18090",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "リコメンダーシステムの場合、この記法をちょっと変更する。",
    "output": "For recommender systems, I'm gonna change this notation a little bit."
  },
  {
    "index": "F18091",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以降の計算を簡略化する為にこのmj項を取り除く。",
    "output": "So to simplify the subsequent math, I with to get rid of this term mj."
  },
  {
    "index": "F18092",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは結局の所、単なる定数だ。でしょ?",
    "output": "So that's just a constant, right?"
  },
  {
    "index": "F18093",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからシータjの値に影響を与えずにこれを削除出来る。",
    "output": "So I can delete it without changing the value of theta j that I get out of this optimization."
  },
  {
    "index": "F18094",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "想像してみよう、この式全体を持ってきて、この式全体にmjを掛けてみる、するとその定数は取り除かれる、そしてそれを最小化した時、得られるシータjは掛ける前と一緒のはずだ。",
    "output": "So if you imagine taking this whole equation, taking this whole expression and multiplying it by mj, get rid of that constant."
  },
  {
    "index": "F18095",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから前のスライドに書いた事を繰り返すと、これが最適化の目的関数だ:シータjを学習する為に、シータjはユーザーjのパラメータだが、そのシータjを学習する為に、この最適化も目的関数をシータjに関して最小化する。",
    "output": "So just to repeat what we wrote on the previous slide, here's our optimization objective. In order to learn theta j which is the parameter for user j, we're going to minimize over theta j of this optimization objectives."
  },
  {
    "index": "F18096",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、通常の二乗誤差項で、これが正規化項だ。",
    "output": "So this is our usual squared error term and then this is our regularizations term."
  },
  {
    "index": "F18097",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、もちろんリコメンダーシステムを作るときには、一人のユーザーだけのパラメータを学習させたい、という事は無い。",
    "output": "Now of course in building a recommender system, we don't just want to learn parameters for a single user."
  },
  {
    "index": "F18098",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは全てのユーザーのパラメータを学習させたい。",
    "output": "We want to learn parameters for all of our users."
  },
  {
    "index": "F18099",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "n下付き添字uのユーザーが居るのだった。だからこれらのパラメータ全てを学習させたい。",
    "output": "I have n subscript u users, so I want to learn all of these parameters."
  },
  {
    "index": "F18100",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから我らがやるのは、この最適化の目的関数に対して、単純に追加のシグマを足すだけ。",
    "output": "And so, what I'm going to do is take this optimization objective and just add the mixture summation there."
  },
  {
    "index": "F18101",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの、ここの式は先頭にはまた1/2があるが、これは上にあるのと、完全に一致する、ただし特定の一ユーザーのシータjについてだけ、これを行う代わりに、ユーザー全員に渡って目的関数を足して、そしてこの最適化の目的関数全体を最小化する。このコスト関数全体を最小化する。",
    "output": "Except that now instead of just doing this for a specific user theta j, I'm going to sum my objective over all of my users and then minimize this overall optimization objective, minimize this overall cost on."
  },
  {
    "index": "F18102",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれをシータ1,シータ2、、、、とシータnuまでの関数として最小化すると、各ユーザーごとに別々のパラメータベクトルが得られ、そしてそれを用いてユーザー全員の、全てのn下付き添字uのユーザーの予測を行う事が出来る。",
    "output": "And when I minimize this as a function of theta 1, theta 2, up to theta nu, I will get a separate parameter vector for each user."
  },
  {
    "index": "F18103",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では全てをつなげると、この上にあるのが、最適化の目的関数だった、これに名前をつけよう、Jのシータ1,点点点シータnu。",
    "output": "And to give this thing a name, I'll just call this J(theta1, ..., theta nu)."
  },
  {
    "index": "F18104",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりJはいつもどおりの最適化の目的関数で、最小化しようとしている対象。",
    "output": "So j as usual is my optimization objective, which I'm trying to minimize."
  },
  {
    "index": "F18105",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、実際に最小化を実行する為に、最急降下法のアップデートルールを導出すると、これらの等式が得られる。",
    "output": "Next, in order to actually do the minimization, if you were to derive the gradient descent update, these are the equations that you would get."
  },
  {
    "index": "F18106",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりシータjのkをとってそこから引く事のアルファ、これはラーニングレートで、掛ける事のこれらの右にある項だ。",
    "output": "So you take theta j, k, and subtract from an alpha, which is the learning rate, times these terms over here on the right."
  },
  {
    "index": "F18107",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "k=0の場合とkが0で無い場合でちょっとだけ異なる場合分けが要る。",
    "output": "So there's slightly different cases when k equals 0 and when k does not equal 0."
  },
  {
    "index": "F18108",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら我らの正規化項はkが0で無い時のシータjのkにだけ存在しているからだ。つまりシータ0を正規化しないので、k=0とkが0以外とでちょっとだけ異なった更新の仕方をする。",
    "output": "Because our regularization term here regularizes only the values of theta jk for k not equal to 0, so we don't regularize theta 0, so with slightly different updates when k equals 0 and k is not equal to 0."
  },
  {
    "index": "F18109",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、例えばこの項は最適化の目的関数のパラメータに関する単なる偏微分に過ぎない。でしょ?",
    "output": "And this term over here, for example, is just the partial derivative with respect to your parameter, that of your optimization objective."
  },
  {
    "index": "F18110",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これは単なる最急降下法で、すでに微分は計算済みで、それをここに代入しただけ。",
    "output": "Right and so this is just gradient descent and I've already computed the derivatives and plugged them into here."
  },
  {
    "index": "F18111",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこれらの最急降下法のアップデートが、線形回帰のそれととても似てると思ったなら、それはこれらが本質的には線形回帰と同じだからだろう。",
    "output": "And if this gradient descent update look a lot like what we have here for linear regression. That's because these are essentially the same as linear regression."
  },
  {
    "index": "F18112",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "唯一の小さな違いとしては、線形回帰なら、これらの1/mの項があった。",
    "output": "The only minor difference is that for linear regression we have these 1 over m terms, this really would've been 1 over mj."
  },
  {
    "index": "F18113",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは実際は1/m(j)だが、でも、前に最適化の目的関数を導出した所で、これを取り除いていた。だからこの1/mの項が無くなっている。",
    "output": "But because earlier when we are deriving the optimization objective, we got rid of this, that's why we don't have this 1 over m term."
  },
  {
    "index": "F18114",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがそれ以外は、ほんとうにトレーニング手本に渡って和をとる事の誤差掛けるxkに、足す事の正規化項の微分への寄与だ。",
    "output": "But otherwise, it's really some of my training examples of the ever times xk plus that regularization term, plus that term of regularization contributes to the derivative."
  },
  {
    "index": "F18115",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから最急降下法を使うとするなら、これがコスト関数Jを最小化するやり方、つまりパラメータを全て学習する方法だ。",
    "output": "And so if you're using gradient descent here's how you can minimize the cost function j to learn all the parameters."
  },
  {
    "index": "F18116",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの微分の式を使ってもしお望みなら、よりアドバンスドな最適化アルゴリズムであるクラスターグラディエントとかLBFGSとかその他なんでもお持ちのアルゴリズムに代入して、同じようにコスト関数Jを最小化する事も出来る。",
    "output": "And using these formulas for the derivative if you want, you can also plug them into a more advanced optimization algorithm, like conjugate gradient or LBFGS or what have you."
  },
  {
    "index": "F18117",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、本質的には線形回帰の一種の何かを使って別々のユーザーの別々の映画へのレーティングを予測するやり方が分かったかな。",
    "output": "And use that to try to minimize the cost function j as well. So hopefully you now know how you can apply essentially a deviation on linear regression in order to predict different movie ratings by different users."
  },
  {
    "index": "F18118",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このアルゴリズムはコンテントベースのリコメンデーション、またはコンテントベースのアプローチと呼ばれている。何故なら個々の映画のフィーチャーが使用可能だと仮定しているからだ。",
    "output": "This particular algorithm is called a content based recommendations, or a content based approach, because we assume that we have available to us features for the different movies."
  },
  {
    "index": "F18119",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり我らは、これらの映画のコンテンツ(内容)がなんなのか、という事を捕捉するフィーチャーがある、という事だ。この映画はどれくらいロマンティックか?",
    "output": "And so where features that capture what is the content of these movies, of how romantic is this movie, how much action is in this movie."
  },
  {
    "index": "F18120",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして映画のコンテンツに関するフィーチャーを実際に用いて予測を行っている。",
    "output": "And we're really using features of a content of the movies to make our predictions."
  },
  {
    "index": "F18121",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも実際には、多くの映画について、そんなフィーチャーなんて持ってない。",
    "output": "But for many movies, we don't actually have such features."
  },
  {
    "index": "F18122",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "またはそんなフィーチャーを全部の映画について取得するのはとても難しい。売ろうとしている商品ならなんでも全てに渡って取るなんて。",
    "output": "Or maybe very difficult to get such features for all of our movies, for all of whatever items we're trying to sell."
  },
  {
    "index": "F18123",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから次のビデオでは、コンテントベースじゃないリコメンダーシステムのアプローチを議論する。それはつまり、他の誰かがこれらのフィーチャー全てを、我らのデータセットの映画全てに与えてくれている、と仮定しない物だ。",
    "output": "And so, in the next video, we'll start to talk about an approach to recommender systems that isn't content based and does not assume that we have someone else giving us all of these features for all of the movies in our data set."
  },
  {
    "index": "F18124",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、協調フィルタリングと呼ばれるリコメンダーシステム構築のアプローチを議論する。",
    "output": "In this video we'll talk about an approach to building a recommender system that's called collaborative filtering."
  },
  {
    "index": "F18125",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これから議論するアルゴリズムはとても興味深い性質を持っていて、それはフィーチャーラーニングと呼ばれている。それの意味する所は、これはアルゴリズムが、なんのフィーチャーを使うかを自分自身で学習しはじめる事を意味する。",
    "output": "The algorithm that we're talking about has a very interesting property that it does what is called feature learning and by that I mean that this will be an algorithm that can start to learn for itself what features to use."
  },
  {
    "index": "F18126",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんなデータセットがあるとしよう。そして各映画ごとに、誰かがやってきてその映画がどのくらいロマンティックか、どれだけのアクションが入っているかを教えてくれるとしよう。",
    "output": "Here was the data set that we had and we had assumed that for each movie, someone had come and told us how romantic that movie was and how much action there was in that movie."
  },
  {
    "index": "F18127",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でも、想像出来るように、実際に誰かが個々の映画を見ていって、それらがどれくらいロマンティックか、どれだけのアクションが詰め込まれているかをいちいち教えてもらうのはとても難しく、とても時間がかかり、とても高く付く場合がありうる。しかもしばしば、フィーチャーはたった2つよりもずっと多く必要となる。",
    "output": "But as you can imagine it can be very difficult and time consuming and expensive to actually try to get someone to, you know, watch each movie and tell you how romantic each movie and how action packed is each movie, and often you'll want even more features than just these two."
  },
  {
    "index": "F18128",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこれらのフィーチャーはどこから得たらいいか?",
    "output": "So where do you get these features from?"
  },
  {
    "index": "F18129",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでちょっと問題を変更してこれらのフィーチャーを知らないデータセットを持っていたとしよう。",
    "output": "So let's change the problem a bit and suppose that we have a data set where we do not know the values of these features."
  },
  {
    "index": "F18130",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり我らは、映画の集合とそれらをユーザーがどうレートしたのかのデータを与えられたとする。だが個々の映画がどのくらいロマンティックか個々の映画にどのくらいアクションが詰まってるのかはまったく分からないとする、つまりこれらをはてなマークで置き換えた。",
    "output": "So we're given the data set of movies and of how the users rated them, but we have no idea how romantic each movie is and we have no idea how action packed each movie is so I've replaced all of these things with question marks."
  },
  {
    "index": "F18131",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがここで、ちょっとだけ別の仮定をおこう。",
    "output": "But now let's make a slightly different assumption."
  },
  {
    "index": "F18132",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ユーザー一人一人の所におもむく事が出来て、各ユーザーが我らに、彼らがどれくらいロマンティックな映画が好きか、どれくらいアクションの映画が好きか、教えてくれるとしよう。",
    "output": "Let's say we've gone to each of our users, and each of our users has told has told us how much they like the romantic movies and how much they like action packed movies."
  },
  {
    "index": "F18133",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Aliceはシータ1に関連付けられているとしよう。",
    "output": "So Alice has associated a current of theta 1."
  },
  {
    "index": "F18134",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Bobはシータ2に。",
    "output": "Bob theta 2."
  },
  {
    "index": "F18135",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Carolはシータ3に。",
    "output": "Carol theta 3."
  },
  {
    "index": "F18136",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Daveはシータ4に。",
    "output": "Dave theta 4."
  },
  {
    "index": "F18137",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれを使う事としよう、Aliceは我らに彼女はロマンティック映画をとても好きだ、と教えてくれて、だからそこには5を、そこはx1の係数に対応する、そしてまたAliceはアクション映画をまったく好きでは無い、とも教えてくれた。",
    "output": "And let's say we also use this and that Alice tells us that she really likes romantic movies and so there's a five there which is the multiplier associated with X1 and lets say that Alice tells us she really doesn't like action movies and so there's a 0 there."
  },
  {
    "index": "F18138",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからそこは0とする。そしてBobも同じような事を言ったとする、だからシータ2はこんな風になる。",
    "output": "And Bob tells us something similar so we have theta 2 over here."
  },
  {
    "index": "F18139",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてまたx0も存在しててそれはイコール1なのも思い出そう。そしてCarolは我らにロマンティックな映画は嫌いだ、とも、教えてくれたとしよう。",
    "output": "Whereas Carol tells us that she really likes action movies which is why there's a 5 there, that's the multiplier associated with X2, and remember there's also X0 equals 1 and let's say that Carol tells us she doesn't like romantic movies and so on, similarly for Dave."
  },
  {
    "index": "F18140",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、どうにかして我らが各ユーザーの元におもむき、各ユーザーjは我らにシータjの値を教えてくれたと想定してみよう。",
    "output": "So let's assume that somehow we can go to users and each user J just tells us what is the value of theta J for them."
  },
  {
    "index": "F18141",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、基本的には彼らが別々の種類の映画をどのくらい好きかを表明してくれるという事だ。",
    "output": "And so basically specifies to us of how much they like different types of movies."
  },
  {
    "index": "F18142",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし我らはこれらのパラメータ、シータをユーザーから得る事が出来れば、各映画のx1とx2の値が幾つかを推測出来る、という事が判明する。",
    "output": "If we can get these parameters theta from our users then it turns out that it becomes possible to try to infer what are the values of x1 and x2 for each movie."
  },
  {
    "index": "F18143",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例を見てみよう。",
    "output": "Let's look at an example."
  },
  {
    "index": "F18144",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "映画1を見てみよう。",
    "output": "Let's look at movie 1."
  },
  {
    "index": "F18145",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "映画1はフィーチャーベクトルx1に関連づけられているのだった。",
    "output": "So that movie 1 has associated with it a feature vector x1."
  },
  {
    "index": "F18146",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの映画がLoveatlastと呼ばれていたのも知ってるだろうが、ここではそれは無視しよう。",
    "output": "And you know this movie is called Love at last but let's ignore that."
  },
  {
    "index": "F18147",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この映画がなんなのか、知らないフリをしよう、だからこの映画のタイトルを無視しよう。",
    "output": "Let's pretend we don't know what this movie is about, so let's ignore the title of this movie."
  },
  {
    "index": "F18148",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Aliceがこの映画を好きだ、というのが我らの知っている全てだ。",
    "output": "All we know is that Alice loved this move."
  },
  {
    "index": "F18149",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Bobはこの映画が好きだ。",
    "output": "Bob loved this movie."
  },
  {
    "index": "F18150",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "CarolとDaveはこの映画を嫌ってる。",
    "output": "Carol and Dave hated this movie."
  },
  {
    "index": "F18151",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではそこから、何が推測出来るだろうか?",
    "output": "So what can we infer?"
  },
  {
    "index": "F18152",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "フィーチャーベクトルから我らはAliceとBobはロマンティックな映画が好きだという事を知っている。だって彼らはここは5だと告げてたのだから。",
    "output": "Well, we know from the feature vectors that Alice and Bob love romantic movies because they told us that there's a 5 here."
  },
  {
    "index": "F18153",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "他方、CarolとDaveは彼らはロマンティックな映画を嫌ってる事を我らは知っている。そして彼らはアクション映画を好んでいる。",
    "output": "Whereas Carol and Dave, we know that they hate romantic movies and that they love action movies."
  },
  {
    "index": "F18154",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりそれらがユーザー3と4、つまりCarolとDaveが我らにくれたパラメーターベクトルなので、そして映画1はAliceとBobに好まれていて、CarolとDaveに嫌われているという事実に基づくと、我らはこれがロマンティックな映画だと合理的に結論出来る。それはたぶん、そんなにアクション映画では無かろう。",
    "output": "And so based on the fact that movie 1 is loved by Alice and Bob and hated by Carol and Dave, we might reasonably conclude that this is probably a romantic movie, it is probably not much of an action movie."
  },
  {
    "index": "F18155",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例はちょっと計算的に単純化してあるが、我らが真に問うている事はシータ1転置x1がだいたい5となるようなx1とはなんだろう?",
    "output": "this example is a little bit mathematically simplified but what we're really asking is what feature vector should X1 be so that theta 1 transpose x1 is approximately equal to 5, that's Alice's rating, and theta 2 transpose x1 is also approximately equal to 5, and theta 3 transpose x1 is approximately equal to 0, so this would be Carol's rating, and theta 4 transpose X1 is approximately equal to 0."
  },
  {
    "index": "F18156",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの事からx1はイコール、まず切片項の1に続いて1.00.0みたいになる。それは既知のAlice、Bob、Carol、Daveの映画に関する嗜好から、そして彼らがこの映画をどう評価するかの知識から、筋が通ってるように思う。",
    "output": "And from this it looks like, you know, X1 equals one that's the intercept term, and then 1.0, 0.0, that makes sense given what we know of Alice, Bob, Carol, and Dave's preferences for movies and the way they rated this movie."
  },
  {
    "index": "F18157",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的にはこのリストを降りていって、これらの他の映画のフィーチャーもどんなだったらリーズナブルか見つけようと試みる事が出来る。",
    "output": "And so more generally, we can go down this list and try to figure out what might be reasonable features for these other movies as well."
  },
  {
    "index": "F18158",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このxiを学習するという問題を定式化しよう。",
    "output": "Let's formalize this problem of learning the features XI."
  },
  {
    "index": "F18159",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ユーザーが我らに自分の嗜好を教えてくれるとする。",
    "output": "Let's say that our users have given us their preferences."
  },
  {
    "index": "F18160",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり我らのユーザーが来てくれて、シータ1からシータnuまでの値を教えてくれるとしよう。そして映画iに関するフィーチャーベクトルxiを学習したい。",
    "output": "So let's say that our users have come and, you know, told us these values for theta 1 through theta of NU and we want to learn the feature vector XI for movie number I."
  },
  {
    "index": "F18161",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから我らがやる事は以下のような最適化問題を解く事だ。",
    "output": "What we can do is therefore pose the following optimization problem."
  },
  {
    "index": "F18162",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その為、映画iをレーティングしたユーザー全員にわたってインデックスjに関する和を取りたい。何故なら我らが学習したいのは映画iのフィーチャーだからで、それはフィーチャーベクトルxiだ。",
    "output": "So we want to sum over all the indices J for which we have a rating for movie I because we're trying to learn the features for movie I that is this feature vector XI."
  },
  {
    "index": "F18163",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、そこでやりたい事はこの二乗誤差を最小化したい、つまりフィーチャーxiをユーザーjが映画iをどうレーティングするかの予測値が二乗誤差の意味で実際の値yij、つまりユーザーjの映画iに対する実際のレーティングの観測値に近い、そんなに離れていないように、フィーチャーxiを選びたい。",
    "output": "So and then what we want to do is minimize this squared error, so we want to choose features XI, so that, you know, the predictive value of how user J rates movie I will be similar, will be not too far in the squared error sense of the actual value YIJ that we actually observe in the rating of user j on movie I."
  },
  {
    "index": "F18164",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではまとめの為に、この項がしている事は、フィーチャーxiを以下の条件を満たすように選ぶ。その条件とは、その映画をレーティングしている全てのユーザーjに対して、アルゴリズムが予測するそのユーザーのその映画のレーティングが、実際の値とそんなに離れていないようにという条件だ。",
    "output": "So, just to summarize what this term does is it tries to choose features XI so that for all the users J that have rated that movie, the algorithm also predicts a value for how that user would have rated that movie that is not too far, in the squared error sense, from the actual value that the user had rated that movie."
  },
  {
    "index": "F18165",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが二乗誤差の項だ。",
    "output": "So that's the squared error term."
  },
  {
    "index": "F18166",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いつも通り、この種の正規化項を足して、フィーチャーが大きくなりすぎるのを防止する事も出来る。",
    "output": "As usual, we can also add this sort of regularization term to prevent the features from becoming too big."
  },
  {
    "index": "F18167",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以上が一つの特定の映画のフィーチャーを学習する方法だが、我らがやりたいのは、全ての映画の全てのフィーチャーを学習する事だ。",
    "output": "So this is how we would learn the features for one specific movie but what we want to do is learn all the features for all the movies and so what I'm going to do is add this extra summation here so I'm going to sum over all Nm movies, N subscript m movies, and minimize this objective on top that sums of all movies."
  },
  {
    "index": "F18168",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからやるべき事は、この追加のシグマをここに追加して、nm個の映画全てに渡って和を取る、n下付き添字m個の映画について、そしてこの目的関数を最小化する、全ての映画についての和の。",
    "output": "And if you do that, you end up with the following optimization problem. And if you minimize this, you have hopefully a reasonable set of features for all of your movies."
  },
  {
    "index": "F18169",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを行うと、結局以下のような最適化の問題となる。",
    "output": "So putting everything together, what we, the algorithm we talked about in the previous video and the algorithm that we just talked about in this video."
  },
  {
    "index": "F18170",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを最小化すると、全ての映画についての、リーズナブルなフィーチャーの集合が得られる事が期待出来る。",
    "output": "In the previous video, what we showed was that you know, if you have a set of movie ratings, so if you have the data the rij's and then you have the yij's that will be the movie ratings."
  },
  {
    "index": "F18171",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "全てをあわせると、我らが前のビデオで議論してきたアルゴリズムと、このビデオで今議論してきたアルゴリズムは前のビデオで見てきたのはえーと、映画のレーティングの集合があったとすると、つまりデータrijとyij、それは映画のレーティングだが、それがあったとすると、別々の映画に対してのフィーチャーが所与であれば、これらのパラメータ、シータが学習出来る、という事だった。",
    "output": "Then given features for your different movies we can learn these parameters theta."
  },
  {
    "index": "F18172",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、もしフィーチャーを知ってたら、別々のユーザーのパラメータ、シータを学習する事が出来る。",
    "output": "So if you knew the features, you can learn the parameters theta for your different users."
  },
  {
    "index": "F18173",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのビデオの前半で見てきたように、もしユーザーがあなたに喜んでパラメータを提供してくれれば、別々の映画のフィーチャーを推計する事が出来る。",
    "output": "And what we showed earlier in this video is that if your users are willing to give you parameters, then you can estimate features for the different movies."
  },
  {
    "index": "F18174",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、鶏と卵の問題だ。",
    "output": "So this is kind of a chicken and egg problem."
  },
  {
    "index": "F18175",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どっちが先に来る?",
    "output": "Which comes first?"
  },
  {
    "index": "F18176",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シータが得られれば、xを知る事が出来る。",
    "output": "You know, do we want if we can get the thetas, we can know the Xs."
  },
  {
    "index": "F18177",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "xが分かっていれば、シータを学習出来る。",
    "output": "If we have the Xs, we can learn the thetas."
  },
  {
    "index": "F18178",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでとりうる手段としては、そしてこれは実際にうまく行くのだが、それは、出来る事は、実際にランダムにシータの何らかの値を推測してしまう、という事だ。",
    "output": "And what you can do is, and then this actually works, what you can do is in fact randomly guess some value of the thetas."
  },
  {
    "index": "F18179",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この最初のランダムなシータの推測に基づいて、前進する事が出来て、別々の映画のフィーチャーを学習する為にここまで話してきた手順が使える事になる。",
    "output": "Now based on your initial random guess for the thetas, you can then go ahead and use the procedure that we just talked about in order to learn features for your different movies."
  },
  {
    "index": "F18180",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、何らかのフィーチャーの初期の値が映画に対して与えられたとき、この前回のビデオで話した最初の手法を用いてパラメータシータの推計を改善する事が出来る。",
    "output": "Now given some initial set of features for your movies you can then use this first method that we talked about in the previous video to try to get an even better estimate for your parameters theta."
  },
  {
    "index": "F18181",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いまやユーザーのシータの改善された値を得たので、それを用いてさらにフィーチャーを改善した物が得られるかもしれない、などなど。",
    "output": "Now that you have a better setting of the parameters theta for your users, we can use that to maybe even get a better set of features and so on."
  },
  {
    "index": "F18182",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは実際に機能する。これを行うと、これは実際に映画のフィーチャーとユーザーごとに異なったパラメータのリーズナブルな組に収束する。",
    "output": "We can sort of keep iterating, going back and forth and optimizing theta, x theta, x theta, nd this actually works and if you do this, this will actually cause your album to converge to a reasonable set of features for you movies and a reasonable set of parameters for your different users."
  },
  {
    "index": "F18183",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が基本的な協調的フィルタリングのアルゴリズムだ。",
    "output": "So this is a basic collaborative filtering algorithm."
  },
  {
    "index": "F18184",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは実際に使う、最終的なアルゴリズムでは無い。",
    "output": "This isn't actually the final algorithm that we're going to use."
  },
  {
    "index": "F18185",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次のビデオで、このアルゴリズムを改善する事が出来る。それでもっとずっと計算量的に効率的になる。",
    "output": "In the next video we are going to be able to improve on this algorithm and make it quite a bit more computationally efficient."
  },
  {
    "index": "F18186",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもこれで、別々の映画から、パラメータとフィーチャーを同時に学習する、という問題をどう定式化するかがなんとなく分かったんじゃないかな。",
    "output": "But, hopefully this gives you a sense of how you can formulate a problem where you can simultaneously learn the parameters and simultaneously learn the features from the different movies."
  },
  {
    "index": "F18187",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの問題に関して言うと、リコメンダーシステムの問題に関して言うと、これが可能なのは、各ユーザーが複数の映画をレーティングしていて、さらに出来たら全ての映画が複数のユーザーにレーティングされてて初めて可能となる。",
    "output": "And for this problem, for the recommender system problem, this is possible only because each user rates multiple movies and hopefully each movie is rated by multiple users."
  },
  {
    "index": "F18188",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この行ったり来たりしてシータとxを推計する手順が使用出来る。",
    "output": "And so you can do this back and forth process to estimate theta and x."
  },
  {
    "index": "F18189",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではまとめよう。このビデオでは、最初の協調的フィルタリングのアルゴリズムを見てきた。",
    "output": "So to summarize, in this video we've seen an initial collaborative filtering algorithm."
  },
  {
    "index": "F18190",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら各ユーザーが映画のサブセットをレーティングする、という行動でもってアルゴリズムがちょっとだけ改善されたフィーチャーを得るのを各ユーザーが助けているからだ。そして助ける事で--2,3の映画を自分でレーティングする事で、私はシステムがより良いフィーチャーを学習するのを助けている事になり、そしてこれらのフィーチャーをシステムがその他全員に対して映画のよりよい予測を行う為に使う事が出来る。",
    "output": "The term collaborative filtering refers to the observation that when you run this algorithm with a large set of users, what all of these users are effectively doing are sort of collaboratively--or collaborating to get better movie ratings for everyone because with every user rating some subset with the movies, every user is helping the algorithm a little bit to learn better features, and then by helping-- by rating a few movies myself, I will be helping the system learn better features and then these features can be used by the system to make better movie predictions for everyone else."
  },
  {
    "index": "F18191",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりある種の協調作業がある訳だ:各ユーザーが、公共の利益の為に、システムがより良いフィーチャーを学習するのを手助けする。",
    "output": "And so there is a sense of collaboration where every user is helping the system learn better features for the common good."
  },
  {
    "index": "F18192",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが協調的フィルタリングだ。",
    "output": "This is this collaborative filtering."
  },
  {
    "index": "F18193",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次のビデオでやる事は、ここまでやってきたアイデアを用いて、より良いアルゴリズムの構築を試みる。協調フィルタリングにとってもうちょっとだけ良い方法を。",
    "output": "And, in the next video what we going to do is take the ideas that have worked out, and try to develop a better an even better algorithm, a slightly better technique for collaborative filtering."
  },
  {
    "index": "F18194",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回までのビデオでまずは映画のフィーチャーが与えられた時それを用いてユーザーのパラメータのデータを学習する事が出来る、という話をした。",
    "output": "In the last couple videos, we talked about the ideas of how, first, if you're given features for movies, you can use that to learn parameters data for users."
  },
  {
    "index": "F18195",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、ユーザーのパラメータを与えられたら、それを使って映画のフィーチャーを学習する事が出来る、という話をした。",
    "output": "And second, if you're given parameters for the users, you can use that to learn features for the movies."
  },
  {
    "index": "F18196",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、これら二つのアイデアを用いて、それらを合わせて協調フィルタのアルゴリズムにたどり着く。",
    "output": "In this video we're going to take those ideas and put them together to come up with a collaborative filtering algorithm."
  },
  {
    "index": "F18197",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり以前やった事の一つには映画のフィーチャーがあれば、最小化問題を解く事でユーザーのパラメータであるシータを見つける事が出来た。",
    "output": "So one of the things we worked out earlier is that if you have features for the movies then you can solve this minimization problem to find the parameters theta for your users."
  },
  {
    "index": "F18198",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその次に、パラメータであるシータがあれば、それを用いてフィーチャーxを推計する事が出来るのだった。それは最小化問題を解く事で出来るのだった。",
    "output": "And then we also worked that out, if you are given the parameters theta, you can also use that to estimate the features x, and you can do that by solving this minimization problem."
  },
  {
    "index": "F18199",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから取りうる手段として一つ考えられるのは行ったり来たりして実行する事だ。",
    "output": "So one thing you could do is actually go back and forth."
  },
  {
    "index": "F18200",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ランダムに初期化されたパラメータで、シータについて解き、xについて解き、シータについて解き、xについて解く。",
    "output": "Maybe randomly initialize the parameters and then solve for theta, solve for x, solve for theta, solve for x."
  },
  {
    "index": "F18201",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、シータとxを行ったり来たりしなくても良いもっと効率的なアルゴリズムがある事が知られている、それは行ったり来たりする代わりにシータとxを同時に解く事が出来る。",
    "output": "But, it turns out that there is a more efficient algorithm that doesn't need to go back and forth between the x's and the thetas, but that can solve for theta and x simultaneously."
  },
  {
    "index": "F18202",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らがやる事は、基本的にはこれら二つの最適化の目的関数をとり、それを一つの目的関数に突っ込む、という事だ。",
    "output": "What we are going to do, is basically take both of these optimization objectives, and put them into the same objective."
  },
  {
    "index": "F18203",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり私は、新しい最適化の目的関数Jを定義する、それはフィーチャーxとパラメータのシータに関する関数としての、コスト関数だ。",
    "output": "So I'm going to define the new optimization objective j, which is a cost function, that is a function of my features x and a function of my parameters theta."
  },
  {
    "index": "F18204",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして基本的には上の所に二つの最適化の目的関数があったが、それを一つにくっつけた。",
    "output": "And, it's basically the two optimization objectives I had on top, but I put together."
  },
  {
    "index": "F18205",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを説明する為に、まず以下の二つの項が同じという事を指摘したい:この項、この二乗誤差の項と、この二乗誤差の項、これ。和の取り方がちょっと違って見えるが、この和が実際に何をやっているかを見てみよう。",
    "output": "So, in order to explain this, first, I want to point out that this term over here, this squared error term, is the same as this squared error term and the summations look a little bit different, but let's see what the summations are really doing."
  },
  {
    "index": "F18206",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の和は、全てのユーザーjに渡ってとっている、そして次にそのユーザーによってレーティングされた全ての映画について和を取る。",
    "output": "The first summation is sum over all users J and then sum over all movies rated by that user."
  },
  {
    "index": "F18207",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これは実際は、ユーザーにレーティングされた全ての映画に対応したiとjのペアに渡って和を取っている。",
    "output": "So, this is really summing over all pairs IJ, that correspond to a movie that was rated by a user."
  },
  {
    "index": "F18208",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "jに渡って取る和は、各ユーザに対して、と言っていてそしてそのユーザーがレーティングした全ての映画に渡って和を取る。",
    "output": "Sum over J says, for every user, the sum of all the movies rated by that user."
  },
  {
    "index": "F18209",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この下の和は、それを反対の順番でやるだけだ。",
    "output": "This summation down here, just does things in the opposite order."
  },
  {
    "index": "F18210",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが言っているのは、各映画iに対してその映画をレーティングした全てのユーザーjに渡って和をとる、つまり、これらの和は、これらは両方とも、rijがイコール1な全てのijのペアに渡って和を取るだけ。",
    "output": "This says for every movie I, sum over all the users J that have rated that movie and so, you know these summations, both of these are just summations over all pairs ij for which r of i J is equal to 1."
  },
  {
    "index": "F18211",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはようするに、レーティングを持っている全てのユーザーと映画の組に渡って和を取る。",
    "output": "It's just something over all the user movie pairs for which you have a rating."
  },
  {
    "index": "F18212",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれら二つの項はこの最初の項と完全に一致している。そして私は和を明示的に書いた、それはようするに、rijがイコール1となる全てのijのペアに渡って和を取る、と言っている。",
    "output": "and so those two terms up there is just exactly this first term, and I've just written the summation here explicitly, where I'm just saying the sum of all pairs IJ, such that RIJ is equal to 1."
  },
  {
    "index": "F18213",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らがやりたい事はxとシータを同時に解く為に最小化をする対象となる、最適化の複合目的関数を定義するという事だ。",
    "output": "So what we're going to do is define a combined optimization objective that we want to minimize in order to solve simultaneously for x and theta."
  },
  {
    "index": "F18214",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最適化の目的関数に残ってる他の項はこれだ。これはシータの正規化項だ。",
    "output": "And then the other terms in the optimization objective are this, which is a regularization in terms of theta."
  },
  {
    "index": "F18215",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはここに来る。そして最後のピースとなるのはこの項で、これはxの為の最適化の目的関数の中に残ってる物でそれはここに来る。",
    "output": "So that came down here and the final piece is this term which is my optimization objective for the x's and that became this."
  },
  {
    "index": "F18216",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの最適化の目的関数Jは興味深い性質を持っている、それはxを定数に固定してみて、シータに関して最小化してみると、するとまさにこの問題を解いている事になる。",
    "output": "And this optimization objective j actually has an interesting property that if you were to hold the x's constant and just minimize with respect to the thetas then you'd be solving exactly this problem, whereas if you were to do the opposite, if you were to hold the thetas constant, and minimize j only with respect to the x's, then it becomes equivalent to this."
  },
  {
    "index": "F18217",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、その反対をやると、シータを仮に定数で固定してみて、Jをxに関してだけ最小化してみると、この項か、またはこの項がxに関してかシータに関してのどちらかに関してだけ最小化するなら、定数になるから。",
    "output": "Because either this term or this term is constant if you're minimizing only the respective x's or only respective thetas."
  },
  {
    "index": "F18218",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、これが、xに関するコスト関数とシータに関するコスト関数をくっつけた最適化の目的関数だ。",
    "output": "So here's an optimization objective that puts together my cost functions in terms of x and in terms of theta."
  },
  {
    "index": "F18219",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして一つの最適化問題に帰着される為には、我らがやるべき事は、このコスト関数をフィーチャーxとユーザーのパラメータシータの両方に関する関数として扱う事だ。",
    "output": "And in order to come up with just one optimization problem, what we're going to do, is treat this cost function, as a function of my features x and of my user pro user parameters data and just minimize this whole thing, as a function of both the Xs and a function of the thetas."
  },
  {
    "index": "F18220",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして単純に全体をxとシータの関数として、最小化すれば良い。",
    "output": "And really the only difference between this and the older algorithm is that, instead of going back and forth, previously we talked about minimizing with respect to theta then minimizing with respect to x, whereas minimizing with respect to theta, minimizing with respect to x and so on."
  },
  {
    "index": "F18221",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際、これと以前のアルゴリズムの唯一の違いは、行ったり来たりする代わりに、以前のはシータに関して最小化した後次にxについて最小化して、その後シータについて最小化して、xについて最小化して、、、などとやったのだったが、この新しいバージョンでは、xとシータという二つのパラメータのセットの間を順番に行ったり来たりする代わりにたんに両方のパラメータに関して同時に最小化する、という事をする。",
    "output": "In this new version instead of sequentially going between the 2 sets of parameters x and theta, what we are going to do is just minimize with respect to both sets of parameters simultaneously."
  },
  {
    "index": "F18222",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に一つ詳細な話だが、フィーチャーをこういう風に学ぶ場合、以前はx0がイコール1となるコンベンションを使っていて、これは切片項に対応した物だった。",
    "output": "Finally one last detail is that when we're learning the features this way. Previously we have been using this convention that we have a feature x0 equals one that corresponds to an interceptor."
  },
  {
    "index": "F18223",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この種の定式化を用いる時には、実際にはフィーチャーも学習する事になるので、このコンベンション無しでいける。",
    "output": "When we are using this sort of formalism where we're are actually learning the features, we are actually going to do away with this convention."
  },
  {
    "index": "F18224",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから我らの学習する事になるフィーチャーxはRnだ。",
    "output": "And so the features we are going to learn x, will be in Rn."
  },
  {
    "index": "F18225",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、以前はフィーチャーxはRn+1だった、切片項を含んでいたから。",
    "output": "Whereas previously we had features x and Rn + 1 including the intercept term."
  },
  {
    "index": "F18226",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこからx0を取り除いたから、xはRnとなる。",
    "output": "By getting rid of x0 we now just have x in Rn."
  },
  {
    "index": "F18227",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして同様に、パラメータのシータは同じ次元なので、シータもRnとなる。何故ならx0が無いなら、シータ0も同様に不要だからだ。",
    "output": "And so similarly, because the parameters theta is in the same dimension, we now also have theta in RN because if there's no x0, then there's no need parameter theta 0 as well."
  },
  {
    "index": "F18228",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのコンベンション無しでいける理由としては、いまや我らはフィーチャーを全て学習する事になった、よね?",
    "output": "And the reason we do away with this convention is because we're now learning all the features, right?"
  },
  {
    "index": "F18229",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからいつも1と等しくなるフィーチャーをハードコードする必要が無い。",
    "output": "So there is no need to hard code the feature that is always equal to one."
  },
  {
    "index": "F18230",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならもしアルゴリズムがいつも1となるフィーチャーを本当に必要としているなら、それは自身で勝手にそう学習するはずだからだ。",
    "output": "Because if the algorithm really wants a feature that is always equal to 1, it can choose to learn one for itself."
  },
  {
    "index": "F18231",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしアルゴリズムがそう選べばx1=1とセットされる。",
    "output": "So if the algorithm chooses, it can set the feature X1 equals 1."
  },
  {
    "index": "F18232",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからフィーチャーx0を1とハードコードする必要は無い。アルゴリズムはそれを自分自身で学習する事が出来るだけの柔軟性がある。",
    "output": "So there's no need to hard code the feature of 001, the algorithm now has the flexibility to just learn it by itself."
  },
  {
    "index": "F18233",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では全部を合わせると、これが協調フィルタリングのアルゴリズムだ。",
    "output": "So, putting everything together, here is our collaborative filtering algorithm."
  },
  {
    "index": "F18234",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず、xとシータをある小さなランダムの値で初期化する。",
    "output": "first we are going to initialize x and theta to small random values."
  },
  {
    "index": "F18235",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはちょっとニューラルネットワークのトレーニングに似てるね。そこでもニューラルネットワークのパラメータを全て小さなランダムの数で初期化したんだった。",
    "output": "And this is a little bit like neural network training, where there we were also initializing all the parameters of a neural network to small random values."
  },
  {
    "index": "F18236",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、コスト関数を、最急降下法なりアドバンスドな最適化のアルゴリズムなりを使って最小化する。だから微分をとれば、こんな感じの最急降下法の更新ルールが得られる。",
    "output": "Next we're then going to minimize the cost function using great intercepts or one of the advance optimization algorithms."
  },
  {
    "index": "F18237",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "全部書いたりはしないが、それをフィーチャーx(i)kで偏微分した物だ。同様に、この項もまた、コスト関数を偏微分した物で今度はパラメータであるシータに関して偏微分した物で、それに関して最小化する。",
    "output": "So, if you take derivatives you find that the great intercept like these and so this term here is the partial derivative of the cost function, I'm not going to write that out, with respect to the feature value Xik and similarly this term here is also a partial derivative value of the cost function with respect to the parameter theta that we're minimizing."
  },
  {
    "index": "F18238",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ちょっと注意を。この式では、もはやx0イコール1が無い、だからxはRnとなり、シータもRnとなる。",
    "output": "And just as a reminder, in this formula that we no longer have this X0 equals 1 and so we have that x is in Rn and theta is a Rn."
  },
  {
    "index": "F18239",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この新しい定式化では、各パラメータ、シータも、各パラメータxnも、正規化している。",
    "output": "In this new formalism, we're regularizing every one of our perimeters theta, you know, every one of our parameters Xn."
  },
  {
    "index": "F18240",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もはやシータ0の特別扱いのケースも存在しない。シータ0は異なる風に正規化していたんだった。",
    "output": "There's no longer the special case theta zero, which was regularized differently, or which was not regularized compared to the parameters theta 1 down to theta."
  },
  {
    "index": "F18241",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もはやシータ0は存在しないので、そんな訳だからこのアップデートでもk=0の特別なケースの場合分けをしていない。",
    "output": "So there is now no longer a theta 0, which is why in these updates, I did not break out a special case for k equals 0."
  },
  {
    "index": "F18242",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最急降下法を使ってコスト関数Jをフィーチャーxとパラメータシータに関して最小化していく。",
    "output": "So we then use gradient descent to minimize the cost function j with respect to the features x and with respect to the parameters theta."
  },
  {
    "index": "F18243",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、あるユーザーに対してユーザーがあるパラメータ、シータを持っているなら、そして映画にはある種の学習されたフィーチャーxがあるなら、そのユーザーが映画に星いくつのレーティングをするかをシータjの転置と、、、または、前の表記と合わせると、もしユーザーjがまだ映画iをレーティングしていなければ、ユーザーjは映画iをシータjの転置xiとレーティングすると予測する。以上が協調フィルタリングアルゴリズムだ。",
    "output": "And finally, given a user, if a user has some parameters, theta, and if there's a movie with some sort of learned features x, we would then predict that that movie would be given a star rating by that user of theta transpose j."
  },
  {
    "index": "F18244",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこのアルゴリズムを実装すれば、おそらく全ての映画の良いフィーチャーと全てのユーザーのパラメータを同時に学習する、かなり良いアルゴリズムを得る事が出来る。そして別々のユーザーが別々の映画をどうレーティングするかのかなり良い予測を与える事も期待出来る。",
    "output": "So that's the collaborative filtering algorithm and if you implement this algorithm you actually get a pretty decent algorithm that will simultaneously learn good features for hopefully all the movies as well as learn parameters for all the users and hopefully give pretty good predictions for how different users will rate different movies that they have not yet rated"
  },
  {
    "index": "F18245",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここ何回かのビデオで、協調的フィルタリングのアルゴリズムについて議論してきた。",
    "output": "In the last few videos, we talked about a collaborative filtering algorithm."
  },
  {
    "index": "F18246",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、このアルゴリズムのベクトル化した実装についてちょこっと話しておきたい。",
    "output": "In this video I'm going to say a little bit about the vectorization implementation of this algorithm."
  },
  {
    "index": "F18247",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "またついでにこのアルゴリズムについて出来るその他の事についても話す。",
    "output": "And also talk a little bit about other things you can do with this algorithm."
  },
  {
    "index": "F18248",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、出来る事の一つにある商品が与えられた時に、これに関連した商品を探す、というのがある。",
    "output": "For example, one of the things you can do is, given one product can you find other products that are related to this so that for example, a user has recently been looking at one product."
  },
  {
    "index": "F18249",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは例えばユーザーが最近ある商品を見たとすると、このユーザーに推薦する、別の関連した商品は無いか?",
    "output": "Are there other related products that you could recommend to this user?"
  },
  {
    "index": "F18250",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではどうやってこれが出来るか見てみよう。",
    "output": "So let's see what we could do about that."
  },
  {
    "index": "F18251",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでやろうとしているのは、協調的フィルタリングのアルゴリズムの予測を別のやり方で練習する、という物。",
    "output": "What I'd like to do is work out an alternative way of writing out the predictions of the collaborative filtering algorithm."
  },
  {
    "index": "F18252",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "はじめに、これが我々のデータセットで五つの映画がある。そして私がやる事は、ユーザーのレーティングを全て取ってきて、一つの行列にグループ化する。",
    "output": "To start, here is our data set with our five movies and what I'm going to do is take all the ratings by all the users and group them into a matrix."
  },
  {
    "index": "F18253",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとここでは、5つの映画と4人のユーザーがいるので、この行列yは5x4行列になる。",
    "output": "So, here we have five movies and four users, and so this matrix y is going to be a 5 by 4 matrix."
  },
  {
    "index": "F18254",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは単純に、これらの要素、これらのデータを全部とってきて、これははてなマークも全部含めて、それをこの行列にグループ化した物だ。",
    "output": "It's just you know, taking all of the elements, all of this data."
  },
  {
    "index": "F18255",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろんこの行列の要素、この行列の要素ijは、実際は、以前にyの上付き添字ijと書いていた物だ。",
    "output": "Including question marks, and grouping them into this matrix."
  },
  {
    "index": "F18256",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはユーザーjによる映画iのレーティング。我らの持つ全てのレーティングを含むこの行列Yが与えられたとすると、レーティングを予測するアルゴリズムを書き下す代替的な方法がある。",
    "output": "And of course the elements of this matrix of the (i, j) element of this matrix is really what we were previously writing as y superscript i, j."
  },
  {
    "index": "F18257",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、もしあなたがあるユーザーのある映画に対する予測を調べたいなら、ユーザーjの映画iに対する予測の式はこうなる。",
    "output": "It's the rating given to movie i by user j."
  },
  {
    "index": "F18258",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、予測されたレーティングの行列があれば、それは以下のような行列だろう、ijエントリがこれがユーザーjが映画iに与えるレーティングの予測値で、それは正確にシータjの転置xiにイコールだ。",
    "output": "Given this matrix y of all the ratings that we have, there's an alternative way of writing out all the predictive ratings of the algorithm."
  },
  {
    "index": "F18259",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この行列は、最初の要素、この要素11は、ユーザー1の映画1に対するレーティングの予測値だ。",
    "output": "And, in particular if you look at what a certain user predicts on a certain movie, what user j predicts on movie i is given by this formula."
  },
  {
    "index": "F18260",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "などなど。そしてこれは、ユーザー1の最後の映画に対するレーティングの予想値。",
    "output": "And so, if you have a matrix of the predicted ratings, what you would have is the following matrix where the i, j entry."
  },
  {
    "index": "F18261",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれは、このレーティングはこの値を予想した物で、このレーティングはこの値を予想した物で、、、などなど。",
    "output": "So this corresponds to the rating that we predict using j will give to movie i is exactly equal to that theta j transpose XI, and so, you know, this is a matrix where this first element the one-one element is a predictive rating of user one or movie one and this element, this is the one-two element is the predicted rating of user two on movie one, and so on, and this is the predicted rating of user one on the last movie and if you want, you know, this rating is what we would have predicted for this value and this rating is what we would have predicted for that value, and so on."
  },
  {
    "index": "F18262",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いま、このレーティングの予想値の行列が与えられたとすると、これらを書きだすより簡単な、またはベクトル化した方法が存在する。",
    "output": "Now, given this matrix of predictive ratings there is then a simpler or vectorized way of writing these out."
  },
  {
    "index": "F18263",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは、以前、線形回帰であった行列に似ている。x1の転置にx2の転置に、、、とxのnmの転置まで降りていく。",
    "output": "In particular if I define the matrix x, and this is going to be just like the matrix we had earlier for linear regression to be sort of x1 transpose x2 transpose down to x of nm transpose."
  },
  {
    "index": "F18264",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり全ての映画のフィーチャーを持ってきて列として積んでいく。",
    "output": "So I'm take all the features for my movies and stack them in rows."
  },
  {
    "index": "F18265",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり各映画を一つの手本とみなして、それら別々の映画のフィーチャーを列として積んでいく訳だ。",
    "output": "So if you think of each movie as one example and stack all of the features of the different movies and rows."
  },
  {
    "index": "F18266",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてまた、行列として大文字のシータも作りたい。その為には、ユーザー毎のパラメータベクトルを取ってきて、同様に列として積んでいく。",
    "output": "And if we also to find a matrix capital theta, and what I'm going to do is take each of the per user parameter vectors, and stack them in rows, like so."
  },
  {
    "index": "F18267",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これがシータ1で、これは最初のユーザーのパラメータベクトルだ。",
    "output": "So that's theta 1, which is the parameter vector for the first user."
  },
  {
    "index": "F18268",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてシータ2、つまり、このように列として積んでいって大文字のシータの行列を定義する。するとこのように列として積み上がったnu個のパラメータベクトルを列に持った物を得る。",
    "output": "And, you know, theta 2, and so, you must stack them in rows like this to define a matrix capital theta and so I have nu parameter vectors all stacked in rows like this."
  },
  {
    "index": "F18269",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、行列Xと行列シータがこう定義されたとして、全ての予測を計算するベクトル化した方法を得るには、単純にX掛けるシータ転置を計算するだけで良い。それがここにあるこの行列を計算するベクトル化した方法となっている。",
    "output": "Now given this definition for the matrix x and this definition for the matrix theta in order to have a vectorized way of computing the matrix of all the predictions you can just compute x times the matrix theta transpose, and that gives you a vectorized way of computing this matrix over here."
  },
  {
    "index": "F18270",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らの使っているこのアルゴリズムはまた、低ランク行列分解とも呼ばれている。",
    "output": "The algorithm that we're using is also called low rank matrix factorization."
  },
  {
    "index": "F18271",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりもし人々が低ランク行列分解について話しているのを耳にした時は、それは本質的には我らがここまで議論して来た物と同一の物だ。",
    "output": "And so if you hear people talk about low rank matrix factorization that's essentially exactly the algorithm that we have been talking about."
  },
  {
    "index": "F18272",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの用語はこの行列X掛けるシータ転置が持つ数学的特徴に、線形代数ではこの低ランク行列と呼ばれる物があって、だからこれらのアルゴリズムには低ランク行列分解という名前がついた。この行列Xシータ転置の持つ低ランクという行列の特徴の為に。",
    "output": "And this term comes from the property that this matrix x times theta transpose has a mathematical property in linear algebra called that this is a low rank matrix and so that's what gives rise to this name low rank matrix factorization for these algorithms, because of this low rank property of this matrix x theta transpose."
  },
  {
    "index": "F18273",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし低ランクというのがなんなのか知らなくて、低ランク行列というのが何なのか知らなくても、まぁ気にすんな。",
    "output": "In case you don't know what low rank means or in case you don't know what a low rank matrix is, don't worry about it."
  },
  {
    "index": "F18274",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このアルゴリズムを使うのに、そんな事を知ってる必要はまったく無い。",
    "output": "You really don't need to know that in order to use this algorithm."
  },
  {
    "index": "F18275",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもしあなたが線形代数のエキスパートなら、そんな理由でこのアルゴリズムに低ランク行列分解という別名がついた、というワケ。",
    "output": "But if you're an expert in linear algebra, that's what gives this algorithm, this other name of low rank matrix factorization."
  },
  {
    "index": "F18276",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、協調的フィルタリングのアルゴリズムを実行する時に、実行出来る別の事として、こんなのがある。それは関連する映画を探す為に学習したフィーチャーを用いる、という物。",
    "output": "Finally, having run the collaborative filtering algorithm here's something else that you can do which is use the learned features in order to find related movies."
  },
  {
    "index": "F18277",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、各商品iに関して実際には各映画iに関して、フィーチャーベクトルxiを学習した。",
    "output": "Specifically for each product i really for each movie i, we've learned a feature vector xi."
  },
  {
    "index": "F18278",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、あるフィーチャー群を学習した時には、どんなフィーチャーがあるのかとかは前もっては知らなかった。",
    "output": "So, you know, when you learn a certain features without really know that can the advance what the different features are going to be, but if you run the algorithm and perfectly the features will tend to capture what are the important aspects of these different movies or different products or what have you."
  },
  {
    "index": "F18279",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがアルゴリズムを実行すると、フィーチャーはこれらの別々の映画のまたは別々の商品の、またはなんでも、これらの重要な性質は何か、を捕捉する傾向にある。",
    "output": "What are the important aspects that cause some users to like certain movies and cause some users to like different sets of movies."
  },
  {
    "index": "F18280",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何がある種のユーザー達がある映画たちを好きになる重要な特徴なのか、そしてあるユーザー達が異なる映画を好む、重要な特徴とは何か。",
    "output": "Then some feature x4 which is, you know, some other thing."
  },
  {
    "index": "F18281",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから例えば、最終的にあなたはフィーチャーx1としてロマンスをx2としてアクションを、これまでの例と同様に学習した上で、さらに別のフィーチャーx3としてコメディー度合いを学習する事になるかもしれない。",
    "output": "And you have N features all together and after you have learned features it's actually often pretty difficult to go in to the learned features and come up with a human understandable interpretation of what these features really are."
  },
  {
    "index": "F18282",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうやってn個のフィーチャーを得る事になったとして、実際はこれらのフィーチャーを見ていって、これらのフィーチャーが本当は何を意味しているのかを人間が理解するのはとても難しい事が多い。",
    "output": "But in practice, you know, the features even though these features can be hard to visualize."
  },
  {
    "index": "F18283",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが実践的には、これらのフィーチャーが、たとえ可視化する事すら大変でも、これらのフィーチャーが何なのかを突き止めるのは大変であっても、だいたいは、それが何にせよあなたが映画を好いたり嫌ったりするもっとも特徴的でとても意義深い特徴を捕捉しているフィーチャーを学習する。",
    "output": "It can be hard to figure out just what these features are. Usually, it will learn features that are very meaningful for capturing whatever are the most important or the most salient properties of a movie that causes you to like or dislike it."
  },
  {
    "index": "F18284",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、以下のような問題に取り組むとしよう。",
    "output": "And so now let's say we want to address the following problem."
  },
  {
    "index": "F18285",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたがある映画iを持ってたとする、そしてあなたはその映画に関連した別の映画jを探したい。",
    "output": "Say you have some specific movie i and you want to find other movies j that are related to that movie."
  },
  {
    "index": "F18286",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、いったいなんでそんな事したいのか?",
    "output": "And so well, why would you want to do this?"
  },
  {
    "index": "F18287",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いいでしょう。例えばユーザーが映画をブラウズしているとする、そして彼は今、映画jを観ているとする、その時、映画jを観終わった後に彼に視聴を推薦すべき、リーズナブルな映画はなんだろう?",
    "output": "Right, maybe you have a user that's browsing movies, and they're currently watching movie j, than what's a reasonable movie to recommend to them to watch after they're done with movie j?"
  },
  {
    "index": "F18288",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または誰かが映画jを購入したとすると、その時に彼らに購入を検討してもらう為に推薦する事がリーズナブルな別の映画はなんだろう?",
    "output": "Or if someone's recently purchased movie j, well, what's a different movie that would be reasonable to recommend to them for them to consider purchasing."
  },
  {
    "index": "F18289",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いまやあなたはこれらの学習したフィーチャーを持っているのだから、これを用いて二つの映画がどの位近いかを測るとても便利な方法がある。",
    "output": "So, now that you have learned these feature vectors, this gives us a very convenient way to measure how similar two movies are."
  },
  {
    "index": "F18290",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、映画iはフィーチャーベクトルxiを持つ訳だが、別の映画jを、フィーチャーベクトルxiとxjの距離が短い物として探す事が出来る。",
    "output": "In particular, movie i has a feature vector xi."
  },
  {
    "index": "F18291",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは二つの映画、iとjが類似しているという極めて強い示唆を与える。少なくとも、映画iを好きな人は映画jも好むだろう、という意味において。",
    "output": "and so if you can find a different movie, j, so that the distance between xi and xj is small, then this is a pretty strong indication that, you know, movies j and i are somehow similar."
  },
  {
    "index": "F18292",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではまとめると、ユーザーが映画iを見ていたとする、そしてあなたはその映画にもっとも似た5つの新しい映画を彼らにリコメンド(推薦)したいとする。",
    "output": "At least in the sense that some of them likes movie i, maybe more likely to like movie j as well."
  },
  {
    "index": "F18293",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたがする事は、これら別々の映画間でフィーチャーの距離が一番小さい5つの映画を探してくる、という事。",
    "output": "So, just to recap, if your user is looking at some movie i and if you want to find the 5 most similar movies to that movie in order to recommend 5 new movies to them, what you do is find the five movies j, with the smallest distance between the features between these different movies."
  },
  {
    "index": "F18294",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これであなたは勧めるべき幾つかの映画を得る事が出来る。",
    "output": "And this could give you a few different movies to recommend to your user."
  },
  {
    "index": "F18295",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で全てのユーザー、全ての映画の予測レーティングをベクトル化して実装し計算するやり方を理解出来ただろう、そして学習したフィーチャーをどのように用いて、どの映画とか商品が関連しているのかを探す方法も。",
    "output": "So with that, hopefully, you now know how to use a vectorized implementation to compute all the predicted ratings of all the users and all the movies, and also how to do things like use learned features to find what might be movies and what might be products that aren't related to each other."
  },
  {
    "index": "F18296",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでで、あなたはリコメンダーシステムのアルゴリズム、または協調フィルリングのアルゴリズムの全ての主要な要素を見終わった事になる。",
    "output": "By now you've seen all of the main pieces of the recommender system algorithm or the collaborative filtering algorithm."
  },
  {
    "index": "F18297",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、ちょっとした最後の実装の詳細を共有しておきたい。それは平均標準化(meannormalize)と呼ばれる物で、それはときどきアルゴリズムをちょっと良く機能させる程度の物。",
    "output": "In this video I want to just share one last implementational detail, namely mean normalization, which can sometimes just make the algorithm work a little bit better."
  },
  {
    "index": "F18298",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "平均標準化のアイデアのモチベーションを理解する為、なんの映画もレーティングしていないユーザーの例を考えてみよう。",
    "output": "To motivate the idea of mean normalization, let's consider an example of where there's a user that has not rated any movies."
  },
  {
    "index": "F18299",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、我らの四人のユーザー、Alice,Bob,Carol,Daveに加えて、さらに5番目のユーザー、Eveを足したとしよう。彼女は一つの映画もレーティングしていないとする。",
    "output": "So, in addition to our four users, Alice, Bob, Carol, and Dave, I've added a fifth user, Eve, who hasn't rated any movies."
  },
  {
    "index": "F18300",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らの協調フィルリングのアルゴリズムがこのユーザーに何をするのか見てみよう。",
    "output": "Let's see what our collaborative filtering algorithm will do on this user."
  },
  {
    "index": "F18301",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてパラメータベクトルのシータ5を学習したい。",
    "output": "Let's say that n is equal to 2 and so we're going to learn two features and we are going to have to learn a parameter vector theta 5, which is going to be in R2, remember this is now vectors in Rn not Rn+1, we'll learn the parameter vector theta 5 for our user number 5, Eve."
  },
  {
    "index": "F18302",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最適化の目的関数の最初の項を見てみると、ユーザーEveはなんの映画もレーティングしていないので、rijが1となる映画は、Eveに関しては一つも無い。",
    "output": "So if we look in the first term in this optimization objective, well the user Eve hasn't rated any movies, so there are no movies for which Rij is equal to one for the user Eve and so this first term plays no role at all in determining theta 5 because there are no movies that Eve has rated."
  },
  {
    "index": "F18303",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからシータ5に影響を与える唯一の項はこの項だ。",
    "output": "And so the only term that effects theta 5 is this term."
  },
  {
    "index": "F18304",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、我らはベクトルシータ5を、最後の正規化項をなるべく小さくするように選ぶ、と主張している。",
    "output": "And so we're saying that we want to choose vector theta 5 so that the last regularization term is as small as possible."
  },
  {
    "index": "F18305",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、我らが最小化しようとするのはこのラムダ/2のシータ5下付き添字1の二乗足すことのシータ5下付き添字2の二乗だ。以上がユーザー5に関する正規化項の要素だ。",
    "output": "In other words we want to minimize this lambda over 2 theta 5 subscript 1 squared plus theta 5 subscript 2 squared so that's the component of the regularization term that corresponds to user 5, and of course if your goal is to minimize this term, then what you're going to end up with is just theta 5 equals 0 0."
  },
  {
    "index": "F18306",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら正規化項はパラメータを0に近くなるように推奨する訳だが、そこでもしパラメータを0から引き離すデータが存在しなければ、何故ならこの最初の項はシータ5には影響しないので、結局シータ5としては全ての要素が0のベクトルを得る事になる。",
    "output": "Because a regularization term is encouraging us to set parameters close to 0 and if there is no data to try to pull the parameters away from 0, because this first term doesn't effect theta 5, we just end up with theta 5 equals the vector of all zeros."
  },
  {
    "index": "F18307",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、ユーザー5が映画をどうレーティングするかを予測しようとすると、どんな映画に対しても、シータ5の転置xiはいかなるiに対しても、イコール0となる。",
    "output": "And so when we go to predict how user 5 would rate any movie, we have that theta 5 transpose xi, for any i, that's just going to be equal to zero."
  },
  {
    "index": "F18308",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シータ5はどのxに対しても0なので、内積は0となる。",
    "output": "Because theta 5 is 0 for any value of x, this inner product is going to be equal to 0."
  },
  {
    "index": "F18309",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから我らは結局、Eveはどの動画も0とレーティングする、と予測する事になる。",
    "output": "And what we're going to have therefore, is that we're going to predict that Eve is going to rate every single movie with zero stars."
  },
  {
    "index": "F18310",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもこの予想はあんまり役に立たない、よね?",
    "output": "But this doesn't seem very useful does it?"
  },
  {
    "index": "F18311",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり別々の映画を見ていくと、LoveatLast、この最初の映画は、何人かは星5とレーティングした。",
    "output": "I mean if you look at the different movies, Love at Last, this first movie, a couple people rated it 5 stars."
  },
  {
    "index": "F18312",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからある人々はある種の映画を好むのだ。",
    "output": "So some people do like some movies."
  },
  {
    "index": "F18313",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからEveは全ての映画を0と付けると予測するのは、あまり便利では無い。",
    "output": "It seems not useful to just predict that Eve is going to rate everything 0 stars."
  },
  {
    "index": "F18314",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらに実際、もし我らがEveは全てに星0のレーティングをする、と予測してしまうと、彼女に推薦する映画を決める良い方法も無くなってしまう。何故ならこれら全ての映画はEveの場合、完全に同じ予測値となってしまうから。",
    "output": "And in fact if we're predicting that eve is going to rate everything 0 stars, we also don't have any good way of recommending any movies to her, because you know all of these movies are getting exactly the same predicted rating for Eve so there's no one movie with a higher predicted rating that we could recommend to her, so, that's not very good."
  },
  {
    "index": "F18315",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはあまり良くない。平均標準化法のアイデアはこの問題を修正する。",
    "output": "The idea of mean normalization will let us fix this problem."
  },
  {
    "index": "F18316",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはこんな風に機能する。",
    "output": "So here's how it works."
  },
  {
    "index": "F18317",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前と同様、全ての映画のレーティングをこの行列Yにグルーピングする。つまりこれらのレーティングを全て持ってきて、この行列Yにグルーピングする。",
    "output": "As before let me group all of my movie ratings into this matrix Y, so just take all of these ratings and group them into matrix Y."
  },
  {
    "index": "F18318",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのここにある行の、全部はてなマークなのは、Eveがなんの映画もレーティングしていない事に対応している。",
    "output": "And this column over here of all question marks corresponds to Eve's not having rated any movies."
  },
  {
    "index": "F18319",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで平均標準化を実行する為には、各映画の得たレーティングの平均を計算する。",
    "output": "Now to perform mean normalization what I'm going to do is compute the average rating that each movie obtained."
  },
  {
    "index": "F18320",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、それをミューというベクトルに保存する。",
    "output": "And I'm going to store that in a vector that we'll call mu."
  },
  {
    "index": "F18321",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、最初の映画は二つの星5と二つの星0のレーティングを得たのだから、その平均は星2.5だ。",
    "output": "So the first movie got two 5-star and two 0-star ratings, so the average of that is a 2.5-star rating."
  },
  {
    "index": "F18322",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二番目の映画の平均は星2.5だ。",
    "output": "The second movie had an average of 2.5-stars and so on."
  },
  {
    "index": "F18323",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後の映画は、0,0,5,0だから0,0,5,0の平均は平均をとると、平均は1.25レーティング。",
    "output": "And the final movie that has 0, 0, 5, 0. And the average of 0, 0, 5, 0, that averages out to an average of 1.25 rating."
  },
  {
    "index": "F18324",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、全映画のレーティングを見ていき、そこから平均のレーティングを引いていく。",
    "output": "And what I'm going to do is look at all the movie ratings and I'm going to subtract off the mean rating."
  },
  {
    "index": "F18325",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの最初の要素、5は、2.5を引くから、2.5となる。",
    "output": "So this first element 5 I'm going to subtract off 2.5 and that gives me 2.5."
  },
  {
    "index": "F18326",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二番目の要素5からは2.5を引くから2.5となる。",
    "output": "And the second element 5 subtract off of 2.5, get a 2.5."
  },
  {
    "index": "F18327",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、私がやってる事は映画のレーティングの行列をもってきて、この幅の広い行列を持ってきて、各列から、その映画の平均のレーティングを引く、という事。",
    "output": "In other words, what I'm going to do is take my matrix of movie ratings, take this wide matrix, and subtract form each row the average rating for that movie."
  },
  {
    "index": "F18328",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、私がやってるのは、各映画を、平均が0になるように標準化しているだけ。",
    "output": "So, what I'm doing is just normalizing each movie to have an average rating of zero."
  },
  {
    "index": "F18329",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に一つ例を見る。",
    "output": "And so just one last example."
  },
  {
    "index": "F18330",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この最後の列を見ると、0050だ。",
    "output": "If you look at this last row, 0 0 5 0."
  },
  {
    "index": "F18331",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1.25を引くから、結局ここの値となる。",
    "output": "We're going to subtract 1.25, and so I end up with these values over here."
  },
  {
    "index": "F18332",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして現在、もちろんこのはてなマークははてなマークのままだ。",
    "output": "So now and of course the question marks stay a question mark."
  },
  {
    "index": "F18333",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、この新しい行列Yにある各映画も平均のレーティングは0となる。",
    "output": "So each movie in this new matrix Y has an average rating of 0."
  },
  {
    "index": "F18334",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで次にやる事としては、このレーティングの集合を使って、協調フィルタリングアルゴリズムを行う。",
    "output": "What I'm going to do then, is take this set of ratings and use it with my collaborative filtering algorithm."
  },
  {
    "index": "F18335",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これをユーザーから取ったデータのフリをさせて、言い換えると、これらをユーザーから取った実際のレーティングのフリをさせて、そしてこれを私のデータセットとしてパラメータのシータjとフィーチャーのxiを学習させる。-これらの平均標準化された映画のレーティングから。",
    "output": "So I'm going to pretend that this was the data that I had gotten from my users, or pretend that these are the actual ratings I had gotten from the users, and I'm going to use this as my data set with which to learn my parameters theta J and my features XI - from these mean normalized movie ratings."
  },
  {
    "index": "F18336",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "映画のレーティングの予測をさせたくなったら、以下のようにする:ユーザーjの映画iに関しては、シータj転置xiと予測する。ここでxとシータはこの、平均標準化したデータセットから学習したパラメータ。",
    "output": "When I want to make predictions of movie ratings, what I'm going to do is the following: for user J on movie I, I'm gonna predict theta J transpose XI, where X and theta are the parameters that I've learned from this mean normalized data set."
  },
  {
    "index": "F18337",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、このデータセットは既に平均が引かれた物だから、映画iに関して予測を行いたいなら、平均を足し戻す必要がある。だからミューiを足し戻す。",
    "output": "But, because on the data set, I had subtracted off the means in order to make a prediction on movie i, I'm going to need to add back in the mean, and so i'm going to add back in mu i."
  },
  {
    "index": "F18338",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が私の予測となる。そこでは、トレーニングデータから平均を全て引いたのだから、予測を行う時には、これらの平均、ミューiを映画iに対して足し戻さなくてはならない。",
    "output": "And so that's going to be my prediction where in my training data subtracted off all the means and so when we make predictions and we need to add back in these means mu i for movie i."
  },
  {
    "index": "F18339",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、もしユーザー5、つまりEveに対して、前のスライドの議論がいまだに適用出来て、Eveはなんの映画もレーティングしてないので、だからユーザー5について学習したパラメータはまだイコール0,0だ。",
    "output": "And so specifically if you user 5 which is Eve, the same argument as the previous slide still applies in the sense that Eve had not rated any movies and so the learned parameter for user 5 is still going to be equal to 0, 0."
  },
  {
    "index": "F18340",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから得られる物は、つまり、特定の映画iについてEveの結果を予測すると、シータ5転置xi足すことの、ミューiを足し戻す、のだから、この最初の要素はシータ5が0なら0となる。",
    "output": "And so what we're going to get then is that on a particular movie i we're going to predict for Eve theta 5, transpose xi plus add back in mu i and so this first component is going to be equal to zero, if theta five is equal to zero."
  },
  {
    "index": "F18341",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから映画iに関しては、ミューiになる、と予測する事になる。",
    "output": "And so on movie i, we are going to end a predicting mu i."
  },
  {
    "index": "F18342",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは納得出来る。",
    "output": "And, this actually makes sense."
  },
  {
    "index": "F18343",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、映画1に関しては、Eveが2.5とレーティングするだろう、と予測する訳だ。",
    "output": "It means that on movie 1 we're going to predict Eve rates it 2.5."
  },
  {
    "index": "F18344",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは実際に筋が通っている、何故ならEveがまだ何もレーティングしていないとするとこの新しいユーザーEveについては我らは何も知らない事になる。だから我らがする事といえば、各映画に対してそれらの映画のレーティングの平均と予測する訳だ。",
    "output": "This actually makes sense, because it says that if Eve hasn't rated any movies and we just don't know anything about this new user Eve, what we're going to do is just predict for each of the movies, what are the average rating that those movies got."
  },
  {
    "index": "F18345",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、平均標準化について議論した。そこでは、Y行列の各列を、平均が0になるように標準化した。",
    "output": "Finally, as an aside, in this video we talked about mean normalization, where we normalized each row of the matrix y, to have mean 0."
  },
  {
    "index": "F18346",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし全くレーティングされていない映画がある場合は、それは何もレーティングしていないユーザーと似ているが、レーティングの一切無い映画がある場合、各行の平均が0になるようなバージョンのアルゴリズムを使う事も出来る。列を平均が0になるように標準化する代わりにだ。",
    "output": "In case you have some movies with no ratings, so it is analogous to a user who hasn't rated anything, but in case you have some movies with no ratings, you can also play with versions of the algorithm, where you normalize the different columns to have means zero, instead of normalizing the rows to have mean zero, although that's maybe less important, because if you really have a movie with no rating, maybe you just shouldn't recommend that movie to anyone, anyway."
  },
  {
    "index": "F18347",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもこちらは比較的重要では無いパターンかもしれない。何故なら、実際にレーティングの無い映画があったら、なんにせよその映画は誰にも推薦すべきでは無いかもしれないから。",
    "output": "And so, taking care of the case of a user who hasn't rated anything might be more important than taking care of the case of a movie that hasn't gotten a single rating."
  },
  {
    "index": "F18348",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではまとめだ。以上が協調フィルタリングの前処理として平均標準化を行う方法だ。",
    "output": "So to summarize, that's how you can do mean normalization as a sort of pre-processing step for collaborative filtering."
  },
  {
    "index": "F18349",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたのデータセットによっては、この手法はあなたの実装をちょっぴり良く振舞わせてくれるかもしれない。",
    "output": "Depending on your data set, this might some times make your implementation work just a little bit better."
  },
  {
    "index": "F18350",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "続く幾つかのビデオで、大規模スケールの機械学習について話す。",
    "output": "In the next few videos, we'll talk about large scale machine learning."
  },
  {
    "index": "F18351",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、アルゴリズムなんだけれど、とてもビッグなデータセットを見る物だ。",
    "output": "That is, algorithms but viewing with big data sets. If you look back at a recent 5 or 10-year history of machine learning."
  },
  {
    "index": "F18352",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここ最近5年とか10年の機械学習の歴史を振り返ると、5年前と比べても凄く学習アルゴリズムがうまく動作するようになった理由の一つには、アルゴリズムのトレーニングに使えるデータの量が単純に増えた、というのがある。",
    "output": "One of the reasons that learning algorithms work so much better now than even say, 5-years ago, is just the sheer amount of data that we have now and that we can train our algorithms on."
  },
  {
    "index": "F18353",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここからの幾つかのビデオでは、そんな巨大なデータセットがあった時にそれを扱うアルゴリズムについて議論していきたい。",
    "output": "In these next few videos, we'll talk about algorithms for dealing when we have such massive data sets."
  },
  {
    "index": "F18354",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、そもそも何故そんな大きなデータセットを使いたいと思うのか?",
    "output": "So why do we want to use such large data sets?"
  },
  {
    "index": "F18355",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは既に、機械学習のシステムで高いパフォーマンスを得たい時には、ベストな方法の一つに低いバイアスの学習アルゴリズムを使い、大量のデータでトレーニングさせる、というのがあった。",
    "output": "We've already seen that one of the best ways to get a high performance machine learning system, is if you take a low-bias learning algorithm, and train that on a lot of data."
  },
  {
    "index": "F18356",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "既に見た前出の例としては、このややこしい単語の分類の例がある。",
    "output": "And so, one early example we have already seen was this example of classifying between confusable words."
  },
  {
    "index": "F18357",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Forbreakfastには、Iateといえばtwoのeggsとなる。この例の問題を解くと、こんな結果となる。",
    "output": "So, for breakfast, I ate two (TWO) eggs and we saw in this example, these sorts of results, where, you know, so long as you feed the algorithm a lot of data, it seems to do very well."
  },
  {
    "index": "F18358",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、これらのような結果から、以下のような事が良く言われる。機械学習においては、勝者はもっとも良いアルゴリズムを持つ物では無く、もっともデータを持っている人だ、と。",
    "output": "And so it's results like these that has led to the saying in machine learning that often it's not who has the best algorithm that wins."
  },
  {
    "index": "F18359",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから大規模なデータセットから学習したい、少なくともそんなデータセットが入手可能ならば。",
    "output": "So you want to learn from large data sets, at least when we can get such large data sets."
  },
  {
    "index": "F18360",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "しかし大規模なデータセットからの学習は、特有の問題もつきまとう。具体的には、計算的な問題だ。",
    "output": "But learning with large data sets comes with its own unique problems, specifically, computational problems."
  },
  {
    "index": "F18361",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニングセットのサイズmが100,000,000だとしよう。",
    "output": "Let's say your training set size is M equals 100,000,000."
  },
  {
    "index": "F18362",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはこんにち的なデータセットしては、普通に現実的な範囲だ。",
    "output": "And this is actually pretty realistic for many modern data sets."
  },
  {
    "index": "F18363",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "USの国勢調査のデータを見ると、そこには、3億の人がUSには居るから、普通に何億ってデータを扱う事になる。",
    "output": "If you look at the US Census data set, if there are, you know, 300 million people in the US, you can usually get hundreds of millions of records."
  },
  {
    "index": "F18364",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "人気のあるwebサイトのトラフィックの量を見ると、簡単に億より多くの手本を得る事になる。",
    "output": "If you look at the amount of traffic that popular websites get, you easily get training sets that are much larger than hundreds of millions of examples."
  },
  {
    "index": "F18365",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "線形回帰のモデルをトレーニングしたいとしよう。またはロジスティック回帰でも良い。",
    "output": "And let's say you want to train a linear regression model, or maybe a logistic regression model, in which case this is the gradient descent rule."
  },
  {
    "index": "F18366",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして勾配を計算する為に必要な事を見ると、それはここの項だ。そしてmが一億の時は、1億に渡る和を取る必要がある、これらの微分項を計算する為には、そして最急降下法の1ステップを実行する為には。",
    "output": "And if you look at what you need to do to compute the gradient, which is this term over here, then when M is a hundred million, you need to carry out a summation over a hundred million terms, in order to compute these derivatives terms and to perform a single step of decent."
  },
  {
    "index": "F18367",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1億に渡る和を取るという計算量的なコストの為に、最急降下法のたった1ステップを計算する為だけに。続く一連のビデオで、これを別の何かに置き換えるテクニックや、この微分項を計算するより効率的な方法について議論する。",
    "output": "Because of the computational expense of summing over a hundred million entries in order to compute just one step of gradient descent, in the next few videos we've spoken about techniques for either replacing this with something else or to find more efficient ways to compute this derivative."
  },
  {
    "index": "F18368",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この大規模スケールの機械学習の一連のビデオを観終わった頃には、線形回帰とかロジスティック回帰とかニューラルネットワークのモデルのフィッティングをこんにち的なデータセット、つまり一億の手本とかに行う方法を理解する事になるだろう。",
    "output": "By the end of this sequence of videos on large scale machine learning, you know how to fit models, linear regression, logistic regression, neural networks and so on even today's data sets with, say, a hundred million examples."
  },
  {
    "index": "F18369",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、一億の手本でモデルをトレーニングするという努力を払う前に、単に1000の手本を使うだけでダメなのか?という事も自らに問うてみなくてはならない。",
    "output": "Of course, before we put in the effort into training a model with a hundred million examples, We should also ask ourselves, well, why not use just a thousand examples."
  },
  {
    "index": "F18370",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "時には1億の手本からランダムに1000の手本をピックアップして、その1000の手本でトレーニングするだけ、でも良いかもしれない。",
    "output": "Maybe we can randomly pick the subsets of a thousand examples out of a hundred million examples and train our algorithm on just a thousand examples."
  },
  {
    "index": "F18371",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれらの巨大なモデルをトレーニングするのに必要なソフトウェアの開発などに投資する前に、1000の手本でトレーニングしてみる事は、良いサニティチェックとなる。",
    "output": "So before investing the effort into actually developing and the software needed to train these massive models is often a good sanity check, if training on just a thousand examples might do just as well."
  },
  {
    "index": "F18372",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より少ないトレーニングセットでサニティチェックを行う方法は、つまりより少ないm=1000のサイズのトレーニングセットを用いる方法は、通常の学習曲線をプロットする、という方法で良いだろう。もし学習曲線をプロットしてみて、トレーニングの目的関数がこんな感じなら、この目的関数はJtrainのシータだが、そしてクロスバリデーションセットの目的関数、Jcvのシータがこんな感じだったとする。",
    "output": "The way to sanity check of using a much smaller training set might do just as well, that is if using a much smaller n equals 1000 size training set, that might do just as well, it is the usual method of plotting the learning curves, so if you were to plot the learning curves and if your training objective were to look like this, that's J train theta."
  },
  {
    "index": "F18373",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、高バリアンスな学習アルゴリズムのようなので、さらに追加でトレーニング手本を足す事は、パフォーマンスを改善すると確信が持てる。",
    "output": "And if your cross-validation set objective, Jcv of theta would look like this, then this looks like a high-variance learning algorithm, and we will be more confident that adding extra training examples would improve performance."
  },
  {
    "index": "F18374",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、対照的に、学習曲線をプロットしたら、トレーニングの目的関数がこんな感じで、クロスバリデーションの目的関数がこんな感じだと、これは高バイアスな学習アルゴリズムに見える。",
    "output": "Whereas in contrast if you were to plot the learning curves, if your training objective were to look like this, and if your cross-validation objective were to look like that, then this looks like the classical high-bias learning algorithm."
  },
  {
    "index": "F18375",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この後者の場合には、例えばこれがm=1000までのプロットだとして、この辺がm=500で、1000までとして、するとたぶん、データを1億に増やしてもたぶんあまり良くはならないだろう。それならばアルゴリズムのスケールを増やす為に努力を費やすよりは、m=1000のままにしておく方が良かろう。",
    "output": "And in the latter case, you know, if you were to plot this up to, say, m equals 1000 and so that is m equals 500 up to m equals 1000, then it seems unlikely that increasing m to a hundred million will do much better and then you'd be just fine sticking to n equals 1000, rather than investing a lot of effort to figure out how the scale of the algorithm."
  },
  {
    "index": "F18376",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、この右側の図のような状況だったら、次に行うべき自然なステップとしては、追加のフィーチャーを足すとか、隠れユニットをニューラルネットワークに足すとか、そういう事で、そういう事を通してm=1000のままだと左側の状態に近づいていったらその時は初めて1億以上の手本を使うように、インフラを追加したり、アルゴリズムを変更したりする事に、より確信を持てるようになり、それは実際に良い自分の時間の使い方だと思えるだろう。",
    "output": "Of course, if you were in the situation shown by the figure on the right, then one natural thing to do would be to add extra features, or add extra hidden units to your neural network and so on, so that you end up with a situation closer to that on the left, where maybe this is up to n equals 1000, and this then gives you more confidence that trying to add infrastructure to change the algorithm to use much more than a thousand examples that might actually be a good use of your time."
  },
  {
    "index": "F18377",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、大規模スケールの機械学習においては、とてもビッグなデータを扱うのに、計算量的にリーズナブルな、または効率的な方法を知りたい。",
    "output": "So in large-scale machine learning, we like to come up with computationally reasonable ways, or computationally efficient ways, to deal with very big data sets."
  },
  {
    "index": "F18378",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "続く幾つかのビデオでは、二つの主なアイデアを見ていく。",
    "output": "In the next few videos, we'll see two main ideas."
  },
  {
    "index": "F18379",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の物は確率的な最急降下法、そして二番目はMapReduceと呼ばれる物。とてもビッグなデータを見るのに。",
    "output": "The first is called stochastic gradient descent and the second is called Map Reduce, for viewing with very big data sets."
  },
  {
    "index": "F18380",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらの手法を学んだ後には、あなたの学習アルゴリズムをビッグにスケールアップ出来るようになり、様々な応用に対して、もっと良いパフォーマンスが得られるようになる事を祈る。",
    "output": "And after you've learned about these methods, hopefully that will allow you to scale up your learning algorithms to big data and allow you to get much better performance on many different applications."
  },
  {
    "index": "F18381",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多くの学習アルゴリズムにとって、線形回帰やロジスティック回帰やニューラルネットワークなどにとって、アルゴリズムを導出する方法は、まずコスト関数、または目的関数を考えて、そしてそれを最急降下法なりのアルゴリズムを使って最小化する、という物だった。",
    "output": "For many learning algorithms, among them linear regression, logistic regression and neural networks, the way we derive the algorithm was by coming up with a cost function or coming up with an optimization objective. And then using an algorithm like gradient descent to minimize that cost function."
  },
  {
    "index": "F18382",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "大量のトレーニングセットがある時は、最急降下法は極めて計算的に高価な手続きとなる。",
    "output": "We have a very large training set gradient descent becomes a computationally very expensive procedure."
  },
  {
    "index": "F18383",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、基本的な最急降下法のアルゴリズムを改変した、確率的最急降下法という物を議論する、それはこれらのアルゴリズムをもっとビッグなトレーニングセットにスケール出来るようにする。",
    "output": "In this video, we'll talk about a modification to the basic gradient descent algorithm called Stochastic gradient descent, which will allow us to scale these algorithms to much bigger training sets."
  },
  {
    "index": "F18384",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最急降下法で線形回帰のモデルをトレーニングしているとしよう。",
    "output": "Suppose you are training a linear regression model using gradient descent."
  },
  {
    "index": "F18385",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "軽く復習しておくと、仮説はこんな形、コスト関数はこんな感じ。それは1/2の仮説の二乗誤差の平均の、mのトレーニング手本に渡る和を取った物だ。",
    "output": "As a quick recap, the hypothesis will look like this, and the cost function will look like this, which is the sum of one half of the average square error of your hypothesis on your m training examples, and the cost function we've already seen looks like this sort of bow-shaped function."
  },
  {
    "index": "F18386",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "パラメータシータ0とシータ1の関数としてプロットすると、コスト関数Jは弓型の関数となる。",
    "output": "So, plotted as function of the parameters theta 0 and theta 1, the cost function J is a sort of a bow-shaped function."
  },
  {
    "index": "F18387",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最急降下法はこんな感じだ。最急降下法の内側のループでは、パラメータシータをこの式を使って繰り返しアップデートしていく。",
    "output": "And gradient descent looks like this, where in the inner loop of gradient descent you repeatedly update the parameters theta using that expression."
  },
  {
    "index": "F18388",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、このビデオの残りの部分で、線形回帰を実行出来る例として使い続ける。",
    "output": "Now in the rest of this video, I'm going to keep using linear regression as the running example."
  },
  {
    "index": "F18389",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが確率的最急降下法のアイデア自体は完全に一般的な物で、それはその他の学習アルゴリズム、ロジスティック回帰とかニューラルネットワークとか、その他なんでも特定のトレーニングセットに対して最急降下法で学習するアルゴリズムになら適用出来る。",
    "output": "But the ideas here, the ideas of Stochastic gradient descent is fully general and also applies to other learning algorithms like logistic regression, neural networks and other algorithms that are based on training gradient descent on a specific training set."
  },
  {
    "index": "F18390",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、これは最急降下法が何をするかだ。もしパラメータがここの点で初期化されたら、最急降下法はパラメータをグローバル最小へと持っていく。",
    "output": "So here's a picture of what gradient descent does, if the parameters are initialized to the point there then as you run gradient descent different iterations of gradient descent will take the parameters to the global minimum."
  },
  {
    "index": "F18391",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな感じの軌跡を通って、だいたいまっすぐグローバル最小へと向かう。",
    "output": "So take a trajectory that looks like that and heads pretty directly to the global minimum."
  },
  {
    "index": "F18392",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、最急降下法の問題点としては、もしmが大きい時には、この微分項を計算するのが、とても高価になってしまう、という事。",
    "output": "Now, the problem with gradient descent is that if m is large."
  },
  {
    "index": "F18393",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故ならこれは全てのm手本に渡って和を取るから。",
    "output": "Then computing this derivative term can be very expensive, because the surprise, summing over all m examples."
  },
  {
    "index": "F18394",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもしmが3億なら、、、アメリカ合衆国にはだいたい3億人の人がいる。",
    "output": "So if m is 300 million, alright. So in the United States, there are about 300 million people."
  },
  {
    "index": "F18395",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、US、またはアメリカ合衆国の国勢調査のデータは、そんなオーダーの数のレコードとなる。",
    "output": "And so the US or United States census data may have on the order of that many records."
  },
  {
    "index": "F18396",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると、そこに線形回帰のモデルをフィッティングしたいとすると、3億のレコードに渡って和をとらなくてはならない。",
    "output": "So you want to fit the linear regression model to that then you need to sum over 300 million records."
  },
  {
    "index": "F18397",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それはとても高価だ。",
    "output": "And that's very expensive."
  },
  {
    "index": "F18398",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このアルゴリズムに名前をつけておく。この特定の最急降下法のバージョンは、バッチ最急降下法とも呼ばれる。",
    "output": "To give the algorithm a name, this particular version of gradient descent is also called Batch gradient descent."
  },
  {
    "index": "F18399",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで「バッチ」という単語は一度に全部のトレーニング手本を見るという事実を表している。",
    "output": "And the term Batch refers to the fact that we're looking at all of the training examples at a time."
  },
  {
    "index": "F18400",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それをある種の、全てのトレーニング手本のバッチ、と呼ぶ。",
    "output": "We call it sort of a batch of all of the training examples."
  },
  {
    "index": "F18401",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは実はあんまりいい名前じゃ、ベストな名前って訳じゃない。だが、機械学習屋の人々がこのバージョンの最急降下法をそう呼んでるんだから仕方がない。",
    "output": "And it really isn't the, maybe the best name but this is what machine learning people call this particular version of gradient descent."
  },
  {
    "index": "F18402",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの3億の国勢調査のレコードをディスクに保存して退避させてしまってる事を想像してみよう。",
    "output": "And if you imagine really that you have 300 million census records stored away on disc."
  },
  {
    "index": "F18403",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このアルゴリズムのまわり方としては、この微分項を計算するのに3億のレコードを全てコンピュータのメモリに読み出す必要がある。",
    "output": "The way this algorithm works is you need to read into your computer memory all 300 million records in order to compute this derivative term."
  },
  {
    "index": "F18404",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのレコード全てをコンピュータにストリーム処理しなくてはいけない。何故ならコンピュータメモリに全て保存する事は出来ないから。",
    "output": "You need to stream all of these records through computer because you can't store all your records in computer memory."
  },
  {
    "index": "F18405",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからそれらを読んでいき、ゆっくりと和を蓄積していく事で、はじめて微分が計算出来る。",
    "output": "So you need to read through them and slowly, you know, accumulate the sum in order to compute the derivative."
  },
  {
    "index": "F18406",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれを全部終えたら、その結果最急降下法を1ステップだけ進める事が出来る、、、そしてまた全体をやりなおさなくてはいけない。",
    "output": "And then having done all that work, that allows you to take one step of gradient descent. And now you need to do the whole thing again."
  },
  {
    "index": "F18407",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、3億のレコードを全部スキャンして、それらの和を蓄積していく。",
    "output": "You know, scan through all 300 million records, accumulate these sums."
  },
  {
    "index": "F18408",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その仕事を全て終えても、最急降下法のちょっとのステップがもう一歩進むだけ。",
    "output": "And having done all that work, you can take another little step using gradient descent."
  },
  {
    "index": "F18409",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてまた、同じ事をする。",
    "output": "And then do that again."
  },
  {
    "index": "F18410",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてまた三歩目が進める。などなど。",
    "output": "And then you take yet a third step."
  },
  {
    "index": "F18411",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、アルゴリズムが収束するのに、凄い長い時間がかかる。",
    "output": "And so it's gonna take a long time in order to get the algorithm to converge."
  },
  {
    "index": "F18412",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バッチ最急降下法と比較して、別のアルゴリズムを考え出していく、それは各イテレーションごとに全てのトレーニング手本を見なくて良く、一回のイテレーションでは一つのトレーニング手本単体だけを見れば良い。",
    "output": "In contrast to Batch gradient descent, what we are going to do is come up with a different algorithm that doesn't need to look at all the training examples in every single iteration, but that needs to look at only a single training example in one iteration."
  },
  {
    "index": "F18413",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "新しいアルゴリズムにうつる前に、ここにバッチ最急降下法のアルゴリズムを再掲しておこう、これがコスト関数で、これがアップデート、そしてここの項はもちろん最急降下法で用いる、偏微分項だ、パラメータシータjによる、我らが最適化の目的関数Jtrainのシータの。",
    "output": "Before moving on to the new algorithm, here's just a Batch gradient descent algorithm written out again with that being the cost function and that being the update and of course this term here, that's used in the gradient descent rule, that is the partial derivative with respect to the parameters theta J of our optimization objective, J train of theta."
  },
  {
    "index": "F18414",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、大規模なデータセットにもっと効率的にスケールするアルゴリズムを見てみよう。",
    "output": "Now, let's look at the more efficient algorithm that scales better to large data sets."
  },
  {
    "index": "F18415",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "確率的最急降下法と呼ばれるアルゴリズムを用いる為に、このベクトル、コスト関数をちょっと違う形にして、トレーニング手本、x(i),y(i)に関するパラメータをシータとするコストを定義する、それはイコール、1/2掛ける、二乗誤差のx(i),y(i)に対して仮説が引き起こす分。",
    "output": "In order to work off the algorithms called Stochastic gradient descent, this vectors the cost function in a slightly different way then they define the cost of the parameter theta with respect to a training example x(i), y(i) to be equal to one half times the squared error that my hypothesis incurs on that example, x(i), y(i)."
  },
  {
    "index": "F18416",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのコスト関数の項は、私の仮説が、単体の手本、x(i)とy(i)に対してどれだけ良いかを実際に測っている。",
    "output": "So this cost function term really measures how well is my hypothesis doing on a single example x(i), y(i)."
  },
  {
    "index": "F18417",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、全体のコスト関数、Jtrainは、等価な形でこう書ける。",
    "output": "Now you notice that the overall cost function j train can now be written in this equivalent form."
  },
  {
    "index": "F18418",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりJtrainは、m個のトレーニング手本の仮説のコストの平均に過ぎない。",
    "output": "So j train is just the average over my m training examples of the cost of my hypothesis on that example x(i), y(i)."
  },
  {
    "index": "F18419",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "線形回帰のコスト関数をこうやってみる見方を身につけた上で、確率的最急降下法が何をする物なのか、書き下してみよう。",
    "output": "Armed with this view of the cost function for linear regression, let me now write out what Stochastic gradient descent does."
  },
  {
    "index": "F18420",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "確率的最急降下法の最初のステップは、データセットをランダムにシャッフルする。",
    "output": "The first step of Stochastic gradient descent is to randomly shuffle the data set."
  },
  {
    "index": "F18421",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ランダムにシャッフルする、という事の意味は、m個のトレーニング手本をランダムに並べ替える、という事。",
    "output": "So by that I just mean randomly shuffle, or randomly reorder your m training examples."
  },
  {
    "index": "F18422",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは普通の前処理だ。後でこの件については考える。",
    "output": "It's sort of a standard pre-processing step, come back to this in a minute."
  },
  {
    "index": "F18423",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが確率的最急降下法の主な部分は、その次に以下のように続く所だ。",
    "output": "But the main work of Stochastic gradient descent is then done in the following."
  },
  {
    "index": "F18424",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "i=1からmまで、リピートする事の、、、つまりトレーニング手本を繰り返しスキャンして、以下のアップデートを実施する。",
    "output": "We're going to repeat for i equals 1 through m. So we'll repeatedly scan through my training examples and perform the following update."
  },
  {
    "index": "F18425",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "パラメータ、シータjをシータj引くことのアルファ掛けるh(x(i))引くことのy(i)に掛けるx(i)j。",
    "output": "Gonna update the parameter theta j as theta j minus alpha times h of x(i) minus y(i) times x(i)j."
  },
  {
    "index": "F18426",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このアップデートをいつも通り、全てのjの値に対して行う。",
    "output": "And we're going to do this update as usual for all values of j."
  },
  {
    "index": "F18427",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここで、ここの項はバッチ最急降下法の和の中にある物と、完全に一致する事が分かるだろう。",
    "output": "Now, you notice that this term over here is exactly what we had inside the summation for Batch gradient descent."
  },
  {
    "index": "F18428",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実のところ、もし解析学が得意なら、このここの項は、costのシータとx(i)を、パラメータシータjで偏微分した物に等しい事が示せる。",
    "output": "In fact, for those of you that are calculus is possible to show that that term here, that's this term here, is equal to the partial derivative with respect to my parameter theta j of the cost of the parameters theta on x(i), y(i)."
  },
  {
    "index": "F18429",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでこのcostはもちろん、前に定義した物。",
    "output": "Where cost is of course this thing that was defined previously."
  },
  {
    "index": "F18430",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このアルゴリズムのまとめとして、、、その前に中括弧を閉じておこう。",
    "output": "And just the wrap of the algorithm, let me close my curly braces over there."
  },
  {
    "index": "F18431",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、確率的最急降下法がやる事は、実際にトレーニング手本をスキャンして、そして最初にトレーニング手本の最初のx(1),y(1)を見る時、この最初の例だけを見て、最初の手本に関してだけのコストによる、最急降下法の小さな一ステップを実行する。",
    "output": "So what Stochastic gradient descent is doing is it is actually scanning through the training examples. And first it's gonna look at my first training example x(1), y(1)."
  },
  {
    "index": "F18432",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、最初の手本を見て、最初の手本のデータだけにもうちょっとだけフィットするように、パラメータを少しだけ変更する。",
    "output": "And then looking at only this first example, it's gonna take like a basically a little gradient descent step with respect to the cost of just this first training example."
  },
  {
    "index": "F18433",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを終えたら、この内側のforループの中で、次の二番目のトレーニング手本に進む。",
    "output": "So in other words, we're going to look at the first example and modify the parameters a little bit to fit just the first training example a little bit better."
  },
  {
    "index": "F18434",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで行う事は、またもう一歩、パラメーター空間内を進む事、つまりちょっとだけ良く二番目のトレーニング手本にフィットするようにパラメータを変更する。",
    "output": "Having done this inside this inner for-loop is then going to go on to the second training example."
  },
  {
    "index": "F18435",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを終えたら、三番目のトレーニング手本に進む。",
    "output": "And what it's going to do there is take another little step in parameter space, so modify the parameters just a little bit to try to fit just a second training example a little bit better."
  },
  {
    "index": "F18436",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして三番目のトーレニング手本にちょっとだけ良くフィットするように、パラメータを変更する。",
    "output": "Having done that, is then going to go onto my third training example."
  },
  {
    "index": "F18437",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをトレーニングセット全体に渡って行う。",
    "output": "And modify the parameters to try to fit just the third training example a little bit better, and so on until you know, you get through the entire training set."
  },
  {
    "index": "F18438",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの外側のループが、トレーニングセット全体を複数回繰り返させる。",
    "output": "And then this ultra repeat loop may cause it to take multiple passes over the entire training set."
  },
  {
    "index": "F18439",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この確率的最急降下法の見方は、データセットをランダムにシャッフルする事から始める理由も分かる。",
    "output": "This view of Stochastic gradient descent also motivates why we wanted to start by randomly shuffling the data set."
  },
  {
    "index": "F18440",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしシャッフルせずにトレーニングセットをここからスキャンして行ったら、むちゃくちゃにソートされてる順番にトレーニング手本を見ていく事になる、その順番はデータが最初からランダムに来たか、変な風にソートされているかに寄ってしまう。",
    "output": "This doesn't show us that when we scan through the training site here, that we end up visiting the training examples in some sort of randomly sorted order."
  },
  {
    "index": "F18441",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実際的には、ランダムにソートする事は確率的最急降下法をちょっとだけスピードアップする、ちょっとだけね。",
    "output": "Depending on whether your data already came randomly sorted or whether it came originally sorted in some strange order, in practice this would just speed up the conversions to Stochastic gradient descent just a little bit."
  },
  {
    "index": "F18442",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから念のため、それがランダムな並びと確信が持てる場合を除いて、普通はとりあえずデータセットをランダムにシャッフルしておく方が良い。",
    "output": "So in the interest of safety, it's usually better to randomly shuffle the data set if you aren't sure if it came to you in randomly sorted order."
  },
  {
    "index": "F18443",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもっと重要な点として、確率的最急降下法のもう一つの見方として、それは、通常の最急降下法ととても似ているが、だがこれらの微分項を全てのmトレーニング手本に渡って足すのではなく、この微分項を単に一つのトレーニング手本に対してだけ取る、という事をしている、そしてそこで既にパラメータの改善を開始してしまう。",
    "output": "But more importantly another view of Stochastic gradient descent is that it's a lot like descent but rather than wait to sum up these gradient terms over all m training examples, what we're doing is we're taking this gradient term using just one single training example and we're starting to make progress in improving the parameters already."
  },
  {
    "index": "F18444",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、全てのアメリカ合衆国の国勢調査のレコード3億件をなめるのを待つのでは無く、パラメータをちょっとだけ改善してグローバル最小へとちょっとだけ歩を進める為に、全てのトレーニング手本をスキャンする事を必要とするのでは無く、確率的最急降下法では、手本は一つしか見る必要が無くて、この場合のパラメータの改善を既に始めてしまって良い、パラメータをグローバル最小へと進めるという。",
    "output": "So rather than, you know, waiting 'till taking a path through all 300,000 United States Census records, say, rather than needing to scan through all of the training examples before we can modify the parameters a little bit and make progress towards a global minimum. For Stochastic gradient descent instead we just need to look at a single training example and we're already starting to make progress in this case of parameters towards, moving the parameters towards the global minimum."
  },
  {
    "index": "F18445",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがアルゴリズムをふたたび書き下した物だ。最初のステップはデータをランダムにシャッフルする事で、二番目のステップは実際の仕事をする所だが、そこでは一つのトレーニング手本、x(i)とy(i)に関してのみでアップデートしてしまう。",
    "output": "So, here's the algorithm written out again where the first step is to randomly shuffle the data and the second step is where the real work is done, where that's the update with respect to a single training example x(i), y(i)."
  },
  {
    "index": "F18446",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこのアルゴリズムがパラメータに何をしていくか、見てみよう。",
    "output": "So, let's see what this algorithm does to the parameters."
  },
  {
    "index": "F18447",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前に、バッチ最急降下法を使っている時に、それは全てのトレーニング手本を一度に見る物だという事を見た。",
    "output": "Previously, we saw that when we are using Batch gradient descent, that is the algorithm that looks at all the training examples in time, Batch gradient descent will tend to, you know, take a reasonably straight line trajectory to get to the global minimum like that."
  },
  {
    "index": "F18448",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バッチ最急降下法はグローバル最小へと向かう、かなりまっすぐな軌跡を描く傾向になる。",
    "output": "In contrast with Stochastic gradient descent every iteration is going to be much faster because we don't need to sum up over all the training examples."
  },
  {
    "index": "F18449",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それに対して確率的最急降下法は、各イテレーションはもっと早い、何故なら全てのトレーニング手本を足し合わせる必要が無いからだが、しかし各イテレーションは一つのトレーニング手本に対してだけより良くフィットするように試みるだけなので、だからもし確率的最急降下法を始めると、あー、確率的最急降下法をこの点とかから始めたとしよう。",
    "output": "But every iteration is just trying to fit single training example better."
  },
  {
    "index": "F18450",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初のイテレーションでは、この方向に進んだとする、二番目のイテレーションでは、うーん、二番目のイテレーションは偶然、ちょっとツイてなかったとしよう。そして実際には悪い方向にこんな感じでパラメータを進めてしまった。",
    "output": "So, if we were to start stochastic gradient descent, oh, let's start stochastic gradient descent at a point like that."
  },
  {
    "index": "F18451",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "三度目のイテレーションでは、三番目のトレーニング手本に対してだけもっと良くフィットするようにパラメータを変更する。",
    "output": "The first iteration, you know, may take the parameters in that direction and maybe the second iteration looking at just the second example maybe just by chance, we get more unlucky and actually head in a bad direction with the parameters like that."
  },
  {
    "index": "F18452",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこんな方向に向かったとする。",
    "output": "In the third iteration where we tried to modify the parameters to fit just the third training examples better, maybe we'll end up heading in that direction."
  },
  {
    "index": "F18453",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして四番目のトレーニング手本を見て、同じ事をする。",
    "output": "And then we'll look at the fourth training example and we will do that."
  },
  {
    "index": "F18454",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "5番目、6番目、7番目、などなど。",
    "output": "The fifth example, sixth example, 7th and so on."
  },
  {
    "index": "F18455",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして確率的最急降下法を実行すると、こんな結果が見られる:だいたいはパラメータはグローバル最小の方向に向かうが、いつもそうだという訳では無い。",
    "output": "And as you run Stochastic gradient descent, what you find is that it will generally move the parameters in the direction of the global minimum, but not always."
  },
  {
    "index": "F18456",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりもっとデタラメに見える、遠回りの軌跡を通ってグローバル最小を探す。",
    "output": "And so take some more random-looking, circuitous path to watch the global minimum."
  },
  {
    "index": "F18457",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実のところ、確率的最急降下法は、バッチ最急降下法がするような意味では収束しない。そして最終的には、それはグローバル最小のそばのある一定の範囲をうろちょろし続けるようになる。",
    "output": "And in fact as you run Stochastic gradient descent it doesn't actually converge in the same same sense as Batch gradient descent does and what it ends up doing is wandering around continuously in some region that's in some region close to the global minimum, but it doesn't just get to the global minimum and stay there."
  },
  {
    "index": "F18458",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが現実的には、それはそんなに問題じゃない、何故なら、パラメータがグローバル最小のきわめてそばの領域に居続けるなら、パラメータは最終的にグローバル最小に極めて近いはずなので、それは仮説としてはとても良い物となるだろう。",
    "output": "But in practice this isn't a problem because, you know, so long as the parameters end up in some region there maybe it is pretty close to the global minimum."
  },
  {
    "index": "F18459",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから通常、確率的最急降下法を実行すると、グローバル最小のそばのパラメータを得る事になり、それは実質的にはほとんどの現実的な目的にとって十分に良い物だ。",
    "output": "So, as parameters end up pretty close to the global minimum, that will be a pretty good hypothesis and so usually running Stochastic gradient descent we get a parameter near the global minimum and that's good enough for, you know, essentially any, most practical purposes."
  },
  {
    "index": "F18460",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に詳細を一つ。",
    "output": "Just one final detail."
  },
  {
    "index": "F18461",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "確率的最急降下法においては、この内側のループを複数回実行するように指示する、外側のループがある。",
    "output": "In Stochastic gradient descent, we had this outer loop repeat which says to do this inner loop multiple times."
  },
  {
    "index": "F18462",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、何回外側のループは繰り返せば良い?",
    "output": "So, how many times do we repeat this outer loop?"
  },
  {
    "index": "F18463",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニングセットのサイズによっては、このループは一回で十分かもしれない。",
    "output": "Depending on the size of the training set, doing this loop just a single time may be enough."
  },
  {
    "index": "F18464",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "典型的には、10回までのどこかって所かな。",
    "output": "And up to, you know, maybe 10 times may be typical so we may end up repeating this inner loop anywhere from once to ten times."
  },
  {
    "index": "F18465",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの内側のループを1回から10回の間のどこかの回数実行すれば良い。",
    "output": "So if we have a you know, truly massive data set like the this US census gave us that example that I've been talking about with 300 million examples, it is possible that by the time you've taken just a single pass through your training set."
  },
  {
    "index": "F18466",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、もし我らが、真に大量のデータセット、例えばこのUS国勢調査のような物で、3億の手本とかあるならば、トレーニングセットを1パスだけなめる頃には、つまりこのforでi=1から3億まで回せば、そのデータセットを1パスなめ終わる頃には、既に十分完璧な良い仮説に到達しているかもしれない。",
    "output": "So, this is for i equals 1 through 300 million. It's possible that by the time you've taken a single pass through your data set you might already have a perfectly good hypothesis."
  },
  {
    "index": "F18467",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合は、この内側のループは一回だけ実行すれば良い。凄い凄い大きなmなら。",
    "output": "In which case, you know, this inner loop you might need to do only once if m is very, very large."
  },
  {
    "index": "F18468",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが一般的には、1と10の間の価数のパスだけデータセットをなめる。この辺が普通だ。",
    "output": "But in general taking anywhere from 1 through 10 passes through your data set, you know, maybe fairly common."
  },
  {
    "index": "F18469",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもそれは本当にトレーニングセットのサイズに依存した話だ。",
    "output": "But really it depends on the size of your training set."
  },
  {
    "index": "F18470",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バッチ最急降下法と比較してみると、バッチ最急降下法だと、一つのパスでトレーニングセット全体をなめて、それだけやってたった一歩の最急降下法のステップしか進まない。",
    "output": "And if you contrast this to Batch gradient descent."
  },
  {
    "index": "F18471",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり最急降下法のこれらの小さなステップの、たった一歩のステップだけ。",
    "output": "With Batch gradient descent, after taking a pass through your entire training set, you would have taken just one single gradient descent steps."
  },
  {
    "index": "F18472",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれが、確率的最急降下法がもっと早くなりうる理由だ。",
    "output": "So one of these little baby steps of gradient descent where you just take one small gradient descent step and this is why Stochastic gradient descent can be much faster."
  },
  {
    "index": "F18473",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が確率的最急降下法アルゴリズムだ。",
    "output": "So, that was the Stochastic gradient descent algorithm."
  },
  {
    "index": "F18474",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれを実装すれば、あなたは多くの学習アルゴリズムをスケールアップして、よりビッグなデータセットに対して、もっと良いパフォーマンスが得られるように出来るだろう。",
    "output": "And if you implement it, hopefully that will allow you to scale up many of your learning algorithms to much bigger data sets and get much more performance that way."
  },
  {
    "index": "F18475",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオでは、確率的最急降下法について議論した、そしてそれがどうバッチ最急降下法に比べて早くなりうるのかも。",
    "output": "In the previous video, we talked about Stochastic gradient descent, and how that can be much faster than Batch gradient descent."
  },
  {
    "index": "F18476",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、ミニバッチ最急降下法と呼ばれる、もう一つのこの種の変種について見ていこう。それは時には、確率的最急降下法よりもちょっと早くなる事すらある。",
    "output": "In this video, let's talk about another variation on these ideas is called Mini-batch gradient descent they can work sometimes even a bit faster than stochastic gradient descent."
  },
  {
    "index": "F18477",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まとめてしまうと、ここまで議論してきたアルゴリズムはバッチ最急降下法は、一回のイテレーションに全てのm個の手本をなめる物だった。",
    "output": "In Batch gradient descent we will use all m examples in each generation."
  },
  {
    "index": "F18478",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方確率的最急降下法は一回のイテレーションで手本一つだけを見る。",
    "output": "Whereas in Stochastic gradient descent we will use a single example in each generation."
  },
  {
    "index": "F18479",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてミニバッチ最急降下法は、この間のどこかしらに位置する。",
    "output": "What Mini-batch gradient descent does is somewhere in between."
  },
  {
    "index": "F18480",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、このアルゴリズムで各イテレーションでb個の手本をなめる、ここでbはミニバッチサイズと呼ばれるパラメータ。つまりこれは、バッチ最急降下法と確率的最急降下法の間あたりを狙うアイデアだ。",
    "output": "Specifically, with this algorithm we're going to use b examples in each iteration where b is a parameter called the \"mini batch size\" so the idea is that this is somewhat in-between Batch gradient descent and Stochastic gradient descent."
  },
  {
    "index": "F18481",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはバッチ最急降下法みたいな物だが、もっと小さなバッチサイズを使う、っていう所が違う。",
    "output": "This is just like batch gradient descent, except that I'm going to use a much smaller batch size."
  },
  {
    "index": "F18482",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "典型的なbの値の選択肢としては、10とかその辺で、典型的と言える範囲はだいたい2から100くらいの間かな。",
    "output": "A typical choice for the value of b might be b equals 10, lets say, and a typical range really might be anywhere from b equals 2 up to b equals 100."
  },
  {
    "index": "F18483",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がミニバッチサイズの典型的なサイズだ。",
    "output": "So that will be a pretty typical range of values for the Mini-batch size."
  },
  {
    "index": "F18484",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてアイデアとしては、一度に手本を一つだけ使うのでもm個全部使うのでも無く、一度にb個だけの手本を使っていく、という物。",
    "output": "And the idea is that rather than using one example at a time or m examples at a time we will use b examples at a time."
  },
  {
    "index": "F18485",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをインフォーマルに書き下すと、まずb個の手本を取り出して、、、この例ではbを10としようか、すると次の10個の手本をトレーニングセットから取り出す。",
    "output": "For this example, let's say b equals 10. So we're going to get, the next 10 examples from my training set so that may be some set of examples xi, yi."
  },
  {
    "index": "F18486",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、10個の手本の場合、インデックスはxi,yiからx(i+9),y(i+9)までの10個の手本の集合が得られる。つまり全部で10個の手本が得られて、それに対して本質的には最急降下法のアップデートを実行する。",
    "output": "If it's 10 examples then the indexing will be up to x (i+9), y (i+9) so that's 10 examples altogether and then we'll perform essentially a gradient descent update using these 10 examples."
  },
  {
    "index": "F18487",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "学習率掛ける1/10掛けるk=iからi+9までの和を取る事のhの下付き添字シータx(k)-y(i)掛けるx(k)のj。",
    "output": "So, that's any rate times one tenth times sum over k equals i through i+9 of h subscript theta of x(k) minus y(k) times x(k)j."
  },
  {
    "index": "F18488",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この式では、微分項の和は10個の手本に渡って取られる。",
    "output": "And so in this expression, where summing the gradient terms over my ten examples."
  },
  {
    "index": "F18489",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれは10個で、これがバッチサイズで、i+9もまた、この9もパラメータbの選択した値から来てる。そしてこれを増やしていって、10番目のiまで来たら、そのあとは次の10個の手本に進む、それを以後続けて進めていく。",
    "output": "So, that's number ten, that's, you know, my mini batch size and just i+9 again, the 9 comes from the choice of the parameter b, and then after this we will then increase, you know, i by tenth, we will go on to the next ten examples and then keep moving like this."
  },
  {
    "index": "F18490",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからアルゴリズム全体を書きだすと、ここでのインデクシングをシンプルにする為に、ミニバッチサイズとして10を、トレーニングセットのサイズとして1000を想定すると、我らがやるのは、こんなforループ、fori=1,11,21...など、つまり10ずつのステップで進む、何故なら一度に10個の手本を見るから。",
    "output": "In order to simplify the indexing for this one at the right top, I'm going to assume we have a mini-batch size of ten and a training set size of a thousand, what we're going to do is have this sort of form, for i equals 1 and that in 21's the stepping, in steps of 10 because we look at 10 examples at a time."
  },
  {
    "index": "F18491",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの種の最急降下法アップデートを一度に10個の手本を用いて実行する。",
    "output": "And then we perform this sort of gradient descent update using ten examples at a time so this 10 and this i+9 those are consequence of having chosen my mini-batch to be ten."
  },
  {
    "index": "F18492",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの外側のforループは、991で終わってる、何故なら、もしトレーニング手本が1000個なら、サイズが10の100ステップが、トレーニングセットを全部なめるのに必要だからだ。",
    "output": "And you know, this ultimate four-loop, this ends at 991 here because if I have 1000 training samples then I need 100 steps of size 10 in order to get through my training set."
  },
  {
    "index": "F18493",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がミニバッチ最急降下法だ。",
    "output": "So this is mini-batch gradient descent."
  },
  {
    "index": "F18494",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バッチ最急降下法と比較すると、この手法もまた、進捗がより早くなる。",
    "output": "Compared to batch gradient descent, this also allows us to make progress much faster."
  },
  {
    "index": "F18495",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合やるべき事は、まず最初の10個の手本を見て、パラメータシータを改善する為に歩を進める。",
    "output": "So we have again our running example of, you know, U.S."
  },
  {
    "index": "F18496",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初の10個の手本しか見る必要がない。",
    "output": "Census data with 300 million training examples, then what we're saying is after looking at just the first 10 examples we can start to make progress in improving the parameters theta so we don't need to scan through the entire training set."
  },
  {
    "index": "F18497",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうすれば歩を進める事が出来て、そして次にまた、二番目の10個の手本を見る、そしてパラメータをちょっと改善する。",
    "output": "We just need to look at the first 10 examples and this will start letting us make progress and then we can look at the second ten examples and modify the parameters a little bit again and so on."
  },
  {
    "index": "F18498",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、こんな訳でミニバッチ最急降下法はバッチ最急降下法よりも早くなりうる。",
    "output": "So, that is why Mini-batch gradient descent can be faster than batch gradient descent."
  },
  {
    "index": "F18499",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、パラメータの改善を、たった10個の手本を見た後ですぐに始められる、各イテレーションで3億個の手本を一つ一つスキャンする必要がある代わりに。",
    "output": "Namely, you can start making progress in modifying the parameters after looking at just ten examples rather than needing to wait 'till you've scan through every single training example of 300 million of them."
  },
  {
    "index": "F18500",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "では、確率的最急降下法と比べた場合ミニバッチ最急降下法はどうだろう?",
    "output": "So, how about Mini-batch gradient descent versus Stochastic gradient descent."
  },
  {
    "index": "F18501",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、一度に一個では無くて、なんでb個の手本を見たい、と思うのだろうか?",
    "output": "So, why do we want to look at b examples at a time rather than look at just a single example at a time as the Stochastic gradient descent?"
  },
  {
    "index": "F18502",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その答えはベクトル化だ。",
    "output": "The answer is in vectorization."
  },
  {
    "index": "F18503",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、ミニバッチ最急降下法が確率的最急降下法を大きくアウトパフォームすると期待出来るのは良いベクトル化実装がある時だけだ。",
    "output": "In particular, Mini-batch gradient descent is likely to outperform Stochastic gradient descent only if you have a good vectorized implementation."
  },
  {
    "index": "F18504",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、10個の手本に渡る和は、よりベクトル化した形で実行出来て、その方が10個の手本に対する計算が部分的に並列化されやすい。",
    "output": "In that case, the sum over 10 examples can be performed in a more vectorized way which will allow you to partially parallelize your computation over the ten examples."
  },
  {
    "index": "F18505",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、微分項を計算する為に適切なベクトル化実装を使えば、時には部分的に良い数値計算代数ライブラリが使えて、b手本に渡る微分の計算を並列化出来る、一方で確率的最急降下法のように一度に一つの手本しか見ないと、一度に一つの手本しか見ないと、並列化するような物が無いまま終わってしまう。",
    "output": "So, in other words, by using appropriate vectorization to compute the rest of the terms, you can sometimes partially use the good numerical algebra libraries and parallelize your gradient computations over the b examples, whereas if you were looking at just a single example of time with Stochastic gradient descent then, you know, just looking at one example at a time their isn't much to parallelize over."
  },
  {
    "index": "F18506",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "少なくとも、相対的によりちょっとしか並列化する物が無い。",
    "output": "At least there is less to parallelize over."
  },
  {
    "index": "F18507",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ミニバッチ最急降下法の欠点の一つには、追加のパラメータbという、ミニバッチサイズという物が増える事で、そこでも時間を浪費するはめになるかもしれない、つまりもっと時間を食うという事だ。",
    "output": "One disadvantage of Mini-batch gradient descent is that there is now this extra parameter b, the Mini-batch size which you may have to fiddle with, and which may therefore take time."
  },
  {
    "index": "F18508",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが良いベクトル化した実装があれば、確率的最急降下法ですらよりも、早く走る事もある。",
    "output": "But if you have a good vectorized implementation this can sometimes run even faster that Stochastic gradient descent."
  },
  {
    "index": "F18509",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がミニバッチ最急降下法だ。これはある意味でバッチ最急降下法と確率的最急降下法の間をやるような物だ。",
    "output": "So that was Mini-batch gradient descent which is an algorithm that in some sense does something that's somewhat in between what Stochastic gradient descent does and what Batch gradient descent does."
  },
  {
    "index": "F18510",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもしbにまともな値を選べば、、、私は普通b=10を使ってるが、それ以外の値でも、2から100の間あたりならこれもまともで良く使われている範囲と言える。",
    "output": "And if you choose their reasonable value of b. I usually use b equals 10, but, you know, other values, anywhere from say 2 to 100, would be reasonably common."
  },
  {
    "index": "F18511",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてbの値を選んで、良いベクトル化した実装を使えば、確率的最急降下法よりもバッチ最急降下法よりも早くなる場合がある。",
    "output": "So we choose value of b and if you use a good vectorized implementation, sometimes it can be faster than both Stochastic gradient descent and faster than Batch gradient descent."
  },
  {
    "index": "F18512",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いまや、あなたは確率的最急降下法のアルゴリズムについて知った。",
    "output": "You now know about the stochastic gradient descent algorithm."
  },
  {
    "index": "F18513",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがアルゴリズムを実行している時、あなたはどうやってバグが無い、とかちゃんと収束している、という事を確認すれば良いだろうか?",
    "output": "But when you're running the algorithm, how do you make sure that it's completely debugged and is converging okay?"
  },
  {
    "index": "F18514",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同じように重要な事として、どうやって確率的最急降下法においてどうやって学習率のアルファをチューンしたら良い?",
    "output": "Equally important, how do you tune the learning rate alpha with Stochastic Gradient Descent."
  },
  {
    "index": "F18515",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、これらを行う幾つかのテクニックを紹介する、収束を確認する方法と学習率アルファを選ぶ方法。",
    "output": "In this video we'll talk about some techniques for doing these things, for making sure it's converging and for picking the learning rate alpha."
  },
  {
    "index": "F18516",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "バッチ最急降下法を使ってた頃を思い返すと、最急降下法が収束していたかを確認する標準的な方法は、最適化の目的関数の値を繰り返しの回数の関数としてプロットする事だった。",
    "output": "Back when we were using batch gradient descent, our standard way for making sure that gradient descent was converging was we would plot the optimization cost function as a function of the number of iterations."
  },
  {
    "index": "F18517",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これがコスト関数で、このコスト関数が各イテレーションで減少している事を確認したい。",
    "output": "So that was the cost function and we would make sure that this cost function is decreasing on every iteration."
  },
  {
    "index": "F18518",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "トレーニングサイズが小さい時はそれが出来た。何故なら和の計算がとても早く行えたからだ。",
    "output": "When the training set sizes were small, we could do that because we could compute the sum pretty efficiently."
  },
  {
    "index": "F18519",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが大量のトレーニングセットのサイズがあると、定期的にアルゴリズムを止めて、、、確率的最急降下法を定期的に止めてこのコスト関数を計算したくは無い、何故なら、このコスト関数の計算には、トレーニングセットサイズ全体に渡る和を必要とするから。",
    "output": "But when you have a massive training set size then you don't want to have to pause your algorithm periodically."
  },
  {
    "index": "F18520",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そもそもに確率的最急降下法のポイントは、全て、アルゴリズムの途中でトレーニングセット全体をスキャンする必要無しに、一つの手本を見ただけで歩を進める事が出来る、という物だった。",
    "output": "You don't want to have to pause stochastic gradient descent periodically in order to compute this cost function since it requires a sum of your entire training set size."
  },
  {
    "index": "F18521",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コスト関数などを計算する為だけにトレーニングセット全体を見る、というような事無しに。",
    "output": "And the whole point of stochastic gradient was that you wanted to start to make progress after looking at just a single example without needing to occasionally scan through your entire training set right in the middle of the algorithm, just to compute things like the cost function of the entire training set."
  },
  {
    "index": "F18522",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから確率的最急降下法においてアルゴリズムが収束しているのを確認する為にやる事としては、代わりにこんな事をやる。",
    "output": "So for stochastic gradient descent, in order to check the algorithm is converging, here's what we can do instead."
  },
  {
    "index": "F18523",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前に定義したコスト関数を使おう。",
    "output": "Let's take the definition of the cost that we had previously."
  },
  {
    "index": "F18524",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "単体のトレーニング手本に関するパラメータシータでのコストは、単にそのトレーニング手本における二乗誤差の半分だ。",
    "output": "So the cost of the parameters theta with respect to a single training example is just one half of the square error on that training example."
  },
  {
    "index": "F18525",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、確率的最急降下法を学習させている間、ある特定のサンプルを学習させる直前、つまり確率的最急降下法で順番に見ていって、あるサンプルxi,yiをこれから見よう、という時、この次にはこのサンプルによるちょっとの更新を行う訳だ。",
    "output": "Then, while stochastic gradient descent is learning, right before we train on a specific example. So, in stochastic gradient descent we're going to look at the examples xi, yi, in order, and then sort of take a little update with respect to this example."
  },
  {
    "index": "F18526",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が確率的最急降下法がやる事だが、つまりアルゴリズムが手本xi,yiを見ているが、まだパラメータシータをその手本を使ってアップデートしていない時の、その手本のコストを計算してみよう。",
    "output": "So, while the algorithm is looking at the example xi, yi, but before it has updated the parameters theta using that an example, let's compute the cost of that example."
  },
  {
    "index": "F18527",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同じ事をちょっとだけ異なる言葉で言い換えてみよう。",
    "output": "Just to say the same thing again, but using slightly different words."
  },
  {
    "index": "F18528",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "確率的最急降下法がトレーニングセットをスキャンしていく訳だが、ある手本x(i),y(i)を使ってシータをアップデートする直前で、そのトレーニング手本に対し仮説がどれだけ良いかを計算してみよう。",
    "output": "A stochastic gradient descent is scanning through our training set right before we have updated theta using a specific training example x(i) comma y(i) let's compute how well our hypothesis is doing on that training example."
  },
  {
    "index": "F18529",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをシータをアップデートする前に行いたいのは、もしシータをその手本でアップデートした後では、その手本については代表的な値よりももっと良くなってしまうから。",
    "output": "And we want to do this before updating theta because if we've just updated theta using example, you know, that it might be doing better on that example than what would be representative."
  },
  {
    "index": "F18530",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、確率的最急降下法が収束しているかをチェックする為に出来る手段としては、各1000繰り返しごとにその前のステップで計算したこれらのコスト関数をプロットする、というのがある。",
    "output": "Finally, in order to check for the convergence of stochastic gradient descent, what we can do is every, say, every thousand iterations, we can plot these costs that we've been computing in the previous step."
  },
  {
    "index": "F18531",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アルゴリズムに処理された最後の1000手本に渡るコストの平均をプロット出来る。",
    "output": "We can plot those costs average over, say, the last thousand examples processed by the algorithm."
  },
  {
    "index": "F18532",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これをやると、アルゴリズムがどれくらいうまく行ってるかのランニングでの推計が得られる。",
    "output": "And if you do this, it kind of gives you a running estimate of how well the algorithm is doing."
  },
  {
    "index": "F18533",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Jtrainを定期的に計算するのと比べると、そちらはトレーニングセット全体をスキャンする必要があるが、この方法だと、確率的最急降下法の一部として、パラメータシータをアップデートする直前にこれらのコストを計算するのは、そんなに高くはつかない。",
    "output": "So, in contrast to computing J With this other procedure, well, as part of stochastic gradient descent, it doesn't cost much to compute these costs as well right before updating to parameter theta."
  },
  {
    "index": "F18534",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らがやる事は、各1000イテレーションごととかに、そこまでに計算した最後の1000コストを平均して、それをプロットする。",
    "output": "And all we're doing is every thousand integrations or so, we just average the last 1,000 costs that we computed and plot that."
  },
  {
    "index": "F18535",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのプロットを見る事で、確率的最急降下法が収束しているかをチェックする事が出来る。",
    "output": "And by looking at those plots, this will allow us to check if stochastic gradient descent is converging."
  },
  {
    "index": "F18536",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに、そのプロットがどんな風になりうるかの例がある。",
    "output": "So here are a few examples of what these plots might look like."
  },
  {
    "index": "F18537",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後1000手本に渡るコストの平均をプロットしたとしよう、これは1000個だけの手本に渡る平均なので、ちょっとノイジーになるだろう、そして各イテレーションで必ず減少する、という訳でも無かろう。",
    "output": "Suppose you have plotted the cost average over the last thousand examples, because these are averaged over just a thousand examples, they are going to be a little bit noisy and so, it may not decrease on every single iteration."
  },
  {
    "index": "F18538",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこんな感じの図が得られたとすると、プロットはノイジーでしょう、何故なら小さなサブセット、1000個のトレーニング手本に渡ってだけの平均だから。",
    "output": "Then if you get a figure that looks like this, So the plot is noisy because it's average over, you know, just a small subset, say a thousand training examples."
  },
  {
    "index": "F18539",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、もしこんな感じの図を得られたなら、これは結構良くアルゴリズムは実行されている、コストが下がっていって、その先である点から台地のように平坦になってる、こんな場合は。",
    "output": "If you get a figure that looks like this, you know that would be a pretty decent run with the algorithm, maybe, where it looks like the cost has gone down and then this plateau that looks kind of flattened out, you know, starting from around that point."
  },
  {
    "index": "F18540",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "コストがこんな感じの時は、学習アルゴリズムはきっと収束している。",
    "output": "look like, this is what your cost looks like then maybe your learning algorithm has converged."
  },
  {
    "index": "F18541",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしもっと小さい学習率を用いて試したければ、その結果はこんな見た目で、アルゴリズムは最初はゆっくりと学習していく。だからコストはもっとゆっくりと下がっていく。",
    "output": "If you want to try using a smaller learning rate, something you might see is that the algorithm may initially learn more slowly so the cost goes down more slowly."
  },
  {
    "index": "F18542",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがやがてより小さい学習率だと、アルゴリズムは、たぶんちょっとだけ良い解となる。",
    "output": "But then eventually you have a smaller learning rate is actually possible for the algorithm to end up at a, maybe very slightly better solution."
  },
  {
    "index": "F18543",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "赤い線で、よりゆっくりな、より小さい学習率を用いた時の確率的最急降下法の場合を表すとする。",
    "output": "So the red line may represent the behavior of stochastic gradient descent using a slower, using a smaller leaning rate."
  },
  {
    "index": "F18544",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合により良い解となる理由は、確率的最急降下法はグローバル最小に収束するのでは無く、グローバル最小の回りをちょっとだけ振動するのだった。",
    "output": "And the reason this is the case is because, you remember, stochastic gradient descent doesn't just converge to the global minimum, is that what it does is the parameters will oscillate a bit around the global minimum."
  },
  {
    "index": "F18545",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからより小さい学習率を使う事で、最終的にはより小さな振幅にする事が出来る。",
    "output": "And so by using a smaller learning rate, you'll end up with smaller oscillations."
  },
  {
    "index": "F18546",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "時にはこの小さな違いは無視出来る物だろう、時にはより小さい方がわずかに良いパラメータの値を得られるだろう。",
    "output": "And sometimes this little difference will be negligible and sometimes with a smaller than you can get a slightly better value for the parameters."
  },
  {
    "index": "F18547",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "確率的最急降下法を走らせて、これらの1000個の手本に渡ってコストを平均してプロットしたとして、こんな結果が得られる場合もある。",
    "output": "Let's say you run stochastic gradient descent and you average over a thousand examples when plotting these costs."
  },
  {
    "index": "F18548",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合も、一種の収束しているように見える。",
    "output": "Then again, it kind of looks like it's converged."
  },
  {
    "index": "F18549",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこの数字、1000を増やして5000手本に渡って平均をとれば、もっとスムースなカーブ、もっとこんな感じのが得られたと思われる。",
    "output": "If you were to take this number, a thousand, and increase to averaging over 5 thousand examples. Then it's possible that you might get a smoother curve that looks more like this."
  },
  {
    "index": "F18550",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1000手本の代わりに5000手本に渡って平均を取るとすると、もっとスムースなカーブ、こんな感じの物が得られるだろう。",
    "output": "And by averaging over, say 5,000 examples instead of 1,000, you might be able to get a smoother curve like this."
  },
  {
    "index": "F18551",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それが平均を取る対象の手本の数を増やす効果だ。",
    "output": "And so that's the effect of increasing the number of examples you average over."
  },
  {
    "index": "F18552",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この値を大きくし過ぎた場合の欠点はもちろん、5000手本につき、たった一つの点しか得られないという事。",
    "output": "The disadvantage of making this too big of course is that now you get one date point only every 5,000 examples."
  },
  {
    "index": "F18553",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからアルゴリズムがどの位良く動いているかのフィードバックを得るのが、より遅れる事になる。何故ならプロット上の1点を得る為に1000手本じゃなくて5000手本ごとになるからだ。",
    "output": "And so the feedback you get on how well your learning learning algorithm is doing is, sort of, maybe it's more delayed because you get one data point on your plot only every 5,000 examples rather than every 1,000 examples."
  },
  {
    "index": "F18554",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "同様に最急降下法を走らせると、こんなプロットが得られる事もある。",
    "output": "Along a similar vein some times you may run a gradient descent and end up with a plot that looks like this."
  },
  {
    "index": "F18555",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このプロットでは、コストは全く減少してないように見える。",
    "output": "And with a plot that looks like this, you know, it looks like the cost just is not decreasing at all."
  },
  {
    "index": "F18556",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アルゴリズムは全く学習していないように見える。",
    "output": "It looks like the algorithm is just not learning."
  },
  {
    "index": "F18557",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここはフラットなカーブで、コストは低下してないように見える。",
    "output": "It's just, looks like this here a flat curve and the cost is just not decreasing."
  },
  {
    "index": "F18558",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "青い線はあまりにもノイジーなので、実際のトレンド、コストが実際に低下しているというトレンドが見えない。そして1000の代わりに5000手本に渡って平均を取るという事が、助けになるかもしれない。",
    "output": "But again if you were to increase this to averaging over a larger number of examples it is possible that you see something like this red line it looks like the cost actually is decreasing, it's just that the blue line averaging over 2, 3 examples, the blue line was too noisy so you couldn't see the actual trend in the cost actually decreasing and possibly averaging over 5,000 examples instead of 1,000 may help."
  },
  {
    "index": "F18559",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、大きな数の手本に渡って平均をとっても、ここでは5000手本に渡って平均をとってみたが、ここでは別の色を使ったが、その時に、こんな風に学習曲線がなる場合もありうる。",
    "output": "Of course we averaged over a larger number examples that we've averaged here over 5,000 examples, I'm just using a different color, it is also possible that you that see a learning curve ends up looking like this."
  },
  {
    "index": "F18560",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "大きな数の手本に渡って平均しても、フラットなままだ。",
    "output": "That it's still flat even when you average over a larger number of examples."
  },
  {
    "index": "F18561",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれが得られたら、それは不運にも何らかの理由で、アルゴリズムがあまり学習出来ていない、という事に、より固く確信を持てる。",
    "output": "And as you get that, then that's maybe just a more firm verification that unfortunately the algorithm just isn't learning much for whatever reason."
  },
  {
    "index": "F18562",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして学習率を変えるなりフィーチャーを変えるなり、またはアルゴリズムに関しての何かを変えるなりをしなくてはならない。",
    "output": "And you need to either change the learning rate or change the features or change something else about the algorithm."
  },
  {
    "index": "F18563",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後にもう一つ、これらの曲線をプロットしてみたら、そうしたらこんな曲線を得たとすると、つまり実際に増加しているようにみえたら、その時はそれはアルゴリズムが発散しているサインだ。",
    "output": "Finally, one last thing that you might see would be if you were to plot these curves and you see a curve that looks like this, where it actually looks like it's increasing."
  },
  {
    "index": "F18564",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合にすべき事は、より小さい値の学習率アルファを使う事だ。",
    "output": "And what you really should do is use a smaller value of the learning rate alpha."
  },
  {
    "index": "F18565",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上で、ある範囲の手本に渡るコスト関数の平均をプロットした時に、どんな事が起こりうるのか、そのそれぞれのプロットごとのオススメの対応について、感じがつかめたかな。",
    "output": "So hopefully this gives you a sense of the range of phenomena you might see when you plot these cost average over some range of examples as well as suggests the sorts of things you might try to do in response to seeing different plots."
  },
  {
    "index": "F18566",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしプロットがあまりにもノイジーに見えたら、またはくねくねあがったり下がったりしすぎているようなら、平均を取る手本の範囲を増やしてみてくれ、するとプロットの全体的なトレンドをより良く分かるようになるだろう。",
    "output": "So if the plots looks too noisy, or if it wiggles up and down too much, then try increasing the number of examples you're averaging over so you can see the overall trend in the plot better."
  },
  {
    "index": "F18567",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして誤差が実際に増加していたら、コストが実際に増加していたら、より小さい値のアルファを試してみてくれ。",
    "output": "And if you see that the errors are actually increasing, the costs are actually increasing, try using a smaller value of alpha."
  },
  {
    "index": "F18568",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、学習率の問題についてもうちょっと良く見てみよう。",
    "output": "Finally, it's worth examining the issue of the learning rate just a little bit more."
  },
  {
    "index": "F18569",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "確率的最急降下法を走らせると、アルゴリズムはここから始まって、最小に向かってくねくねと歩くのを見た。そしてそれは実際には収束せずに、そのかわりに最小の付近を永遠にうろちょろし続ける。",
    "output": "We saw that when we run stochastic gradient descent, the algorithm will start here and sort of meander towards the minimum And then it won't really converge, and instead it'll wander around the minimum forever."
  },
  {
    "index": "F18570",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、最終的にはグローバル最小に近いパラメータが得られる事が期待出来るが、完全にグローバル最小に一致する訳では無い。",
    "output": "And so you end up with a parameter value that is hopefully close to the global minimum that won't be exact at the global minimum."
  },
  {
    "index": "F18571",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もっとも典型的な確率的最急降下法の実装では、学習率アルファは定数のまま据え置くのが普通だ。",
    "output": "In most typical implementations of stochastic gradient descent, the learning rate alpha is typically held constant."
  },
  {
    "index": "F18572",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり最終的に得られるのはまさにこんな図となる。",
    "output": "And so what you we end up is exactly a picture like this."
  },
  {
    "index": "F18573",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし確率的最急降下法に実際にグローバル最小に収束してほしい、と思うなら、一つ考えられる手としては、学習率アルファを時間がたつにつれて徐々に下げていく、という物がある。",
    "output": "If you want stochastic gradient descent to actually converge to the global minimum, there's one thing which you can do which is you can slowly decrease the learning rate alpha over time."
  },
  {
    "index": "F18574",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "割と良くやるのは、アルファをイコール、constant1割る事のイテレーション数+constant2、とかそんな数字にする。",
    "output": "So, a pretty typical way of doing that would be to set alpha equals some constant 1 divided by iteration number plus constant 2."
  },
  {
    "index": "F18575",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "イテレーション数というのは確率的最急降下法の何回目のイテレーションか、を表す物で、ようするにそこまで見たトレーニング手本の数だ。そしてconst1とconst2はアルゴリズムの追加のパラメータで、良いパフォーマンスを得る為にいじらなくてはいけないかもしれない物だ。",
    "output": "So, iteration number is the number of iterations you've run of stochastic gradient descent, so it's really the number of training examples you've seen And const 1 and const 2 are additional parameters of the algorithm that you might have to play with a bit in order to get good performance."
  },
  {
    "index": "F18576",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この方法を人々があんまり取らない理由としては、これら二つの追加のパラメータ、constant1とconstant2を調整するのに時間を食われるからだ。",
    "output": "One of the reasons people tend not to do this is because you end up needing to spend time playing with these 2 extra parameters, constant 1 and constant 2, and so this makes the algorithm more finicky."
  },
  {
    "index": "F18577",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのせいでアルゴリズムが気難しくなる。つまりアルゴリズムがうまく行くように時間を浪費するハメになるパラメータが増えるのだ。",
    "output": "You know, it's just more parameters able to fiddle with in order to make the algorithm work well."
  },
  {
    "index": "F18578",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもしパラメータをいい感じにチューン出来たら、得られる図は、アルゴリズムが最初はふらつきつつ、最小に向かっていくが、だが近づくと、学習率もそれにつれてどんどん下がっていくので、ふらつきは小さくなり、グローバル最小に至るまで小さくなり続ける。",
    "output": "But if you manage to tune the parameters well, then the picture you can get is that the algorithm will actually around towards the minimum, but as it gets closer because you're decreasing the learning rate the meanderings will get smaller and smaller until it pretty much just to the global minimum."
  },
  {
    "index": "F18579",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは納得出来るだろう。",
    "output": "I hope this makes sense, right?"
  },
  {
    "index": "F18580",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの式が納得出来る理由は、アルゴリズムが走るにつれて、イテレーション回数も大きくなっていくので、アルファはゆっくりと小さくなっていき、すると一歩一歩がどんどん小さくなっていき、それはグローバル最小に収束するまで小さくなり続ける。",
    "output": "And the reason this formula makes sense is because as the algorithm runs, the iteration number becomes large So alpha will slowly become small, and so you take smaller and smaller steps until it hopefully converges to the global minimum."
  },
  {
    "index": "F18581",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、アルファをゆっくりと0へと減少させていくと、最終的にはちょっとだけ良い仮説が得られる。",
    "output": "So If you do slowly decrease alpha to zero you can end up with a slightly better hypothesis."
  },
  {
    "index": "F18582",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、定数をいじるのにかかる余計な仕事と、さらにざっくばらんに言ってしまえばグローバル最小に近いんなら、どんなパラメータの値でもまったく幸せなので、典型的には、このアルファをゆっくり減少させる、という手続きは、普通はやらん。",
    "output": "But because of the extra work needed to fiddle with the constants and because frankly usually we're pretty happy with any parameter value that is, you know, pretty close to the global minimum."
  },
  {
    "index": "F18583",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "で、確率的最急降下法を適用する時には、アルファは定数のままにしておく方がもっと普通だ。どちらのバージョンを使う人も見かけはするけど。",
    "output": "Typically this process of decreasing alpha slowly is usually not done and keeping the learning rate alpha constant is the more common application of stochastic gradient descent although you will see people use either version."
  },
  {
    "index": "F18584",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まとめると、このビデオでは、確率的最急降下法がどうなってるかをコスト関数の観点から近似的にモニタリングする方法を議論した。",
    "output": "To summarize in this video we talk about a way for approximately monitoring how the stochastic gradient descent is doing in terms for optimizing the cost function."
  },
  {
    "index": "F18585",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはコスト関数を計算する為に定期的にトレーニングセット全体をスキャンする必要が無くて、代わりに例えば最後の1000手本とかを見る手法だ。",
    "output": "And this is a method that does not require scanning over the entire training set periodically to compute the cost function on the entire training set."
  },
  {
    "index": "F18586",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの手法は確率的最急降下法がうまく機能していて、収束している、という事を確認するのにも、学習率アルファをチューンするのにも用いる事が出来る。",
    "output": "And you can use this method both to make sure the stochastic gradient descent is okay and is converging or to use it to tune the learning rate alpha."
  },
  {
    "index": "F18587",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここ数回のビデオでは、確率的最急降下法についてや、その他の確率的最急降下法の変種についてーーオンライン学習アルゴリズムの適用などーー議論して来た。だがそれらは全て一つのマシン、または一つのコンピュータで実行出来る物だった。",
    "output": "In the last few videos, we talked about stochastic gradient descent, and, you know, other variations of the stochastic gradient descent algorithm, including those adaptations to online learning, but all of those algorithms could be run on one machine, or could be run on one computer."
  },
  {
    "index": "F18588",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "幾つかの機械学習の問題は一つのマシンで実行するにはあまりにも大きくて、時には単純にデータがあまりにも大きい為に一つのコンピュータで走らせてみようとは考えもしないような状況もあるかもしれない、そのコンピュータでどんなアルゴリズムを使うにせよ、だ。",
    "output": "And some machine learning problems are just too big to run on one machine, sometimes maybe you just so much data you just don't ever want to run all that data through a single computer, no matter what algorithm you would use on that computer."
  },
  {
    "index": "F18589",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでこのビデオでは、大規模機械学習における、異なるアプローチである所の、MapReduceアプローチと呼ばれる物を議論したい。",
    "output": "So in this video I'd like to talk about different approach to large scale machine learning, called the map reduce approach."
  },
  {
    "index": "F18590",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは確率的最急降下法について結構たくさんのビデオを費やしMapReduceに関しては相対的にはちょっとしか扱わないが、それを持って確率的最急降下法に比べるとMapReduceはそんなに重要では無い、とは判断しないでもらいたい、このそれぞれのアイデアに関して私が費やす時間を元に判断するのは。",
    "output": "And even though we have quite a few videos on stochastic gradient descent and we're going to spend relative less time on map reduce--don't judge the relative importance of map reduce versus the gradient descent based on the amount amount of time I spend on these ideas in particular."
  },
  {
    "index": "F18591",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多くの人々が、MapReduceは少なくとも同程度には、そして人によってはもっと重要なアイデアだと言うだろう、確率的最急降下法と比較した時に。単にMapReduceは相対的には説明するのが簡単なだけ、その為にそれについてあんまり時間を使わないだけだ。",
    "output": "Many people will say that map reduce is at least an equally important, and some would say an even more important idea compared to gradient descent, only it's relatively simpler to explain, which is why I'm going to spend less time on it, but using these ideas you might be able to scale learning algorithms to even far larger problems than is possible using stochastic gradient descent."
  },
  {
    "index": "F18592",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのアイデアとはこうだ。",
    "output": "Here's the idea."
  },
  {
    "index": "F18593",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば線形回帰とかロジスティック回帰とか、とにかくその辺のモデルにフィッティングしたい、としよう。ここではバッチ最急降下法から始める事としよう。",
    "output": "Let's say we want to fit a linear regression model or a logistic regression model or some such, and let's start again with batch gradient descent, so that's our batch gradient descent learning rule."
  },
  {
    "index": "F18594",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これが我らのバッチ最急降下法のルールとなる。そしてこのスライドの記述が追いやすいように、ここではm=400の手本と仮定して進めていく事にする。",
    "output": "And to keep the writing on this slide tractable, I'm going to assume throughout that we have m equals 400 examples."
  },
  {
    "index": "F18595",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どちらかといえばより一般的な状況としては4億個とかそれに近い数字の方がより典型的な数字と言える。だが、スライドの記述をシンプルにする為に、我らの手持ちの手本が400個であるフリをしてみよう。",
    "output": "Of course, by our standards, in terms of large scale machine learning, you know m might be pretty small and so, this might be more commonly applied to problems, where you have maybe closer to 400 million examples, or some such, but just to make the writing on the slide simpler, I'm going to pretend we have 400 examples."
  },
  {
    "index": "F18596",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その場合、バッチ最急降下法の学習ルールはこの400個に対して行われ、和をi=1から400までのこの400個の手本に渡って取る、もしmが大きければこれは、計算量的に高くつくステップとなる。",
    "output": "So in that case, the batch gradient descent learning rule has this 400 and the sum from i equals 1 through 400 through my 400 examples here, and if m is large, then this is a computationally expensive step."
  },
  {
    "index": "F18597",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこでMapReduceのアイデアが行う事は以下のような事だ。ここでMapReduceのアイデアは二人の研究者による物だと言及しておくべきだろう、JeffDeanとSanjayGimawatだ。",
    "output": "So, what the MapReduce idea does is the following, and I should say the map reduce idea is due to two researchers, Jeff Dean and Sanjay Gimawat."
  },
  {
    "index": "F18598",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところでJeffDeanはシリコンバレー中でももっとも伝説的なエンジニアの一人で、今日Googleで動いているアーキテクチャ的なインフラのかなりの部分を創り上げた男だ。",
    "output": "Jeff Dean, by the way, is one of the most legendary engineers in all of Silicon Valley and he kind of built a large fraction of the architectural infrastructure that all of Google runs on today."
  },
  {
    "index": "F18599",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "話を戻して、これがMapReduceというアイデアだ。",
    "output": "But here's the map reduce idea."
  },
  {
    "index": "F18600",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あるトレーニングセットがあるとして、この箱でxyのペアを表すとして、これはx1,y1から400個の手本までxm,ymまで降りていく。",
    "output": "So, let's say I have some training set, if we want to denote by this box here of X Y pairs, where it's X1, Y1, down to my 400 examples, Xm, Ym."
  },
  {
    "index": "F18601",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれがトレーニングセットで400個の手本がある。",
    "output": "So, that's my training set with 400 training examples."
  },
  {
    "index": "F18602",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "MapReduceのアイデアでは、一つのやり方としては、このトレーニングセットを別々のサブセットに分割する。",
    "output": "In the MapReduce idea, one way to do, is split this training set in to different subsets."
  },
  {
    "index": "F18603",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると4台のマシンがトレーニングセットに渡って並列に走る。そんな訳だから4台のマシンに分割した。",
    "output": "assume for this example that I have 4 computers, or 4 machines to run in parallel on my training set, which is why I'm splitting this into 4 machines."
  },
  {
    "index": "F18604",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたの手持ちが10台のマシンだったり100台のマシンなら、その場合はそれに応じてトレーニングセットを10個とか100個とか、持ってるマシンの台数に応じて分割する事になる。",
    "output": "If you have 10 machines or 100 machines, then you would split your training set into 10 pieces or 100 pieces or what have you."
  },
  {
    "index": "F18605",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして4台のマシンの最初の一台がやるべき事は、トレーニングセットのうちの最初の1/4を用いてつまり最初の100個のトレーニング手本を用いて、具体的に言うと、それがやる事はこの和を見てくれ。",
    "output": "And what the first of my 4 machines is to do, say, is use just the first one quarter of my training set--so use just the first 100 training examples."
  },
  {
    "index": "F18606",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最初の100個のトレーニング手本に対してこの和を計算する事だ。書きだしてみよう。",
    "output": "And in particular, what it's going to do is look at this summation, and compute that summation for just the first 100 training examples."
  },
  {
    "index": "F18607",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはイコール和を取る事の1から100までのここでここにある項を代入する。つまり、hシータのxi引く事のyi、掛けるxij。",
    "output": "So let me write that up I'm going to compute a variable temp 1 to superscript 1 the first machine J equals sum from equals 1 through 100, and then I'm going to plug in exactly that term there--so I have X-theta, Xi, minus Yi times Xij, right?"
  },
  {
    "index": "F18608",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは単なるここの最急降下法の項だ。",
    "output": "So that's just that gradient descent term up there."
  },
  {
    "index": "F18609",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは同様の和をインデックスが101から200までに対するトレーニングセットに対し取った物だ。",
    "output": "And then similarly, I'm going to take the second quarter of my data and send it to my second machine, and my second machine will use training examples 101 through 200 and you will compute similar variables of a temp to j which is the same sum for index from examples 101 through 200."
  },
  {
    "index": "F18610",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして以下同様にマシン3と4はトレーニングセットの三番目の1/4と、四番目の1/4を使う事になる。",
    "output": "And similarly machines 3 and 4 will use the third quarter and the fourth quarter of my training set."
  },
  {
    "index": "F18611",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりいまや、各マシンは400に渡る和では無くて100手本に対する和だけとなる。つまりやらなくてはならない仕事が1/4となり、つまり4倍早く終えられる事が期待される。",
    "output": "So now each machine has to sum over 100 instead of over 400 examples and so has to do only a quarter of the work and thus presumably it could do it about four times as fast."
  },
  {
    "index": "F18612",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、これら全てのマシンが仕事を終えたら、これらのtemp変数を取り出して一つに戻さないといけない。",
    "output": "Finally, after all these machines have done this work, I am going to take these temp variables and put them back together."
  },
  {
    "index": "F18613",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからこれらの変数を中央のサーバーに送りつける。そしてマスターサーバーがやる事はこれらの結果を一つに結合する事。",
    "output": "So I take these variables and send them all to a You know centralized master server and what the master will do is combine these results together."
  },
  {
    "index": "F18614",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、パラメータのシータjをシータjを、以下のように更新する:シータj引く事の学習率のアルファ掛ける事の1/400掛ける事のtemp(i)j足す事のtemp(2)j足す事のtemp(3)j足すことのtemp(4)j。",
    "output": "and in particular, it will update my parameters theta j according to theta j gets updated as theta j minus Of the learning rate alpha times one over 400 times temp, 1, J, plus temp 2j plus temp 3j plus temp 4j and of course we have to do this separately for J equals 0."
  },
  {
    "index": "F18615",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもちろん、これをj=0からnまで、nはフィーチャーの数だが、それらのjに対してそれぞれ行わなくてはならない。",
    "output": "You know, up to and within this number of features."
  },
  {
    "index": "F18616",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな風に複数の等式に分割出来る。この式がやってる事は、完全にこれと同じだ。",
    "output": "So operating this equation into I hope it's clear."
  },
  {
    "index": "F18617",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "中央のマスターサーバーがこれらの結果を受け取り、つまりtemp1j、temp2j、temp3j、temp4jと、それらを足し合わせるとつまり当然これら4つの和となる訳だ。いいかい?",
    "output": "So what this equation is doing is exactly the same is that when you have a centralized master server that takes the results, the ten one j the ten two j ten three j and ten four j and adds them up and so of course the sum of these four things."
  },
  {
    "index": "F18618",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは単にこれ、足す、この和、足す、この和、足す、この和。そしてこれら4つの物を足しあわせると、この和と等しくなる。",
    "output": "Right, that's just the sum of this, plus the sum of this, plus the sum of this, plus the sum of that, and those four things just add up to be equal to this sum that we're originally computing a batch stream descent."
  },
  {
    "index": "F18619",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれは厳密にバッチ最急降下法と等価である。ただ、400個のトレーニング手本にたいして和を取らなくてはいけなかった代わりに、ワークロードを4つのマシンに分割出来るようになっている。",
    "output": "And then we have the alpha times 1 of 400, alpha times 1 of 100, and this is exactly equivalent to the batch gradient descent algorithm, only, instead of needing to sum over all four hundred training examples on just one machine, we can instead divide up the work load on four machines."
  },
  {
    "index": "F18620",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、これは、一般的なMapReduceのテクニックがどんなかを表した図だ。",
    "output": "So, here's what the general picture of the MapReduce technique looks like."
  },
  {
    "index": "F18621",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あるトレーニングセットがあって、それを4つのマシンに渡って並列化したいとすると、トレーニングセットを同じサイズに分割する、4つのサブセットに等しく分割する。",
    "output": "We have some training sets, and if we want to paralyze across four machines, we are going to take the training set and split it, you know, equally. Split it as evenly as we can into four subsets."
  },
  {
    "index": "F18622",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、この4つのトレーニングデータのサブセットを4つの別々のコンピュータに送る。",
    "output": "Then we are going to take the 4 subsets of the training data and send them to 4 different computers."
  },
  {
    "index": "F18623",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして4つのコンピュータはおのおの、1/4のトレーニングセットについてだけ和を計算する事が出来る。そして次に、最終的に各コンピュータの結果を取り出して、中央のサーバーに送る。",
    "output": "And each of the 4 computers can compute a summation over just one quarter of the training set, and then finally take each of the computers takes the results, sends them to a centralized server, which then combines the results together."
  },
  {
    "index": "F18624",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前のスライドの例では最急降下法の仕事の大半は、i=1から400までの何かの和を計算する事だった。",
    "output": "So, on the previous line in that example, the bulk of the work in gradient descent, was computing the sum from i equals 1 to 400 of something."
  },
  {
    "index": "F18625",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的に言うと、最急降下法の式のi=1からmまでの和。",
    "output": "So more generally, sum from i equals 1 to m of that formula for gradient descent."
  },
  {
    "index": "F18626",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてここで、各4つのコンピュータはその仕事の1/4しか行わないので、潜在的には4倍のスピードアップの可能性がある。",
    "output": "And now, because each of the four computers can do just a quarter of the work, potentially you can get up to a 4x speed up."
  },
  {
    "index": "F18627",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、もし仮にネットワークの遅延も無くデータをあちこちに送るのにネットワークのコミュニケーションのコストも存在しないとすると、4倍のスピードアップの可能性がある。",
    "output": "In particular, if there were no network latencies and no costs of the network communications to send the data back and forth, you can potentially get up to a 4x speed up."
  },
  {
    "index": "F18628",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、現実には、ネットワークのレイテンシもあるし、結果を結合するオーバーヘッドもあるし、その他のファクターもあるので、実際には4倍のスピードアップよりはわずかに少ないだろう。",
    "output": "Of course, in practice, because of network latencies, the overhead of combining the results afterwards and other factors, in practice you get slightly less than a 4x speedup."
  },
  {
    "index": "F18629",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、それにも関わらず、この種のMapReduceのアプローチは単体のコンピュータのみを使う事に比べるとより大きなデータセットを処理する方法を提供してくれるアプローチと言える。",
    "output": "But, none the less, this sort of macro juice approach does offer us a way to process much larger data sets than is possible using a single computer."
  },
  {
    "index": "F18630",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたがある学習アルゴリズムを複数のコンピュータに渡って計算を並列化する事でスピードアップをする為にMapReduceを適用することを検討している時は、自身に問うてみるべき鍵となる問いは、あなたの使おうとしている学習アルゴリズムはトレーニングセットに渡る和の形で表現出来るのか?",
    "output": "If you are thinking of applying Map Reduce to some learning algorithm, in order to speed this up."
  },
  {
    "index": "F18631",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "という事だ。そして多くの学習アルゴリズムは実際にトレーニングセットに渡って関数の和を取る形に表現出来る事が分かっている。",
    "output": "By paralleling the computation over different computers, the key question to ask yourself is, can your learning algorithm be expressed as a summation over the training set?"
  },
  {
    "index": "F18632",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして大きなデータセットに対してそれらを走らせる時の計算量のコストはとても大きなトレーニングセットに渡って和を取る必要がある事に起因している。",
    "output": "And it turns out that many learning algorithms can actually be expressed as computing sums of functions over the training set and the computational expense of running them on large data sets is because they need to sum over a very large training set."
  },
  {
    "index": "F18633",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、あなたの学習アルゴリズムがなんであれ、トレーニングセットに渡る和として表現する事が出来て、そして学習アルゴリズムの仕事の大部分がトレーニングセットに渡る和として表現出来れば、MapReduceはあなたの学習アルゴリズムをとても大きなデータセットに対してスケールさせてくれる為の、とても有力な候補となる。",
    "output": "So, whenever your learning algorithm can be expressed as a sum of the training set and whenever the bulk of the work of the learning algorithm can be expressed as the sum of the training set, then map reviews might a good candidate for scaling your learning algorithms through very, very good data sets."
  },
  {
    "index": "F18634",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もう一つ例を見てみよう。",
    "output": "Lets just look at one more example."
  },
  {
    "index": "F18635",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "アドバンスドな最適化のアルゴリズムの一つを使いたいとしよう。",
    "output": "Let's say that we want to use one of the advanced optimization algorithm."
  },
  {
    "index": "F18636",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、L-BFGSとかconjugategradientとかそういう奴。そして、ロジスティック回帰をそのアルゴリズムを使って訓練したいとする。",
    "output": "So, things like, you know, l, b, f, g, s constant gradient and so on, and let's say we want to train a logistic regression of the algorithm."
  },
  {
    "index": "F18637",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その為には、我らは二つの主な値を計算する必要がある。",
    "output": "For that, we need to compute two main quantities."
  },
  {
    "index": "F18638",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つめは、L-BFGSやconjugategradientのような最適化アルゴリズムに対して我らは最適化の目的関数である、コスト関数を計算するルーチンを渡してやらないといけない。",
    "output": "One is for the advanced optimization algorithms like, you know, LPF and constant gradient."
  },
  {
    "index": "F18639",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてロジスティック回帰においてはコスト関数はこんな感じの物をトレーニングセットに渡って和を取る物だった。",
    "output": "We need to provide it a routine to compute the cost function of the optimization objective."
  },
  {
    "index": "F18640",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "すると10台のマシンに渡って並列化したいなら、トレーニングセットを10分割して、10台のマシンに割り振り、そして10台の各マシンが、トレーニングデータの十分の一に渡ってこの量の和を計算する。",
    "output": "And so for logistic regression, you remember that a cost function has this sort of sum over the training set, and so if youre paralizing over ten machines, you would split up the training set onto ten machines and have each of the ten machines compute the sum of this quantity over just one tenth of the training data."
  },
  {
    "index": "F18641",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、他にアドバンスドな最適化アルゴリズムが必要としている物としては、これらの偏微分項を計算するルーチンだ。",
    "output": "Then, the other thing that the advanced optimization algorithms need, is a routine to compute these partial derivative terms."
  },
  {
    "index": "F18642",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ふたたび、これらの微分項は、ロジスティック回帰なら、トレーニングセットの和として表現する事が出来て、だからふたたび、前の例と同様に、各マシンに、トレーニングデータの少しの部分ずつだけを計算させる事が出来る。",
    "output": "Once again, these derivative terms, for which it's a logistic regression, can be expressed as a sum over the training set, and so once again, similar to our earlier example, you would have each machine compute that summation over just some small fraction of your training data."
  },
  {
    "index": "F18643",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、これらを全て計算し終えたら、各マシンがその結果を中央のサーバーに送りつける。",
    "output": "And finally, having computed all of these things, they could then send their results to a centralized server, which can then add up the partial sums."
  },
  {
    "index": "F18644",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはこれらのtempi、あるいはtempij変数を足し合わせる事に対応する、ここでiはマシン番号iを表し、そのマシンローカルで計算された物。つまり中央のサーバーはこれらの物を足し合わせる事が出来て、それで全体のコスト関数を得る事が出来る、そこから全体の偏微分項が得られて、それをアドバンスドな最適化アルゴリズムに渡す事が出来る。",
    "output": "This corresponds to adding up those tenth i or tenth ij variables, which were computed locally on machine number i, and so the centralized server can sum these things up and get the overall cost function and get the overall partial derivative, which you can then pass through the advanced optimization algorithm."
  },
  {
    "index": "F18645",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "より一般的に言うと、その他の学習アルゴリズムでも、それらを和の形に表現すれば、あるいはトレーニングセットに渡る関数の和の形に表現すれば、その他のアルゴリズムでもMapReduceのテクニックを用いて同様に並列化する事が出来る。そしてとても大きなトレーニングセットに対してスケールさせる事が出来る。",
    "output": "So, more broadly, by taking other learning algorithms and expressing them in sort of summation form or by expressing them in terms of computing sums of functions over the training set, you can use the MapReduce technique to parallelize other learning algorithms as well, and scale them to very large training sets."
  },
  {
    "index": "F18646",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、一つコメントを。ここまで我らはMapReduceのアルゴリズムを複数のコンピューターに渡って、またはコンピュータクラスタの複数のコンピュータ、またはデータセンターの複数のコンピュータに渡って並列化する為の物として議論してきた。",
    "output": "Finally, as one last comment, so far we have been discussing MapReduce algorithms as allowing you to parallelize over multiple computers, maybe multiple computers in a computer cluster or over multiple computers in the data center."
  },
  {
    "index": "F18647",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが時には、一つしかコンピュータが無くてもMapReduceが適応可能な場合がある事が分かっている。",
    "output": "It turns out that sometimes even if you have just a single computer, MapReduce can also be applicable."
  },
  {
    "index": "F18648",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、こんにちの多くのコンピュータは、複数のプロセッサコアを持っている。",
    "output": "In particular, on many single computers now, you can have multiple processing cores."
  },
  {
    "index": "F18649",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "複数のCPUを持っている事があり得るし、各CPU内にも複数のプロセッサコアがある場合もある。",
    "output": "You can have multiple CPUs, and within each CPU you can have multiple proc cores."
  },
  {
    "index": "F18650",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし大きなトレーニングセットがある時に、あなたがとれる手段としては、もしあなたの手元に4つの計算コアを持つコンピュータがあったとすると、それが一つのコンピュータでしか無かったとしても、トレーニングセットを複数のピースに分割して一つのマシンの中の別々のコアにトレーニングセットを送り込む、という事が出来る。一つのマシンとは一つのデスクトップコンピュータかもしれないし、一つのサーバーかもしれない。",
    "output": "If you have a large training set, what you can do if, say, you have a computer with 4 computing cores, what you can do is, even on a single computer you can split the training sets into pieces and send the training set to different cores within a single box, like within a single desktop computer or a single server and use MapReduce this way to divvy up work load."
  },
  {
    "index": "F18651",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして各コアはトレーニングセットの1/4に渡る和を実行出来る。そして次にそれらの部分和を取ってきて結合する事が出来る、トレーニングセット全体に渡る和を得る為に。",
    "output": "Each of the cores can then carry out the sum over, say, one quarter of your training set, and then they can take the partial sums and combine them, in order to get the summation over the entire training set."
  },
  {
    "index": "F18652",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "MapReduceをこんな風に、一つのマシンの中の複数コアに対する並列化と考えるメリットは、複数のマシンに渡る並列化として考えることに比べて、ネットワークのレイテンシを気にする必要が無くなる、という事がある。何故なら全てのコミュニケーションは、tempj変数を行ったり来たり送る事は、それらは全て一つのマシン内で起こる事だから。",
    "output": "The advantage of thinking about MapReduce this way, as paralyzing over cause within a single machine, rather than parallelizing over multiple machines is that, this way you don't have to worry about network latency, because all the communication, all the sending of the back and forth, all that happens within a single machine."
  },
  {
    "index": "F18653",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからデータセンター内の別々のマシンを使う事と比べると、ネットワークのレイテンシはより重要度が低くなる。",
    "output": "And so network latency becomes much less of an issue compared to if you were using this to over different computers within the data sensor."
  },
  {
    "index": "F18654",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後にマルチコアのマシンでの並列化の落とし穴の最後の一つを挙げておこう。",
    "output": "Finally, one last caveat on parallelizing within a multi-core machine."
  },
  {
    "index": "F18655",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "実装の詳細によっては、もしマルチコアのマシンがあって、もしある種の数値計算線形代数ライブラリがあるなら、幾つかの数値計算線形代数ライブラリは線形代数計算を自動的にマシン内の複数コアに並列化する物がある。",
    "output": "It turns out that the sum numerical linear algebra libraries that can automatically parallelize their linear algebra operations across multiple cores within the machine."
  },
  {
    "index": "F18656",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、それらの線形代数数値計算ライブラリの一つを使えるような程度の幸運に恵まれたなら、そしてこれは確かにどのライブラリでも適用出来るという訳でも無いが、もしあなたがそれらのライブラリの一つを使っていてそしてとても良いベクトル化した実装の学習アルゴリズムを用いているなら、ただ標準的な学習アルゴリズムをベクトル化した形で実装するだけで、そして並列化について思い煩う事無く、数値計算線形代数ライブラリがそのうちのいくらかをあなたの代わりに受け持ってくれる。",
    "output": "So if you're fortunate enough to be using one of those numerical linear algebra libraries and certainly this does not apply to every single library."
  },
  {
    "index": "F18657",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがそれ以外の学習問題では、この種のMapReduceの実装を有効利用する事により、このMapReduceの定式化を用いる事で明示的に複数コアに渡る並列化を自分自身で行う事もまた同様に良いアイデアだと思う事もあるだろう、そしてそれを用いてあなたの学習アルゴリズムを高速化する事が可能かもしれない。",
    "output": "Sometimes you can just implement you standard learning algorithm in a vectorized fashion and not worry about parallelization and numerical linear algebra libararies could take care of some of it for you."
  },
  {
    "index": "F18658",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、機械学習を並列化するためのアプローチとしてMapReduceを議論する。",
    "output": "for other any problems, taking advantage of this sort of map reducing commentation, finding and using this MapReduce formulation and to paralelize a cross coarse except yourself might be a good idea as well and could let you speed up your learning algorithm."
  },
  {
    "index": "F18659",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "データセンターのたくさんのマシンに対してデータをばらまくやり方だ。",
    "output": "In this video, we talked about the MapReduce approach to parallelizing machine learning by taking a data and spreading them across many computers in the data center."
  },
  {
    "index": "F18660",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、このアイデアは一つのマシン内の複数コアで並列化する為にも極めて重要な物である。",
    "output": "Although these ideas are critical to paralysing across multiple cores within a single computer as well."
  },
  {
    "index": "F18661",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "Hadoopと呼ばれるオープンソースのシステムにはたくさんのユーザーが居る。だから、自分の独自実装を使うにせよ誰かの作ったオープンソース実装を使うにせよ、これらのアイデアを用いて学習アルゴリズムを並列化する事が出来て、それらを一つのマシンだけを使う場合と比べたらより大きなデータセットに対して走らせる事が可能となる。",
    "output": "Today there are some good open source implementations of MapReduce, so there are many users in open source system called Hadoop and using either your own implementation or using someone else's open source implementation, you can use these ideas to parallelize learning algorithms and get them to run on much larger data sets than is possible using just a single machine."
  },
  {
    "index": "F18662",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオと続く幾つかのビデオで機械学習の応用例であるところのPhotoOCRの話、およびそのPhotoOCRにまつわる歴史の話をしていきたい。",
    "output": "In this and the next few videos, I want to tell you about a machine learning application example, or a machine learning application history centered around an application called Photo OCR ."
  },
  {
    "index": "F18663",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それをやりたいと思う3つの理由がある。1つ目、複雑な機械学習のシステムがどのように組み合わせられるのかをお見せしたい。",
    "output": "There are three reasons why I want to do this, first I wanted to show you an example of how a complex machine learning system can be put together."
  },
  {
    "index": "F18664",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二つ目、機械学習パイプラインのコンセプトをお伝えし、次にやるべき事を決める時に、どうリソースを配分するかを話したい。",
    "output": "Second, once told the concepts of a machine learning a type line and how to allocate resources when you're trying to decide what to do next."
  },
  {
    "index": "F18665",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは一人で大きなアプリケーションに従事している場合でも、または複雑なアプリケーションをデベロッパのチームで一緒に作ろうという文脈もあり得る。",
    "output": "And this can either be in the context of you working by yourself on the big application Or it can be the context of a team of developers trying to build a complex application together."
  },
  {
    "index": "F18666",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、PhotoOCRの問題は機械学習のより興味深い幾つかのアイデアについてお伝えする口実にもなる。",
    "output": "And then finally, the Photo OCR problem also gives me an excuse to tell you about just a couple more interesting ideas for machine learning."
  },
  {
    "index": "F18667",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つには機械学習をコンピュータの画像に適用するやり方だ。そして二番目に、人工的なデータ合成のアイデアだ。",
    "output": "One is some ideas of how to apply machine learning to computer vision problems, and second is the idea of artificial data synthesis, which we'll see in a couple of videos."
  },
  {
    "index": "F18668",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではPhotoOCRの問題とは何なのかを話すところから始めよう。",
    "output": "So, let's start by talking about what is the Photo OCR problem."
  },
  {
    "index": "F18669",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PhotoOCRはPhotoOpticalCharacterRecognitionの略だ。",
    "output": "Photo OCR stands for Photo Optical Character Recognition."
  },
  {
    "index": "F18670",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "デジタル写真の分野が成長するに連れて、さらに最近では携帯にカメラがついた事により、我らはそこら中で撮った大量の画像写真を所持している。",
    "output": "With the growth of digital photography and more recently the growth of camera in our cell phones we now have tons of visual pictures that we take all over the place."
  },
  {
    "index": "F18671",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして多くのデベロッパが興味を持っている事の一つに、これらの写真をコンピュータにもうちょっと良く理解させるにはどうしたら良いか、というのがある。",
    "output": "And one of the things that has interested many developers is how to get our computers to understand the content of these pictures a little bit better."
  },
  {
    "index": "F18672",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PhotoOCR問題は撮った写真の中のテキストをコンピュータにどうやって読ませるか、という事にフォーカスする。",
    "output": "The photo OCR problem focuses on how to get computers to read the text to the purest in images that we take."
  },
  {
    "index": "F18673",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えばもしこの写真を後でもう一度探したい時にLULAB'sとかANTIQUEMALLとタイプしたら自動的にこの写真を取ってこれるように出来るだろう。そうすれば大量の時間を費やして何百とか何千とかのあなたの写真のコレクションから頑張ってひっくり返して探さなくて済むように出来るだろう。",
    "output": "Given an image like this it might be nice if a computer can read the text in this image so that if you're trying to look for this picture again you type in the words, lulu bees and and have it automatically pull up this picture, so that you're not spending lots of time digging through your photo collection Maybe hundreds of thousands of pictures in."
  },
  {
    "index": "F18674",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PhotoOCRの問題はまさにこれをやる問題だ。そしてそれは複数のステップで行う。",
    "output": "The Photo OCR problem does exactly this, and it does so in several steps."
  },
  {
    "index": "F18675",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず、与えられた写真に対し、画像を見ていって、写真の中のどこにテキストがあるかを検出する。",
    "output": "First, given the picture it has to look through the image and detect where there is text in the picture."
  },
  {
    "index": "F18676",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれを終えた後で、あるいはそれが成功裡に終えられたら、次にこれらのテキストの領域を見て、その領域のテキストを実際に読む。それが正しく読めたら、画像の中に現れたテキストが何なのか、という転写が得られる。",
    "output": "And after it has done that or if it successfully does that it then has to look at these text regions and actually read the text in those regions, and hopefully if it reads it correctly, it'll come up with these transcriptions of what is the text that appears in the image."
  },
  {
    "index": "F18677",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "スキャンしたドキュメントのOCR、つまりOpticalCharacterRecognitionはより簡単な問題だが、写真のOCRはこんにちでもまだ、とても難しい機械学習の問題である。",
    "output": "Whereas OCR, or optical character recognition of scanned documents is relatively easier problem, doing OCR from photographs today is still a very difficult machine learning problem, and you can do this."
  },
  {
    "index": "F18678",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこれが出来れば、コンピュータに自動で画像の内容をより良く理解させる助けになるだけでなく、例えば盲目の人を助ける、というような応用例もある。もし盲目の人にカメラを持たせて、その前にある物が何なのかを見る事が出来るようにすると、その前にある道路の標識とかの標識が何なのかを教えてくれたり出来る。",
    "output": "Not only can this help our computers to understand the content of our though images better, there are also applications like helping blind people, for example, if you could provide to a blind person a camera that can look at what's in front of them, and just tell them the words that my be on the street sign in front of them."
  },
  {
    "index": "F18679",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたの車が道路の標識を読む事が出来て、目的地までのナビゲーションを助けてくれる事を想像してみてくれ。",
    "output": "For example, imagine if your car could read the street signs and help you navigate to your destination."
  },
  {
    "index": "F18680",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PhotoOCRを実現する為には、こんな手段が可能だ。",
    "output": "In order to perform photo OCR, here's what we can do."
  },
  {
    "index": "F18681",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まず最初に、画像を見ていって、画像内のどこにテキストがあるかを探す。",
    "output": "First we can go through the image and find the regions where there's text and image."
  },
  {
    "index": "F18682",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここに見せたのは、PhotoOCRのシステムが見つけたテキストと画像の一例だ。",
    "output": "So, shown here is one example of text and image that the photo OCR system may find."
  },
  {
    "index": "F18683",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、テキストの範囲のあたりの矩形を元に、次に文字分割を行う事が出来る。例えばこのANTIQUEMALLと書いてあるテキストの箱を取って、これを個々の文字の場所に分割する事を試みる。",
    "output": "Second, given the rectangle around that text region, we can then do character segmentation, where we might take this text box that says \"Antique Mall\" and try to segment it out into the locations of the individual characters."
  },
  {
    "index": "F18684",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、個々の文字に分割された物を元に、分類器を実行する、そこでは画像の文字を見ていってそして書かれている文字の認識を試みる:最初の文字がAで、二番目の文字がN、三番目の文字がT、という具合に。そうして、これらを全て終えたら、その時はこのフレーズがLULAB'sANTIQUEMALLだと判明する事が期待出来る。",
    "output": "And finally, having segmented out into individual characters, we can then run a crossfire, which looks at the images of the visual characters, and tries to figure out the first character's an A, the second character's an N, the third character is a T, and so on, so that up by doing all this how that hopefully you can then figure out that this phrase is Rulegee's antique mall and similarly for some of the other words that appear in that image."
  },
  {
    "index": "F18685",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いくつかのPhotoOCRのシステムはこれよりさらに複雑な事もやる、という事は言っておくべきだろう。例えば最後にちょっとしたスペル修正を行ったりとか。",
    "output": "I should say that there are some photo OCR systems that do even more complex things, like a bit of spelling correction at the end."
  },
  {
    "index": "F18686",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、文字分割と文字分類システムが、c1eaningという単語を見た、と言ってきたら、その時は、お分かりの通りある種のスペリング修正システムがそれは多分cleaningだ、と言う事だろう。",
    "output": "Then, you know, a sort of spelling correction system might tell you that this is probably the word 'cleaning', and your character classification algorithm had just mistaken the l for a 1."
  },
  {
    "index": "F18687",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、このビデオでやりたい目的の為に、この最後のステップを無視して、これら三つの事をするシステム、つまりテキスト検出、文字分割、そして文字分類に集中しよう。",
    "output": "But for the purpose of what we want to do in this video, let's ignore this last step and just focus on the system that does these three steps of text detection, character segmentation, and character classification."
  },
  {
    "index": "F18688",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このようなシステムを機械学習パイプラインと呼んでいる。",
    "output": "A system like this is what we call a machine learning pipeline."
  },
  {
    "index": "F18689",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "特に、ここでお見せしているのは、フォトOCRパイプラインだ。",
    "output": "In particular, here's a picture showing the photo OCR pipeline."
  },
  {
    "index": "F18690",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "画像があって、それをテキスト検出システムに食わせて、テキストの領域から、次に文字を分割する--テキスト内の各文字に--そして最後に個々の文字を認識する。",
    "output": "We have an image, which then fed to the text detection system text regions, we then segment out the characters--the individual characters in the text--and then finally we recognize the individual characters."
  },
  {
    "index": "F18691",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "多くの複雑な機械学習のシステムではこの種のパイプラインは一般的で、そこでは複数のモジュールがありうる--この例では、テキスト検出、文字分割、文字認識モジュール--それらはおのおの、機械学習のコンポーネントかもしれないし、時には機械学習のコンポーネントでは無い物もあるかもしれない。何にせよ、あるデータ片に対して次々と機能する一連のモジュールを用いる事で、望みの出力を生成する。",
    "output": "In many complex machine learning systems, these sorts of pipelines are common, where you can have multiple modules--in this example, the text detection, character segmentation, character recognition modules--each of which may be machine learning component, or sometimes it may not be a machine learning component but to have a set of modules that act one after another on some piece of data in order to produce the output you want, which in the photo OCR example is to find the transcription of the text that appeared in the image."
  },
  {
    "index": "F18692",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしあなたが機械学習のシステムをデザインする事になったら、もっとも重要な決定の一つは、しばしば組み合わせるパイプラインは何であるか?",
    "output": "If you're designing a machine learning system one of the most important decisions will often be what exactly is the pipeline that you want to put together."
  },
  {
    "index": "F18693",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、PhotoOCRの問題が与えられた時に、この問題をどう別々のモジュールへと分割するか、という事。",
    "output": "In other words, given the photo OCR problem, how do you break this problem down into a sequence of different modules."
  },
  {
    "index": "F18694",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてパイプラインを設計する。",
    "output": "And you design the pipeline and each the performance of each of the modules in your pipeline."
  },
  {
    "index": "F18695",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "するとパイプライン内の各モジュールのパフォーマンスはしばしば最終的なアルゴリズムのパフォーマンスにとても大きな影響を与える。",
    "output": "will often have a big impact on the final performance of your algorithm."
  },
  {
    "index": "F18696",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このような問題に取り組んでいるエンジニアチームがある場合、それぞれのモジュールに対する作業を別々の人がするのもまた、とても一般的な事だ。",
    "output": "If you have a team of engineers working on a problem like this is also very common to have different individuals work on different modules."
  },
  {
    "index": "F18697",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんな風に簡単に想像出来る:文字列検出を1〜5人のエンジニアが文字分割を別の一人から5人程度のエンジニアが、文字認識をまた別の1〜5人のエンジニアが受け持つ、というような事を。だからこのようなパイプラインを持つ事は、エンジニアのチームのそれぞれのメンバーに仕事を分割する自然な区分を提供する。",
    "output": "So I could easily imagine tech easily being the of anywhere from 1 to 5 engineers, character segmentation maybe another 1-5 engineers, and character recognition being another 1-5 engineers, and so having a pipeline like often offers a natural way to divide up the workload amongst different members of an engineering team, as well."
  },
  {
    "index": "F18698",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、これらの仕事全てを一人で全部やったって構わないが、それがお望みならね。",
    "output": "Although, or course, all of this work could also be done by just one person if that's how you want to do it."
  },
  {
    "index": "F18699",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "複雑な機械学習のシステムでは、パイプラインのアイデアは、機械のパイプラインというアイデアは、極めて広く行き渡っている。",
    "output": "In complex machine learning systems the idea of a pipeline, of a machine of a pipeline, is pretty pervasive."
  },
  {
    "index": "F18700",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして今見て来たのは、PhotoOCRパイプラインがどう機能するかという具体的な例だ。",
    "output": "And what you just saw is a specific example of how a Photo OCR pipeline might work."
  },
  {
    "index": "F18701",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次の一連のビデオで、このパイプラインについてもうちょっと議論をしていく。そこでもこの例を用いて、機械学習のーー私が思うに重要なーーキーコンセプトの幾つかを例示していく。",
    "output": "In the next few videos I'll tell you a little bit more about this pipeline, and we'll continue to use this as an example to illustrate--I think--a few more key concepts of machine learning."
  },
  {
    "index": "F18702",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前回のビデオではPhotoOCRのパイプラインについてと、それがどう機能するかについて議論してきた。",
    "output": "In the previous video, we talked about the photo OCR pipeline and how that worked."
  },
  {
    "index": "F18703",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは画像をとり、それを一連の機械学習のコンポーネントを通過させて、画像の中にあるテキストを読み取ることを試みる、という物だった。",
    "output": "In which we would take an image and pass the Through a sequence of machine learning components in order to try to read the text that appears in an image."
  },
  {
    "index": "F18704",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、個々のパイプラインのコンポーネントがどう機能するかについてもう少し議論していきたい。",
    "output": "In this video I like to. A little bit more about how the individual components of the pipeline works."
  },
  {
    "index": "F18705",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、このビデオの大半を、スライディングウィンドウ分類器(classifier)と呼ばれる物に関する議論に費やしたいと思う。",
    "output": "In particular most of this video will center around the discussion. of whats called a sliding windows."
  },
  {
    "index": "F18706",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "PhotoOCRのパイプラインの最初のステージはテキスト検出だった。そこではこんな画像を見ていってこの画像の中でテキストがある位置を見つける事を試みる。",
    "output": "The first stage of the filter was the Text detection where we look at an image like this and try to find the regions of text that appear in this image."
  },
  {
    "index": "F18707",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "テキスト検出はコンピュータビジョンにとっては普通でない問題だ。",
    "output": "Text detection is an unusual problem in computer vision."
  },
  {
    "index": "F18708",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら見つけたいテキストの長さに応じて見つけようとするこれらの矩形も異なるアスペクト比となるからだ。",
    "output": "Because depending on the length of the text you're trying to find, these rectangles that you're trying to find can have different aspect."
  },
  {
    "index": "F18709",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから画像の中の物事を検出する話をする為に、もっと簡単な例である、歩行者の検出から始めよう。",
    "output": "So in order to talk about detecting things in images let's start with a simpler example of pedestrian detection and we'll then later go back to."
  },
  {
    "index": "F18710",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その後に話を戻して歩行者の検出で構築したアイデアをテキスト検出に用いよう。",
    "output": "Ideas that were developed in pedestrian detection and apply them to text detection."
  },
  {
    "index": "F18711",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、歩行者検出においては、こんな画像をとり、画像の中に居る個々の歩行者を見つける事を試みる物だ。",
    "output": "So in pedestrian detection you want to take an image that looks like this and the whole idea is the individual pedestrians that appear in the image."
  },
  {
    "index": "F18712",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり歩行者が一人見つかり、ここに二人目、三人目、四人目、五人目、そして六人目、と。",
    "output": "So there's one pedestrian that we found, there's a second one, a third one a fourth one, a fifth one."
  },
  {
    "index": "F18713",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この問題はテキスト検出の問題よりもちょっとだけ簡単だろう。何故ならほとんどの歩行者のアスペクト比はきわめて似通っているからだ。",
    "output": "This problem is maybe slightly simpler than text detection just for the reason that the aspect ratio of most pedestrians are pretty similar."
  },
  {
    "index": "F18714",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らが見つけようと試みている矩形に対し固定されたアスペクト比を用いる事で、ここでアスペクト比という言葉はこれらの矩形の高さと幅の比の事を言う。",
    "output": "So by aspect ratio I mean the ratio between the height and the width of these rectangles."
  },
  {
    "index": "F18715",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それらは別々の歩行者に対しても同じだ。",
    "output": "They're all the same."
  },
  {
    "index": "F18716",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、テキスト検出の場合は、異なる行のテキストで縦と横の比は異なる。歩行者の検出の場合は歩行者の居る場所までのカメラからの距離が異なる事はあり得るので、これらの矩形の高さは、どれだけ離れているかに応じて違いうるけれど。",
    "output": "for different pedestrians but for text detection the height and width ratio is different for different lines of text Although for pedestrian detection, the pedestrians can be different distances away from the camera and so the height of these rectangles can be different depending on how far away they are."
  },
  {
    "index": "F18717",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "でもアスペクト比は同一だ。",
    "output": "but the aspect ratio is the same."
  },
  {
    "index": "F18718",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "歩行者検出システムを構築する為にはこんな方法が考えられる。このアスペクト比を82x36に標準化する事にしよう。",
    "output": "In order to build a pedestrian detection system here's how you can go about it."
  },
  {
    "index": "F18719",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "別に丸めた数字、例えば80x40とかにしても良いんだけど、82x36で特に問題無さそうなのでこれで行く。",
    "output": "Let's say that we decide to standardize on this aspect ratio of 82 by 36 and we could have chosen some rounded number like 80 by 40 or something, but 82 by 36 seems alright."
  },
  {
    "index": "F18720",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にやるべき事は外に出て大量の陽性と陰性のトレーニングセットを集めてくる事だ。",
    "output": "What we would do is then go out and collect large training sets of positive and negative examples."
  },
  {
    "index": "F18721",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このスライドには、y=1となる12の陽性の手本とy=0となる12の手本をお見せしている。",
    "output": "On this slide I show 12 positive examples of y1 and 12 examples of y0."
  },
  {
    "index": "F18722",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もっと典型的な歩行者検出のアプリケーションでは1000トレーニング手本から1万トレーニング手本くらいまで、またはもっとトレーニングセットが集められる時はそれ以上の場合すらあるのが一般的だ。",
    "output": "In a more typical pedestrian detection application, we may have anywhere from a 1,000 training examples up to maybe 10,000 training examples, or even more if you can get even larger training sets."
  },
  {
    "index": "F18723",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてその時にとれる手段としては、ニューラルネットワークか、それ以外の何らかの学習アルゴリズムを訓練して、この画像のパッチ、82x36の次元のパッチを入力として、それをyかどうか、を分類出来るようにする。",
    "output": "And what you can do, is then train in your network or some other learning algorithm to take this input, an MS patch of dimension 82 by 36, and to classify 'y' and to classify that image patch as either containing a pedestrian or not."
  },
  {
    "index": "F18724",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらの画像パッチに歩行者が含まれているかどうかを分類出来るようにする。以上のように、画像のパッチを受け取りそこに歩行者が居るかどうかを区別する為に教師あり学習を用いる事が出来る。",
    "output": "So this gives you a way of applying supervised learning in order to take an image patch can determine whether or not a pedestrian appears in that image capture."
  },
  {
    "index": "F18725",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、新しい画像を与えられたとして、こんなテストセットの画像だとして、写真の画像から歩行者を見つけたいとする。",
    "output": "Now, lets say we get a new image, a test set image like this and we want to try to find a pedestrian's picture image."
  },
  {
    "index": "F18726",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで我らがやる事はこの画像から矩形のパッチをとっていき、ここにあげたような感じで、これは82x36の画像のパッチとかで、そしてその画像のパッチを分類器に通してその画像パッチに歩行者が居るかどうかを決定する。",
    "output": "What we would do is start by taking a rectangular patch of this image."
  },
  {
    "index": "F18727",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのパッチについては分類器がy=0を返すのを期待する。",
    "output": "Like that shown up here, so that's maybe a 82 X 36 patch of this image, and run that image patch through our classifier to determine whether or not there is a pedestrian in that image patch, and hopefully our classifier will return y equals 0 for that patch, since there is no pedestrian."
  },
  {
    "index": "F18728",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら歩行者は居ないから。次に、その緑の矩形を少しだけスライドさせて、その後にその新しい画像パッチに対して分類器を走らせて、歩行者がそこに居るかを判定する。",
    "output": "Next, we then take that green rectangle and we slide it over a bit and then run that new image patch through our classifier to decide if there's a pedestrian there."
  },
  {
    "index": "F18729",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを終えたら、その後はさらにウィンドウを右にスライドさせてそのパッチを分類器にふたたび通す。",
    "output": "And having done that, we then slide the window further to the right and run that patch through the classifier again."
  },
  {
    "index": "F18730",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "矩形を一度にどれだけシフトさせるかはパラメータだ。それはステップサイズのパラメータと呼ばれる事もあるし、また、ストライドパラメータと呼ばれる事もある。",
    "output": "The amount by which you shift the rectangle over each time is a parameter, that's sometimes called the step size of the parameter, sometimes also called the slide parameter, and if you step this one pixel at a time."
  },
  {
    "index": "F18731",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてもし一度に1ピクセルしか動かさなければ、つまりステップ幅、あるいは歩幅1を使う事が出来れば、それは普通一番良い実行結果が得られるが、計算量的にはより高価となる。だからステップサイズで4ピクセルとか8ピクセルとか、またはそれより大きな適当なピクセルを用いるのがより一般的だ。",
    "output": "So you can use the step size or stride of 1, that usually performs best, that is more cost effective, and so using a step size of maybe 4 pixels at a time, or eight pixels at a time or some large number of pixels might be more common, since you're then moving the rectangle a little bit more each time."
  },
  {
    "index": "F18732",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このプロセスを用いる事で、毎回ちょっとずつ矩形を右に動かしていく事で、そしてこれらの各パッチを分類器にかけていく事で、最終的に、、、このウィンドウを画像の異なる場所へとスライドし続けて最初は最初の行から始めてその後に画像のさらなる先の行へと進めていき、なんらかのステップサイズであるいはあるストライドのサイズでこれら別々の画像のパッチに対し実行していく。",
    "output": "So, using this process, you continue stepping the rectangle over to the right a bit at a time and running each of these patches through a classifier, until eventually, as you slide this window over the different locations in the image, first starting with the first row and then we go further rows in the image, you would then run all of these different image patches at some step size or some stride through your classifier."
  },
  {
    "index": "F18733",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここまでは、とても小さな矩形だった。これは一つの特定のサイズの歩行者しか検出出来ない。",
    "output": "Now, that was a pretty small rectangle, that would only detect pedestrians of one specific size."
  },
  {
    "index": "F18734",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次にやる事は、より大きな画像のパッチを見て、つまりより大きな画像のパッチを取って、ここに示したような、そしてふたたび同様に分類器を走らせる。",
    "output": "What we do next is start to look at larger image patches."
  },
  {
    "index": "F18735",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、より大きな画像のパッチを取る、と言った時に私が実際に意味している事は、こんな画像パッチをとった時、実際にやる事は、この画像のパッチを取り、これを82x36に縮小する、という事。",
    "output": "So now let's take larger images patches, like those shown here and run those through the crossfire as well."
  },
  {
    "index": "F18736",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこのより大きなパッチを取り、それをより小さい画像にリサイズして、そしてその小さくした画像こそが、分類器に渡す物で、そこで歩行者がパッチにいないか決定する事を試みる。",
    "output": "And by the way when I say take a larger image patch, what I really mean is when you take an image patch like this, what you're really doing is taking that image patch, and resizing it down to 82 X 36, say."
  },
  {
    "index": "F18737",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、さらに大きなスケールでこれを行う事が出来て、そのスライディングウィンドウを最後まで実行する。",
    "output": "So you take this larger patch and re-size it to be smaller image and then it would be the smaller size image that is what you would pass through your classifier to try and decide if there is a pedestrian in that patch."
  },
  {
    "index": "F18738",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらのプロセスが全て終わったら、あなたのアルゴリズムはこの画像の中にこれらの歩行者が居るかを検出する事が期待出来る訳だ。以上が分類器を訓練する方法と、そしてスライディングウィンドウの分類器、またはスライディングウィンドウの検出器を用いて、画像の中の歩行者を探す方法だ。",
    "output": "And finally you can do this at an even larger scales and run that side of Windows to the end And after this whole process hopefully your algorithm will detect whether theres pedestrian appears in the image, so thats how you train a the classifier, and then use a sliding windows classifier, or use a sliding windows detector in order to find pedestrians in the image."
  },
  {
    "index": "F18739",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "テキスト検出の例に立ち戻って、PhotoOCRパイプラインでのテキスト検出のステージについて議論しよう、そこでは我らの目標は画像内のテキストの領域を見つける事だ。",
    "output": "Let's have a turn to the text detection example and talk about that stage in our photo OCR pipeline, where our goal is to find the text regions in unit."
  },
  {
    "index": "F18740",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "歩行者の検出と同様に、テキストが現れる場所に対応した陽性と陰性の手本を作り出す事ができる。",
    "output": "similar to pedestrian detection you can come up with a label training set with positive examples and negative examples with examples corresponding to regions where text appears."
  },
  {
    "index": "F18741",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり歩行者を検出する代わりに、今度はテキストを検出したい。",
    "output": "So instead of trying to detect pedestrians, we're now trying to detect texts."
  },
  {
    "index": "F18742",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これで分類器をトレーニングし終えたら、それを新規の画像、テストセットの画像に適用出来る。",
    "output": "And so positive examples are going to be patches of images where there is text."
  },
  {
    "index": "F18743",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これは例として使ってきた画像だ。",
    "output": "And negative examples is going to be patches of images where there isn't text."
  },
  {
    "index": "F18744",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでは、この例においては、スライディングウィンドウをたった一つの固定されたスケールの物で実行する事にしよう。これは例示の為だ。",
    "output": "Having trained this we can now apply it to a new image, into a test set image."
  },
  {
    "index": "F18745",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、小さな、スライディングウィンドウの分類器を、たくさんのちいさな画像のパッチに対して実行するとしよう、こんな感じに。",
    "output": "So here's the image that we've been using as example."
  },
  {
    "index": "F18746",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうすると、結局はこんな結果が得らる。",
    "output": "Now, last time we run, for this example we are going to run a sliding windows at just one fixed scale just for purpose of illustration, meaning that I'm going to use just one rectangle size."
  },
  {
    "index": "F18747",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこの領域はこの領域に対応している。そしてここが黒であるという事実は、分類器がここにテキストは見つからない、と思っている事を表している。",
    "output": "But lets say I run my little sliding windows classifier on lots of little image patches like this if I do that, what Ill end up with is a result like this where the white region show where my text detection system has found text and so the axis' of these two figures are the same."
  },
  {
    "index": "F18748",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方でここにはたくさんの白い物があるという事実は、分類器がたくさんのテキストがここにある、と思っている事を反映している。",
    "output": "So there is a region up here, of course also a region up here, so the fact that this black up here represents that the classifier does not think it's found any texts up there, whereas the fact that there's a lot of white stuff here, that reflects that classifier thinks that it's found a bunch of texts."
  },
  {
    "index": "F18749",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この左下の絵で私がやった事は分類器がテキストを見つけた、と実際に思った場所を見せる為に、白を使ったという事だ。",
    "output": "over there on the image. What i have done on this image on the lower left is actually use white to show where the classifier thinks it has found text."
  },
  {
    "index": "F18750",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりグレイの影は分類器がテキストを発見したように思ってはいるが、だがそんなに自信は無い、と思っている所。明るい白は分類器がとても高い確率でテキストがある場所だ、と推計している場所に対応している。",
    "output": "And different shades of grey correspond to the probability that was output by the classifier, so like the shades of grey corresponds to where it thinks it might have found text but has lower confidence the bright white response to whether the classifier, up with a very high probability, estimated probability of there being pedestrians in that location."
  },
  {
    "index": "F18751",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何故なら我らが望んでいるのは、画像の中のこのテキストの全体の回りの領域に対して四角で囲む、という事だからだ。だからさらにもう一段階ステップを踏む、それは分類器の出力をとり、そこにexpansionoperatorと呼ばれる物を適用する、という事をする。",
    "output": "We aren't quite done yet because what we actually want to do is draw rectangles around all the region where this text in the image, so were going to take one more step which is we take the output of the classifier and apply to it what is called an expansion operator."
  },
  {
    "index": "F18752",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それがやる事は、この画像に対して、それぞれの白のシミに対して、それぞれの白い領域に対して、その白の領域を拡大する、という事をする。",
    "output": "So what that does is, it take the image here, and it takes each of the white blobs, it takes each of the white regions and it expands that white region."
  },
  {
    "index": "F18753",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "数学的には、それを実装する方法は、右の画像を見てみると、右の画像を作る為にやれる事としては、各ピクセルに対し、以下のように尋ねてみる事だ:このピクセルは、左の画像の白いピクセルから一定の距離以内にあるだろうか?と。",
    "output": "Mathematically, the way you implement that is, if you look at the image on the right, what we're doing to create the image on the right is, for every pixel we are going to ask, is it withing some distance of a white pixel in the left image."
  },
  {
    "index": "F18754",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "白いピクセルの5ピクセルとか10ピクセル以内にあるのなら、その時は一番右のそのピクセルも白に塗る。",
    "output": "And so, if a specific pixel is within, say, five pixels or ten pixels of a white pixel in the leftmost image, then we'll also color that pixel white in the rightmost image."
  },
  {
    "index": "F18755",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このような操作が与える効果は、一番左の画像の白いしみを取り出して、ちょっとだけ拡張したような物となる。",
    "output": "And so, the effect of this is, we'll take each of the white blobs in the leftmost image and expand them a bit, grow them a little bit, by seeing whether the nearby pixels, the white pixels, and then coloring those nearby pixels in white as well."
  },
  {
    "index": "F18756",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、これで終わりになるが、この一番右の画像を見ていき、くっついている構成要素を見ていき、つながった白い領域のバウンディングボックスをその回りに描く。",
    "output": "Finally, we are just about done."
  },
  {
    "index": "F18757",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "具体的には、これらの白い領域を全部見ていくとすると、例えばこれとか、これとか、これとか。",
    "output": "We can now look at this right most image and just look at the connecting components and look at the as white regions and draw bounding boxes around them."
  },
  {
    "index": "F18758",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして簡単な経験則でアスペクト比がおかしい、と思うような物を除外していくと、、、何故なら我らはテキストの回りの箱は高さよりも幅の方が大きいべきだという事を知っているから。だから痩せてて高い箱を無視していくと、例えばこれとかこれとか。",
    "output": "And in particular, if we look at all the white regions, like this one, this one, this one, and so on, and if we use a simple heuristic to rule out rectangles whose aspect ratios look funny because we know that boxes around text should be much wider than they are tall."
  },
  {
    "index": "F18759",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてアスペクト比がテキストの領域っぽい物の回りを囲んだ矩形を描く、ここでアスペクト比とは高さと幅の比の事。",
    "output": "And so if we ignore the thin, tall blobs like this one and this one, and we discard these ones because they are too tall and thin, and we then draw a the rectangles around the ones whose aspect ratio thats a height to what ratio looks like for text regions, then we can draw rectangles, the bounding boxes around this text region, this text region, and that text region, corresponding to the Lula B's antique mall logo, the Lula B's, and this little open sign."
  },
  {
    "index": "F18760",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "所で、この例は実の所、一片のテキストを見逃している。",
    "output": "This example by the actually misses one piece of text."
  },
  {
    "index": "F18761",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはとても読みにくいが、だがここには実際は一片のテキストがある。",
    "output": "This is very hard to read, but there is actually one piece of text there."
  },
  {
    "index": "F18762",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここにもLULAB'sがあって、それはこれに対応しているのだが、だがこのアスペクト比は間違いっぽいので、それを捨てたのだった。",
    "output": "That says are corresponding to this but the aspect ratio looks wrong so we discarded that one."
  },
  {
    "index": "F18763",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、この画像に関しては問題無さそうだが、この具体例においては分類器は実は一片のテキストを見逃している。",
    "output": "So you know it's ok on this image, but in this particular example the classifier actually missed one piece of text."
  },
  {
    "index": "F18764",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは凄い読みにくい、何故なら透明の窓に対して書かれたテキストだから。",
    "output": "It's very hard to read because there's a piece of text written against a transparent window."
  },
  {
    "index": "F18765",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がスライディングウィンドウを用いたテキスト検出だ。",
    "output": "So that's text detection using sliding windows."
  },
  {
    "index": "F18766",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "テキストを囲むこれらの矩形を見つけたのちには、我らはこれらの画像の領域を切り抜いて、それをあとに続くパイプラインのステージでテキストを読む為に用いる事が出来る。",
    "output": "And having found these rectangles with the text in it, we can now just cut out these image regions and then use later stages of pipeline to try to meet the texts."
  },
  {
    "index": "F18767",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、パイプラインの二番目のステージは文字分割だったのを覚えているだろうか?つまり上に見せたような画像を与えられた時に、この画像の各文字に、どうやって分割出来るだろうか?",
    "output": "Now, you recall that the second stage of pipeline was character segmentation, so given an image like that shown on top, how do we segment out the individual characters in this image?"
  },
  {
    "index": "F18768",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ある陽性の手本の集合と、ある陰性の手本の集合を共に用いた。それでやる事は画像のパッチを見て、画像のパッチのちょうど真ん中に二つの文字の区切りがあるかどうかを決めたい。",
    "output": "So what we can do is again use a supervised learning algorithm with some set of positive and some set of negative examples, what were going to do is look in the image patch and try to decide if there is split between two characters right in the middle of that image match."
  },
  {
    "index": "F18769",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、これらは陽性の手本だ。",
    "output": "So for initial positive examples."
  },
  {
    "index": "F18770",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その画像の真ん中は、隙間、または二つの文字を分ける区切りで、他方、陰性の手本は、真ん中で二つの文字を分割したい、と思わないような物。",
    "output": "This first cross example, this image patch looks like the middle of it is indeed the middle has splits between two characters and the second example again this looks like a positive example, because if I split two characters by putting a line right down the middle, that's the right thing to do."
  },
  {
    "index": "F18771",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれらは陰性の手本だ、何故ならこれらは二つの文字の真ん中を表していないからだ。",
    "output": "So, these are positive examples, where the middle of the image represents a gap or a split between two distinct characters, whereas the negative examples, well, you know, you don't want to split two characters right in the middle, and so these are negative examples because they don't represent the midpoint between two characters."
  },
  {
    "index": "F18772",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで我らが行う事は、分類器をトレーニングする事で、ニューラルネットワークを使ってもいいし、別のアルゴリズムでもいいが、とにかく陽性と陰性の手本を分類しようとするアルゴリズムをトレーニングする。",
    "output": "So what we will do is, we will train a classifier, maybe using new network, maybe using a different learning algorithm, to try to classify between the positive and negative examples."
  },
  {
    "index": "F18773",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そんな分類器をトレーニングし終えたら、その後我らはこれをテキスト検出器が取り出したこんな画像に対して走らせる。",
    "output": "Having trained such a classifier, we can then run this on this sort of text that our text detection system has pulled out."
  },
  {
    "index": "F18774",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この矩形から始めて、こう問う:緑の矩形の真ん中は、、、2つの文字の間っぽく見えるか?と。",
    "output": "As we start by looking at that rectangle, and we ask, \"Gee, does it look like the middle of that green rectangle, does it look like the midpoint between two characters?\"."
  },
  {
    "index": "F18775",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてウィンドウをスライドさせて、、、所でこれは1次元のスライディングウィンドウの分類器だ。何故ならウィンドウを一直線に左から右へとスライドさせるだけでここでは別の行、というのが無いから。",
    "output": "And hopefully, the classifier will say no, then we slide the window over and this is a one dimensional sliding window classifier, because were going to slide the window only in one straight line from left to right, theres no different rows here."
  },
  {
    "index": "F18776",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここには一行しか無い。",
    "output": "There's only one row here."
  },
  {
    "index": "F18777",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、ここで、分類器がこの場所に来た時に、二つの文字をここで分割すべきか、または二つの文字の区切りをこの矩形の真ん中に置くべきか?と問う。",
    "output": "But now, with the classifier in this position, we ask, well, should we split those two characters or should we put a split right down the middle of this rectangle."
  },
  {
    "index": "F18778",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして期待される事としては、分類器はy=1と出力する事。つまりその場合はそこに線を引くと決定し、二つの文字を分割しようとする訳だ。",
    "output": "And hopefully, the classifier will output y equals one, in which case we will decide to draw a line down there, to try to split two characters."
  },
  {
    "index": "F18779",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にウィンドウをまたスライドさせて、この場合は分類器はここでは分割しない、と出力する事が期待されて、さらにまたスライドさせて、yes、ここでスプリットせよ、と言う事が期待される。",
    "output": "Then we slide the window over again, optic process, don't close the gap, slide over again, optic says yes, do split there and so on, and we slowly slide the classifier over to the right and hopefully it will classify this as another positive example and so on."
  },
  {
    "index": "F18780",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのウィンドウを右側へとスライドさせていき、各ステップで分類器を実行し、そしてそれをもって我らにこれらの文字列を分割する適切な場所を教えてくれる事を期待する訳だ、つまりこの画像を個々の文字へと分割する。",
    "output": "And we will slide this window over to the right, running the classifier at every step, and hopefully it will tell us, you know, what are the right locations to split these characters up into, just split this image up into individual characters."
  },
  {
    "index": "F18781",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が1Dスライディングウィンドウによる文字分割だ。",
    "output": "And so thats 1D sliding windows for character segmentation."
  },
  {
    "index": "F18782",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここにPhotoOCRパイプラインの全体像を再掲した。",
    "output": "So, here's the overall photo OCR pipe line again."
  },
  {
    "index": "F18783",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、テキスト検出のステップを議論して来た。そこでは、テキストを検出する為に、スライディングウィンドウを使った。",
    "output": "In this video we've talked about the text detection step, where we use sliding windows to detect text."
  },
  {
    "index": "F18784",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてまた、文字分割でも1次元のスライディングウィンドウを分割する為に使った。このテキストの画像を、各文字に分割する。",
    "output": "And we also use a one-dimensional sliding windows to do character segmentation to segment out, you know, this text image in division of characters."
  },
  {
    "index": "F18785",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "パイプラインの最後のステップは文字分類ステップだ。そしてそのステップに関しては、以前の教師あり学習のところのビデオでやったので、既になじみの物だろう、そこでは、通常の教師あり学習の、例えばニューラルネットワークとかそれ以外のなんでも良いが、その辺を使って画像を入力として、こんなような、そしてどのアルファベットか、言い換えると26文字のaからzまでのどの文字か、数字も入れるなら36文字にすべきかもしれない。",
    "output": "The final step through the pipeline is the character qualification step and that step you might already be much more familiar with the early videos on supervised learning where you can apply a standard supervised learning within maybe on your network or maybe something else in order to take it's input, an image like that and classify which alphabet or which 26 characters A to Z, or maybe we should have 36 characters if you have the numerical digits as well, the multi class classification problem where you take it's input and image contained a character and decide what is the character that appears in that image?"
  },
  {
    "index": "F18786",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がPhotoOCRのパイプラインだ。そしてどうやってスライディングウィンドウの分類器などのアイデアを、、、これら別々のコンポーネントを組み合わせてPhotoOCRシステムを開発するか、という話だ。",
    "output": "So that was the photo OCR pipeline and how you can use ideas like sliding windows classifiers in order to put these different components to develop a photo OCR system."
  },
  {
    "index": "F18787",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次の一連のビデオでは、引き続きPhotoOCRの問題を用いてこのようなアプリケーションを開発する時にまつわるいくらか興味深い問題を探求していく。",
    "output": "In the next few videos we keep on using the problem of photo OCR to explore somewhat interesting issues surrounding building an application like this."
  },
  {
    "index": "F18788",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何度も見てきたが高いパフォーマンスの機械学習システムを得るもっとも信頼出来る方法の一つに、低バイアスの学習アルゴリズムに大量のトレーニングセットで訓練する、というのがある。",
    "output": "I've seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias learning algorithm and to train it on a massive training set."
  },
  {
    "index": "F18789",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、そんな大量のトレーニングデータをどこから得たら良いだろうか?",
    "output": "But where did you get so much training data from?"
  },
  {
    "index": "F18790",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "機械学習においては、人工データ合成、と呼ばれる魅力的なアイデアが考え出されている。このアイデアはどの問題でも使えるという訳では無いし特定の問題に適用する時にもなんらかの思索、イノベーション、そして洞察が必要となる事が多い。",
    "output": "Turns out that the machine earnings there's a fascinating idea called artificial data synthesis, this doesn't apply to every single problem, and to apply to a specific problem, often takes some thought and innovation and insight."
  },
  {
    "index": "F18791",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもしこのアイデアがあなたの機械学習の問題に適用出来たら、それはあなたの学習アルゴリズムに膨大なトレーニングセットを与える簡単な方法となる事がある。",
    "output": "But if this idea applies to your machine, only problem, it can sometimes be a an easy way to get a huge training set to give to your learning algorithm."
  },
  {
    "index": "F18792",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "人工データ合成は2つのバリエーションから構成されている。最初の主要なバリエーションは本質的には無からデータを作り出す、つまり新しいデータをスクラッチから作り出す、という物。",
    "output": "The idea of artificial data synthesis comprises of two variations, main the first is if we are essentially creating data from , creating new data from scratch."
  },
  {
    "index": "F18793",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして二番目は、もし既に少量のラベルつきトレーニングセットを持っていたら、そのトレーニングセットをどうにか増幅する。",
    "output": "And the second is if we already have it's small label training set and we somehow have amplify that training set or use a small training set to turn that into a larger training set and in this video we'll go over both those ideas."
  },
  {
    "index": "F18794",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "人口データ合成のアイデアを議論する為にPhotoOCRパイプラインの中の文字認識の部分を例にとろう。入力の画像を取り、その文字が何なのかを認識する。",
    "output": "To talk about the artificial data synthesis idea, let's use the character portion of the photo OCR pipeline, we want to take it's input image and recognize what character it is."
  },
  {
    "index": "F18795",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "外に出て多くのラベルづけされたデータセットを収集してくると、こんな感じとなる。",
    "output": "If we go out and collect a large label data set, here's what it is and what it look like."
  },
  {
    "index": "F18796",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この具体例に関しては、正方形のアスペクト比を選んだ。",
    "output": "For this particular example, I've chosen a square aspect ratio."
  },
  {
    "index": "F18797",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり正方形の画像パッチをとる。",
    "output": "So we're taking square image patches."
  },
  {
    "index": "F18798",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして目標は、画像のパッチを取り、その画像パッチの真ん中にある文字を認識する事だ。",
    "output": "And the goal is to take an image patch and recognize the character in the middle of that image patch."
  },
  {
    "index": "F18799",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シンプルにする為に、これらの画像はカラー画像では無くグレースケール画像として扱う。",
    "output": "And for the sake of simplicity, I'm going to treat these images as grey scale images, rather than color images."
  },
  {
    "index": "F18800",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "色を使ってもこの問題に関しては大して変わらない事が分かっている。",
    "output": "It turns out that using color doesn't seem to help that much for this particular problem."
  },
  {
    "index": "F18801",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、この画像パッチが与えられて、これがTだ、と認識したい。",
    "output": "So given this image patch, we'd like to recognize that that's a T."
  },
  {
    "index": "F18802",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この画像パッチが与えられたらこれはSだと認識したい。",
    "output": "Given this image patch, we'd like to recognize that it's an 'S'."
  },
  {
    "index": "F18803",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この画像パッチを与えられたら、これはIだと認識したい、などなど。",
    "output": "Given that image patch we would like to recognize that as an 'I' and so on."
  },
  {
    "index": "F18804",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれら全て、我らの生画像の手本に対し、どうやったらもっと多くのトレーニングセットが得られるか?",
    "output": "So all of these, our examples of row images, how can we come up with a much larger training set?"
  },
  {
    "index": "F18805",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最近のコンピュータなら、普通膨大なフォントのライブラリを持ってる物だ。もしワープロのソフトを使ってるならどのワープロを使っているかに応じてこれらのフォントを全て持ってるかもしれないし、さらにもっと多くの物が既に内部に保存されているだろう。",
    "output": "Modern computers often have a huge font library and if you use a word processing software, depending on what word processor you use, you might have all of these fonts and many, many more Already stored inside."
  },
  {
    "index": "F18806",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実の所、いろいろなwebサイトにはそこにもまた、膨大なフリーのフォントのライブラリがインターネット上にはある。我らは様々な種類のフォント、それこそ何百とか何千ものフォントをダウンロード出来る。",
    "output": "And, in fact, if you go different websites, there are, again, huge, free font libraries on the internet we can download many, many different types of fonts, hundreds or perhaps thousands of different fonts."
  },
  {
    "index": "F18807",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもっと多くのトレーニング手本が欲しければ考えられる手としては一つには別々のフォントから文字を取り出して、別々のランダムの背景にペーストしていく、というのが考えられる。",
    "output": "So if you want more training examples, one thing you can do is just take characters from different fonts and paste these characters against different random backgrounds."
  },
  {
    "index": "F18808",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれを取って、このCをランダムの背景にペーストする。",
    "output": "So you might take this ---- and paste that c against a random background."
  },
  {
    "index": "F18809",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうすれば、文字Cの画像のトレーニングセットを得る事が出来る。",
    "output": "If you do that you now have a training example of an image of the character C."
  },
  {
    "index": "F18810",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "いくらかの仕事を行えば、本当っぽく見せる為のこれらの行程はちょっとした仕事ではあるが、だがこの幾らかの仕事を行ったあとには、こんな感じの合成されたトレーニングセットが得られる。",
    "output": "So after some amount of work, you know this, and it is a little bit of work to synthisize realistic looking data. But after some amount of work, you can get a synthetic training set like that."
  },
  {
    "index": "F18811",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "右側に示した各画像は全て実際に合成された画像だ。",
    "output": "Every image shown on the right was actually a synthesized image."
  },
  {
    "index": "F18812",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前面にはフォント、例えばwebからダウンロードしたランダムのフォントなどから一文字か数文字を背景画像の上にペーストした物でその背景画像はそれぞれ別々のランダムの背景画像。",
    "output": "Where you take a font, maybe a random font downloaded off the web and you paste an image of one character or a few characters from that font against this other random background image."
  },
  {
    "index": "F18813",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそこに、ちょっとしたブラーの操作をしても良い---適当なアフィン変換で歪めたり、アフィン変換というのはシアーしたり拡大縮小したり、ちょっとだけ回転させたりといった操作の事だ。",
    "output": "And then apply maybe a little blurring operators -----of app finder, distortions that app finder, meaning just the sharing and scaling and little rotation operations and if you do that you get a synthetic training set, on what the one shown here."
  },
  {
    "index": "F18814",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それを行うと、合成されたトレーニングセットが出来上がり、それはここに示したような物となる。",
    "output": "And this is work, grade, it is, it takes thought at work, in order to make the synthetic data look realistic, and if you do a sloppy job in terms of how you create the synthetic data then it actually won't work well."
  },
  {
    "index": "F18815",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのやり方は、合成されたデータが本物っぽく見えるように真面目に考えて頑張ればかなりうまく機能する。",
    "output": "But if you look at the synthetic data looks remarkably similar to the real data."
  },
  {
    "index": "F18816",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがもし実際のデータと驚くほど似たデータを合成する事が出来たら、その合成されたデータを使う事で、人工トレーニングセット合成による実質的には無制限の量のトレーニング手本を供給出来る。つまり、この合成データを使う事でラベルデータの供給を実質無制限に行う事が出来、それを用いて文字認識の教師あり学習のアルゴリズムをトレーニング出来る。",
    "output": "And so by using synthetic data you have essentially an unlimited supply of training examples for artificial training synthesis And so, if you use this source synthetic data, you have essentially unlimited supply of label data to create a improvised learning algorithm for the character recognition problem."
  },
  {
    "index": "F18817",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上が人工データ合成の例だ。そこでは、基本的にはデータをスクラッチから作る、スクラッチから全く新しい画像を生成する。",
    "output": "So this is an example of artificial data synthesis where youre basically creating new data from scratch, you just generating brand new images from scratch."
  },
  {
    "index": "F18818",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これとは別の、人工データ合成の良くあるアプローチとしては、既に持っている手本を持ってきて、つまり本当の画像などの本当の手本を持ってきて、追加のデータを作成して、トレーニングセットを増幅する。",
    "output": "The other main approach to artificial data synthesis is where you take a examples that you currently have, that we take a real example, maybe from real image, and you create additional data, so as to amplify your training set."
  },
  {
    "index": "F18819",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さてここに文字Aの画像がある、これは実際の画像から取ってきた物だ、合成された画像では無い。そして格子上の線を例示の為に重ねて表示している。",
    "output": "So here is an image of a compared to a from a real image, not a synthesized image, and I have overlayed this with the grid lines just for the purpose of illustration."
  },
  {
    "index": "F18820",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、ここで取れる手段として、このここにあるアルファベットを取り、この画像を取り、人工的なたわみ、あるいは人工的な歪みを画像に導入する。つまり画像を取り、そこから16個の新しい手本を生成する。",
    "output": "So what you can do is then take this alphabet here, take this image and introduce artificial warpings or artificial distortions into the image so they can take the image a and turn that into 16 new examples."
  },
  {
    "index": "F18821",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこうやって、少量のラベル付きトレーニングセットを持ってきてそれを一気に増幅してもっとたくさんの手本を、元のトレーニングセットから得る事が出来る。",
    "output": "So in this way you can take a small label training set and amplify your training set to suddenly get a lot more examples, all of it."
  },
  {
    "index": "F18822",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "繰り返しになるが、具体的なアプリケーションの為にこれを行うには、我らにとっての合理的な歪みとはどんな物か、トレーニングセットを増幅する合理的な方法はどんな物か、について、良く考えて、洞察を元に探す必要がある。この文字認識の具体例に関しては、これらのたわみを導入するのは、自然な選択に思える。",
    "output": "Again, in order to do this for application, it does take thought and it does take insight to figure out what our reasonable sets of distortions, or whether these are ways that amplify and multiply your training set, and for the specific example of character recognition, introducing these warping seems like a natural choice, but for a different learning machine application, there may be different the distortions that might make more sense."
  },
  {
    "index": "F18823",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "全く異なる分野の例として、音声認識の例を見てみよう。",
    "output": "Let me just show one example from the totally different domain of speech recognition."
  },
  {
    "index": "F18824",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "音声認識とは、オーディオクリップがあったとして、オーディオクリップから、その中になんという単語が喋られているかを認識するように学習したい。",
    "output": "So the speech recognition, let's say you have audio clips and you want to learn from the audio clip to recognize what were the words spoken in that clip."
  },
  {
    "index": "F18825",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ラベル付きトレーニング手本の一つがどんな物か、見てみよう。",
    "output": "So let's see how one labeled training example."
  },
  {
    "index": "F18826",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたは一つのラベル付き手本を持ってるとして、それは誰かが幾つかの特定の単語を喋ってる物だとしよう。",
    "output": "So let's say you have one labeled training example, of someone saying a few specific words."
  },
  {
    "index": "F18827",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そのオーディオクリップを再生してみる。",
    "output": "So let's play that audio clip here."
  },
  {
    "index": "F18828",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり誰かが0から5まで数えているとする、そしてその中で言われている単語が何なのかを認識する為に学習アルゴリズムを適用したいとする。",
    "output": "Alright, so someone counting from 0 to 5, and so you want to try to apply a learning algorithm to try to recognize the words said in that."
  },
  {
    "index": "F18829",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その時、どうやってデータセットを増幅するか?",
    "output": "So, how can we amplify the data set?"
  },
  {
    "index": "F18830",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "うーん、一つ考えられるのは、データセットに追加のオーディオ的な歪みを加えるという事。",
    "output": "Well, one thing we do is introduce additional audio distortions into the data set."
  },
  {
    "index": "F18831",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ビープ音が聞こえても、それは実際にオーディオトラックの一部だからスピーカーの故障じゃありません。では再生しよう。",
    "output": "When you hear beeping sounds, that's actually part of the audio track, that's nothing wrong with the speakers, I'm going to play this now."
  },
  {
    "index": "F18832",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "0-1-2-3-4-5。",
    "output": "0-1-2-3-4-5."
  },
  {
    "index": "F18833",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この種のオーディオクリップもあなたは聞き取れて、音を認識出来るのだから、これもまた追加するに値するトレーニング手本に思える。もう一つ、また別の例で、うるさい背後の音がある例を聞いてみよう。",
    "output": "Right, so you can listen to that sort of audio clip and recognize the sounds, that seems like another useful training example to have, here's another example, noisy background."
  },
  {
    "index": "F18834",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これもまた先ほどとは別の物になっている。元のクリーンなオーディオクリップを取ってきて、つまり誰かがクリアに言っている、0,1,2,3,4,5というオーディオを取ってきて、そして自動的にこれらの追加的なトレーニング手本を合成する事が出来、かくして一つのトレーニング手本を4つの別々のトレーニング手本に増幅出来る。",
    "output": "Zero, one, two, three four five you know of cars driving past, people walking in the background, here's another one, so taking the original clean audio clip so taking the clean audio of someone saying 0 1 2 3 4 5 we can then automatically synthesize these additional training examples and thus amplify one training example into maybe four different training examples."
  },
  {
    "index": "F18835",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではこの最後の例を再生してみよう。",
    "output": "So let me play this final example, as well."
  },
  {
    "index": "F18836",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、一つのラベル付き手本を持ってくるだけで、一つのラベルつき手本を収集する労力を払うだけで、0,1,2,3,4,5と言っている手本を得るだけで、歪みを追加して合成する事によって、異なる背後の音を導入するだけで、いまやこの一つの手本を何倍ものたくさんの手本に増やす事が出来た、そんなにたくさんの仕事をせずに。",
    "output": "0-1 3-4-5 So by taking just one labelled example, we have to go through the effort to collect just one labelled example fall of the 01205, and by synthesizing additional distortions, by introducing different background sounds, we've now multiplied this one example into many more examples."
  },
  {
    "index": "F18837",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "歪みを導入する事でデータを合成する事に関して一言警告をしておく。これを自分でやる時には、あなたが導入する歪みは、テストセットで見られそうなノイズ音源や歪みを代表しているべきだ。",
    "output": "Much work by just automatically adding these different background sounds to the clean audio Just one word of warning about synthesizing data by introducing distortions: if you try to do this yourself, the distortions you introduce should be representative the source of noises, or distortions, that you might see in the test set."
  },
  {
    "index": "F18838",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "文字認識の例では、先ほど導入したこの類の歪みは実際にアリだと思われる物だ、何故なら画像のAはAっぽく見えるから。つまり実際にテストセットでも見そうな物だから。",
    "output": "So, for the character recognition example, you know, the working things begin introduced are actually kind of reasonable, because an image A that looks like that, that's, could be an image that we could actually see in a test set.Reflect a fact And, you know, that image on the upper-right, that could be an image that we could imagine seeing."
  },
  {
    "index": "F18839",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてオーディオの例に関しては、以下のような悪条件でも会話を認識したい:携帯の接続が悪かったり、異なる種類の背後のノイズの中だったり。だからオーディオの場合も、我らが合成した手本は、実際に分類したい、実際に正しく認識したい種類の物を本当に代表している。",
    "output": "And for audio, well, we do wanna recognize speech, even against a bad self internal connection, against different types of background noise, and so for the audio, we're again synthesizing examples are actually representative of the sorts of examples that we want to classify, that we want to recognize correctly."
  },
  {
    "index": "F18840",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これとは逆に、無意味でランダムなノイズをデータに付加するのは、だいたいの場合にはたぶん役に立たないだろう。",
    "output": "In contrast, usually it does not help perhaps you actually a meaning as noise to your data."
  },
  {
    "index": "F18841",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これを見ても分からないかもしれないが、これにやった事は、画像を持ってきて、各ピクセルに対し、これら4つの画像のそれぞれに、ランダムのガウス分布のノイズを各ピクセルに付加したのだ。",
    "output": "I'm not sure you can see this, but what we've done here is taken the image, and for each pixel, in each of these 4 images, has just added some random Gaussian noise to each pixel."
  },
  {
    "index": "F18842",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "各ピクセルに対し、ピクセルの明るさとかに、各ピクセルにガウス分布のランダムのノイズを付加する。",
    "output": "To each pixel, is the pixel brightness, it would just add some, you know, maybe Gaussian random noise to each pixel."
  },
  {
    "index": "F18843",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、それは完全に無意味なノイズだ。でしょ?",
    "output": "So it's just a totally meaningless noise, right?"
  },
  {
    "index": "F18844",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから、テストセットにこの種のピクセル全体に渡るノイズを観測する場合があると想定出来る場合を除いては、この種の純粋にランダムで無意味なノイズは、あまり役に立たない。",
    "output": "And so, unless you're expecting to see these sorts of pixel wise noise in your test set, this sort of purely random meaningless noise is less likely to be useful."
  },
  {
    "index": "F18845",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "しかし、人工データ合成のプロセスにおいては、ちょっとしたアートも必要となるし、時には試してみてうまく行くかを見てみるというのも必要だ。",
    "output": "But the process of artificial data synthesis it is you know a little bit of an art as well and sometimes you just have to try it and see if it works."
  },
  {
    "index": "F18846",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが、もしどの種の歪みを加えるかを決めようとしているなら、それ以外にどんな意味がありそうな歪みがありそうかを真面目に考える必要がある。その歪みは少なくとも幾らかはテストセットを代表した画像を生成すると、つまりテストセットで実際に見そうなものが得られそうな範囲で。",
    "output": "But if you're trying to decide what sorts of distortions to add, you know, do think about what other meaningful distortions you might add that will cause you to generate additional training examples that are at least somewhat representative of the sorts of images you expect to see in your test sets."
  },
  {
    "index": "F18847",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最後に、このビデオをまとめる為に人工データ合成からたくさんのデータを得るというアイデアについて2,3言っておきたい事がある。",
    "output": "Finally, to wrap up this video, I just wanna say a couple of words, more about this idea of getting loss of data via artificial data synthesis."
  },
  {
    "index": "F18848",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "毎度の事だが、人工的にトレーニングデータを作り出す方法を編み出す為にたくさんの労力を払う前に、本当に手持ちの分類器が低バイアスか、そしてより多くのトレーニングデータが本当に役に立つのかを確認しておくのは多くの場合で良い習慣だ。",
    "output": "As always, before expending a lot of effort, you know, figuring out how to create artificial training examples, it's often a good practice is to make sure that you really have a low biased crossfire, and having a lot more training data will be of help."
  },
  {
    "index": "F18849",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれをやる標準的な方法は、学習曲線をプロットして、確かに低バイアスの分類器を持っていて、高バリアンスの分類器では無い事を確認する。",
    "output": "And standard way to do this is to plot the learning curves, and make sure that you only have a low as well, high variance falsifier."
  },
  {
    "index": "F18850",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "または、もし低バイアスの分類器を持っていなければ、もう一つ試す価値のある事としては、分類器の持つフィーチャーの数を増やしてみる、というのがある。",
    "output": "Or if you don't have a low bias falsifier, you know, one other thing that's worth trying is to keep increasing the number of features that your classifier has, increasing the number of hidden units in your network, saying, until you actually have a low bias falsifier, and only then, should you put the effort into creating a large, artificial training set, so what you really want to avoid is to, you know, spend a whole week or spend a few months figuring out how to get a great artificially synthesized data set."
  },
  {
    "index": "F18851",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "本当に避けなくてはいけない事態はまるまる一週間とか何ヶ月も費やしてとても良い人工合成されたデータセットを作る方法を発見した後で、結局あなたの学習アルゴリズムのパフォーマンスは大量のトレーニングセットがあってもあまり改善しない、と判明する事だ。",
    "output": "Only to realize afterward, that, you know, your learning algorithm, performance doesn't improve that much, even when you're given a huge training set."
  },
  {
    "index": "F18852",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以上がいつも通りのアドバイスである、大量のデータを実際にあなたが有効活用出来るかのテストを、大量のトレーニングセットを収集する努力を費やす前に行え、という事だ。二番目。",
    "output": "So that's about my usual advice about of a testing that you really can make use of a large training set before spending a lot of effort going out to get that large training set."
  },
  {
    "index": "F18853",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私が機械学習の問題の仕事をしている時に、一緒に働いているチームにしょっちゅう尋ねる質問は、しょっちゅう生徒に尋ねる質問は、現在持ってるデータセットの10倍を得るのにどれだけの仕事が必要だろうか?という物がある。",
    "output": "Second is, when i'm working on machine learning problems, one question I often ask the team I'm working with, often ask my students, which is, how much work would it be to get 10 times as much date as we currently had."
  },
  {
    "index": "F18854",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私が新しい機械学習の適用の場に直面した時には、とてもよくチームと共に座ってまさにこの質問を尋ねる。この問いを何度も何度もなーんども尋ねてきた。",
    "output": "When I face a new machine learning application very often I will sit down with a team and ask exactly this question, I've asked this question over and over and over and I've been very surprised how often this answer has been that."
  },
  {
    "index": "F18855",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして私はこの問いの答えが何度も以下のようであるかを知る事になり、しばしば驚く:実際はそんなに大変じゃなくて、せいぜい2〜3日の仕事で現在持っているデータの10倍のデータを得る事が出来て機械学習のアプリケーションに使う事が出来、そしてしょっちゅう、もし10倍のデータが得られたら、あなたのアルゴリズムがずっと良い仕事をする、という事が起こる。",
    "output": "You know, it's really not that hard, maybe a few days of work at most, to get ten times as much data as we currently have for a machine running application and very often if you can get ten times as much data there will be a way to make your algorithm do much better."
  },
  {
    "index": "F18856",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからもし何らかの機械学習のアプリケーションの仕事をしているチームに参加する時には、これは自問してみるのにとても良い問いであり、チームに尋ねてみるのにとても良い問いだ。",
    "output": "So, you know, if you ever join the product team working on some machine learning application product this is a very good questions ask yourself ask the team don't be too surprised if after a few minutes of brainstorming if your team comes up with a way to get literally ten times this much data, in which case, I think you would be a hero to that team, because with 10 times as much data, I think you'll really get much better performance, just from learning from so much data."
  },
  {
    "index": "F18857",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして数分のブレーンストーミングの後に文字通り10倍のデータを得る方法を考え出したとしても、それほど驚くべき事では無い。",
    "output": "So there are several waysand that comprised both the ideas of generating data from scratch using random fonts and so on."
  },
  {
    "index": "F18858",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それは二つのアイデアから構成されている:一つ目は適当なフォントなどを使ってデータをスクラッチから作る方法で、二つ目は、すでに存在する手本を取ってきてそれに歪ませて、トレーニングセットをさらにたくさんに増幅する。その他の手段としては、自分でデータを収集してラベルづけしていく、という物。",
    "output": "As well as the second idea of taking an existing example and and introducing distortions that amplify to enlarge the training set A couple of other examples of ways to get a lot more data are to collect the data or to label them yourself."
  },
  {
    "index": "F18859",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから私が良くやる有用な計算として、ある数の手本を集めるのに何分かかるか何時間かかるか、を考えてみるというのがある。実際に座って、考えてみる、1つの手本をラベルづけするのに10秒かかるとして、我らのアプリケーションは現在1000個のラベルづけされた手本があるとしてみよう。",
    "output": "So one useful calculation that I often do is, you know, how many minutes, how many hours does it take to get a certain number of examples, so actually sit down and figure out, you know, suppose it takes me ten seconds to label one example then and, suppose that, for our application, currently we have 1000 labeled examples examples so ten times as much of that would be if n were equal to ten thousand."
  },
  {
    "index": "F18860",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二番目のデータをたくさん集める方法は、単にデータを収集して自分でラベルづけする事だった。",
    "output": "A second way to get a lot of data is to just collect the data and you label it yourself."
  },
  {
    "index": "F18861",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どれだけの時間が、何時間かかるか、何日かかるか、椅子に座って考えてみるのだ。現在持っているデータの10倍のデータを集めてきて、自分たちの手で集めてきて自分たちの手でラベル付けして行ったらどれだけかかるのか、椅子に座って考えてみるのだ。",
    "output": "So what I mean by this is I will often set down and do a calculation to figure out how much time, you know just like how many hours will it take, how many hours or how many days will it take for me or for someone else to just sit down and collect ten times as much data, as we have currently, by collecting the data ourselves and labeling them ourselves."
  },
  {
    "index": "F18862",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば、我らの機械学習のアプリケーションは現在、1000個の手本があるとする、つまりm=1000。",
    "output": "So, for example, that, for our machine learning application, currently we have 1,000 examples, so M 1,000."
  },
  {
    "index": "F18863",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そこで我らがやるべき事は、椅子に座って、こう問うてみる事だ:一つのラベルつき手本を集めるのには、実際どれだけの時間がかかるだろう?",
    "output": "That what we do is sit down and ask, how long does it take me really to collect and label one example."
  },
  {
    "index": "F18864",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "と。例えばそれは、10秒かかるとする、新しいラベル付き手本1つを得るのに。",
    "output": "And sometimes maybe it will take you, you know ten seconds to label one new example, and so if I want 10 X as many examples, I'd do a calculation."
  },
  {
    "index": "F18865",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、10倍の数の手本を得たいと思えば、計算を行ってみると、もし一つの手本に10秒かかり、10倍の数の手本を得たいと思えば、その場合は1万個の手本が必要となる。",
    "output": "If it takes me 10 seconds to get one training example."
  },
  {
    "index": "F18866",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "人力で1万個のラベルつき手本を集めたらどれだけの時間がかかるか?を。",
    "output": "If I wanted to get 10 times as much data, then I need 10,000 examples."
  },
  {
    "index": "F18867",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "1手本につき10秒かかるとすると、その場合にこの計算を行うと、割とよく、あなたがたは驚くことになる、いかにちょっとの仕事で済むのか、時にはほんの2、3日の仕事で、またある時にはほんの数日で済む事を知る事で。",
    "output": "So I do the calculation, how long is it gonna take to label, to manually label 10,000 examples, if it takes me 10 seconds to label 1 example."
  },
  {
    "index": "F18868",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてそれをあなたが成し遂げたら、あなたはどんな製品を開発していようと、どんなチームで働いていようと、きっとヒーローになる。",
    "output": "So when you do this calculation, often I've seen many you would be surprised, you know, how little, or sometimes a few days at work, sometimes a small number of days of work, well I've seen many teams be very surprised that sometimes how little work it could be, to just get a lot more data, and let that be a way to give your learning app to give you a huge boost in performance, and necessarily, you know, sometimes when you've just managed to do this, you will be a hero and whatever product development, whatever team you're working on, because this can be a great way to get much better performance."
  },
  {
    "index": "F18869",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "三番目は、これが最後だが、データをたくさん集めるのに時には良い方法たりえる物として、クラウドソーシングと呼ばれる物がある。",
    "output": "Third and finally, one sometimes good way to get a lot of data is to use what's now called crowd sourcing."
  },
  {
    "index": "F18870",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんにちでは、幾つかのサービスであなたがかなり安い価格であたなの為にラベル付きトレーニングセットをたくさん集めてくれる人を雇わせてくれるサービスが存在している。",
    "output": "So today, there are a few websites or a few services that allow you to hire people on the web to, you know, fairly inexpensively label large training sets for you."
  },
  {
    "index": "F18871",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "クラウドソーシング、あるいはデータのラベル付のクラウドソースは、それはそれでいろいろと問題がある事もあるが、たとえばラベルが信頼できるか、とか。",
    "output": "So this idea of crowd sourcing, or crowd sourced data labeling, is something that has, is obviously, like an entire academic literature, has some of it's own complications and so on, pertaining to labeler reliability."
  },
  {
    "index": "F18872",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "世界中の何百、何千ものラベル付けを手伝ってくれる人が比較的安くあなたのデータのラベルづけを手伝ってくれる訳なのだから。私が既に言及したように、そういう選択肢もまたありうる。",
    "output": "Maybe, you know, hundreds of thousands of labelers, around the world, working fairly inexpensively to help label data for you, and that I've just had mentioned, there's this one alternative as well."
  },
  {
    "index": "F18873",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてたぶん、AmazonのMechanicalTurkシステムが現時点ではもっとも人気のあるクラウドソースの選択肢だ。",
    "output": "And probably Amazon Mechanical Turk systems is probably the most popular crowd sourcing option right now."
  },
  {
    "index": "F18874",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはしばしば、ちゃんと機能させる為にはかなりの作業を必要とする。もし高いクオリティのラベルを得たいと思えば。",
    "output": "This is often quite a bit of work to get to work, if you want to get very high quality labels, but is sometimes an option worth considering as well."
  },
  {
    "index": "F18875",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしラベル付けをあなたの為に比較的安価に行ってくれるような大量の人を、web上で探したい時には。",
    "output": "If you want to try to hire many people, fairly inexpensively on the web, our labels launch miles of data for you."
  },
  {
    "index": "F18876",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "その中でもスクラッチからデータ全体を作る方法:この例としては適当にフォントを持ってくる例を見た。それと、既に存在しているトレーニングセットを増幅する、という方法:既存のラベル付きトレーニング手本を持ってきてそこに歪みを導入し、そこから新しい手本を生成する、という方法を見た。",
    "output": "So this video, we talked about the idea of artificial data synthesis of either creating new data from scratch, looking, using the ramming funds as an example, or by amplifying an existing training set, by taking existing label examples and introducing distortions to it, to sort of create extra label examples."
  },
  {
    "index": "F18877",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、このビデオからあなたに覚えておいて欲しい事としては、もしあなたが機械学習の問題に直面していたら、二つの事はしばしば試してみる価値がある。",
    "output": "And finally, one thing that I hope you remember from this video this idea of if you are facing a machine learning problem, it is often worth doing two things."
  },
  {
    "index": "F18878",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一つは単純なサニティチェックを学習曲線で行い、もっと多くのデータが役に立つかを確認する事。",
    "output": "One just a sanity check, with learning curves, that having more data would help."
  },
  {
    "index": "F18879",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二つ目は、もっと多くのデータが役に立つ場合には、椅子に座って自分自身に真剣にこう問うてみる:現在持っているデータの10倍のデータを得るには、どれだけ時間がかかるか、を。そしていつもでは無いにしても、ときには、それがいかに簡単かが判明して驚く事になる。",
    "output": "And second, assuming that that's the case, I will often seat down and ask yourself seriously: what would it take to get ten times as much creative data as you currently have, and not always, but sometimes, you may be surprised by how easy that turns out to be, maybe a few days, a few weeks at work, and that can be a great way to give your learning algorithm a huge boost in performance"
  },
  {
    "index": "F18880",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以前のビデオで、私は以下の事を繰り返し言ってきた:機械学習システムを開発する時にもっとも貴重なリソースの一つはデベロッパとしてのあなたの時間だ、とーー次に作業すべき事を選ぶ時には。",
    "output": "In earlier videos, I've said over and over that, when you're developing a machine learning system, one of the most valuable resources is your time as the developer, in terms of picking what to work on next."
  },
  {
    "index": "F18881",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あるいは、あなたはデベロッパのチームなりエンジニアのチーム一丸となって機械学習のシステムを開発する時もまた、もっとも貴重なリソースの一つはそのシステムを開発しているエンジニアとかデベロッパの時間だ。",
    "output": "Again, one of the most valuable resources is the time of the engineers or the developers working on the system."
  },
  {
    "index": "F18882",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして本当に避けたい事として、あなたなり、あなたの同僚なりあなたの友人なりが、あるコンポーネントに対してたくさんの作業をした後で何週間とか何ヶ月とか時間を費やした後ではじめてそれらの作業全てが最終的なシステムのパフォーマンスには大した違いを生まない、と気づく、という事だ。",
    "output": "And what you really want to avoid is that you or your colleagues your friends spend a lot of time working on some component. Only to realize after weeks or months of time spent, that all that worked just doesn't make a huge difference on the performance of the final system."
  },
  {
    "index": "F18883",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このビデオでは、シーリング(天井)分析と呼ばれる物について議論したい。",
    "output": "In this video what I'd like to do is something called ceiling analysis."
  },
  {
    "index": "F18884",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなた、あるいはあなたのチームが機械学習パイプラインのシステムの仕事をしている時には、この手法はパイプラインのどの部分を改善するのがもっとも良いのかについての強力なシグナル、あるいはガイダンスを提供してくれる事がある。",
    "output": "When you're the team working on the pipeline machine on your system, this can sometimes give you a very strong signal, a very strong guidance on what parts of the pipeline might be the best use of your time to work on."
  },
  {
    "index": "F18885",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シーリング分析の議論を行う為にここでもPhotoOCRの例を引き続き採用していく。",
    "output": "To talk about ceiling analysis I'm going to keep on using the example of the photo OCR pipeline."
  },
  {
    "index": "F18886",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "前にも言った通りこれらの箱、テキスト検出、文字分割、そして文字認識、、、これらの各箱はそれぞれ小規模かチームが担当する場合もあるし、システム全体をあなた一人だけで構築することもあるだろう。",
    "output": "And see right here each of these boxes, text detection, character segmentation, character recognition, each of these boxes can have even a small engineering team working on it. Or maybe the entire system is just built by you, either way."
  },
  {
    "index": "F18887",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "どちらにせよ、問題は、どこにリソースを割くべきか、という事だ。",
    "output": "But the question is where should you allocate resources?"
  },
  {
    "index": "F18888",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらの箱のうちどれがパフォーマンスを改善しよう、と労力を払うのにもっとも価値がある物だろうか?",
    "output": "Which of these boxes is most worth your effort of trying to improve the performance of."
  },
  {
    "index": "F18889",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シーリング分析のアイデアを説明する為に、このPhotoOCRパイプラインの例を使い続けていく。",
    "output": "In order to explain the idea of ceiling analysis, I'm going to keep using the example of our photo OCR pipeline."
  },
  {
    "index": "F18890",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "以前述べたように、これらの各箱は、これらの各機械学習のコンポーネントはエンジニアの小さなチームでそれぞれ従事しても良いし、またはシステム全体を一人の人間が見ても良い。",
    "output": "As I mentioned earlier, each of these boxes here, each of these machines and components could be the work of a small team of engineers, or the whole system could be built by just one person."
  },
  {
    "index": "F18891",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが何にせよ問題は、いったいどこに貴重なリソースを割り振るべきだろうか?",
    "output": "But the question is, where should you allocate scarce resources?"
  },
  {
    "index": "F18892",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この場合だと、これらのコンポーネントの一つ目か、二つ目か、または三つ目に時間を使うのがパフォーマンスを改善するのにもっとも有益か?",
    "output": "That is, which of these components, which one or two or maybe all three of these components is most worth your time, to try to improve the performance of."
  },
  {
    "index": "F18893",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シーリング分析とはこんな物だ。",
    "output": "So here's the idea of ceiling analysis."
  },
  {
    "index": "F18894",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これ以外の機械学習の問題と同様、機械学習の開発の過程において、システムを開発する時の様々な決断を行う為には、学習システムに関する単一で実数の評価指標があるととても役に立つ。",
    "output": "As in the development process for other machine learning systems as well, in order to make decisions on what to do for developing the system is going to be very helpful to have a single rolled number evaluation metric for this learning system."
  },
  {
    "index": "F18895",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "例えば文字レベルでの正確さを選んだとしよう。",
    "output": "So let's say we pick character level accuracy."
  },
  {
    "index": "F18896",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、、、あるテストセットの画像が与えられた時に、テストセットの画像内にある文字をどれだけの割合で正しく認識出来たのか、の割合。",
    "output": "So if you're given a test set image, what is the fraction of alphabets or characters in a test image that we recognize correctly?"
  },
  {
    "index": "F18897",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "別にこれ以外の単一の実数の評価指標を選んでも良い。お望みならね。",
    "output": "Or you can pick some other single road number evaluation that you could, if you want."
  },
  {
    "index": "F18898",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがどんな評価指標を使うにせよ、とにかくシステム全体として、現在の所72%の正確さ(accuracy)だった、と分かったとしよう。",
    "output": "But let's say for whatever evaluation measure we pick, we find that the overall system currently has 72% accuracy."
  },
  {
    "index": "F18899",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、我らはあるテストセットの画像を持っていて、それらテストセットの各画像にテキスト検出、文字分割、文字認識を、順番に走らせてそして我らのテストセットに対してはシステム全体に対して、あなたの選んだ指標に関して72%の正確さだと分かった、とする。",
    "output": "And from each test set images, we run it through text detection, then character segmentation, then character recognition. And we find that on our test set the overall accuracy of the entire system was 72% on whatever metric you chose."
  },
  {
    "index": "F18900",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、以下にシーリング分析のアイデアの背景を述べる。それは、我らは最初のモジュールを見てこの機械学習パイプラインの最初のモジュールはテキスト検出だ。",
    "output": "Now here's the idea behind ceiling analysis, which is that we're going to go through, let's say the first module of our machinery pipeline, say text detection."
  },
  {
    "index": "F18901",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして我らがやる事は、テストセットに細工をする事だ。",
    "output": "And what we're going to do, is we're going to monkey around with the test set."
  },
  {
    "index": "F18902",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "テストセットに直接おもむき、各テスト手本に対し正解のテキスト検出の出力を直接提供する。",
    "output": "For every test example, which is going to provide it the correct text detection outputs, so in other words, we're going to go to the test set and just manually tell the algorithm where the text is in each of the test examples."
  },
  {
    "index": "F18903",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "言い換えると、テストセットにおもむき、単に手動でアルゴリズムに各テスト手本のどこにテキストがあるかを伝える。",
    "output": "So in other words gonna simulate what happens if you have a text detection system with a hundred percent accuracy, for the purpose of detecting text in an image."
  },
  {
    "index": "F18904",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さらに言い換えると、我らがもし100%正確なテキスト検出のシステムがあったら何が起こるのかをシミュレートする訳だ。",
    "output": "And really the way you do that's pretty simple, right? Instead of letting your learning algorhtim detect the text in the images."
  },
  {
    "index": "F18905",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "画像のテキスト検出の目的の為に。そしてそれを実際に行う方法はとてもシンプルだ。",
    "output": "You wouldn't say go to the images and just manually label what is the location of the text in my test set image."
  },
  {
    "index": "F18906",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "学習アルゴリズムに画像のテキストを検出さえる代わりに、あなたが直接画像に赴き人力でテストセットの画像のどこにテキストがあるかをラベル付けする。",
    "output": "And you would then let these correct or let these ground truth labels of where is the text be part of your test set."
  },
  {
    "index": "F18907",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこれらを正解にした上で、つまりテストセットの画像の中のどこにテキストがあるかの完璧に正しいラベルを付けた上で、これらの完璧に正しいラベルを用いて次のステージのパイプラインに食わせる、つまり文字分割のパイプラインに。",
    "output": "And just use these ground truth labels as what you feed in to the next stage of the pipeline, so the character segmentation pipeline."
  },
  {
    "index": "F18908",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もう一度言おう。",
    "output": "Okay? So just to say that again."
  },
  {
    "index": "F18909",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "このチェックマークは私がテストセットに実際におもむき、単純に正解の答えを与える、正しいラベルを与える、という事を意味している。",
    "output": "By putting a checkmark over here, what I mean is I'm going to go to my test set and just give it the correct answers."
  },
  {
    "index": "F18910",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "パイプラインのテキスト検出の部分を。",
    "output": "Give it the correct labels for the text detection part of the pipeline."
  },
  {
    "index": "F18911",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そうする事で、まるでテストセットに対して完璧なテキスト検出を持っているフリをする為に。",
    "output": "So that as if I have a perfect test detection system on my test set."
  },
  {
    "index": "F18912",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次にやることはこのデータを残りのパイプライン、つまり文字分割と文字認識に流す。",
    "output": "What we need to do then is run this data through the rest of the pipeline. Through character segmentation and character recognition."
  },
  {
    "index": "F18913",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、以前と同様の評価指標を用いて、システム全体の正確さを計測する。",
    "output": "And then use the same evaluation metric as before, to measure what was the overall accuracy of the entire system."
  },
  {
    "index": "F18914",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして完璧なテキスト検出を用いるのだから、たぶんパフォーマンスは向上する事が期待される。",
    "output": "And with perfect text detection, hopefully the performance will go up."
  },
  {
    "index": "F18915",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ここでは89%に向上したとしよう。",
    "output": "And in this example, it goes up by by 89%."
  },
  {
    "index": "F18916",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、そのまま続けて、次のパイプラインのセクションに進み、文字分割に対して、またテストセットの画像におもむき、いまや正確なテキスト検出の出力が与えられている所に今度は正確な文字分割の出力を与える。",
    "output": "And then we're gonna keep going, let's got o the next stage of the pipeline, so character segmentation. So again, I'm gonna go to my test set, and now I'm going to give it the correct text detection output and give it the correct character segmentation output."
  },
  {
    "index": "F18917",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり手動で正確な文字の分割のラベル付けを行い個々の文字に分割する。そしてそれがどれだけ改善するかを見てみるのだ。",
    "output": "So go to the test set and manually label the correct segmentations of the text into individual characters, and see how much that helps."
  },
  {
    "index": "F18918",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして例えば、全体のシステムが90%の正確さに改善したとしよう。",
    "output": "And let's say it goes up to 90% accuracy for the overall system."
  },
  {
    "index": "F18919",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり正確さといったらいつでもシステム全体の正確さだ。",
    "output": "So as always the accuracy of the overall system."
  },
  {
    "index": "F18920",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり文字認識システムの最終的なアウトプットが何かだ。",
    "output": "So is whatever the final output of the character recognition system is."
  },
  {
    "index": "F18921",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりパイプライン全体のアウトプットが何かだ。それが正確さの指標となる。",
    "output": "Whatever the final output of the overall pipeline, is going to measure the accuracy of that."
  },
  {
    "index": "F18922",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、文字認識システムにも正しいラベルを与える。もしそれをやったら、当たり前だが、100%の正確さを得られる。",
    "output": "And finally I'm going to build a character recognition system and give that correct labels as well, and if I do that too then no surprise I should get 100% accuracy."
  },
  {
    "index": "F18923",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今、この分析を行う良い点としては、我らはこれらの各コンポーネントを改善する時のポテンシャルの上限が理解出来る、という事だ。",
    "output": "Now the nice thing about having done this analysis is, we can now understand what is the upside potential of improving each of these components?"
  },
  {
    "index": "F18924",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりもし、我らが完璧なテキスト検出を得たら、我らのパフォーマンスは72%から89%へと上昇する事が分かる。",
    "output": "So we see that if we get perfect text detection, our performance went up from 72 to 89%."
  },
  {
    "index": "F18925",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり17%のパフォーマンス向上が得られる。",
    "output": "So that's a 17% performance gain."
  },
  {
    "index": "F18926",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、あなたが現在のシステムに対してテキスト検出の改善にたくさんの時間を費やしたら、その場合は我らのシステムのパフォーマンスを17%向上出来る可能性がある、という事を意味している。",
    "output": "So this means that if we take our current system we spend a lot of time improving text detection, that means that we could potentially improve our system's performance by 17%."
  },
  {
    "index": "F18927",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはやる価値がありそうに見える。",
    "output": "It seems like it's well worth our while."
  },
  {
    "index": "F18928",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一方、対照的に、テキスト検出から離れて、完璧な文字分割を与えても、パフォーマンスはたったの1%しか向上しない。これはより信頼出来るメッセージだ。",
    "output": "Whereas in contrast, when going from text detection when we gave it perfect character segmentation, performance went up only by 1%, so that's a more sobering message."
  },
  {
    "index": "F18929",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これの意味する所は、どれだけ文字分割に時間を費やそうとも、潜在的な上限は、とても小さい、という事だ。",
    "output": "It means that no matter how much time you spend on character segmentation."
  },
  {
    "index": "F18930",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だからきっと、あなたは大きなエンジニアのチームをこの文字分割の仕事に従事させたい、とは思わないだろう。",
    "output": "Maybe the upside potential is going to be pretty small, and maybe you do not want to have a large team of engineers working on character segmentation."
  },
  {
    "index": "F18931",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この種の分析は、仮に完璧な文字分割を与えられたとしても、パフォーマンスは1%しか向上しない、という事を示してくれる。",
    "output": "This sort of analysis shows that even when you give it the perfect character segmentation, you performance goes up by only one percent."
  },
  {
    "index": "F18932",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何が天井(シーリング)なのか、何が上限なのか。これらのうちの一つのコンポーネントに対し作業を行った時に、どれだけシステムのパフォーマンスを改善出来るか?",
    "output": "That really estimates what is the ceiling, or what is an upper bound on how much you can improve the performance of your system and working on one of these components."
  },
  {
    "index": "F18933",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後に、文字まで行って、より良い文字認識を得たら、パフォーマンスはさらに10%上がる。",
    "output": "And finally, going from character, when we get better character recognition with the forms went up by ten percent."
  },
  {
    "index": "F18934",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ここでも10%の改善に、どれだけ時間を使うか、を決める事が出来る。",
    "output": "So again you can decide is ten percent improvement, how much is worth your while?"
  },
  {
    "index": "F18935",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはパイプラインの終着駅に、もっと労力を集中すべき、と言っているかもしれない。システム全体のパフォーマンスも改善出来る。",
    "output": "This tells you that maybe with more effort spent on the last stage of the pipeline, you can improve the performance of the systems as well."
  },
  {
    "index": "F18936",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これについての別の考え方としては、この種の分析を行っていく事で、これらの各コンポーネントを改善した時の上限のポテンシャルを調べている、と考えても良い。",
    "output": "Another way of thinking about this, is that by going through these sort of analysis you're trying to think about what is the upside potential of improving each of these components."
  },
  {
    "index": "F18937",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あるいは、これらのコンポーネントが一つ究極的に完璧になったら、どれだけの物が得られるか、を調べていると考えても良い。",
    "output": "Or how much could you possibly gain if one of these components became absolutely perfect?"
  },
  {
    "index": "F18938",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これでシステムのパフォーマンスの上限が設定出来る。",
    "output": "And this really places an upper bound on the performance of that system."
  },
  {
    "index": "F18939",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シーリング分析のアイデアはとても重要だ。このアイデアを再び、もっと複雑な別の例で例示しよう。",
    "output": "So the idea of ceiling analysis is pretty important, let me just answer this idea again but with a different example but more complex one."
  },
  {
    "index": "F18940",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたは画像の顔認識をしたいとしよう。",
    "output": "Let's say that you want to do face recognition from images."
  },
  {
    "index": "F18941",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり写真を見て、この写真の人があなたの特定の友人かどうかを認識したい。この画像にいる人物を認識したいとする。",
    "output": "You want to look at the picture and recognize whether or not the person in this picture is a particular friend of yours, and try to recognize the person Shown in this image."
  },
  {
    "index": "F18942",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはちょっと人工的な例で、実際の現場で顔認識がどう行われているか、というのを正確に反映した物では無い。",
    "output": "This is a slightly artificial example, this isn't actually how face recognition is done in practice."
  },
  {
    "index": "F18943",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがシーリング分析のプロセスがどんな感じになるか、という例をもう一つ挙げる為にこのパイプラインがどうなっているのかを見ていきたい。",
    "output": "But we're going to set for an example, what a pipeline might look like to give you another example of how a ceiling analysis process might look."
  },
  {
    "index": "F18944",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "さて、我らはカメラの画像を持っていて、以下のようなパイプラインをデザインするとしよう。最初にやるのは画像の前処理としよう。",
    "output": "So we have a camera image, and let's say that we design a pipeline as follows, the first thing you wanna do is pre-processing of the image."
  },
  {
    "index": "F18945",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この右上に見せたような画像を取ってきて、そして背景を除去したい、としてみよう。",
    "output": "So let's take this image like we have shown on the upper right, and let's say we want to remove the background."
  },
  {
    "index": "F18946",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり前処理を通す事で背景が消える。",
    "output": "So do pre-processing and the background disappears."
  },
  {
    "index": "F18947",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に人の顔を認識したい、とする。これは通常学習アルゴリズムを用いて行われる。",
    "output": "Next we want to say detect the face of the person, that's usually done on the learning So we'll run a sliding Windows crossfire to draw a box around a person's face."
  },
  {
    "index": "F18948",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "一旦顔を認識出来たら人を識別する為には、目というのはとても有力な手がかりだと分かってる。",
    "output": "Having detected the face, it turns out that if you want to recognize people, it turns out that the eyes is a highly useful cue."
  },
  {
    "index": "F18949",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "我らは実際の所、友人を認識する時に、目がどんな見た目かというのは、実際にもっとも重視している手がかりだ。",
    "output": "We actually are, in terms of recognizing your friends the appearance of their eyes is actually one of the most important cues that you use."
  },
  {
    "index": "F18950",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だから人間の目を検出する為の別の分類器を走らせよう。",
    "output": "So lets run another crossfire to detect the eyes of the person."
  },
  {
    "index": "F18951",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "目の部分を切りだして、というのは人物を認識するのにこれは有用なフィーチャー(特徴)だからだが、そして次に、顔の他の部分で使えそうな所、例えば鼻を切り出す。",
    "output": "So the segment of the eyes and then since this will give us useful features to recognize the person."
  },
  {
    "index": "F18952",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして口を切り出す。",
    "output": "Maybe segment of the nose, segment of the mouth."
  },
  {
    "index": "F18953",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして次に、目、鼻、口を見つけた後に、これら全てが恐らく、ロジスティック回帰などの分類器に食わせるのに有用なフィーチャーとなる。",
    "output": "And then having found the eyes, the nose, and the mouth, all of these give us useful features to maybe feed into a logistic regression classifier."
  },
  {
    "index": "F18954",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこの分類器の仕事は、我らに全体として、この人物が誰だと思っているかのラベルを与える。",
    "output": "And there's a job with a cost priority, they'd give us the overall label, to find the label for who we think is the identity of this person."
  },
  {
    "index": "F18955",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もし実際に人を認識したいなら。だがシーリング分析を考えてみる上では示唆に富む例だ。",
    "output": "So this is a kind of complicated pipeline, it's actually probably more complicated than you should be using if you actually want to recognize people, but there's an illustrative example that's useful to think about for ceiling analysis."
  },
  {
    "index": "F18956",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ではどうやってこのパイプラインのシーリング分析を行えば良いだろう?",
    "output": "So how do you go through ceiling analysis for this pipeline."
  },
  {
    "index": "F18957",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これらのピースを一度に一つづつ見ていこう。",
    "output": "Well se step through these pieces one at a time."
  },
  {
    "index": "F18958",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "あなたのシステムが全体として85%の正確さを持つとしよう。",
    "output": "Let's say your overall system has 85% accuracy."
  },
  {
    "index": "F18959",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "最初にやる事は、テストセットにおもむき、そして手動で前景と背景の分割を行う、という事。",
    "output": "The first thing I do is go to my test set and manually give it the full background segmentation."
  },
  {
    "index": "F18960",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりテストセットにおもむきPhotoshopなりなんなりを用いてどれが背景かを伝えて、そして手動で背景を除去する。",
    "output": "So manually go to the test set."
  },
  {
    "index": "F18961",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり完全に正しい背景で、どれだけ正確さが向上するかを見る。",
    "output": "And use Photoshop or something to just tell it where's the background and just manually remove the graph background, so this is a ground true background, and see how much the accuracy changes."
  },
  {
    "index": "F18962",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "この例では、正確さは0.1%向上した。",
    "output": "In this example the accuracy goes up by 0.1%."
  },
  {
    "index": "F18963",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまりこれは、強烈なサインとなる。たとえ完璧な背景分離が、本当に完璧な背景除去であったとしても、あなたのシステムのパフォーマンスは大して向上しない。",
    "output": "So this is a strong sign that even if you have perfect background segmentation, the form is, even with perfect background removal the performance or your system isn't going to go up that much."
  },
  {
    "index": "F18964",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはつまり、前処理の背景除去にはこれ以上莫大な労力を投入する価値は無い、という事だろう。",
    "output": "So it's maybe not worth a huge effort to work on pre-processing on background removal."
  },
  {
    "index": "F18965",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "次に、またテストセットにおもむき、正確な顔検出の領域を与えて、また次に目、鼻、口の分割と順番に見ていく。順番を一つ選ぶ。",
    "output": "Then quickly goes to test set give it the correct face detection images then again step though the eyes nose and mouth segmentation in some order just pick one order."
  },
  {
    "index": "F18966",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "目の正確な位置を与えて、鼻の正確な位置を与えて、口の正確な位置を与えて、そして最後に全体として正解のラベルを与えると、100%正確となる。",
    "output": "Just give the correct location of the eyes."
  },
  {
    "index": "F18967",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、システムを順番に見ていってどんどん各コンポーネントにテストセットの正解のラベルを与えていくと、パフォーマンス、、、つまりシステム全体のパフォーマンスは向上する。",
    "output": "Correct location in noses, correct location in mouth, and then finally if I just give it the correct overall label I can get 100% accuracy."
  },
  {
    "index": "F18968",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして各別々のステップでどれだけパフォーマンスが向上するかを見ていく事が出来る。",
    "output": "And so as I go through the system and just give more and more components, the correct labels in the test set, the performance of the overall system goes up and you can look at how much the performance went up on different steps."
  },
  {
    "index": "F18969",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "完璧な顔検出が提供されると、システム全体のパフォーマンスは5.9%向上する。",
    "output": "So from giving it the perfect face detection, it looks like the overall performance of the system went up by 5.9%."
  },
  {
    "index": "F18970",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "これはかなり大きなジャンプだ。",
    "output": "So that's a pretty big jump."
  },
  {
    "index": "F18971",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、顔検出を改善するのに、かなりの労力を費やす価値がありそうだ。",
    "output": "It means that maybe it's worth quite a bit effort on better face detection."
  },
  {
    "index": "F18972",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、どうやら最も我らが頑張るに値するコンポーネントは、完璧な顔検出を提供出来たらシステムは5.9%向上し、完璧な目の分割を行えたら4%向上する。",
    "output": "1% there, and 3% there. So it looks like the components that most work are while are, when I gave it perfect face detection system went up by 5.9 performance when given perfect eyes segmentation went to four percent."
  },
  {
    "index": "F18973",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして最後のロジスティック分類器は、またさらに3%のギャップがある。つまりこうして、我らが取り組む価値がもっともありそうなコンポーネントを教えてくれる。",
    "output": "And then my final which is cost for well there's another three percent, gap there maybe."
  },
  {
    "index": "F18974",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "ところで、これは本当にあった話なのだが、このプリプロセスの背景除去をここに含めた理由は、現実にこんな話があったのを知っているからだ。",
    "output": "And so this tells maybe whether the components are most worthwhile working on."
  },
  {
    "index": "F18975",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "分かるだろ?だがとにかく、コンピュータビジョンのアプリケーションがあってさ。",
    "output": "And by the way I want to tell you a true cautionary story."
  },
  {
    "index": "F18976",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "二人のエンジニアのチームが文字通り1.5年間、背景除去の仕事に従事したのだ。実際に彼らはとても複雑なアルゴリズムを用いて最終的には1本の研究論文まで出した。",
    "output": "The reason I put this is in this in preprocessing background removal is because I actually know of a true story where there was a research team that actually literally had to people spend about a year and a half, spend 18 months working on better background removal."
  },
  {
    "index": "F18977",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だがそれらの仕事を終えた後になって彼らは気づいたのだ。彼らの実際のアプリケーションに関しては、その違いは全体のパフォーマンスについては大した違いを生まない、という事を。",
    "output": "But actually I'm obscuring the details for obvious reasons, but there was a computer vision application where there's a team of two engineers that literally spent about a year and a half working on better background removal, actually worked out really complicated algorithms and ended up publishing one research paper."
  },
  {
    "index": "F18978",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてお分かりの通り、もし誰かが、もし仮にシーリング分析を前もって行っていたら、この事が分かったと思われる。",
    "output": "But after all that work they found that it just did not make huge difference to the overall performance of the actual application they were working on and if only someone were to do ceiling analysis before hand maybe they could have realized."
  },
  {
    "index": "F18979",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして彼らの一人が後になってこう言った。",
    "output": "And one of them said to me afterward."
  },
  {
    "index": "F18980",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もしこの種の分析をやっていたら、たぶん彼らは18ヶ月の仕事をする前に、彼らが文字通り18ヶ月の仕事を背景除去に従事する前に彼らの労力を別のコンポーネントに注ぐべきだと気づけたんじゃないかな。",
    "output": "If only you've did this sort of analysis like this maybe they could have realized before their 18 months of work."
  },
  {
    "index": "F18981",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "まとめよう。パイプラインは複雑な機械学習のアプリケーションにおいては広く使われている物だ。",
    "output": "So to summarize, pipelines are pretty pervasive in complex machine learning applications."
  },
  {
    "index": "F18982",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして大きな機械学習のアプリケーションに従事している時は、あなたのデベロッパーとしての時間はあまりにも貴重だ。だからあなたは、あまり重要では無い事に多くの時間を費やしてはならない。",
    "output": "And when you're working on a big machine learning application, your time as developer is so valuable, so just don't waste your time working on something that ultimately isn't going to matter."
  },
  {
    "index": "F18983",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてこのビデオで、我らはシーリング分析のアイデアを議論してきた。この手法は作業すべきコンポーネントを見つけ出すのに、とても便利なツールだと、しばしば思う物だ。",
    "output": "And in this video we'll talk about this idea of ceiling analysis, which I've often found to be a very good tool for identifying the component of a video as you put focus on that component and make a big difference."
  },
  {
    "index": "F18984",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そしてあなたがそのコンポーネントに労力を集中して、大きく改善出来たら、最終的なシステム全体にも巨大な影響を与えるだろう。",
    "output": "Will actually have a huge effect on the overall performance of your final system."
  },
  {
    "index": "F18985",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "何年も機械学習の仕事をしてきて、私は本当にどのコンポーネントに取り組むべきかの自分自身の直感を、あまり信じてはいけない、という事を学んできた。",
    "output": "So over the years working machine learning, I've actually learned to not trust my own gut feeling about what components to work on."
  },
  {
    "index": "F18986",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、ほんとうに良く、長い期間機械学習の仕事をしていると、機械学習の問題を見た時に、直感がこう言う事がある:このコンポーネントにもっと時間を注ぎ込んだらいいんじゃないか、と。",
    "output": "So very often, I've work on machine learning for a long time, but often I look at a machine learning problem, and I may have some gut feeling about oh, let's jump on that component and just spend all the time on that."
  },
  {
    "index": "F18987",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "だが何年もかかって、自分自身の直感すら信じてはいけない、むしろ自分の直感をそれほどは信じない、という事を学習した。",
    "output": "But over the years, I've come to even trust my own gut feelings and learn not to trust gut feelings that much."
  },
  {
    "index": "F18988",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "シーリング分析などはしばしば、どこに労力を集中すべきかを決めるもっと良い、そしてもっと信頼出来る方法だ。それによってどこのコンポーネントのパフォーマンスを実際に改善すべきかが分かる。",
    "output": "And instead, if you have a sort of machine learning problem where it's possible to structure things and do a ceiling analysis, often there's a much better and much more reliable way for deciding where to put a focused effort, to really improve the performance of some component."
  },
  {
    "index": "F18989",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして実際にそれを行えば、全体的なシステムの最終的なパフォーマンスが実際に大きく改善する事を確認出来るだろう。",
    "output": "And be kind of reassured that, when you do that, it won't actually have a huge effect on the final performance of the overall system."
  },
  {
    "index": "F18990",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんにちは、競争戦略上級クラスへようこそ。",
    "output": "Hello, and welcome back to the second module of advanced competitive strategy."
  },
  {
    "index": "F18991",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "私は、ミュンヘンのルドウィグ-マキシミリアン大学の戦略、テクノロジー、組織の教授をしているトビアス・クレチメルです。私たちはあたらしいコースという冒険を始めようとしています。",
    "output": "We're on board the Ludwig Fessler a side-wheeler motor vessel on Lake Chiemsee, and we're going to talk about price discrimination."
  },
  {
    "index": "F18992",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "それぞれの異なる7つのモジュールは約10のビデオからなっており、それぞれに、一つか二つのクイズがついています。そこで皆さんが新しく学んだ知識をすぐにテストすることができます。",
    "output": "So, if you want to do a boat trip with them, you will have to pay a different price depending on whether you're an adult, whether you're a child from 16 to 15 years of age, part of a family consisting of two adults and two children, or whether you're travelling in a group of at least 20 people."
  },
  {
    "index": "F18993",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "もちろん、これらは楽しくなくてはいけません。それぞれのモジュールの最後に長い小テストがあります。",
    "output": "At the same time, you also pay different prices, that depend on the services that you require."
  },
  {
    "index": "F18994",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "つまり、皆さんが7つのすべてのモジュールを終えると自動的に最終試験を受けているのです。",
    "output": "By getting a combined ticket for both, the boat trip and the steam engine, you essentially pay less than if you purchase the two tickets individually. So that sounds pretty complicated right?"
  },
  {
    "index": "F18995",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "そして、私たちのフォーラムを確認してください。なぜなら、クラスメイトとつながって、スタディグループなどを作る素晴らしい機会だからです。",
    "output": "Watch the following videos, if you want to find out more about these strategic advantages and benefits, that you can achieve by implementing a suitable price discrimination strategy."
  },
  {
    "index": "F18996",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "こんにちはキース・ダブリンですようこそ数学的思考に関するこのオンラインコースへ。",
    "output": "Hello, I'm Keith Devlin. Welcome to this online course on mathematical thinking."
  },
  {
    "index": "F18997",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "3,000年以上にわたって発展してきた強力な考え方、かつ貴重な精神能力を育成していきましょう。",
    "output": "The goal of the course is to help you develop a valuable mental ability, a powerful way of thinking that people have developed over 3,000 years."
  },
  {
    "index": "F18998",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "今日は、学習の準備についてと、授業の流れについての、2つのことをお話しします。",
    "output": "What I want to do today is get you ready for the course and tell you a little bit about the way the course will work."
  },
  {
    "index": "F18999",
    "instruction": "次の日本語を英語に翻訳してください。",
    "input": "なぜなら、このコースの数学に関する見解は、大抵の人のそれと、大きく異なるからです。",
    "output": "I'm doing this because for most of you, this will be a very different perspective on what mathematics is."
  }
]