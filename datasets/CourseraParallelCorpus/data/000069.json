[
  {
    "index": "F15000",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There would be terms like x1 squared, x1x2, x1x3, you know, x1x4 up to x1x100 and then you have x2 squared, x2x3 and so on.",
    "output": "そして、x2の二乗、x2x3、などと続いていく。"
  },
  {
    "index": "F15001",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you include just the second order terms, that is, the terms that are a product of, you know, two of these terms, x1 times x1 and so on, then, for the case of n equals 100, you end up with about five thousand features.",
    "output": "そして二次の項だけを含めたとしても、つまり、これらの2つの積の項、つまりx1掛けるx1などで、そしてnが100の場合だと、結局は約5000程のフィーチャーとなる。"
  },
  {
    "index": "F15002",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, asymptotically, the number of quadratic features grows roughly as order n squared, where n is the number of the original features, like x1 through x100 that we had.",
    "output": "そして漸近的に、二次のフィーチャーの数はオーダーnの二乗で成長する。ここでnはもとのフィーチャーの数。"
  },
  {
    "index": "F15003",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And its actually closer to n squared over two.",
    "output": "たとえばx1からx100まであった訳だが、その場合は実際nの二乗割る2に近かった。"
  },
  {
    "index": "F15004",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So including all the quadratic features doesn't seem like it's maybe a good idea, because that is a lot of features and you might up overfitting the training set, and it can also be computationally expensive, you know, to be working with that many features.",
    "output": "だから、すべての2次式フィーチャーを含めるのはよいアイデアではなさそうだ。"
  },
  {
    "index": "F15005",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One thing you could do is include only a subset of these, so if you include only the features x1 squared, x2 squared, x3 squared, up to maybe x100 squared, then the number of features is much smaller.",
    "output": "また、そのようなたくさんのフィーチャーを扱うと、計算コストが高くなりうる。"
  },
  {
    "index": "F15006",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here you have only 100 such quadratic features, but this is not enough features and certainly won't let you fit the data set like that on the upper left.",
    "output": "一つできることはこれらのサブセットのみを入れることだ。"
  },
  {
    "index": "F15007",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In fact, if you include only these quadratic features together with the original x1, and so on, up to x100 features, then you can actually fit very interesting hypotheses.",
    "output": "するとフィーチャーの数はかなり少なくなる。そのような2次式フィーチャーが100個だけあるとして、これは不十分なフィーチャーであり左上のようなデータセットにはまずフィットさせられないだろう。"
  },
  {
    "index": "F15008",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you can fit things like, you know, access a line of the ellipses like these, but you certainly cannot fit a more complex data set like that shown here.",
    "output": "実のところ、これらの2次式フィーチャーと共に元々のx1等々からx100までを入れるだけでも、かなり興味深い仮説をフィットさせることは出来る。"
  },
  {
    "index": "F15009",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So 5000 features seems like a lot, if you were to include the cubic, or third order known of each others, the x1, x2, x3.",
    "output": "例えば楕円の仮説、例えばこんなのとかにはフィットさせる事が出来る。だが、ここに示したようなより複雑なデータにはフィットさせられない。"
  },
  {
    "index": "F15010",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, x1 squared, x2, x10 and x11, x17 and so on.",
    "output": "5000のフィーチャーと聞くと凄いたくさんに感じるだろうが、三乗の項を含めるとつまり三次の多項式を含めると、x1x2x3とか、x1の二乗掛けるx2、x10x11x17などなど。"
  },
  {
    "index": "F15011",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You can imagine there are gonna be a lot of these features.",
    "output": "こんなフィーチャーがたくさんになるのは想像出来るだろう。"
  },
  {
    "index": "F15012",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In fact, they are going to be order and cube such features and if any is 100 you can compute that, you end up with on the order of about 170,000 such cubic features and so including these higher auto-polynomial features when your original feature set end is large this really dramatically blows up your feature space and this doesn't seem like a good way to come up with additional features with which to build none many classifiers when n is large.",
    "output": "だから元々のフィーチャーセットが大きいときこれらの高次の多項式フィーチャーを含めることは、実に劇的にフィーチャー空間を膨張させる。nが大きいときに非線形の分類器を作るのに、追加のフィーチャーを用意することはよい方法ではなさそうだ。"
  },
  {
    "index": "F15013",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For many machine learning problems, n will be pretty large.",
    "output": "多くの機械学習問題にとって、nはかなり大きいだろう。"
  },
  {
    "index": "F15014",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's consider the problem of computer vision.",
    "output": "コンピュータビジョンの問題について考える。"
  },
  {
    "index": "F15015",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And suppose you want to use machine learning to train a classifier to examine an image and tell us whether or not the image is a car.",
    "output": "機械学習を使って分類器を学習させたいとする。画像を調べ、それが車かどうかを判定するものだ。"
  },
  {
    "index": "F15016",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Many people wonder why computer vision could be difficult.",
    "output": "多くの人は、コンピュータビジョンがどうして難しいのかと疑問に思う。"
  },
  {
    "index": "F15017",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I mean when you and I look at this picture it is so obvious what this is.",
    "output": "例えばあなたや私がこの写真を見ると、これが何であるかはとても明白だ。"
  },
  {
    "index": "F15018",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You wonder how is it that a learning algorithm could possibly fail to know what this picture is.",
    "output": "どうして学習アルゴリズムがこの写真が何であるかを判定し損ねるだろうかと思うだろう。"
  },
  {
    "index": "F15019",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To understand why computer vision is hard let's zoom into a small part of the image like that area where the little red rectangle is.",
    "output": "コンピュータビジョンが難しい理由を理解するために、画像の小さな部分を拡大してみよう。この小さな赤い矩形の領域だ。"
  },
  {
    "index": "F15020",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that where you and I see a car, the computer sees that.",
    "output": "あなたや私が車を見るとき、コンピュータは実はこのように見ている。"
  },
  {
    "index": "F15021",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What it sees is this matrix, or this grid, of pixel intensity values that tells us the brightness of each pixel in the image.",
    "output": "コンピュータが見ているのはピクセル輝度値のマトリックスあるいはグリッドで、画像の各ピクセルの明るさを表している。"
  },
  {
    "index": "F15022",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the computer vision problem is to look at this matrix of pixel intensity values, and tell us that these numbers represent the door handle of a car.",
    "output": "つまりコンピュータビジョン問題はこのようなピクセル輝度値のマトリックスを見て、数値が車のドアハンドルを表しているとわかることだ。"
  },
  {
    "index": "F15023",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, when we use machine learning to build a car detector, what we do is we come up with a label training set, with, let's say, a few label examples of cars and a few label examples of things that are not cars, then we give our training set to the learning algorithm trained a classifier and then, you know, we may test it and show the new image and ask, \"What is this new thing?\".",
    "output": "具体的には、我々が機械学習を使って車の検出器を作るときに行うのは、分類トレーニングセットを考えること、例えばそれは、車の分類例をいくつか含み、そして車ではない物の例をいくつか含む。そしてそのトレーニングセットを学習アルゴリズムに与え分類器を学習させる。"
  },
  {
    "index": "F15024",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And hopefully it will recognize that that is a car.",
    "output": "」と尋ね、上手くいけば検出器はそれが車であると認識する。"
  },
  {
    "index": "F15025",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To understand why we need nonlinear hypotheses, let's take a look at some of the images of cars and maybe non-cars that we might feed to our learning algorithm.",
    "output": "どうして非線形の仮説が必要かを理解するために、我々が学習アルゴリズムに与えるであろう車の画像や車でない画像をいくつか見てみよう。"
  },
  {
    "index": "F15026",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's pick a couple of pixel locations in our images, so that's pixel one location and pixel two location, and let's plot this car, you know, at the location, at a certain point, depending on the intensities of pixel one and pixel two.",
    "output": "画像の中から一対のピクセルを選んで、ピクセル1の座標、ピクセル2の座標とする。そしてこの車を特定の位置にプロットする。"
  },
  {
    "index": "F15027",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's do this with a few other images.",
    "output": "他の画像についてもいくつかやってみよう。"
  },
  {
    "index": "F15028",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's take a different example of the car and you know, look at the same two pixel locations and that image has a different intensity for pixel one and a different intensity for pixel two.",
    "output": "車の別の例をとると、同じ2つのピクセル座標を見て、この画像はピクセル1が異なる輝度を持ち、ピクセル2も別の輝度を持っている。"
  },
  {
    "index": "F15029",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, it ends up at a different location on the figure.",
    "output": "そのため結局図上の異なる位置に置かれる。"
  },
  {
    "index": "F15030",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then let's plot some negative examples as well.",
    "output": "次に陰性の例もいくつか同様にプロットしてみよう。"
  },
  {
    "index": "F15031",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That's a non-car, that's a non-car .",
    "output": "プラスは車を表しマイナスは車でない物を表す。"
  },
  {
    "index": "F15032",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if we do this for more and more examples using the pluses to denote cars and minuses to denote non-cars, what we'll find is that the cars and non-cars end up lying in different regions of the space, and what we need therefore is some sort of non-linear hypotheses to try to separate out the two classes.",
    "output": "最終的にわかるのは車と車でない物が空間内の異なる領域に分布しているということで、我々に必要なのは2つのクラスを分離しようとする何らかの非線形の仮説だ。"
  },
  {
    "index": "F15033",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What is the dimension of the feature space?",
    "output": "フィーチャー空間の次元は何だろうか?"
  },
  {
    "index": "F15034",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose we were to use just 50 by 50 pixel images.",
    "output": "仮にたった50x50のピクセル画像を使うとしよう。"
  },
  {
    "index": "F15035",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now that suppose our images were pretty small ones, just 50 pixels on the side.",
    "output": "かなり小さな画像、一辺が50ピクセルしかない画像を想定する。"
  },
  {
    "index": "F15036",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then we would have 2500 pixels, and so the dimension of our feature size will be N equals 2500 where our feature vector x is a list of all the pixel testings, you know, the pixel brightness of pixel one, the brightness of pixel two, and so on down to the pixel brightness of the last pixel where, you know, in a typical computer representation, each of these may be values between say 0 to 255 if it gives us the grayscale value.",
    "output": "すると2500ピクセルあることになるので、フィーチャーサイズの次元はn=2500、ここでフィーチャーベクトルxはすべてのピクセル検査のリストだ。これはピクセル1の明るさ、ピクセル2の明るさ、等々から最後のピクセルの明るさまでのことで、典型的なコンピュータ表現では各々は、グレイスケール値の場合例えば0から255までの値となるだろう。"
  },
  {
    "index": "F15037",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we were using RGB images with separate red, green and blue values, we would have n equals 7500.",
    "output": "RGB画像を使い赤、緑、青を分ける場合は、n=7500となるだろう。"
  },
  {
    "index": "F15038",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if we were to try to learn a nonlinear hypothesis by including all the quadratic features, that is all the terms of the form, you know, Xi times Xj, while with the 2500 pixels we would end up with a total of three million features.",
    "output": "だからもしすべての2次式フィーチャーを取り込んで、非線形の仮説を学習させようとしたらつまり、XiかけるXjの形のすべての項を取り込み、それが2500ピクセルあったら結局合計で300万ピクセルになる。"
  },
  {
    "index": "F15039",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's just too large to be reasonable; the computation would be very expensive to find and to represent all of these three million features per training example.",
    "output": "1つのトレーニング例当たりに、これら300万フィーチャーのすべてを見つけて、表現する計算は非常に高くつくだろう。"
  },
  {
    "index": "F15040",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, simple logistic regression together with adding in maybe the quadratic or the cubic features - that's just not a good way to learn complex nonlinear hypotheses when n is large because you just end up with too many features.",
    "output": "だから、単純なロジスティック回帰に2次や3次のフィーチャーを加えたもの-これはnが大きい時に非線形の仮説を学習させるのにはまったく向いていない。フィーチャーが多くなりすぎるからだ。"
  },
  {
    "index": "F15041",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos, I would like to tell you about Neural Networks, which turns out to be a much better way to learn complex hypotheses, complex nonlinear hypotheses even when your input feature space, even when n is large.",
    "output": "以降のいくつかのビデオでは、ニューラルネットワークについて教えよう。これは複雑な仮説、複雑な非線形の仮説を学習させるのにずっとよい方法だということがわかる。"
  },
  {
    "index": "F15042",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And along the way I'll also get to show you a couple of fun videos of historically important applications of Neural networks as well that I hope those videos that we'll see later will be fun for you to watch as well.",
    "output": "そしてついでにニューラルネットワークの歴史的に重要な応用についてのおもしろい動画をいくつかお見せしよう。あなたも後ほど見るこれらの動画をおもしろいと思ってくれるといいが。"
  },
  {
    "index": "F15043",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Neural Networks are a pretty old algorithm that was originally motivated by the goal of having machines that can mimic the brain.",
    "output": "ニューラルネットワークは極めて古いアルゴリズムで、もともとそれは脳を模倣する機械を得る事を目標としていた。"
  },
  {
    "index": "F15044",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now in this class, of course I'm teaching Neural Networks to you because they work really well for different machine learning problems and not, certainly not because they're logically motivated.",
    "output": "このクラスではもちろん、ニューラルネットワークを教えるのは異なる種類の機械学習の問題で、それがとても役立つからで、論理的な動機づけによる、という訳では無い。"
  },
  {
    "index": "F15045",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to give you some of the background on Neural Networks. So that we can get a sense of what we can expect them to do.",
    "output": "だがこのビデオではニューラルネットワークのある程度の背景知識を教えたい、それらが何をしてくれるか、の感覚を養うために。"
  },
  {
    "index": "F15046",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Both in the sense of applying them to modern day machinery problems, as well as for those of you that might be interested in maybe the big AI dream of someday building truly intelligent machines.",
    "output": "こんにち的な機械学習野問題に適用する、という意味でも、あなたがたの中に将来の大きなAIの夢を見る人々がいつの日か真の知的な機械を作る為にも。"
  },
  {
    "index": "F15047",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Also, how Neural Networks might pertain to that.",
    "output": "ニューラルネットワークがそれにどのように関係するのかも。"
  },
  {
    "index": "F15048",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The origins of Neural Networks was as algorithms that try to mimic the brain and those a sense that if we want to build learning systems while why not mimic perhaps the most amazing learning machine we know about, which is perhaps the brain.",
    "output": "ニューラルネットワークの起源は脳を模倣しようとしたアルゴリズムにあり、それは理解も出来る。学習システムを作ろう、と思ったら何故この、恐らくもっとも驚くべき学習する機械である所の、この脳を模倣しないというのか?"
  },
  {
    "index": "F15049",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Neural Networks came to be very widely used throughout the 1980's and 1990's and for various reasons as popularity diminished in the late 90's.",
    "output": "いや、する(反語)ニューラルネットワークは1980年代から1990年代にかけて、とても広く使われていた。だが様々な理由で90年代の後半には人気は下火となっていた。"
  },
  {
    "index": "F15050",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But more recently, Neural Networks have had a major recent resurgence.",
    "output": "だがより最近になって、ニューラルネットワークは大復活を遂げた。"
  },
  {
    "index": "F15051",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One of the reasons for this resurgence is that Neural Networks are computationally some what more expensive algorithm and so, it was only, you know, maybe somewhat more recently that computers became fast enough to really run large scale Neural Networks and because of that as well as a few other technical reasons which we'll talk about later, modern Neural Networks today are the state of the art technique for many applications.",
    "output": "この復活劇の理由の一つにはニューラルネットワークは計算量的になかなか高く付くアルゴリズムだというのがある。だから近年になってようやく大規模なニューラルネットワークを走らせるのに十分なくらいコンピュータが速くなった。"
  },
  {
    "index": "F15052",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, when you think about mimicking the brain while one of the human brain does tell me same things, right?",
    "output": "では脳を模倣しよう、と考えると、人間の脳というのはとてもたくさんの素晴らしい事が出来る、でしょ?"
  },
  {
    "index": "F15053",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The brain can learn to see process images than to hear, learn to process our sense of touch.",
    "output": "脳は画像処理を学習する事も出来るし、聞き取りも学習出来るし、感触の処理も学習出来る。"
  },
  {
    "index": "F15054",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can, you know, learn to do math, learn to do calculus, and the brain does so many different and amazing things.",
    "output": "数学を学ぶ事も出来るし、計算学ぶ事も出来る。脳はそれらたくさんの素晴らしい事が出来る。"
  },
  {
    "index": "F15055",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It seems like if you want to mimic the brain it seems like you have to write lots of different pieces of software to mimic all of these different fascinating, amazing things that the brain tell us, but does is this fascinating hypothesis that the way the brain does all of these different things is not worth like a thousand different programs, but instead, the way the brain does it is worth just a single learning algorithm.",
    "output": "そんなに素晴らしいのだから、その脳を模倣しようとすればたくさんの異なるソフトウェアを書かなきゃいけなさそうに見える、これら脳が教えてくれる、様々な魅力的で驚くべき物達を模倣する為に。"
  },
  {
    "index": "F15056",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is just a hypothesis but let me share with you some of the evidence for this.",
    "output": "これは単なる仮説だが、この証拠とも取れる事を幾つか共有したい。"
  },
  {
    "index": "F15057",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This part of the brain, that little red part of the brain, is your auditory cortex and the way you're understanding my voice now is your ear is taking the sound signal and routing the sound signal to your auditory cortex and that's what's allowing you to understand my words.",
    "output": "脳のこの部分、この小さな赤い部分は聴覚皮質だ。そして今あなたが私の声を理解している方法はあなたの耳が音声信号を拾い上げて、その音声信号をあなたの聴覚皮質へと送り、そしてその聴覚皮質こそが私の言葉を理解してくれる訳だ。"
  },
  {
    "index": "F15058",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Neuroscientists have done the following fascinating experiments where you cut the wire from the ears to the auditory cortex and you re-wire, in this case an animal's brain, so that the signal from the eyes to the optic nerve eventually gets routed to the auditory cortex.",
    "output": "神経学者は以下のような興味深い実験を行なってきた、それは耳から聴覚皮質への線をカットして、それをつなぎなおすーーこの場合は動物の脳だが、目から視神経へのシグナルを聴覚皮質に送られるように。"
  },
  {
    "index": "F15059",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you do this it turns out, the auditory cortex will learn to see.",
    "output": "これをやってみると、聴覚皮質は見る事を学習する事が分かった!"
  },
  {
    "index": "F15060",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is in every single sense of the word see as we know it.",
    "output": "これは、見る、という言葉から想起される全ての事について言える。"
  },
  {
    "index": "F15061",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you do this to the animals, the animals can perform visual discrimination task and as they can look at images and make appropriate decisions based on the images and they're doing it with that piece of brain tissue.",
    "output": "だからこれを動物に行うと、その動物は視覚による区分のタスクが行え、つまりそれらの動物は画像を見て、それらの画像に基づいて適切な決定を行う事が出来る。そしてそれらは、ここの脳の組織で行うのだ。"
  },
  {
    "index": "F15062",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's another example.",
    "output": "ここにもう一つ別の例がある。"
  },
  {
    "index": "F15063",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That red piece of brain tissue is your somatosensory cortex.",
    "output": "脳のこの赤い部分は体性感覚皮質だ。"
  },
  {
    "index": "F15064",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you do a similar re-wiring process then the somatosensory cortex will learn to see. Because of this and other similar experiments, these are called neuro-rewiring experiments.",
    "output": "同じような再結合処理を行うと体性感覚皮質は見る事を学習する。"
  },
  {
    "index": "F15065",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's this sense that if the same piece of physical brain tissue can process sight or sound or touch then maybe there is one learning algorithm that can process sight or sound or touch.",
    "output": "これやその他の似たような実験のため、、、これらは神経再接続実験と呼ばれるが、こんな意味で、物理的に同じ脳の組織が視覚や音や触覚を処理出来るのなら、学習アルゴリズムも一つで、視覚や音や触覚を処理出来ているのかもしれない。"
  },
  {
    "index": "F15066",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And instead of needing to implement a thousand different programs or a thousand different algorithms to do, you know, the thousand wonderful things that the brain does, maybe what we need to do is figure out some approximation or to whatever the brain's learning algorithm is and implement that and that the brain learned by itself how to process these different types of data.",
    "output": "そして何千もの異なるプログラムを実装する代わりにまたは何千もの異なるアルゴリズムを使う代わりに脳が行なっているような、何千もの素晴らしい事をするのに必要なのは、ひょっとしたら脳の学習アルゴリズムが行なっている事の、何らかの近似を見出して、それを実装する事かもしれない。"
  },
  {
    "index": "F15067",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To a surprisingly large extent, it seems as if we can plug in almost any sensor to almost any part of the brain and so, within the reason, the brain will learn to deal with it.",
    "output": "そしてその脳が実際にこれら様々な種類のデータを処理する方法を学習する方法は、驚く程大部分は、どうやらどんな種類のセンサーも脳のほとんどどこにでもつなぐ事が出来るようなのだ。そしてどうやら、そうすると、脳はそれをどう扱うかを学習するらしい。"
  },
  {
    "index": "F15068",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here are a few more examples.",
    "output": "ここにさらなる例を幾つか用意した。"
  },
  {
    "index": "F15069",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "On the upper left is an example of learning to see with your tongue.",
    "output": "上部左は、舌で物を見る事を学習する例だ。"
  },
  {
    "index": "F15070",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The way it works is--this is actually a system called BrainPort undergoing FDA trials now to help blind people see--but the way it works is, you strap a grayscale camera to your forehead, facing forward, that takes the low resolution grayscale image of what's in front of you and you then run a wire to an array of electrodes that you place on your tongue so that each pixel gets mapped to a location on your tongue where maybe a high voltage corresponds to a dark pixel and a low voltage corresponds to a bright pixel and, even as it does today, with this sort of system you and I will be able to learn to see, you know, in tens of minutes with our tongues.",
    "output": "それがどうなってるかというと、これはBrainPortという、FDAが行なっているシステムで盲目の人が見えるようになるのを手伝うという物。それがどうなってるかというと、おでこにグレースケールのカメラを付けて前に向けて、それが低解像度のグレースケールの像であなたの前にある物を映す。"
  },
  {
    "index": "F15071",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's a second example of human echo location or human sonar.",
    "output": "これは二番目の例でエコー位置、または人力ソナーだ。"
  },
  {
    "index": "F15072",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there are two ways you can do this.",
    "output": "これを行うには2つ方法がある。"
  },
  {
    "index": "F15073",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You can either snap your fingers, or click your tongue.",
    "output": "指を鳴らすか舌を打つかだ。"
  },
  {
    "index": "F15074",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But there are blind people today that are actually being trained in schools to do this and learn to interpret the pattern of sounds bouncing off your environment - that's sonar.",
    "output": "だが盲目の人がこんにちこれを実際に学校で練習して、環境から音が跳ね返るパターンの解釈を学習している、それがソナーだ。"
  },
  {
    "index": "F15075",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if after you search on YouTube, there are actually videos of this amazing kid who tragically because of cancer had his eyeballs removed, so this is a kid with no eyeballs.",
    "output": "youtubeを検索してみると、実際の動画があるよ。"
  },
  {
    "index": "F15076",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But by snapping his fingers, he can walk around and never hit anything.",
    "output": "悲運にも目玉にガンを患った為目玉を失った子供が、ーーつまりこの子供は目玉が無いのだーーだが指を鳴らして、歩き回って何にもぶつからないで済んでる。"
  },
  {
    "index": "F15077",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "He can shoot a basketball into a hoop and this is a kid with no eyeballs.",
    "output": "スケートボードにも乗れて、バスケットボールのフープにシュートも出来る。これが目玉の無い子供なのだ。"
  },
  {
    "index": "F15078",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Third example is the Haptic Belt where if you have a strap around your waist, ring up buzzers and always have the northmost one buzzing.",
    "output": "三番目の例はHapticBeltで、腰の回りにストラップをつけて、ブサーをたくさんつける、そして一番北のブザーがいつも鳴るようにする。"
  },
  {
    "index": "F15079",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You can give a human a direction sense similar to maybe how birds can, you know, sense where north is.",
    "output": "すると人間に、鳥などが北がどちらかを感じるような方向感覚を身につける事が出来る。"
  },
  {
    "index": "F15080",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, some of the bizarre example, but if you plug a third eye into a frog, the frog will learn to use that eye as well.",
    "output": "そしてちょっと気味が悪い例としては、カエルに三番目の目をつなげると、カエルはその目をどう使うかを学習する、という物。"
  },
  {
    "index": "F15081",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, it's pretty amazing to what extent is as if you can plug in almost any sensor to the brain and the brain's learning algorithm will just figure out how to learn from that data and deal with that data.",
    "output": "以上は、極めて驚くべきほどじゃないか。ほとんどどんなセンサーを脳につなげても、脳はそのデータから、そのデータをどう扱うかを勝手に学んでいく様が、いかに広い範囲で成立しているかという事は。"
  },
  {
    "index": "F15082",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And there's a sense that if we can figure out what the brain's learning algorithm is, and, you know, implement it or implement some approximation to that algorithm on a computer, maybe that would be our best shot at, you know, making real progress towards the AI, the artificial intelligence dream of someday building truly intelligent machines.",
    "output": "そして脳の学習アルゴリズムがどんな物かを見つけ出し、それを実装する、またはその近似でもコンピュータで実装出来れば、それはたぶん、それはAI、人工知能、その夢みる所の、真に知能を持った機械を作るという野望への、とても大きな一歩となりうる。"
  },
  {
    "index": "F15083",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, of course, I'm not teaching Neural Networks, you know, just because they might give us a window into this far-off AI dream, even though I'm personally, that's one of the things that I personally work on in my research life.",
    "output": "今回はもちろん、ニューラルネットワークを教えるのはこの遥か彼方のAIの夢への扉を開くから、という訳では無い。とは言うものの、私は個人的にはその夢が私の研究人生で追求している物の一つだが。"
  },
  {
    "index": "F15084",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the main reason I'm teaching Neural Networks in this class is because it's actually a very effective state of the art technique for modern day machine learning applications.",
    "output": "だがこのクラスで私がニューラルネットワークを教える主な理由はそれは実際にとても有効なステートオブジアートな、こんにちの機械学習への応用となっているからだ。"
  },
  {
    "index": "F15085",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in the next few videos, we'll start diving into the technical details of Neural Networks so that you can apply them to modern-day machine learning applications and get them to work well on problems.",
    "output": "だから今後の一連のビデオで、ニューラルネットワークの技術的詳細に入っていく。あなたがこんにち的な機械学習の応用が出来るようになって、問題に対してうまく機能させられるようになる為に。"
  },
  {
    "index": "F15086",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But for me, you know, one of the reasons the excite me is that maybe they give us this window into what we might do if we're also thinking of what algorithms might someday be able to learn in a manner similar to humankind.",
    "output": "だが私にとっては、それがとってもエキサイトな理由は、たぶんそれらが将来には、人間のように学習するアルゴリズムとはどんなものか、を想像させる扉を開いてくれる事なのかもしれないけどね。"
  },
  {
    "index": "F15087",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I want to start telling you about how we represent neural networks.",
    "output": "このビデオでは、ニューラルネットワークをどう表現するかについて話す。"
  },
  {
    "index": "F15088",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In other words, how we represent our hypothesis or how we represent our model when using neural networks.",
    "output": "言い換えると、我らの仮説をどう表現するか、または、ニューラルネットワークを使う時はどうモデルを表現するか、という事。"
  },
  {
    "index": "F15089",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Neural networks were developed as simulating neurons or networks of neurons in the brain.",
    "output": "ニューラルネットワークは脳内のニューロン、またはニューロンのネットワークをシミュレートする事で発展した。"
  },
  {
    "index": "F15090",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, to explain the hypothesis representation let's start by looking at what a single neuron in the brain looks like.",
    "output": "だから仮説の表現を説明するために、脳の中のニューロンを一つ取り出すとどんな感じか見てみよう。"
  },
  {
    "index": "F15091",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Your brain and mine is jam packed full of neurons like these and neurons are cells in the brain.",
    "output": "あなたや私の脳は、こんなニューロンがたくさんごちゃごちゃ詰まっている。ニューロンとは脳の細胞で特徴的なのは2つある。"
  },
  {
    "index": "F15092",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The neuron has a cell body, like so, and moreover, the neuron has a number of input wires, and these are called the dendrites.",
    "output": "一つ目はニューロンには細胞体があり、ーこんな感じのーで、さらに、ニューロンは幾つかの入力のワイヤがある事で、これらはdendriteと呼ばれていて、それは入力のワイヤと考えられる。"
  },
  {
    "index": "F15093",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You think of them as input wires, and these receive inputs from other locations.",
    "output": "そしてこれらが他の場所からの入力を受け取る。"
  },
  {
    "index": "F15094",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And a neuron also has an output wire called an Axon, and this output wire is what it uses to send signals to other neurons, so to send messages to other neurons.",
    "output": "そしてニューロンは出力ワイヤも持っていて、それはaxonと呼ばれる。そしてこの出力ワイヤは他のニューロンにシグナルを送るのに、言い換えると他のニューロンにメッセージを送るのに使われる。"
  },
  {
    "index": "F15095",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, at a simplistic level what a neuron is, is a computational unit that gets a number of inputs through it input wires and does some computation and then it says outputs via its axon to other nodes or to other neurons in the brain.",
    "output": "つまり一番単純なレベルでは、ニューロンとは何かというと、たくさんの入力を入力ワイヤから受け取りなんらかの計算を行い、その出力をaxonを通して脳内の他のニューロンに送る計算ユニットと考えることが出来る。"
  },
  {
    "index": "F15096",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's a illustration of a group of neurons.",
    "output": "これはニューロンのグループのイラストだ。"
  },
  {
    "index": "F15097",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The way that neurons communicate with each other is with little pulses of electricity, they are also called spikes but that just means pulses of electricity.",
    "output": "ニューロンがお互いにコミュニケートする方法は僅かな電気信号によってだ。それらはスパイクとも呼ばれている。"
  },
  {
    "index": "F15098",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here is one neuron and what it does is if it wants a send a message what it does is sends a little pulse of electricity.",
    "output": "だけどそれらは単なる小さな電気を意味するに過ぎない。ここに一つニューロンがあり、それがする事といえば、もしメッセージを送りたい時はaxonを通して、別のニューロンにわずかな電気のパルスを送る。"
  },
  {
    "index": "F15099",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Varis axon to some different neuron and here, this axon that is this open wire, connects to the dendrites of this second neuron over here, which then accepts this incoming message that some computation.",
    "output": "そしてこれがaxonだ。この出力ワイヤがあって、それがこうして、二番目のニューロンの入力ワイヤ、またの名をdendriteにつながっている。"
  },
  {
    "index": "F15100",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And they, in turn, decide to send out this message on this axon to other neurons, and this is the process by which all human thought happens.",
    "output": "そしてそこからこの信号に入力のメッセージとして受け取って、なんらかの計算を行い、またさらに他のニューロンへ出力メッセージをaxonを通して送るかもしれない。"
  },
  {
    "index": "F15101",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's these Neurons doing computations and passing messages to other neurons as a result of what other inputs they've got.",
    "output": "以上が、全ての人類が考えた時に起こるプロセスで、これらのニューロンが計算をして、メッセージを他のニューロンに送る。与えられた入力に対する結果として。"
  },
  {
    "index": "F15102",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, by the way, this is how our senses and our muscles work as well.",
    "output": "ところで、これはまた、我らの感覚や筋肉が機能する方法でもある。"
  },
  {
    "index": "F15103",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you want to move one of your muscles the way that where else in your neuron may send this electricity to your muscle and that causes your muscles to contract and your eyes, some senses like your eye must send a message to your brain while it does it senses hosts electricity entity to a neuron in your brain like so.",
    "output": "もし筋肉の一つを動かそうとすれば、それが実現されるのは、ニューロンが筋肉に電気のパルスを送り、それが筋肉を収縮させたり、目の場合、何らかのセンサー、例えば目とかの場合、脳にメッセージを送るには電気のパルスを脳にあるニューロンに送るという手段を通してだ。"
  },
  {
    "index": "F15104",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In a neuro network, or rather, in an artificial neuron network that we've implemented on the computer, we're going to use a very simple model of what a neuron does we're going to model a neuron as just a logistic unit.",
    "output": "ニューラルネットワークにおいては、いや我らがコンピュータで実装する人工的なニューラルネットワークにおいては、と言うべきか、その場合、ニューロンがやってる事のとてもシンプルなモデルを使う事になる。ニューロンを単なるロジスティックの単位としてモデル化する。"
  },
  {
    "index": "F15105",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, when I draw a yellow circle like that, you should think of that as a playing a role analysis, who's maybe the body of a neuron, and we then feed the neuron a few inputs who's various dendrites or input wiles.",
    "output": "だから黄色で円をこんな感じで描いたら、これはニューロンの本体みたいな役割をしている、と考えてくれ。そしてそこに、いくつかの入力をdendritesまたの名を入力ワイヤを通して食わす。"
  },
  {
    "index": "F15106",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And output some value on this output wire, or in the biological neuron, this is an axon.",
    "output": "するとニューロンはなんらかの計算行い、この出力ワイヤからなんらかの値を出力する。生物的なニューロンならそれはaxonに相当する。"
  },
  {
    "index": "F15107",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And whenever I draw a diagram like this, what this means is that this represents a computation of h of x equals one over one plus e to the negative theta transpose x, where as usual, x and theta are our parameter vectors, like so.",
    "output": "そしてこんなダイアグラムを描いた時はいつでも、これの意味する所は、h(x)の計算であり、それは1足すeのマイナスシータ転置x分の1で、xとシータはいつも通り、パラメータベクトルとかを表している。"
  },
  {
    "index": "F15108",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is a very simple, maybe a vastly oversimplified model, of the computations that the neuron does, where it gets a number of inputs, x1, x2, x3 and it outputs some value computed like so.",
    "output": "つまりこれはとても単純化した、ちょっとあまりにも単純化しすぎた感じのニューロンが行なっている事のモデルでいくつかの入力、x1、x2、x3を受け取りそんな風に計算された何らか値を出力する、という。"
  },
  {
    "index": "F15109",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When I draw a neural network, usually I draw only the input nodes x1, x2, x3.",
    "output": "ニューラルネットワークを書く時は普通は入力のノードだけをx1、x2、x3と描くのだが、たまに、そちらの方が便利な時に限り追加のノード、x0を描く事もある。"
  },
  {
    "index": "F15110",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Sometimes when it's useful to do so, I'll draw an extra node for x0.",
    "output": "だがx0は1と決まっているので、このノードは描いたり描かなかったりする。"
  },
  {
    "index": "F15111",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This x0 now that's sometimes called the bias unit or the bias neuron, but because x0 is already equal to 1, sometimes, I draw this, sometimes I won't just depending on whatever is more notationally convenient for that example.",
    "output": "それは単に、その例にとって記しておいた方が便利かどうかで決めてる。最後に、もう一つだけ用語を導入しておく。"
  },
  {
    "index": "F15112",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, one last bit of terminology when we talk about neural networks, sometimes we'll say that this is a neuron or an artificial neuron with a Sigmoid or logistic activation function. So this activation function in the neural network terminology.",
    "output": "ニューラルネットワークについて話してる時は、このニューロン、人工的なニューロンのsigmoid関数またはロジスティック関数を、アクティベーション関数と呼ぶ事がある。"
  },
  {
    "index": "F15113",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is just another term for that function for that non-linearity g(z) = 1 over 1+e to the -z.",
    "output": "このアクティベーション関数というのはニューラルネットワークの用語でこれはこの非線形のg(z)イコール1足すeの-z分の1の、もう一つの呼び名に過ぎない。"
  },
  {
    "index": "F15114",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And whereas so far I've been calling theta the parameters of the model, I'll mostly continue to use that terminology.",
    "output": "一方でここまではシータをモデルのパラメータと呼んできたし、今後もだいたいはその用語を使い続けるが、ニューラルネットワークではニューラルネットワークの文献では人々はたまにモデルのウェイトと呼んでいるのを見かけるかもしれない。"
  },
  {
    "index": "F15115",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here, it's a copy to the parameters, but in neural networks, in the neural network literature sometimes you might hear people talk about weights of a model and weights just means exactly the same thing as parameters of a model.",
    "output": "このウェイトというのはこのモデルのパラメータと完全に同じ意味だ。"
  },
  {
    "index": "F15116",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But I'll mostly continue to use the terminology parameters in these videos, but sometimes, you might hear others use the weights terminology.",
    "output": "このクラスのビデオではだいたいパラメータという用語を使うが、たまに他の人がウェイトって用語を使ってるのを聞くことがあるかもしれない。"
  },
  {
    "index": "F15117",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this little diagram represents a single neuron.",
    "output": "このちっぽけなダイアグラムは単体のニューロンを表している。"
  },
  {
    "index": "F15118",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What a neural network is, is just a group of this different neurons strong together.",
    "output": "ニューラルネットワークとは単にこれらのニューロンが幾つか集まったグループの事だ。"
  },
  {
    "index": "F15119",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Completely, here we have input units x1, x2, x3 and once again, sometimes you can draw this extra note x0 and Sometimes not, just flow that in here.",
    "output": "具体的には、ここに我らの入力単位、x1、x2、x3があり、繰り返しになるがたまにこの追加のx0のノードを描いたり描かなかったりする。"
  },
  {
    "index": "F15120",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And here we have three neurons which have written 81, 82, 83. I'll talk about those indices later.",
    "output": "そしてここに、我らは3つのニューロンを持っている。"
  },
  {
    "index": "F15121",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And once again we can if we want add in just a0 and add the mixture bias unit there.",
    "output": "そして繰り返すが、もし必要ならこのa0という追加のバイアスユニットをここに足す事もある。"
  },
  {
    "index": "F15122",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then finally we have this third node and the final layer, and there's this third node that outputs the value that the hypothesis h(x) computes.",
    "output": "そして最後に、この三番目のノードが最後のレイヤーにある。そしてこの三番目のノードが仮説であるh(x)の計算結果を出力する。"
  },
  {
    "index": "F15123",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To introduce a bit more terminology, in a neural network, the first layer, this is also called the input layer because this is where we Input our features, x1, x2, x3.",
    "output": "この最初のレイヤーは入力レイヤーとも呼ばれる。何故ならこれが我らのフィーチャーであるx1、x2、x3をインプットする所だから。"
  },
  {
    "index": "F15124",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The final layer is also called the output layer because that layer has a neuron, this one over here, that outputs the final value computed by a hypothesis.",
    "output": "最後のレイヤーは出力レイヤーとも呼ばれる。何故ならこのレイヤーがここにあるニューロンこそが、仮説による最終的な計算結果を出力するから。"
  },
  {
    "index": "F15125",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then, layer 2 in between, this is called the hidden layer.",
    "output": "そして2つの間にあるレイヤーを隠れたレイヤー(hiddenlayer)と呼ぶ。"
  },
  {
    "index": "F15126",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The term hidden layer isn't a great terminology, but this ideation is that, you know, you supervised early, where you get to see the inputs and get to see the correct outputs, where there's a hidden layer of values you don't get to observe in the training setup.",
    "output": "隠れたレイヤーという用語はそんな良い用語とは思わないが、直感的には教師あり学習では、入力と正解の出力は見れる訳だが隠れたレイヤーはトレーニングセットでは観測出来ない値だ。"
  },
  {
    "index": "F15127",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's not x, and it's not y, and so we call those hidden.",
    "output": "だからxでもyでも無い物は、隠れたレイヤーと呼んでいる。"
  },
  {
    "index": "F15128",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And they try to see neural nets with more than one hidden layer but in this example, we have one input layer, Layer 1, one hidden layer, Layer 2, and one output layer, Layer 3.",
    "output": "そしてのちほど、一つよりも多い隠れたレイヤーの例を見ていく。だがこの例では入力レイヤーであるレイヤー1が一つに、一つの隠れたレイヤーのレイヤー2に、出力レイヤーのレイヤー3がある。"
  },
  {
    "index": "F15129",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But basically, anything that isn't an input layer and isn't an output layer is called a hidden layer.",
    "output": "だが基本的には入力レイヤーでなく、そして出力レイヤーでも無い物はなんでも隠れたレイヤーと呼ぶ。"
  },
  {
    "index": "F15130",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I want to be really clear about what this neural network is doing.",
    "output": "さて、私はこのニューラルネットワークが何をするのかをとっても分かりやすくしたい。"
  },
  {
    "index": "F15131",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's step through the computational steps that are and body represented by this diagram.",
    "output": "これに埋め込まれた計算過程を順番に見ていこう。このダイアグラムで表現されている物の。"
  },
  {
    "index": "F15132",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To explain these specific computations represented by a neural network, here's a little bit more notation.",
    "output": "ニューラルネットワークで表現されている特定の計算を説明する為に、もうちょっと記法を追加しておく。"
  },
  {
    "index": "F15133",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to use a superscript j subscript i to denote the activation of neuron i or of unit i in layer j.",
    "output": "上付き添字のjと下付き添字のiをレイヤーjにあるニューロンiまたはユニットiのアクティベーションを示すのに使う。"
  },
  {
    "index": "F15134",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So completely this gave superscript to sub group one, that's the activation of the first unit in layer two, in our hidden layer.",
    "output": "具体的には、これは上付き添字2の下付き添字1でこれは二番目のレイヤーのつまり隠れたレイヤーの最初のユニットだ。"
  },
  {
    "index": "F15135",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by activation I just mean the value that's computed by and as output by a specific.",
    "output": "そしてアクティベーションという言葉で、特定のニューロンからの計算結果の値、つまり出力の値を指す。"
  },
  {
    "index": "F15136",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In addition, new network is parametrize by these matrixes, theta super script j Where theta j is going to be a matrix of weights controlling the function mapping form one layer, maybe the first layer to the second layer, or from the second layer to the third layer.",
    "output": "さらに、我らのニューラルネットワークはこれらの行列、シータの上付き添字jでパラメータ化される。ここでシータjは一つのレイヤーから、、、例えば最初のレイヤーから二番目のレイヤーへと、とか、二番目のレイヤーから三番目のレイヤーへと、などをマッピングする関数を制御するウェイトとなる行列だ。"
  },
  {
    "index": "F15137",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here are the computations that are represented by this diagram.",
    "output": "つまり、これがこのダイアグラムで表現される計算だ。"
  },
  {
    "index": "F15138",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This first hidden unit here has it's value computed as follows, there's a is a21 is equal to the sigma function of the sigma activation function, also called the logistics activation function, apply to this sort of linear combination of these inputs.",
    "output": "ここにある、最初の隠れたユニットは以下のように計算された値だ:a(2)1イコールsigmoid関数、またはsigmoidアクティベーション関数またはロジスティックアクティベーション関数と呼ばれるが、それがこんな形の入力の線形の組み合わせに適用される。"
  },
  {
    "index": "F15139",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then this second hidden unit has this activation value computer as sigmoid of this.",
    "output": "そしてこの二番目の隠れたユニットはこのsigmoid関数で計算される、このアクティベーションの値だ。"
  },
  {
    "index": "F15140",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly for this third hidden unit is computed by that formula.",
    "output": "以下同様に、この三番目の隠れたユニットはこの式で計算される。"
  },
  {
    "index": "F15141",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here we have 3 theta 1 which is matrix of parameters governing our mapping from our three different units, our hidden units. Theta 1 is going to be a 3.",
    "output": "つまりシータ1の次元はそれはパラメータの行列で3つの入力ユニットと3つの隠れユニットを合わせた物だから、つまりシータ1は3、、、シータ1はつまり3x4次元の行列だ。"
  },
  {
    "index": "F15142",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And more generally, if a network has SJU units in there j and sj + 1 units and sj + 1 then the matrix theta j which governs the function mapping from there sj + 1.",
    "output": "より一般的にはネットワークがレイヤーjにsのjだけのユニットを、j+1番目のレイヤーにsのj+1個のユニットがあるとすると、行列であるところのシータjはレイヤーjからレイヤーj+1のマッピングの関数を決定する訳だが、その次元は、sのj+1掛けるsのj足す1となる。"
  },
  {
    "index": "F15143",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That will have to mention sj +1 by sj + 1 I'll just be clear about this notation right.",
    "output": "それがsjに1を足す、という事。オーケー?"
  },
  {
    "index": "F15144",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is Subscript j + 1 and that's s subscript j, and then this whole thing, plus 1, this whole thing (sj + 1), okay?",
    "output": "つまりsの下付き添字j+1に足す事の、、、じゃなかった、掛けるだ。"
  },
  {
    "index": "F15145",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's s subscript j + 1 by, So that's s subscript j + 1 by sj + 1 where this plus one is not part of the subscript.",
    "output": "えーと、sのj+1に、掛ける事のsのjに足すことの1、最後の足す1は添字じゃないよ。"
  },
  {
    "index": "F15146",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, there's a loss of this final and after that we have one more unit which computer h of x and that's equal can also be written as a(3)1 and that's equal to this.",
    "output": "それはイコール、、、ところでそれは、a(3)の1と書けて、それはイコール、これとなる。"
  },
  {
    "index": "F15147",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you notice that I've written this with a superscript two here, because theta of superscript two is the matrix of parameters, or the matrix of weights that controls the function that maps from the hidden units, that is the layer two units to the one layer three unit, that is the output unit.",
    "output": "そして気づいたかもしれないが私は上付き添字で2とここでは書いた。その理由は、シータの上付き添字の2はパラメータの行列、またの名をウェイトの行列でそれは隠れユニット、それはレイヤー2のユニットの事だが、それとレイヤー3のユニットをそれは出力ユニットだが、それをマップする関数を制御する。"
  },
  {
    "index": "F15148",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To summarize, what we've done is shown how a picture like this over here defines an artificial neural network which defines a function h that maps with x's input values to hopefully to some space that provisions y.",
    "output": "まとめると、我らがやった事はここにあるような絵がどう人工的なニューラルネットワークを定義するかを見た。それは入力値のxを願わくばあるyの予測値にマップする関数を定義する。"
  },
  {
    "index": "F15149",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And these hypothesis are parameterized by parameters denoting with a capital theta so that, as we vary theta, we get different hypothesis and we get different functions.",
    "output": "そしてこれらの仮説はパラメータでパラメトライズされていて、そのパラメータは大文字のシータで示している。こうする事でシータを変える事で異なる仮説が得られる訳だ。"
  },
  {
    "index": "F15150",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Mapping say from x to y.",
    "output": "つまりxからyへとマップする、別の関数が得られるって事だ。"
  },
  {
    "index": "F15151",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this gives us a mathematical definition of how to represent the hypothesis in the neural network.",
    "output": "そして、これは仮説をニューラルネットワークで表す方法の数学による定義を与えてくれる。"
  },
  {
    "index": "F15152",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos what I would like to do is give you more intuition about what these hypothesis representations do, as well as go through a few examples and talk about how to compute them efficiently.",
    "output": "以後の一連のビデオではこれらの仮説の表現が何をするかの直感を伝えたいと思う。それと同時に、例を幾つか見て、それらをどう効率的に計算するかもお話する。"
  },
  {
    "index": "F15153",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this and the next video I want to work through a detailed example showing how a neural network can compute a complex non linear function of the input.",
    "output": "このビデオとこの次のビデオで具体的な詳細例をやっていく事で、ニューラルネットワークがどのように入力の複雑な非線形の関数を計算出来るのかを見ていき、これを通して何故ニューラルネットワークが複雑で非線形な仮説を学習するのに用いる事が出来るか、その心を伝えたいと思う。"
  },
  {
    "index": "F15154",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Consider the following problem where we have features X1 and X2 that are binary values.",
    "output": "二値の値のフィーチャー、x1とx2があるとしよう。"
  },
  {
    "index": "F15155",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, either 0 or 1.",
    "output": "値は0か1のどちらか。"
  },
  {
    "index": "F15156",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, X1 and X2 can each take on only one of two possible values.",
    "output": "つまりx1とx2は2つの可能な値のどちらかしかとれない。"
  },
  {
    "index": "F15157",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this example, I've drawn only two positive examples and two negative examples. That you can think of this as a simplified version of a more complex learning problem where we may have a bunch of positive examples in the upper right and lower left and a bunch of negative examples denoted by the circles.",
    "output": "この例では、2つの陽性の手本と2つの陰性の手本だけを描いたが、これを、より複雑な学習問題を単純化した物と考える事が出来て、その複雑な問題では、たくさんの陽性の手本が右上と左下にあり、そしてたくさんの陰性の手本が丸で示されていて、やりたい事は非線型の陽性と陰性の手本を分離する為の決定境界を学習させたい、という物。"
  },
  {
    "index": "F15158",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we'd like to do is learn a non-linear division of boundary that may need to separate the positive and negative examples.",
    "output": "では、どのようにニューラルネットワークがこれを行う事が出来るだろうか?そこで右の例をそのまま使うのでは無く、この左側の、より簡単に精査出来る例を使っていく。"
  },
  {
    "index": "F15159",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how can a neural network do this and rather than using the example and the variable to use this maybe easier to examine example on the left. Concretely what this is, is really computing the type of label y equals x 1 x or x 2.",
    "output": "具体的には、これが実際になんなのかというと、ターゲットのラベルyを計算する物で、それはイコールx1XORx2だ。"
  },
  {
    "index": "F15160",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that these specific examples in the works out a little bit better if we use the XNOR example instead.",
    "output": "具体例としては実はXNORを使う方がちょっとだけ良い事が判明している。"
  },
  {
    "index": "F15161",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "These two are the same of course.",
    "output": "これら二つはもちろん等しい。"
  },
  {
    "index": "F15162",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This means not x1 or x2 and so, we're going to have positive examples of either both are true or both are false and what have as y equals 1, y equals 1.",
    "output": "そしてどちらかだけがtrueの時はy=0となる。"
  },
  {
    "index": "F15163",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we're going to have y equals 0 if only one of them is true and we're going to figure out if we can get a neural network to fit to this sort of training set.",
    "output": "そして我らはこの種のトレーニングセットにフィットするようなニューラルネットワークが得られるのか?という事を見出したい。"
  },
  {
    "index": "F15164",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to build up to a network that fits the XNOR example we're going to start with a slightly simpler one and show a network that fits the AND function.",
    "output": "xnorの例に適合するネットワークを構築する為に、もう少し簡単な例であるAND関数に適合するネットワークを見る事から始めて行こう。"
  },
  {
    "index": "F15165",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, let's say we have input x1 and x2 that are again binaries so, it's either 0 or 1 and let's say our target labels y = x1 AND x2.",
    "output": "つまりそれは0か1だ。そしてターゲットのラベルyはイコールx1ANDx2としよう。"
  },
  {
    "index": "F15166",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is a logical AND.",
    "output": "これは論理積のANDだ。"
  },
  {
    "index": "F15167",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, can we get a one-unit network to compute this logical AND function?",
    "output": "さて、一ユニットのネットワークで、このAND関数を計算する物が得られるだろうか?"
  },
  {
    "index": "F15168",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to do so, I'm going to actually draw in the bias unit as well the plus one unit.",
    "output": "それを行う為に、バイアスユニットも描いておく、この+1のユニットを。"
  },
  {
    "index": "F15169",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let me just assign some values to the weights or parameters of this network.",
    "output": "いま、単に適当な値をこのネットワークのウェイト、あるいはパラメータに割り振ってみよう。"
  },
  {
    "index": "F15170",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm gonna write down the parameters on this diagram here, -30 here.",
    "output": "この図にパラメータを書きこんでいく。"
  },
  {
    "index": "F15171",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what this mean is just that I'm assigning a value of -30 to the value associated with X0 this +1 going into this unit and a parameter value of +20 that multiplies to X1 a value of +20 for the parameter that multiplies into x 2.",
    "output": "このx0は+1で、このユニットになる。そしてパラメータの値+20は、x1に掛ける物で、そして+20はx2に掛けるパラメータだ。"
  },
  {
    "index": "F15172",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, concretely it's the same that the hypothesis h(x)=g(-30+20 X1 plus 20 X2.",
    "output": "具体的には、これは仮説hのxがイコールg(-30+20x1+20x2)だと言っている訳だ。"
  },
  {
    "index": "F15173",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, sometimes it's just convenient to draw these weights. Draw these parameters up here in the diagram within and of course this- 30.",
    "output": "つまり、こういう風にニューラルネットワークの図の上にこれらのウェイト、パラメータを、書きこむコンベンションもある、という事だ。"
  },
  {
    "index": "F15174",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is theta 1 of 1 1 and that's theta 1 of 1 2 but it's just easier to think about it as associating these parameters with the edges of the network.",
    "output": "そしてこれはシータ1の1,2。だがそうするよりも、これらのパラメータがネットワークのエッジに関連づけられている、と考える方が分かりやすい。"
  },
  {
    "index": "F15175",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at what this little single neuron network will compute.",
    "output": "ではこの小さな一つのニューロンのネットワークが何を計算しているか、見ていこう。"
  },
  {
    "index": "F15176",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just to remind you the sigmoid activation function g(z) looks like this.",
    "output": "思い出せるように、sigmoidのアクティベーション関数であるgのzはこんな物だった。"
  },
  {
    "index": "F15177",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It starts from 0 rises smoothly crosses 0.5 and then it asymptotic as 1 and to give you some landmarks, if the horizontal axis value z is equal to 4.6 then the sigmoid function is equal to 0.99.",
    "output": "それは0から始まり、スムースに上昇していき、0.5で交わり、そして1に漸近していく。目印になりそうな点を置いておくと、横軸の値、zがイコール4.6だと、sigmoid関数はイコール0.99となる。"
  },
  {
    "index": "F15178",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is very close to 1 and kind of symmetrically, if it's -4.6 then the sigmoid function there is 0.01 which is very close to 0.",
    "output": "これはとても1に近い。そしてある種対称的な位置として-4.6の時は、sigmoid関数はイコール0.01となる、これはとても0に近い。"
  },
  {
    "index": "F15179",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at the four possible input values for x1 and x2 and look at what the hypotheses will output in that case.",
    "output": "x1とx2の、4つの可能な入力値の組み合わせを見ていこう、そしてそれぞれの場合に仮説が何を出力するかを見ていこう。"
  },
  {
    "index": "F15180",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If x1 and x2 are both equal to 0.",
    "output": "x1とx2がどちらも0なら、これを見ると、x1とx2がどちらもイコール0なら、その場合は仮説はgの-30の点となる。"
  },
  {
    "index": "F15181",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you look at this, if x1 x2 are both equal to 0 then the hypothesis of g of -30.",
    "output": "これはとても0に近い。もしx1=0でx2=1なら、この式は評価するとgとなる、つまりsigmoid関数が-10に適用される。"
  },
  {
    "index": "F15182",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is a very far to the left of this diagram so it will be very close to 0.",
    "output": "そしてこれもまた、このプロット上では左の遥か彼方なので、これもまたとても0に近い。"
  },
  {
    "index": "F15183",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If x 1 equals 0 and x equals 1, then this formula here evaluates the g that is the sigma function applied to -10, and again that's you know to the far left of this plot and so, that's again very close to 0.",
    "output": "これもまたgの-10となる。"
  },
  {
    "index": "F15184",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is also g of minus 10 that is f x 1 is equal to 1 and x 2 0, this minus 30 plus 20 which is minus 10 and finally if x 1 equals 1 x 2 equals 1 then you have g of minus 30 plus 20 plus 20.",
    "output": "つまりx1がイコール1で、x2が0だと、これは-30+20で、-10。"
  },
  {
    "index": "F15185",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's g of positive 10 which is there for very close to 1.",
    "output": "そして最後に、x1=1でx2=1の時は、その時はgの-30+20+20で、つまりそれはgの+10となる、それはとても1に近い値となる。"
  },
  {
    "index": "F15186",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you look in this column this is exactly the logical and function.",
    "output": "そしてこの列を見てみると、これは論理積のAND関数だ。"
  },
  {
    "index": "F15187",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is computing h of x is approximately x 1 and x 2.",
    "output": "つまり、これはhのxは、だいたいx1ANDx2を計算している。"
  },
  {
    "index": "F15188",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In other words it outputs one If and only if x2, x1 and x2, are both equal to 1.",
    "output": "言い換えると、それはx1とx2の両方が1の時にだけ、1を出力する。"
  },
  {
    "index": "F15189",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, by writing out our little truth table like this we manage to figure what's the logical function that our neural network computes.",
    "output": "そこで簡単な真理値表を書いてみると、我らのニューラルネットワークが計算してる論理関数が何か、が分かりやすい。"
  },
  {
    "index": "F15190",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This network showed here computes the OR function.",
    "output": "ここのネットワークはOR関数を計算している。"
  },
  {
    "index": "F15191",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you are write out the hypothesis that this confusing g of -10 + 20 x 1 + 20 x 2 and so you fill in these values.",
    "output": "どう機能するか見てみよう。仮説を書き下してみればそれが計算する事はgの-10+20x1+20x2だ。"
  },
  {
    "index": "F15192",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You find that's g of minus 10 which is approximately 0.",
    "output": "つまり、もしこれらの値を埋めれば、gの-10はだいたい0となり、gの10はだいたい1、という感じになる。"
  },
  {
    "index": "F15193",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "g of 10 which is approximately 1 and so on and these are approximately 1 and approximately 1 and these numbers are essentially the logical OR function.",
    "output": "これらはだいたい1、そしてだいたい1、そしてこれらの数は本質的には論理和のOR関数となっている。"
  },
  {
    "index": "F15194",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, hopefully with this you now understand how single neurons in a neural network can be used to compute logical functions like AND and OR and so on.",
    "output": "以上で、いまやニューラルネットワークの一つのニューロンが、論理関数である所のANDとかORを計算するのにどう使えるのか、分かったんじゃないかなぁ。"
  },
  {
    "index": "F15195",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video we'll continue building on these examples and work through a more complex example.",
    "output": "次のビデオでは、引き続きこれらの例を構築していき、さらにより複雑な例を見ていく。"
  },
  {
    "index": "F15196",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We'll get to show you how a neural network now with multiple layers of units can be used to compute more complex functions like the XOR function or the XNOR function.",
    "output": "そこではニューラルネットワークがどのように、ユニットの複数のレイヤーで、もっと複雑な関数を計算する事が出来るのかを見ていく、xor関数とかxnor関数のような。"
  },
  {
    "index": "F15197",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I'd like to keep working through our example to show how a Neural Network can compute complex non linear hypothesis.",
    "output": "このビデオではニューラルネットワークがどのように複雑な非線形の仮説を計算するかを見ていく。"
  },
  {
    "index": "F15198",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last video we saw how a Neural Network can be used to compute the functions x1 AND x2, and the function x1 OR x2 when x1 and x2 are binary, that is when they take on values 0,1.",
    "output": "前回のビデオではニューラルネットワークがどのように関数x1ANDx2とか、関数x1ORx2を計算するかを見てきた、ここでx1とx2は二値の値をとる。つまり、取りうる可能な値は0か1だけ。"
  },
  {
    "index": "F15199",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can also have a network to compute negation, that is to compute the function not x1.",
    "output": "また、我らは否定を計算するネットワークも持ちうる。つまりNOTx1関数。"
  },
  {
    "index": "F15200",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me just write down the ways associated with this network.",
    "output": "このネットワークを書き出してみよう。"
  },
  {
    "index": "F15201",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We have only one input feature x1 in this case and the bias unit +1.",
    "output": "たった一つだけの入力フィーチャーx1がこの場合にはあり、そしてバイアスユニットとして+1がある。"
  },
  {
    "index": "F15202",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if I associate this with the weights plus 10 and -20, then my hypothesis is computing this h(x) equals sigmoid (10- 20 x1).",
    "output": "そしてこれに、ウェイト+10と-20を付与すると、仮説はこれを計算する事になる。hのxは、イコール、sigmoid関数の10-20*x1となる。"
  },
  {
    "index": "F15203",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when x1 is equal to 0, my hypothesis would be computing g(10- 20 x 0) is just 10.",
    "output": "つまり、x1が0の時には仮説はgの10-20*0を計算する事になり、これはgの10となる。"
  },
  {
    "index": "F15204",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's approximately 1, and when x is equal to 1, this will be g(-10) which is approximately equal to 0.",
    "output": "そしてxがイコール1の時はgの-10となりこれはだいたい0となる。"
  },
  {
    "index": "F15205",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you look at what these values are, that's essentially the not x1 function.",
    "output": "そしてこれらの値が何なのかを眺めてみると、これはようするにNOTx1関数だ。"
  },
  {
    "index": "F15206",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Cells include negations, the general idea is to put that large negative weight in front of the variable you want to negate.",
    "output": "つまり否定を行うには基本的には負の大きなウェイトを否定したい変数の前におけば良い。"
  },
  {
    "index": "F15207",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Minus 20 multiplied by x1 and that's the general idea of how you end up negating x1.",
    "output": "この場合は-20を、x1に掛けてる。これがx1を否定するやり方の基本的な考え方。"
  },
  {
    "index": "F15208",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so in an example that I hope that you can figure out yourself.",
    "output": "以上で、例えば以下のような関数を計算したい時に自力でやり方を見つけ出せるだろう:(NOTx1)AND(NOTx2)解答の一部分としては、たぶん負の大きなウェイトをx1とx2に置く事になるだろう、そしてそれを出力が一つだけのニューラルネットワークを得る為に食わせる。"
  },
  {
    "index": "F15209",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you want to compute a function like this NOT x1 AND NOT x2, part of that will probably be putting large negative weights in front of x1 and x2, but it should be feasible.",
    "output": "いいかい?つまりこの大きな論理関数、(NOTx1)AND(NOTx2)がイコール1となるのはx1=x2=0の時で、その時のみ1となる。"
  },
  {
    "index": "F15210",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you get a neural network with just one output unit to compute this as well. All right, so this logical function, NOT x1 AND NOT x2, is going to be equal to 1 if and only if x1 equals x2 equals 0.",
    "output": "つまり、これは論理関数で、これはNOTx1、つまりX1は0でなくてはいけない、という意味で、さらにNOTx2。"
  },
  {
    "index": "F15211",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "All right since this is a logical function, this says NOT x1 means x1 must be 0 and NOT x2, that means x2 must be equal to 0 as well.",
    "output": "それはx2もまた0でなくてはならない、という意味だ。"
  },
  {
    "index": "F15212",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this logical function is equal to 1 if and only if both x1 and x2 are equal to 0 and hopefully you should be able to figure out how to make a small neural network to compute this logical function as well.",
    "output": "だからこの論理関数が1となるのは、x1とx2の両方がイコール0の時だけだ。以上で、この論理関数を計算する小さなニューラルネットワークをどうやって構築すれば良いかも分かるだろう。"
  },
  {
    "index": "F15213",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, taking the three pieces that we have put together as the network for computing x1 AND x2, and the network computing for computing NOT x1 AND NOT x2.",
    "output": "さて、今度はこの三つの部品を一つに組み合わせて、つまりx1ANDx2を計算するネットワークと(NOTx1)AND(NOTx2)を計算するネットワークと、最後はx1ORx2を計算する為のネットワーク。"
  },
  {
    "index": "F15214",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And one last network computing for computing x1 OR x2, we should be able to put these three pieces together to compute this x1 XNOR x2 function.",
    "output": "我らはこれら三つの要素を組み合わせる事により、このx1XNORx2関数が計算出来るはずだ。"
  },
  {
    "index": "F15215",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just to remind you if this is x1, x2, this function that we want to compute would have negative examples here and here, and we'd have positive examples there and there.",
    "output": "ここで再掲しておくと、これがx1とx2とすると、我らがこれから計算しようとしているこの関数は陰性の手本がこことここに、そして陽性の手本がこことここにあるような物だった。"
  },
  {
    "index": "F15216",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so clearly this will need a non linear decision boundary in order to separate the positive and negative examples.",
    "output": "これから、明らかに陽性と陰性の手本を分離するには非線型の決定境界が要る。"
  },
  {
    "index": "F15217",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's draw the network.",
    "output": "ネットワークを書き下してみよう。"
  },
  {
    "index": "F15218",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to take my input +1, x1, x2 and create my first hidden unit here.",
    "output": "入力として、+1,x1,x2を取り、そして最初の隠れユニットをここに作る。"
  },
  {
    "index": "F15219",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm gonna call this a 21 cuz that's my first hidden unit.",
    "output": "これをa(2)1と呼ぶ事にする、何故ならこれは最初の隠れユニットだから。"
  },
  {
    "index": "F15220",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm gonna copy the weight over from the red network, the x1 and x2.",
    "output": "そしてウェイトを赤のネットワークから、つまりx1ANDx2のネットワークからコピーする。"
  },
  {
    "index": "F15221",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As well so then -30, 20, 20.",
    "output": "つまり-30,20,20。"
  },
  {
    "index": "F15222",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next let me create a second hidden unit which I'm going to call a 2 2.",
    "output": "次に、二番目の隠れユニットを作ろう。それをa(2)2と呼ぶ事にする。"
  },
  {
    "index": "F15223",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is the second hidden unit of layer two.",
    "output": "これはレイヤー2の二番目の隠れユニットだ。"
  },
  {
    "index": "F15224",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, let's pull some of the truth table values.",
    "output": "ここで真理値表から値をひっぱってこよう。"
  },
  {
    "index": "F15225",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For the red network, we know that was computing the x1 and x2, and so this will be approximately 0 0 0 1, depending on the values of x1 and x2, and for a 2 2, the cyan network.",
    "output": "赤いネットワークはx1ANDx2を計算している事を知っている。つまりこれは、だいたい0,0,0,1となる、x1とx2の値に応じて。"
  },
  {
    "index": "F15226",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The function NOT x1 AND NOT x2, that outputs 1 0 0 0, for the 4 values of x1 and x2.",
    "output": "そしてa(2)2については、シアン色のネットワークで、これは(NOTx1)AND(NOTx2)関数だと知っているから、出力は4つのx1とx2の入力に対して1,0,0,0となる。"
  },
  {
    "index": "F15227",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, I'm going to create my output node, my output unit that is a 3 1.",
    "output": "最後に、出力ノードを作る。"
  },
  {
    "index": "F15228",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is one more output h(x) and I'm going to copy over the old network for that.",
    "output": "これはh(x)の出力で、ここにはORのネットワークからコピーして、ここにはバイアスユニットの+1が必要だ。"
  },
  {
    "index": "F15229",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm going to need a +1 bias unit here, so you draw that in, And I'm going to copy over the weights from the green networks.",
    "output": "だからそれを書きこむ、緑のネットワークからウェイトをコピーする。"
  },
  {
    "index": "F15230",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's -10, 20, 20 and we know earlier that this computes the OR function.",
    "output": "つまり、-10,20,20で、これがOR関数を実装する事を前に見ている。"
  },
  {
    "index": "F15231",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's fill in the truth table entries.",
    "output": "では真理値表を見ていこう。"
  },
  {
    "index": "F15232",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the first entry is 0 OR 1 which can be 1 that makes 0 OR 0 which is 0, 0 OR 0 which is 0, 1 OR 0 and that falls to 1.",
    "output": "最初のエントリでは、0OR1だから1となる。次は0OR0だから、0となり、0OR0は0。"
  },
  {
    "index": "F15233",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And thus h(x) is equal to 1 when either both x1 and x2 are zero or when x1 and x2 are both 1 and concretely h(x) outputs 1 exactly at these two locations and then outputs 0 otherwise.",
    "output": "1OR0は1で、つまり、hのxがイコール1となるのは、x1とx2がどちらも0の時か、またはx1とx2がどちらも1の時だけだ。具体的には、hのxはこれら二つの位置の時にはぴったり1を出力し、それ以外の場合は0を出力する。"
  },
  {
    "index": "F15234",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And thus will this neural network, which has a input layer, one hidden layer, and one output layer, we end up with a nonlinear decision boundary that computes this XNOR function.",
    "output": "かくして、このニューラルネットワークでもって、それは入力レイヤーに1つの隠れレイヤーと1つの出力レイヤーがあるような物だが、これでXNOR関数のような非線型の決定境界を計算出来る、このような関数を。"
  },
  {
    "index": "F15235",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the more general intuition is that in the input layer, we just have our four inputs.",
    "output": "そしてより一般的には直感的には、入力レイヤーには単に生の入力を置いて、そして隠れレイヤーがあり、そこでは、ここに示したもうちょっとだけ複雑な入力の関数を計算している。"
  },
  {
    "index": "F15236",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then we have a hidden layer, which computed some slightly more complex functions of the inputs that its shown here this is slightly more complex functions. And then by adding yet another layer we end up with an even more complex non linear function.",
    "output": "これらはさらにちょっとだけ複雑な関数で、さらにもう一つレイヤーを追加する事で、さらに複雑な非線型の関数も計算する事が出来る。"
  },
  {
    "index": "F15237",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is a sort of intuition about why neural networks can compute pretty complicated functions.",
    "output": "以上が、ニューラルネットワークを用いると、極めて複雑な関数を計算しうるかの直感的な説明だ。"
  },
  {
    "index": "F15238",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That when you have multiple layers you have relatively simple function of the inputs of the second layer. But the third layer I can build on that to complete even more complex functions, and then the layer after that can compute even more complex functions.",
    "output": "複数のレイヤーがある時に、二番目のレイヤーは比較的単純な入力の関数でも、三番目のレイヤーを足す事でさらに複雑な関数を計算する為にネットワークを作りあげる事が出来、そしてその次のレイヤーで、さらに複雑な関数を計算する事も出来る。"
  },
  {
    "index": "F15239",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To wrap up this video, I want to show you a fun example of an application of a the Neural Network that captures this intuition of the deeper layers computing more complex features.",
    "output": "このビデオのまとめとして、楽しい例を紹介したい、これはニューラルネットワークを適用した例で、より深い所のレイヤーがより複雑な計算をしていく、という直感を捉えた物となっている。"
  },
  {
    "index": "F15240",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I want to show you a video of that customer a good friend of mine Yann LeCunj.",
    "output": "私がお見せするビデオは私の良き友人、YonKhunからもらった物だ。"
  },
  {
    "index": "F15241",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Yann is a professor at New York University, NYU and he was one of the early pioneers of Neural Network reasearch and is sort of a legend in the field now and his ideas are used in all sorts of products and applications throughout the world now.",
    "output": "YonはNewYorkUniversity、NYUの教授だ。彼はニューラルネットワークの初期の頃の研究のパイオニアであり、今ではこの分野での、ある種伝説になった男で、彼のアイデアは今や世界中のあらゆる所であらゆる製品、アプリケーションに使われている。"
  },
  {
    "index": "F15242",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I wanna show you a video from some of his early work in which he was using a neural network to recognize handwriting, to do handwritten digit recognition.",
    "output": "さて、私があなたにお見せするのは、彼の初期の頃の仕事の一つ、手書き認識の為にニューラルネットワークを用いる、という物だ。手書きの数字認識の為に。"
  },
  {
    "index": "F15243",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You might remember early in this class, at the start of this class I said that one of the earliest successes of neural networks was trying to use it to read zip codes to help USPS Laws and read postal codes.",
    "output": "このクラスの最初の方で、このクラスの最初で、最初期のニューラルネットワークの成功した使い道はそれをzipコード(郵便番号)を読むのに用いるという物だった、と言った。郵便を送る助けとなるように、郵便番号を読む為に。"
  },
  {
    "index": "F15244",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is one of the attempts, this is one of the algorithms used to try to address that problem.",
    "output": "つまり、これがその試みの一種だ。つまりこれこそがあの問題を解決しようとするアルゴリズムの一種だ。"
  },
  {
    "index": "F15245",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the video that I'll show you this area here is the input area that shows a canvasing character shown to the network.",
    "output": "ビデオでは、みえているこの、ここの領域、これがネットワークに入力する手書きの文字がみえている領域だ。"
  },
  {
    "index": "F15246",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This column here shows a visualization of the features computed by sort of the first hidden layer of the network. So that the first hidden layer of the network and so the first hidden layer, this visualization shows different features.",
    "output": "このここの列はネットワークの最初の隠れレイヤーによる計算結果のフィーチャーを可視化した物を表示している、つまり最初の隠れレイヤだ。"
  },
  {
    "index": "F15247",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Different edges and lines and so on detected.",
    "output": "この可視化は別々のフィーチャーを表示している、検出された様々なエッジや線を。"
  },
  {
    "index": "F15248",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is a visualization of the next hidden layer.",
    "output": "これは次の隠れレイヤーを可視化した物だ。"
  },
  {
    "index": "F15249",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's kinda harder to see, harder to understand the deeper, hidden layers, and that's a visualization of why the next hidden layer is confusing.",
    "output": "より深い位置の隠れレイヤーはどう解釈したらいいか難しい。そしてこれが、次の隠れレイヤーが計算している物を可視化した物だ。"
  },
  {
    "index": "F15250",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You probably have a hard time seeing what's going on much beyond the first hidden layer, but then finally, all of these learned features get fed to the upper layer.",
    "output": "最初の隠れレイヤーよりもずっと分かりにくい。だが、最後に、これらの学習されたフィーチャーを全て出力レイヤーに食わせて、ここに最終的な答えを表示する。"
  },
  {
    "index": "F15251",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And shown over here is the final answer, it's the final predictive value for what handwritten digit the neural network thinks it is being shown.",
    "output": "最終的な、ニューラルネットワークが見た手書き文字が何なのかの予測を。"
  },
  {
    "index": "F15252",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's take a look at the video.",
    "output": "では、ビデオを見てみよう。"
  },
  {
    "index": "F15253",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I hope you enjoyed the video and that this hopefully gave you some intuition about the source of pretty complicated functions neural networks can learn.",
    "output": "さて、お楽しみいただけただろうか。このビデオでニューラルネットワークが学習出来る極めて複雑な関数がどんな感じかの感覚をつかめただろうか。"
  },
  {
    "index": "F15254",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And these features can then be used by essentially the final layer of the logistic classifiers to make accurate predictions without the numbers that the network sees.",
    "output": "そこでは、この入力画像を取り、この生のピクセルを入力に取り、そして最初のレイヤーの終わりまでに何らかのフィーチャーの集合を計算し、次のレイヤーの終わりまでにさらに複雑なフィーチャーを計算し、さらに複雑なフィーチャー、これらのフィーチャーは最後の、本質的にはロジスティック回帰の分類器の入力として使われ、その分類器が、ネットワークが見た数字がなんなのかを正確に予測するのに使われる。"
  },
  {
    "index": "F15255",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I want to tell you about how to use neural networks to do multiclass classification where we may have more than one category that we're trying to distinguish amongst.",
    "output": "このビデオでは、ニューラルネットワークを用いてマルチクラスの分類問題を行う方法を話す。そこでは、1つより多くのカテゴリがあって、それらのどれかを区別したい。"
  },
  {
    "index": "F15256",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last part of the last video, where we had the handwritten digit recognition problem, that was actually a multiclass classification problem because there were ten possible categories for recognizing the digits from 0 through 9 and so, if you want us to fill you in on the details of how to do that.",
    "output": "前回のビデオの最後の所で、手書き数字認識の問題を見た、あれは実の所、マルチクラスの分類問題だ。何故なら10個の可能なカテゴリがあり、そのどれかを認識したい、という問題だったから、0から9までの。"
  },
  {
    "index": "F15257",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The way we do multiclass classification in a neural network is essentially an extension of the one versus all method.",
    "output": "ニューラルネットワークでマルチクラスの分類を行う方法は、本質的には1vsALL法の拡張だ。"
  },
  {
    "index": "F15258",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's say that we have a computer vision example, where instead of just trying to recognize cars as in the original example that I started off with, but let's say that we're trying to recognize, you know, four categories of objects and given an image we want to decide if it is a pedestrian, a car, a motorcycle or a truck.",
    "output": "コンピュータビジョンの例を考える、今回は始めにやった車だけを認識する、という例の代わりに、4つのカテゴリの物体を認識したいとする。画像が与えられた時にそれが歩行者か、車か、バイクか、トラックかを決めたい。"
  },
  {
    "index": "F15259",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If that's the case, what we would do is we would build a neural network with four output units so that our neural network now outputs a vector of four numbers.",
    "output": "その場合には、我らがやる事としては、4つの出力ユニットのあるニューラルネットワークを構築する、という事が考えられる、我らのニューラルネットワークが4つの数のベクトルを出力するように。"
  },
  {
    "index": "F15260",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, the output now is actually needing to be a vector of four numbers and what we're going to try to do is get the first output unit to classify: is the image a pedestrian, yes or no.",
    "output": "つまり、今回は出力が実際に4つの数のベクトルである必要がある。"
  },
  {
    "index": "F15261",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The second unit to classify: is the image a car, yes or no.",
    "output": "そしてやろうとしている事は、最初の出力ユニットに画像が歩行者かどうかをyesかnoで分類させ、二番目のユニットに画像が車かどうかをyesかnoで分類させ、このユニットには画像がバイクかどうかをyesまたはnoで分類させ、そしてこれに、画像がトラックかどうかをyesかnoで分類させる。"
  },
  {
    "index": "F15262",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This unit to classify: is the image a motorcycle, yes or no, and this would classify: is the image a truck, yes or no.",
    "output": "かくして、歩行者の画像の時には、理想的にはネットワークに1,0,0,0を出力して欲しい。車の時には0,1,0,0を出力して欲しい。"
  },
  {
    "index": "F15263",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And thus, when the image is of a pedestrian, we would ideally want the network to output 1, 0, 0, 0, when it is a car we want it to output 0, 1, 0, 0, when this is a motorcycle, we get it to or rather, we want it to output 0, 0, 1, 0 and so on.",
    "output": "これがバイクの時には、0,0,1,0が得られる、というか出力して欲しい、などなど。"
  },
  {
    "index": "F15264",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is just like the \"one versus all\" method that we talked about when we were describing logistic regression, and here we have essentially four logistic regression classifiers, each of which is trying to recognize one of the four classes that we want to distinguish amongst.",
    "output": "つまりこれは、ロジスティック回帰の時にやった1vsALL法にすぎない。"
  },
  {
    "index": "F15265",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, rearranging the slide of it, here's our neural network with four output units and those are what we want h of x to be when we have the different images, and the way we're going to represent the training set in these settings is as follows.",
    "output": "そしてここでは、ようするに4つのロジスティック回帰の分類器があり、各分類器はおのおの、見分けたい4つのクラスの一つを認識しようと試みる。"
  },
  {
    "index": "F15266",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, when we have a training set with different images of pedestrians, cars, motorcycles and trucks, what we're going to do in this example is that whereas previously we had written out the labels as y being an integer from 1, 2, 3 or 4.",
    "output": "では、スライドをちょっと並べ替えて、これは4つの出力ユニットを持ったニューラルネットワークで、そしてこれらは、それぞれの画像の時にhのxに期待する物だ。このような前提でトレーニングセットを表す方法は、以下のようになる。"
  },
  {
    "index": "F15267",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so one training example will be one pair Xi colon Yi where Xi is an image with, you know one of the four objects and Yi will be one of these vectors.",
    "output": "つまり一つのトレーニング手本としては、xi,yiのペアとなり、xiは4つの物体のどれかの画像で、そしてyiはこれらのベクトルのどれか一つ。"
  },
  {
    "index": "F15268",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And hopefully, we can find a way to get our Neural Networks to output some value.",
    "output": "そして望むらくは、我らのニューラルネットワークが以下のような、何らかの値を出力する方法を見つける事だ。"
  },
  {
    "index": "F15269",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, the h of x is approximately y and both h of x and Yi, both of these are going to be in our example, four dimensional vectors when we have four classes.",
    "output": "それは、hのxがだいたいyとなるような物、ここでhのxとyiは両方とも、我らの例では、4つのクラスがある時には4つの次元となる。"
  },
  {
    "index": "F15270",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's how you get neural network to do multiclass classification.",
    "output": "以上がマルチクラスの分類問題をどうニューラルネットワークにやらせるか、だ。"
  },
  {
    "index": "F15271",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This wraps up our discussion on how to represent Neural Networks that is on our hypotheses representation.",
    "output": "この話で我らの仮説をニューラルネットワークでどう表現するか、をまとめられたと思う。"
  },
  {
    "index": "F15272",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next set of videos, let's start to talk about how take a training set and how to automatically learn the parameters of the neural network.",
    "output": "続く一連のビデオでは、どうトレーニングセットを取るか、そしてどうやってニューラルネットワークの仮説のパラメータを自動的に学習するかを議論したい。"
  },
  {
    "index": "F15273",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Neural networks are one of the most powerful learning algorithms that we have today.",
    "output": "ニューラルネットワークはこんにち使える中でもっとも強力な学習アルゴリズムの一つだ。"
  },
  {
    "index": "F15274",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this and in the next few videos, I'd like to start talking about a learning algorithm for fitting the parameters of a neural network given a training set.",
    "output": "このビデオとその後の一連のビデオで所与のトレーニングセットに対してニューラルネットワークのパラメータをフィッティングする為の学習アルゴリズムの話をしていく。"
  },
  {
    "index": "F15275",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As with the discussion of most of our learning algorithms, we're going to begin by talking about the cost function for fitting the parameters of the network.",
    "output": "ほとんどの学習アルゴリズムの議論と同様、ニューラルネットワークのパラメータのフィッティングでもコスト関数から始める事にする。"
  },
  {
    "index": "F15276",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to focus on the application of neural networks to classification problems.",
    "output": "ニューラルネットワークの分類問題への応用に私はフォーカスしたいと思う。"
  },
  {
    "index": "F15277",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So suppose we have a network like that shown on the left.",
    "output": "左に描いてあるようなネットワークがあったとする。"
  },
  {
    "index": "F15278",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And suppose we have a training set like this is x I, y I pairs of M training example.",
    "output": "そしてこんな感じのトレーニングセット、xiとyiのペアがm個あるようなのがあったとする。"
  },
  {
    "index": "F15279",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to use upper case L to denote the total number of layers in this network.",
    "output": "大文字のLをネットワークに存在するレイヤーの総数を表すのに使う。"
  },
  {
    "index": "F15280",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for the network shown on the left we would have capital L equals 4.",
    "output": "例えば左に見えるネットワークの場合、大文字のLはイコール4だ。"
  },
  {
    "index": "F15281",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to use S subscript L to denote the number of units, that is the number of neurons.",
    "output": "そしてsの下付き添字lでユニットの総数を表す事にする。"
  },
  {
    "index": "F15282",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Not counting the bias unit in their L of the network.",
    "output": "それはネットワークのレイヤーlに存在するニューロンの総数のうち、バイアスユニットは含めない数だ。"
  },
  {
    "index": "F15283",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for example, we would have a S one, which is equal there, equals S three unit, S two in my example is five units.",
    "output": "例えば、s1と言えば、入力レイヤーの事でイコール3ユニット。s2はこの例だと5ユニット。"
  },
  {
    "index": "F15284",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the output layer S four, which is also equal to S L because capital L is equal to four.",
    "output": "そして出力レイヤのs4はそれはsLとも等しい。というのは大文字のLは4だから。"
  },
  {
    "index": "F15285",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The output layer in my example under that has four units.",
    "output": "この例における出力レイヤは4ユニットとなっている。"
  },
  {
    "index": "F15286",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to consider two types of classification problems.",
    "output": "2つの種類の分類問題を扱っていく事になる。"
  },
  {
    "index": "F15287",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first is Binary classification, where the labels y are either 0 or 1.",
    "output": "一つ目はバイナリ分類、それはyの取りうる値は0か1のどちらかだけの場合。"
  },
  {
    "index": "F15288",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this case, we will have 1 output unit, so this Neural Network unit on top has 4 output units, but if we had binary classification we would have only one output unit that computes h(x).",
    "output": "この場合は出力ユニットは一つだけとなる。"
  },
  {
    "index": "F15289",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the output of the neural network would be h(x) is going to be a real number.",
    "output": "この上にあるニューラルネットワークの場合は4つの出力ユニットがあるが、バイナリ分類なら、出力ユニットは一つだけで、それはhのxを計算する。"
  },
  {
    "index": "F15290",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this case the number of output units, S L, where L is again the index of the final layer.",
    "output": "そしてそのニューラルネットワークの出力は、つまりhのxは実数となる。"
  },
  {
    "index": "F15291",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Cuz that's the number of layers we have in the network so the number of units we have in the output layer is going to be equal to 1.",
    "output": "そしてこのケースでは、出力ユニットの総数sLはところでLは最後のレイヤのインデックスなのでというのはLはそのネットワークのレイヤーの総数だからだが、すると出力レイヤに存在するユニットの総数は、1に等しい。"
  },
  {
    "index": "F15292",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this case to simplify notation later, I'm also going to set K=1 so you can think of K as also denoting the number of units in the output layer.",
    "output": "この場合、あとでノーテーションを簡素化する為、K=1とも書く事にする。つまりKもまた、出力レイヤのユニットの総数を表す訳だ。"
  },
  {
    "index": "F15293",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The second type of classification problem we'll consider will be multi-class classification problem where we may have K distinct classes.",
    "output": "2つ目のタイプの分類問題はマルチクラスの分類問題だ。それは、K個の異なる分類がある問題。"
  },
  {
    "index": "F15294",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So our early example had this representation for y if we have 4 classes, and in this case we will have capital K output units and our hypothesis or output vectors that are K dimensional.",
    "output": "これは4つのクラスがある場合だった。このタイプでは、大文字のK個だけの出力ユニットがあり、我らの仮説はK次元のベクトルを出力することになる。"
  },
  {
    "index": "F15295",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the number of output units will be equal to K.",
    "output": "そして出力ユニットの総数はKと等しくなり、このKは通常は、3以上なのがこのタイプとなる。"
  },
  {
    "index": "F15296",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And usually we would have K greater than or equal to 3 in this case, because if we had two causes, then we don't need to use the one verses all method.",
    "output": "何故なら、もし2クラスしか無い時はonevsall法を使う必要が無いからだ。"
  },
  {
    "index": "F15297",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We use the one verses all method only if we have K greater than or equals V classes, so having only two classes we will need to use only one upper unit.",
    "output": "onevsall法を使わなくてはいけないのはKが3以上の時だけなので2つしかクラスが無い時は一つの出力ユニットしか必要としない。"
  },
  {
    "index": "F15298",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's define the cost function for our neural network.",
    "output": "ではここで、ニューラルネットワークのコスト関数を定義しよう。"
  },
  {
    "index": "F15299",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The cost function we use for the neural network is going to be a generalization of the one that we use for logistic regression.",
    "output": "ニューラルネットワークで使うコスト関数は、ロジスティック回帰で使った物を一般化した物だ。"
  },
  {
    "index": "F15300",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For logistic regression we used to minimize the cost function J(theta) that was minus 1/m of this cost function and then plus this extra regularization term here, where this was a sum from J=1 through n, because we did not regularize the bias term theta0.",
    "output": "ロジスティック回帰ではコスト関数Jのシータを最小化したのだった。そのJのシータとは-1/mのこのコスト関数にこの追加の正規化の項を足した物だった。"
  },
  {
    "index": "F15301",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For a neural network, our cost function is going to be a generalization of this.",
    "output": "何故ならバイアス項であるシータ0は正規化しないからだった。ニューラルネットワーク向けにはコスト関数はこれを一般化したものとなる。"
  },
  {
    "index": "F15302",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where instead of having basically just one, which is the compression output unit, we may instead have K of them.",
    "output": "そこでは基本的には単に一つのロジスティック回帰の出力ユニットしか無いのではなく、その代わりにK個のユニットがある訳だ。"
  },
  {
    "index": "F15303",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Our new network now outputs vectors in R K where R might be equal to 1 if we have a binary classification problem.",
    "output": "ニューラルネットワークはRのKのベクトルを出力する。ここでKは1となる事もある、その時はバイナリ分類問題という事。"
  },
  {
    "index": "F15304",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to use this notation h(x) subscript i to denote the ith output.",
    "output": "h(x)の添字iという記法でi番目の出力を示す。"
  },
  {
    "index": "F15305",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, h(x) is a k-dimensional vector and so this subscript i just selects out the ith element of the vector that is output by my neural network.",
    "output": "つまり、h(x)はK次元ベクトルなのでこの添字iは単にニューラルネットワークからの出力のベクトルからi番目を選び取るだけだ。"
  },
  {
    "index": "F15306",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "My cost function J(theta) is now going to be the following.",
    "output": "コスト関数のJのシータは今や以下のようになる。"
  },
  {
    "index": "F15307",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Is - 1 over M of a sum of a similar term to what we have for logistic regression, except that we have the sum from K equals 1 through K.",
    "output": "-1/mにロジスティック回帰の時と似たような項の和となっている。"
  },
  {
    "index": "F15308",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This summation is basically a sum over my K output.",
    "output": "だが、この和がkが1からKまでというのがあるのが違いか。"
  },
  {
    "index": "F15309",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if I have four output units, that is if the final layer of my neural network has four output units, then this is a sum from k equals one through four of basically the logistic regression algorithm's cost function but summing that cost function over each of my four output units in turn.",
    "output": "その和は基本的にはK個の出力ユニットに対する和だ。"
  },
  {
    "index": "F15310",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so you notice in particular that this applies to Yk Hk, because we're basically taking the K upper units, and comparing that to the value of Yk which is that one of those vectors saying what cost it should be. And finally, the second term here is the regularization term, similar to what we had for the logistic regression.",
    "output": "もし出力ユニットが4つあるとすると、それはつまりニューラルネットワークの最後のレイヤーが4つの出力ユニットを持つという事だが、するとその時この和はこれはkが1から4までの基本的にはロジスティック回帰のアルゴリズムのコスト関数なんだが、しかしそのコスト関数を4つの出力ユニット分を一つずつ足していく所が違う。"
  },
  {
    "index": "F15311",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This summation term looks really complicated, but all it's doing is it's summing over these terms theta j i l for all values of i j and l.",
    "output": "そして気づいたかもしれないが、これは特に、yのkとhのkに適用される。"
  },
  {
    "index": "F15312",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Except that we don't sum over the terms corresponding to these bias values like we have for logistic progression.",
    "output": "何故なら、基本的には我らはk番目の出力ユニットをとり、それをyのkと比較しているから。"
  },
  {
    "index": "F15313",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Completely, we don't sum over the terms responding to where i is equal to 0.",
    "output": "でも実際にやってるのはこれらの項、シータのijlの和を全てのijlの値に渡って取るだけだ。"
  },
  {
    "index": "F15314",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that is because when we're computing the activation of a neuron, we have terms like these.",
    "output": "それはというと、ニューロンの活性化を計算してるときこういう感じの項があるΘ(セータ)i0プラスセータのi1x1プラス、、、などなど。"
  },
  {
    "index": "F15315",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the values with a zero there, that corresponds to something that multiplies into an x0 or an a0.",
    "output": "つまり、ここにある0に対応する値はx0とかa0の係数となっている、つまり、これはバイアスユニットの係数って事。"
  },
  {
    "index": "F15316",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is kinda like a bias unit and by analogy to what we were doing for logistic progression, we won't sum over those terms in our regularization term because we don't want to regularize them and string their values as zero.",
    "output": "そしてロジスティック回帰から類推出来るように、正規化の項ではそれらの項は足し合わせない。何故ならそれらは正規化したくないから。"
  },
  {
    "index": "F15317",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But this is just one possible convention, and even if you were to sum over i equals 0 up to Sl, it would work about the same and doesn't make a big difference.",
    "output": "だがこれは単なる一つの慣習に過ぎず、別にiを0からslまで足し合わせたとしても、大差なく、だいたいうまく行く。"
  },
  {
    "index": "F15318",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But maybe this convention of not regularizing the bias term is just slightly more common.",
    "output": "だがこっちの慣習、つまりバイアス項は正規化しない流派の方がちょっとだけ普及してると思う。"
  },
  {
    "index": "F15319",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the cost function we're going to use for our neural network.",
    "output": "以上がネットワークをフィッティングするのに使用するコスト関数です。"
  },
  {
    "index": "F15320",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video we'll start to talk about an algorithm for trying to optimize the cost function.",
    "output": "次のビデオでは、コスト関数を最適化するアルゴリズムに入っていきます。"
  },
  {
    "index": "F15321",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, we talked about a cost function for the neural network.",
    "output": "前回のビデオではニューラルネットワークのコスト関数について話した。"
  },
  {
    "index": "F15322",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, let's start to talk about an algorithm, for trying to minimize the cost function.",
    "output": "このビデオでは、コスト関数の最小化を試みるアルゴリズムの話に入っていこう。"
  },
  {
    "index": "F15323",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, we'll talk about the back propagation algorithm.",
    "output": "特に、バックプロパゲーション(後に伝播)アルゴリズムについて話す。"
  },
  {
    "index": "F15324",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's the cost function that we wrote down in the previous video.",
    "output": "これは前回のビデオで書き下したコスト関数だ。"
  },
  {
    "index": "F15325",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we'd like to do is try to find parameters theta to try to minimize j of theta.",
    "output": "今回やりたいのはJのシータを最小化するようなシータを探したい。"
  },
  {
    "index": "F15326",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to use either gradient descent or one of the advance optimization algorithms.",
    "output": "最急降下法なりより進んだアルゴリズムなりを使うためにはこの入力であるパラメータ、シータをとり、Jのシータとこれらの偏微分の項を計算するコードを書く必要がある。"
  },
  {
    "index": "F15327",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we need to do therefore is to write code that takes this input the parameters theta and computes j of theta and these partial derivative terms.",
    "output": "つまりこれらが、計算しなくてはならない偏微分の項だ。"
  },
  {
    "index": "F15328",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Remember, that the parameters in the the neural network of these things, theta superscript l subscript ij, that's the real number and so, these are the partial derivative terms we need to compute.",
    "output": "コスト関数であるJのシータを計算するには、この上の式をただ使うだけで良い。"
  },
  {
    "index": "F15329",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to compute the cost function j of theta, we just use this formula up here and so, what I want to do for the most of this video is focus on talking about how we can compute these partial derivative terms.",
    "output": "だからこのビデオの残りのほとんどでフォーカスしたいのは、どうやってこれらの偏微分の項を計算出来るか、という方。"
  },
  {
    "index": "F15330",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's start by talking about the case of when we have only one training example, so imagine, if you will that our entire training set comprises only one training example which is a pair xy.",
    "output": "一つしかトレーニング手本が無い場合から始めよう。トレーニングセットの全体がたった一つの手本から構成されている場合を思い浮かべてくれ。"
  },
  {
    "index": "F15331",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm not going to write x1y1 just write this.",
    "output": "x1とかy1とは書かずにただこう書く事にする。"
  },
  {
    "index": "F15332",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Write a one training example as xy and let's tap through the sequence of calculations we would do with this one training example.",
    "output": "一つのトレーニングの手本をx、yと書いて、この一つのトレーニング手本に対してどんな計算を行うのか順番に見ていこう。"
  },
  {
    "index": "F15333",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first thing we do is we apply forward propagation in order to compute whether a hypotheses actually outputs given the input.",
    "output": "最初にやる事はフォワードプロパゲーション(前方に伝播)を与えられた入力に対して仮説が実際に何を出力するかを計算する為、適用する。"
  },
  {
    "index": "F15334",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, the called the a(1) is the activation values of this first layer that was the input there.",
    "output": "具体的には、a(1)と呼ばれている物はこの最初のレイヤーのアクティベーションの値、つまり入力の値だったのを思い出そう。"
  },
  {
    "index": "F15335",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I'm going to set that to x and then we're going to compute z(2) equals theta(1) a(1) and a(2) equals g, the sigmoid activation function applied to z(2) and this would give us our activations for the first middle layer.",
    "output": "だからそれをxに設定し、次にz(2)=シータ(1)a(1)を計算して、そこからa(2)イコールg、つまりsigmoid関数をz(2)に適用する。この結果が最初の中間レイヤーのアクティベーションとなる。"
  },
  {
    "index": "F15336",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is for layer two of the network and we also add those bias terms.",
    "output": "つまりネットワークのレイヤー2に対応する。また、これらのバイアス項も足す。"
  },
  {
    "index": "F15337",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next we apply 2 more steps of this four and propagation to compute a(3) and a(4) which is also the upwards of a hypotheses h of x.",
    "output": "そこから伝播させる為に、もう2ステップさらに適用してa(3)と、、、a(4)を計算する。これはh(x)の出力でもある。"
  },
  {
    "index": "F15338",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is our vectorized implementation of forward propagation and it allows us to compute the activation values for all of the neurons in our neural network.",
    "output": "以上がフォワードプロパゲーションのベクトル化された実装だ。以上でまた、ニューラルネットワーク内の全てのアクティベーション値も計算出来る。"
  },
  {
    "index": "F15339",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, in order to compute the derivatives, we're going to use an algorithm called back propagation.",
    "output": "次に、微分を計算する為にバックプロパゲーション(後方に伝播)と呼ばれるアルゴリズムを使っていく。"
  },
  {
    "index": "F15340",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The intuition of the back propagation algorithm is that for each note we're going to compute the term delta superscript l subscript j that's going to somehow represent the error of note j in the layer l.",
    "output": "デルタには上付き添字のlに下付き添字のjがつく。それはある意味でレイヤーlにあるノードjの誤差を表す物だ。"
  },
  {
    "index": "F15341",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, recall that a superscript l subscript j that does the activation of the j of unit in layer l and so, this delta term is in some sense going to capture our error in the activation of that neural duo.",
    "output": "もういちど思い出すとaの添字のl、下の添え字のjはレイヤーlにあるj番目のアクティベーションだった。そして、このデルタの項はある意味で、そのノードのアクティベーションの誤差を捕捉する、と考えられる。"
  },
  {
    "index": "F15342",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how we might wish the activation of that note is slightly different.",
    "output": "つまり、そのノードのアクティベーションの期待される値からどのくらいずれているかだ。"
  },
  {
    "index": "F15343",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, taking the example neural network that we have on the right which has four layers.",
    "output": "具体的に見てみると、右のニューラルネットワークは4つのレイヤーを持ってる。"
  },
  {
    "index": "F15344",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so capital L is equal to 4.",
    "output": "だから、大文字のLは4。"
  },
  {
    "index": "F15345",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For each output unit, we're going to compute this delta term.",
    "output": "それぞれの出力のユニットに対し、いまこのデルタ項を計算しようとしている。"
  },
  {
    "index": "F15346",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, delta for the j of unit in the fourth layer is equal to just the activation of that unit minus what was the actual value of 0 in our training example.",
    "output": "で、この4つ目のレイヤーの、このjのユニットのデルタは以下に等しい、このユニットのアクティベーションの値から引くことの、この、ぼくたちのトレーニングセットの中の、実際に観測された値。"
  },
  {
    "index": "F15347",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this term here can also be written h of x subscript j, right.",
    "output": "そして、ここのこの項は、h(x)の下添字jとも書ける。でしょ?"
  },
  {
    "index": "F15348",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this delta term is just the difference between when a hypotheses output and what was the value of y in our training set whereas y subscript j is the j of element of the vector value y in our labeled training set.",
    "output": "つまりこのデルタ項は単に我らの仮説の出力した値と、トレーニングセットでのyの値との差分でしか無い。ここでyの下添字jはトレーニング手本のベクトルの値yのj番目の要素って意味だった。"
  },
  {
    "index": "F15349",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way, if you think of delta a and y as vectors then you can also take those and come up with a vectorized implementation of it, which is just delta 4 gets set as a4 minus y.",
    "output": "ところで、もしデルタ、a、yをベクトルだとしてもこれはやはり成立し、これはベクトル化された実装となる。それは単にデルタ4にa4-yをセットする。"
  },
  {
    "index": "F15350",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where here, each of these delta 4 a4 and y, each of these is a vector whose dimension is equal to the number of output units in our network.",
    "output": "ここで、デルタ4、a4、yはそれぞれベクトルでその次元はネットワークの出力ユニットの数に等しい。"
  },
  {
    "index": "F15351",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we've now computed the era term's delta 4 for our network.",
    "output": "これで我らは今やネットワークの誤差項であるところのデルタ4を計算した。"
  },
  {
    "index": "F15352",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we do next is compute the delta terms for the earlier layers in our network.",
    "output": "次にやる事はネットワーク内のより手前のレイヤーのデルタ項を計算する事だ。"
  },
  {
    "index": "F15353",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's a formula for computing delta 3 is delta 3 is equal to theta 3 transpose times delta 4.",
    "output": "これがデルタ3を計算する為の式だ。デルタ3イコールシータ3の転置掛けるデルタ4。"
  },
  {
    "index": "F15354",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this dot times, this is the element y's multiplication operation that we know from MATLAB.",
    "output": "そしてこのドット掛け算は、MATLABなんかにある、要素毎の積。"
  },
  {
    "index": "F15355",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So delta 3 transpose delta 4, that's a vector; g prime z3 that's also a vector and so dot times is in element y's multiplication between these two vectors.",
    "output": "g'(z3)は、これもベクトル。そしてこのドット掛け算はこれら2つのベクトルを要素ごとに掛け合わせた物。"
  },
  {
    "index": "F15356",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This term g prime of z3, that formally is actually the derivative of the activation function g evaluated at the input values given by z3.",
    "output": "この項、g'(z3)は正式にはアクティベーション関数のgを入力値がz3の所で微分した値。"
  },
  {
    "index": "F15357",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you know calculus, you can try to work it out yourself and see that you can simplify it to the same answer that I get. But I'll just tell you pragmatically what that means.",
    "output": "もし解析学を知ってるなら自分自身で実行してみて私が得たのと同じ答えになる事を確かめられるはずだが、現実的にはようするにどういう意味か、答えを教えちゃおう。"
  },
  {
    "index": "F15358",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What you do to compute this g prime, these derivative terms is just a3 dot times1 minus A3 where A3 is the vector of activations.",
    "output": "実際にこのg'を計算する為にやるべき事はこれらの微分項はa3ドット掛ける1-a3で、a3はアクティベーションのベクトルだ。"
  },
  {
    "index": "F15359",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "1 is the vector of ones and A3 is again the activation the vector of activation values for that layer.",
    "output": "この1はベクトルで全要素が1を意味し、このa3もそのレイヤーのアクティベーションの値のベクトルだ。"
  },
  {
    "index": "F15360",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next you apply a similar formula to compute delta 2 where again that can be computed using a similar formula.",
    "output": "次に似たような式をデルタ2にも適用する。それも似たような公式で計算出来る。"
  },
  {
    "index": "F15361",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Only now it is a2 like so and I then prove it here but you can actually, it's possible to prove it if you know calculus that this expression is equal to mathematically, the derivative of the g function of the activation function, which I'm denoting by g prime.",
    "output": "解析学を知ってればこの式が数学的にアクティベーション関数であるところのgの微分と等しい事を示せるだろう。それがまさにg'の事だった。"
  },
  {
    "index": "F15362",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, that's it and there is no delta1 term, because the first layer corresponds to the input layer and that's just the feature we observed in our training sets, so that doesn't have any error associated with that.",
    "output": "だって最初のレイヤーは入力レイヤに対応してるのだから、それって単にトレーニングセットで実際に観察される値なのでそれに関連した誤差も何も無い。"
  },
  {
    "index": "F15363",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's not like, you know, we don't really want to try to change those values.",
    "output": "ようするに、その値はまったく変更したいなんて思ってない訳だ。"
  },
  {
    "index": "F15364",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so we have delta terms only for layers 2, 3 and for this example.",
    "output": "だからデルタ項はこの例だとレイヤー2、3、4にしか無いって訳だ。"
  },
  {
    "index": "F15365",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The name back propagation comes from the fact that we start by computing the delta term for the output layer and then we go back a layer and compute the delta terms for the third hidden layer and then we go back another step to compute delta 2 and so, we're sort of back propagating the errors from the output layer to layer 3 to their to hence the name back complication.",
    "output": "バックプロパゲーションという名前はデルタ項を出力レイヤから計算しはじめてレイヤーを遡っていき隠れレイヤのデルタ項を計算していき、その次にさらにもう一歩戻ってデルタ2を計算して、、、という事からついた名前。つまりある意味で誤差を出力レイヤーからレイヤー3に、そこからさらに前にと伝播(プロパゲート)させていくから、バックプロパゲーションという名前な訳。"
  },
  {
    "index": "F15366",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, the derivation is surprisingly complicated, surprisingly involved but if you just do this few steps steps of computation it is possible to prove viral frankly some what complicated mathematical proof.",
    "output": "最後に、微分はめちゃくちゃ大変だが凄い時間かかるが、単にこれらを一つ一つ計算していけばとても平凡なやり方でかなり面倒だけど数学的に示せるんだがーー本当に示せるんだけど、、、ほんとほんと。"
  },
  {
    "index": "F15367",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's possible to prove that if you ignore authorization then the partial derivative terms you want are exactly given by the activations and these delta terms.",
    "output": "もし正規化の項を無視すれば我らが欲しい偏微分の項は正確にアクティベーションとこれらのデルタ項で与えられる。"
  },
  {
    "index": "F15368",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is ignoring lambda or alternatively the regularization term lambda will equal to 0.",
    "output": "これはラムダ無視してる、言い換えると正規化項を。ラムダが0の場合って事だ。"
  },
  {
    "index": "F15369",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We'll fix this detail later about the regularization term, but so by performing back propagation and computing these delta terms, you can, you know, pretty quickly compute these partial derivative terms for all of your parameters.",
    "output": "この細かい事、正規化項については後で修正するがバックプロパゲーションを実行してこれらのデルタ項を計算する事で全てのパラメータについてこれらの偏微分の項を手早く計算出来る。"
  },
  {
    "index": "F15370",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is a lot of detail.",
    "output": "たくさんの事が出てきたね。"
  },
  {
    "index": "F15371",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's take everything and put it all together to talk about how to implement back propagation to compute derivatives with respect to your parameters.",
    "output": "それらを全部合わせてパラメータに関する微分の計算をどう実装するのか議論しよう。"
  },
  {
    "index": "F15372",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for the case of when we have a large training set, not just a training set of one example, here's what we do.",
    "output": "しかもトレーニングセットがたくさんあるケースを、一つしか無いケースではなく。こんな風にやる。"
  },
  {
    "index": "F15373",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose we have a training set of m examples like that shown here.",
    "output": "m個のトレーニング手本があるとする。"
  },
  {
    "index": "F15374",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first thing we're going to do is we're going to set these delta l subscript i j. So this triangular symbol?",
    "output": "最初にやる事はこれらのデルタl下付き添字ijを、、、ところでこの三角の記号、これは実際はギリシャ文字のアルファベットで大文字のデルタだ。"
  },
  {
    "index": "F15375",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The symbol we had on the previous slide was the lower case delta.",
    "output": "前のスライドにあったデルタは小文字のデルタ。"
  },
  {
    "index": "F15376",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the triangle is capital delta.",
    "output": "この三角形は大文字のデルタ。"
  },
  {
    "index": "F15377",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Eventually, this capital delta l i j will be used to compute the partial derivative term, partial derivative respect to theta l i j of J of theta.",
    "output": "最終的には、この大文字のデルタlijは偏微分の項、、、Jのシータの、シータlijに関する偏微分の項を計算するのに使う事になる。"
  },
  {
    "index": "F15378",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So as we'll see in a second, these deltas are going to be used as accumulators that will slowly add things in order to compute these partial derivatives.",
    "output": "すぐに見る事となるがこれらのデルタはこれらの偏微分を計算する為にちょっとずつ値を足していく為のアキュームレーターとして使う事になる。"
  },
  {
    "index": "F15379",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, we're going to loop through our training set.",
    "output": "次にトレーニングセットをforループで回す。"
  },
  {
    "index": "F15380",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we'll say for i equals 1 through m and so for the i iteration, we're going to working with the training example xi, yi.",
    "output": "つまり、iが1からmまでのfor文、つまりi番目のイテレーションの時はトレーニング手本のxiとyiに関する計算をしてるという事。では、最初にやることは、a1、つまり入力レイヤーのアクティベーションに対しxiをセットする。"
  },
  {
    "index": "F15381",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the first thing we're going to do is set a1 which is the activations of the input layer, set that to be equal to xi is the inputs for our i training example, and then we're going to perform forward propagation to compute the activations for layer two, layer three and so on up to the final layer, layer capital L.",
    "output": "それはi番目のトレーニング手本の入力を表すから。そして次に、フォワードプロパゲーションを適用してレイヤー2、レイヤー3と最後のレイヤーである所のレイヤーLまでを計算していく。"
  },
  {
    "index": "F15382",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, we're going to use the output label yi from this specific example we're looking at to compute the error term for delta L for the output there.",
    "output": "次に、yiとラベルづけされた現在見ているトレーニング手本の出力を用いて、ここの出力の誤差を計算する。つまりデルタLは仮説の出力結果引くことのターゲットにしてる観測値。"
  },
  {
    "index": "F15383",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we're going to use the back propagation algorithm to compute delta L minus 1, delta L minus 2, and so on down to delta 2 and once again there is now delta 1 because we don't associate an error term with the input layer.",
    "output": "そしてバックプロパゲーションのアルゴリズムを用いて、デルタL-1、デルタL-2、、、とデルタ2までを計算する。ここでもデルタ1は求めない。"
  },
  {
    "index": "F15384",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, we're going to use these capital delta terms to accumulate these partial derivative terms that we wrote down on the previous line.",
    "output": "何故なら入力レイヤに対応した誤差というのは想定しないから。そして最後に大文字のΔ(デルタ)の項を使う前の行で書いた偏微分の項を蓄積していく為に。"
  },
  {
    "index": "F15385",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way, if you look at this expression, it's possible to vectorize this too.",
    "output": "ところでこの式を眺めて見ると、これもベクトル化出来そうだ。"
  },
  {
    "index": "F15386",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, if you think of delta ij as a matrix, indexed by subscript ij.",
    "output": "具体的にはデルタijを添字のijがインデックスの行列とみなすと、デルタlは行列としてこう書き直せる。"
  },
  {
    "index": "F15387",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then, if delta L is a matrix we can rewrite this as delta L, gets updated as delta L plus lower case delta L plus one times aL transpose.",
    "output": "デルタlはデルタl足すことの小文字のデルタl+1にalの転置を掛けた物で更新する、と。"
  },
  {
    "index": "F15388",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's a vectorized implementation of this that automatically does an update for all values of i and j.",
    "output": "以上がベクトル化したこれの実装でこれは自動的に全てのi,jに対して値を更新してくれる。"
  },
  {
    "index": "F15389",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, after executing the body of the four-loop we then go outside the four-loop and we compute the following.",
    "output": "最後に、forループの中身を実行した後でforループの外に出るが、そこで以下を計算する。"
  },
  {
    "index": "F15390",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We compute capital D as follows and we have two separate cases for j equals zero and j not equals zero.",
    "output": "大文字のDを二つの場合、jが0の時とjが0以外の時に分けて以下のように計算していく。"
  },
  {
    "index": "F15391",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The case of j equals zero corresponds to the bias term so when j equals zero that's why we're missing is an extra regularization term.",
    "output": "jが0の時とはバイアス項に対応するのでだからこのケースでは追加の正規化項が無いのだ。"
  },
  {
    "index": "F15392",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, while the formal proof is pretty complicated what you can show is that once you've computed these D terms, that is exactly the partial derivative of the cost function with respect to each of your perimeters and so you can use those in either gradient descent or in one of the advanced authorization algorithms.",
    "output": "最後に正式な証明は極めて複雑だが、頑張れば示せる事としてはこれらDの項をひとたび計算してしまえば、コスト関数のパラメータでの偏微分にぴったりと一致する、という事。だからそれらを最急降下法にでもより高度な最適化アルゴリズムにでも使う事が出来る。"
  },
  {
    "index": "F15393",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the back propagation algorithm and how you compute derivatives of your cost function for a neural network.",
    "output": "これが、バックプロパゲーションアルゴリズムだ。それとニューラルネットワークのコスト関数の偏微分係数を計算する方法。"
  },
  {
    "index": "F15394",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I know this looks like this was a lot of details and this was a lot of steps strung together.",
    "output": "これがまるですごいたくさんのディティールとステップが数珠つなぎになった代物に見えることはよくわかってるんだけど。"
  },
  {
    "index": "F15395",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But both in the programming assignments write out and later in this video, we'll give you a summary of this so we can have all the pieces of the algorithm together so that you know exactly what you need to implement if you want to implement back propagation to compute the derivatives of your neural network's cost function with respect to those parameters.",
    "output": "でも、プログラミングの宿題と、あとのビデオでまた、まとめ直すのですべてのピースのアルゴリズムを全部まとめて。きみがもしバックプロパゲーションを用いてニューラルネットワークのコスト関数のパラメータによる偏微分係数を実装したくなったときに何しなくてはいけないかがはっきり分かるようにね。"
  },
  {
    "index": "F15396",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, we talked about the backpropagation algorithm.",
    "output": "前回の動画では、バックプロパゲーションについて話した。"
  },
  {
    "index": "F15397",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To a lot of people seeing it for the first time, their first impression is often that wow this is a really complicated algorithm, and there are all these different steps, and I'm not sure how they fit together.",
    "output": "多くの人にとっては最初に見たら第一印象は「うげぇ。"
  },
  {
    "index": "F15398",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it's kinda this black box of all these complicated steps.",
    "output": "そして「私、実はそれらがどう組み合わせて使うのか良く分かってないんだぁ、、、」とか、さらに「これらの込み入ったステップはなんだかブラックボックスみたいだ!"
  },
  {
    "index": "F15399",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case that's how you're feeling about backpropagation, that's actually okay.",
    "output": "あなたがバックプロパゲーションを、もしそんな風に感じていたとしても、実は問題ありません。"
  },
  {
    "index": "F15400",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Backpropagation maybe unfortunately is a less mathematically clean, or less mathematically simple algorithm, compared to linear regression or logistic regression.",
    "output": "バックプロパゲーションは残念な事に線形回帰やロジスティック回帰に比べると数学的にクリーンという訳でも数学的にシンプルなアルゴリズムという訳でもありません。"
  },
  {
    "index": "F15401",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I've actually used backpropagation, you know, pretty successfully for many years.",
    "output": "でも実の所、私も長年、ひょっとしたら今日ですら、バックプロパゲーションが何やってるのかいまいち直感的に理解出来てないなぁ、と思う事はあるけれど、使う分には問題なくとてもしっかりと使えてきました。"
  },
  {
    "index": "F15402",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And even today I still don't sometimes feel like I have a very good sense of just what it's doing, or intuition about what back propagation is doing.",
    "output": "そうすれば自分で動かす事は出来るようになるよ。そしてこのビデオでやりたい事はバックプロパゲーションの手順を、もうちょっとだけ機械的に見てみたい。"
  },
  {
    "index": "F15403",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If, for those of you that are doing the programming exercises, that will at least mechanically step you through the different steps of how to implement back prop.",
    "output": "そうする事で、もうちょっと感覚的にバックプロパゲーションを機械的に行う手順はどんな感じかを伝えたい。そうする事でこれは少なくとも良さそうなアルゴリズムだな、と思ってもらえたら幸い。"
  },
  {
    "index": "F15404",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you'll be able to get it to work for yourself.",
    "output": "と思った君に、ちょっとだけ魔法をかけてあげよう。実際の所、それでも問題無いです!"
  },
  {
    "index": "F15405",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what I want to do in this video is look a little bit more at the mechanical steps of backpropagation, and try to give you a little more intuition about what the mechanical steps the back prop is doing to hopefully convince you that, you know, it's at least a reasonable algorithm.",
    "output": "もしそう感じてたとしても、私はバックプロパゲーションを長年使ってきたよ。"
  },
  {
    "index": "F15406",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case even after this video in case back propagation still seems very black box and kind of like a, too many complicated steps and a little bit magical to you, that's actually okay.",
    "output": "たまに理解の難しい事のあるアルゴリズムだけどね。"
  },
  {
    "index": "F15407",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And Even though I've used back prop for many years, sometimes this is a difficult algorithm to understand, but hopefully this video will help a little bit.",
    "output": "でも願わくばこの動画がバックプロパゲーションを理解する助けにならんことを!"
  },
  {
    "index": "F15408",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to better understand backpropagation, let's take another closer look at what forward propagation is doing.",
    "output": "まずフォワードプロパゲーションが何をしてるのか別の角度から詳しく見ていこう。"
  },
  {
    "index": "F15409",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's a neural network with two input units that is not counting the bias unit, and two hidden units in this layer, and two hidden units in the next layer.",
    "output": "バイアスユニットを数えない。そして2つの隠れユニットがこのレイヤーにありさらに2つの隠れユニットがその次のレイヤーにある。"
  },
  {
    "index": "F15410",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then, finally, one output unit.",
    "output": "そして最後に出力ユニット。"
  },
  {
    "index": "F15411",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Again, these counts two, two, two, are not counting these bias units on top.",
    "output": "そしてこれらのカウント、2、2、2は、このトップのバイアスユニットを数えていない。"
  },
  {
    "index": "F15412",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to illustrate forward propagation, I'm going to draw this network a little bit differently.",
    "output": "フォーワードプロパゲーションをわかりやすく説明するためにこのネットワークをちょっと違う風に描いてみよう。"
  },
  {
    "index": "F15413",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular I'm going to draw this neuro-network with the nodes drawn as these very fat ellipsis, so that I can write text in them.",
    "output": "特に、このニューラルネットのノードを大きな楕円で描くことで、その中にテキストを書けるようにする。"
  },
  {
    "index": "F15414",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it'll be this x i that we feed into the input layer. So this maybe x i 2 and x i 2 are the values we set the input layer to.",
    "output": "フォワードプロパゲーションを実行する時はある手本に対してそれを手本x(i)、y(i)としよう、そしてこのx(i)こそが入力レイヤーに食わせる物だ。"
  },
  {
    "index": "F15415",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And when we forward propagated to the first hidden layer here, what we do is compute z (2) 1 and z (2) 2. So these are the weighted sum of inputs of the input units.",
    "output": "つまり、このx(i)の1とx(i)の2の2つの値が入力レイヤーにセットする値で、そしてその値を最初の隠れレイヤーへとフォワードプロパゲートするには、z(2)の1とz(2)の2を計算し、ところで、これらは入力ユニットからの入力の重み付け和だ。"
  },
  {
    "index": "F15416",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we apply the sigmoid of the logistic function, and the sigmoid activation function applied to the z value. Here's are the activation values.",
    "output": "で、ロジスティック関数のsigmoid関数を適用するーーーsigmoidアクティベーション関数をzの値に適用すると、これらのアクティベーションの値が得られる。"
  },
  {
    "index": "F15417",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that gives us a (2) 1 and a (2) 2.",
    "output": "つまり以上で、a(2)の1とa(2)の2が得られる。"
  },
  {
    "index": "F15418",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Apply the sigmoid of the logistic function, the activation function to that to get a (3) 1.",
    "output": "次にそれをまたフォワードプロパゲートして、ここのz(3)の1にsigmoidのロジスティック関数、アクティベーション関数を適用し、そしてa(3)の1を得る。"
  },
  {
    "index": "F15419",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, like so until we get z (4) 1. Apply the activation function.",
    "output": "同様に、z(4)の1を得て、そこにアクティベーション関数を適用してa(4)の1を得るまで続ける。"
  },
  {
    "index": "F15420",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This gives us a (4)1, which is the final output value of the neural network.",
    "output": "これこそが、ネットワークの最終的な出力の値だ。"
  },
  {
    "index": "F15421",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's erase this arrow to give myself some more space.",
    "output": "ちょっとスペース開ける為にこの矢印を消す。"
  },
  {
    "index": "F15422",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you look at what this computation really is doing, focusing on this hidden unit, let's say.",
    "output": "そしてこの計算が実際に何をやってるのか見てみよう。"
  },
  {
    "index": "F15423",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We have to add this weight. Shown in magenta there is my weight theta (2) 1 0, the indexing is not important.",
    "output": "この隠れユニットにフォーカスしてみると、このウェイト、マゼンダで示したがウェイト、シータ(2)10だとしよう。"
  },
  {
    "index": "F15424",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this way here, which I'm highlighting in red, that is theta (2) 1 1 and this weight here, which I'm drawing in cyan, is theta (2) 1 2.",
    "output": "で、こんな風に、ここは赤でシータ(2)11、そしてここのウェイトは緑、、、というよりはシアンで描いたのがシータ(2)12。"
  },
  {
    "index": "F15425",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the way we compute this value, z(3)1 is, z(3)1 is as equal to this magenta weight times this value.",
    "output": "以上がz(3)1をz(3)1を計算する方法。このウェイトにこの値を掛ける。"
  },
  {
    "index": "F15426",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's theta (2) 10 x 1. And then plus this red weight times this value, so that's theta(2) 11 times a(2)1.",
    "output": "つまりシータ(2)10掛ける1、足すことの赤のウェイト掛けるこの値、つまりシータ(2)11掛けるa(2)1。"
  },
  {
    "index": "F15427",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally this cyan weight times this value, which is therefore plus theta(2)12 times a(2)1.",
    "output": "そして最後にこのシアン、掛けるこの値。つまり、足すことのシータ(2)12掛けるa(2)1。"
  },
  {
    "index": "F15428",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it turns out that as we'll see later in this video, what backpropagation is doing is doing a process very similar to this.",
    "output": "そしてこのビデオの後半で見ると明らかになるが、バックプロパゲーションでやる事もとてもこれと似たプロセスだったりする。"
  },
  {
    "index": "F15429",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Except that instead of the computations flowing from the left to the right of this network, the computations since their flow from the right to the left of the network.",
    "output": "違いは計算の流れがネットワークの左から右へと流れる代わりに、そこでは計算はネットワークの左から右へと流れる。"
  },
  {
    "index": "F15430",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And using a very similar computation as this.",
    "output": "そしてこれととても似た計算を用いて、二枚のスライドでちゃんと説明する。"
  },
  {
    "index": "F15431",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To better understand what backpropagation is doing, let's look at the cost function.",
    "output": "バックプロパゲーションが何やってるのかより良く理解する為に、コスト関数を見てみよう。"
  },
  {
    "index": "F15432",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's just the cost function that we had for when we have only one output unit.",
    "output": "一つしか出力ユニットが無い時のコスト関数を。"
  },
  {
    "index": "F15433",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we have more than one output unit, we just have a summation you know over the output units indexed by k there.",
    "output": "もし一つよりも多くの出力ユニットがある時は単に出力ユニットのインデックスについて足し合わせてやれば良い。"
  },
  {
    "index": "F15434",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have only one output unit then this is a cost function.",
    "output": "もし出力ユニットが一つしかなければこれがコスト関数。"
  },
  {
    "index": "F15435",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we do forward propagation and backpropagation on one example at a time.",
    "output": "フォワードプロパゲーションとバックプロパゲーションを一度に一つの手本データずつに対して行う。"
  },
  {
    "index": "F15436",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's just focus on the single example, x (i) y (i) and focus on the case of having one output unit.",
    "output": "だから一つの手本、x(i)、y(i)にフォーカスしよう。そして出力ユニットは一つのケースについてフォーカスしよう。"
  },
  {
    "index": "F15437",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So y (i) here is just a real number.",
    "output": "つまりここのy(i)は単なる実数。"
  },
  {
    "index": "F15438",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's ignore regularization, so lambda equals 0.",
    "output": "さらに正規化は無視しよう。つまりラムダは0。"
  },
  {
    "index": "F15439",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this final term, that regularization term, goes away.",
    "output": "そうすればこの最後の正規化の項は消えるからね。"
  },
  {
    "index": "F15440",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now if you look inside the summation, you find that the cost term associated with the training example, that is the cost associated with the training example x(i), y(i).",
    "output": "今、この和の中を見てみれば、i番目のトレーニング手本に対応するコストの項はつまりx(i)とy(i)に対応したコストはこの式で与えられる。"
  },
  {
    "index": "F15441",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That's going to be given by this expression. So, the cost to live off examplie i is written as follows.",
    "output": "これがi番目のトレーニング手本に対応したコストという事になる。"
  },
  {
    "index": "F15442",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what this cost function does is it plays a role similar to the squared arrow.",
    "output": "そしてこのコスト関数がやってるのは誤差の二乗に似た感じの役割。"
  },
  {
    "index": "F15443",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, rather than looking at this complicated expression, if you want you can think of cost of i being approximately the square difference between what the neural network outputs, versus what is the actual value.",
    "output": "だからこの複雑な式を見る代わりにi番目に対応したコストをニューラルネットワークの出力と実際の値との差分の二乗みたいなもんだと考えても良い。"
  },
  {
    "index": "F15444",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just as in logistic repression, we actually prefer to use the slightly more complicated cost function using the log. But for the purpose of intuition, feel free to think of the cost function as being the sort of the squared error cost function.",
    "output": "ロジスティック回帰の時と同様に実際にはこちらのlogを使ったちょびっと複雑なコスト関数の方がいいんだけど、感覚的に理解する、という点では誤差の二乗のコスト関数なんだと考えて差し支えない。"
  },
  {
    "index": "F15445",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this cost(i) measures how well is the network doing on correctly predicting example i.",
    "output": "つまりこのコストのiはどのくらいネットワークがうまく手本iを予測しているかを測ってる。"
  },
  {
    "index": "F15446",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How close is the output to the actual observed label y(i)?",
    "output": "どのくらい出力が実際の観測値、y(i)と近いかを。"
  },
  {
    "index": "F15447",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's look at what backpropagation is doing.",
    "output": "ではバックプロパゲーションが何をやってるかを見ていこう。"
  },
  {
    "index": "F15448",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One useful intuition is that backpropagation is computing these delta superscript l subscript j terms.",
    "output": "有用な直感的な説明としては、バックプロパゲーションはこれらのデルタ上付き添字l下付き添字jの項を計算している、というのがある。"
  },
  {
    "index": "F15449",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we can think of these as the quote error of the activation value that we got for unit j in the layer, in the lth layer.",
    "output": "これらをアクティベーションの値の「誤差」と考える事が出来る、l番目のレイヤーのj番目のユニットのアクティベーション。"
  },
  {
    "index": "F15450",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "More formally, for, and this is maybe only for those of you who are familiar with calculus.",
    "output": "より正式には解析学が得意な人向けだろうけど、より正式には、デルタ項が実際になんなのかというと、これだ!"
  },
  {
    "index": "F15451",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "More formally, what the delta terms actually are is this, they're the partial derivative with respect to z,l,j, that is this weighted sum of inputs that were confusing these z terms.",
    "output": "それらはz(l)jによる偏微分係数。このzは入力の重み付き和を計算したもので、これらによるコスト関数の偏微分となる。"
  },
  {
    "index": "F15452",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So concretely, the cost function is a function of the label y and of the value, this h of x output value neural network.",
    "output": "具体的にはコスト関数はラベルyとこのh(x)の、ネットワークの出力した値の関数。"
  },
  {
    "index": "F15453",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if we could go inside the neural network and just change those z l j values a little bit, then that will affect these values that the neural network is outputting. And that will end up changing the cost function.",
    "output": "そしてもしこのネットワークの中に入ってそれらのz(l)jの値をちょっとずらしたら、これらの値がニューラルネットに影響を与えて最終的にはコスト関数も変わる。"
  },
  {
    "index": "F15454",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And again really, this is only for those of you who are expert in Calculus.",
    "output": "もう一度言っておくとこれはほんと解析得意な人向けの話。"
  },
  {
    "index": "F15455",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you're comfortable with partial derivatives, what these delta terms are is they turn out to be the partial derivative of the cost function, with respect to these intermediate terms that were confusing.",
    "output": "偏微分に慣れ親しんでて、快適に使える人向け。これらのデルタの項は我らの計算している、これらの中間項によるコスト関数の偏微分となっている。"
  },
  {
    "index": "F15456",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so they're a measure of how much would we like to change the neural network's weights, in order to affect these intermediate values of the computation. So as to affect the final output of the neural network h(x) and therefore affect the overall cost.",
    "output": "つまりそれらは、これらの中間の値の計算に影響を与えて、ニューラルネットワークの最終出力、h(x)に影響を与えて、結果として全体のコストに影響を与える為にどれだけニューラルネットワークのウェイトを変更すれば良いかの指標となっている。"
  },
  {
    "index": "F15457",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case this lost part of this partial derivative intuition, in case that doesn't make sense.",
    "output": "この最後の部分の偏微分の直感がいまいちピンと来て無くても、心配ご無用。"
  },
  {
    "index": "F15458",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Don't worry about the rest of this, we can do without really talking about partial derivatives.",
    "output": "ここから後は偏微分なんて計算出来なくてもやっていけるから。"
  },
  {
    "index": "F15459",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But let's look in more detail about what backpropagation is doing.",
    "output": "でも、バックプロパゲーションが何をやっているのかもうちょっと詳しく見てみよう。"
  },
  {
    "index": "F15460",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For the output layer, the first set's this delta term, delta (4) 1, as y (i) if we're doing forward propagation and back propagation on this training example i. That says y(i) minus a(4)1.",
    "output": "出力レイヤについては、最初にセットされるデルタ項だが、それはデルタ(4)1をy(i)、、、もしこのトレーニング手本のiについてフォワードプロパゲーションを行い、さらにバックプロパゲーションを行うとすると、y(i)-a(4)1となる。"
  },
  {
    "index": "F15461",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is really the error, right?",
    "output": "つまり、それは本当に誤差だ。"
  },
  {
    "index": "F15462",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's the difference between the actual value of y minus what was the value predicted, and so we're gonna compute delta(4)1 like so.",
    "output": "実際の値であるyから、引くことの予言された値なのだから。こんな風にデルタ(4)1を計算する。"
  },
  {
    "index": "F15463",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next we're gonna do, propagate these values backwards.",
    "output": "次に、これらの値を後ろ(バック)へと伝播(プロパゲート)させていく。"
  },
  {
    "index": "F15464",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'll explain this in a second, and end up computing the delta terms for the previous layer.",
    "output": "すぐに説明する。で、一つ前のデルタ項を計算する事になる。"
  },
  {
    "index": "F15465",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're gonna end up with delta(3)1. Delta(3)2.",
    "output": "結局、デルタ(3)1とデルタ(3)2になる。"
  },
  {
    "index": "F15466",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we're gonna propagate this further backward, and end up computing delta(2)1 and delta(2)2.",
    "output": "そしてその後に、これをさらに後ろへと伝播させていき、デルタ(2)1とデルタ(2)2を計算する事になる。"
  },
  {
    "index": "F15467",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now the backpropagation calculation is a lot like running the forward propagation algorithm, but doing it backwards.",
    "output": "ここまでくると、バックプロパゲーションの計算はかなりフォワードプロパゲーションのアルゴリズムを実行するのに似通ってくる。"
  },
  {
    "index": "F15468",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's what I mean.",
    "output": "それの意味する所はこうだ。"
  },
  {
    "index": "F15469",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at how we end up with this value of delta(2)2. So we have delta(2)2.",
    "output": "どうやってこのデルタ(2)2まで来たのか見てみよう。"
  },
  {
    "index": "F15470",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similar to forward propagation, let me label a couple of the weights.",
    "output": "フォワードプロパゲーションみたいに幾つかのウェイトにラベルをつけよう。"
  },
  {
    "index": "F15471",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this weight, which I'm going to draw in cyan. Let's say that weight is theta(2)1 2, and this one down here when we highlight this in red.",
    "output": "このウェイトはシアンの色で、シータ(2)の12で、そしてこの下のウェイトは、赤で書こう、これを以後、シータ(2)の22と呼ぼう。"
  },
  {
    "index": "F15472",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is going to be let's say theta(2) of 2 2.",
    "output": "さて、デルタ(2)の2がどう計算されるかこのノードはどう計算されるのか?見てみよう。"
  },
  {
    "index": "F15473",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if we look at how delta(2)2, is computed, how it's computed with this note.",
    "output": "計算する為にやる事はこの値を取ってこのウェイトを掛ける、そしてそれを足し合わせる事のこの値掛けるそのウェイト。"
  },
  {
    "index": "F15474",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that what we're going to do, is gonna take this value and multiply it by this weight, and add it to this value multiplied by that weight.",
    "output": "つまりそれは本当にこれらのデルタの値の重み付き和だ。"
  },
  {
    "index": "F15475",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it's really a weighted sum of these delta values, weighted by the corresponding edge strength.",
    "output": "対応するエッジの重みで重みづけする。具体的に書き下してみよう。"
  },
  {
    "index": "F15476",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So completely, let me fill this in, this delta(2)2 is going to be equal to, Theta(2)1 2 is that magenta lay times delta(3)1. Plus, and the thing I had in red, that's theta (2)2 times delta (3)2.",
    "output": "このデルタ(2)2はイコール、シータ(2)の12ーーこれはマゼンダ色で描いた重みーー掛ける事のデルタ(3)1、足すことの次は赤の奴。"
  },
  {
    "index": "F15477",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it's really literally this red wave times this value, plus this magenta weight times this value.",
    "output": "つまり本当に、文字通り、この赤のウェイト掛けるこの値、足すことのこのマゼンダのウェイト掛けることのこの値。"
  },
  {
    "index": "F15478",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's how we wind up with that value of delta.",
    "output": "それがこの値、デルタを計算する方法。もう一つ見てみよう。"
  },
  {
    "index": "F15479",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just as another example, let's look at this value.",
    "output": "この値を見てみる。"
  },
  {
    "index": "F15480",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How do we get that value?",
    "output": "この値はどうやったら得られるか?"
  },
  {
    "index": "F15481",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then we have that delta (3) 2 is going to be equal to that green weight, theta (3) 12 times delta (4) 1.",
    "output": "それは似た感じで、このウェイトをとりあえず緑で表しておくと、このウェイトをシータ(3)の12とすると、デルタ(3)2はその場合、イコール緑のウェイト、シータ(3)12掛けるデルタ(4)1だ。"
  },
  {
    "index": "F15482",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way, so far I've been writing the delta values only for the hidden units, but excluding the bias units.",
    "output": "ところで、ここまでの所、デルタの値を隠れユニットにだけ書いてきた。そしてバイアスのユニットには書いて来なかった。"
  },
  {
    "index": "F15483",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Depending on how you define the backpropagation algorithm, or depending on how you implement it, you know, you may end up implementing something that computes delta values for these bias units as well.",
    "output": "バックプロパゲーションのアルゴリズムをどう定義するか、またはどう実装するかによって、これらのバイアスユニットに対するデルタの値を計算する事になったりする。"
  },
  {
    "index": "F15484",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The bias units always output the value of plus one, and they are just what they are, and there's no way for us to change the value.",
    "output": "そしてその値を変更する方法は存在しない。"
  },
  {
    "index": "F15485",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, depending on your implementation of back prop, the way I usually implement it.",
    "output": "だからバックプロパゲーションの実装方法によるが、私の普段の実装方法だと、これらのデルタの値は計算してる。"
  },
  {
    "index": "F15486",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I do end up computing these delta values, but we just discard them, we don't use them.",
    "output": "でも、ただその結果を捨てていて、使っていない。"
  },
  {
    "index": "F15487",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because they don't end up being part of the calculation needed to compute a derivative.",
    "output": "何故ならそれらは結局のところ微分を計算するのに必要な計算に含まれていないから。"
  },
  {
    "index": "F15488",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully that gives you a little better intuition about what back propegation is doing.",
    "output": "さて、以上でバックプロパゲーションが何をしているか、感覚的な理解がちょっとでも与えられたら幸い。"
  },
  {
    "index": "F15489",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case of all of this still seems sort of magical, sort of black box, in a later video, in the putting it together video, I'll try to get a little bit more intuition about what backpropagation is doing.",
    "output": "もしこれらを見てもなお、全部魔法のようであまりにもブラックボックスに感じられた場合はあとのビデオ、「PuttingItTogether」(ビデオのタイトル)の中で、バックプロパゲーションが何をしているかについて、さらなる直感を提供したいと思っている。"
  },
  {
    "index": "F15490",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But unfortunately this is a difficult algorithm to try to visualize and understand what it is really doing.",
    "output": "これは、可視化したり、それが本当の所何をやっているのかを理解するのは難しいアルゴリズムだ。"
  },
  {
    "index": "F15491",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you implement the algorithm you can have a very effective learning algorithm. Even though the inner workings of exactly how it works can be harder to visualize.",
    "output": "私の推測するところによると、たくさんの人がとてもうまくこのアルゴリズムを使えていて、実装してみると、とても効率的な学習アルゴリズムだ、たとえ中がどうなってるのか、正確に何が起こるかを可視化するのが難しいにせよ。"
  },
  {
    "index": "F15492",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, we talked about how to use back propagation to compute the derivatives of your cost function.",
    "output": "前回のビデオではコスト関数の微分を計算する為にバックプロパゲーションをどう使うかを議論した。"
  },
  {
    "index": "F15493",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I want to quickly tell you about one implementational detail of unrolling your parameters from matrices into vectors, which we need in order to use the advanced optimization routines.",
    "output": "このビデオでは、行列からベクトルへのパラメータのアンロールという、細かい実装の話を簡単に行う、アドバンスドな最適化ルーチンを使うのに必要となるから。"
  },
  {
    "index": "F15494",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, let's say you've implemented a cost function that takes this input, you know, parameters theta and returns the cost function and returns derivatives.",
    "output": "具体的にいこう、パラメータシータを受け取ってコスト関数とその微分を返す関数を実装したとしよう。"
  },
  {
    "index": "F15495",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then you can pass this to an advanced authorization algorithm by fminunc and fminunc isn't the only one by the way.",
    "output": "すると、これをfminuncのようなアドバンスドな最適化アルゴリズムに渡す事が出来る、ところで、このfminuncは唯一の選択肢という訳では無い。"
  },
  {
    "index": "F15496",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But what all of them do is take those input pointedly the cost function, and some initial value of theta.",
    "output": "でもそれらは全てコスト関数のポインタとシータの初期値を受け取る。"
  },
  {
    "index": "F15497",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And both, and these routines assume that theta and the initial value of theta, that these are parameter vectors, maybe Rn or Rn plus 1.",
    "output": "そしてこれらのルーチンはシータとシータの初期値をどちらもパラメータベクトルと想定する、RのnとかRのn+1とか。"
  },
  {
    "index": "F15498",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But these are vectors and it also assumes that, you know, your cost function will return as a second return value this gradient which is also Rn and Rn plus 1.",
    "output": "これらはベクトルだが、コスト関数の実装が二番目の返値として返す微分項もRのnなりRのn+1なりを仮定する。"
  },
  {
    "index": "F15499",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So also a vector.",
    "output": "つまりこれもベクトルだ。"
  },
  {
    "index": "F15500",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This worked fine when we were using logistic progression but now that we're using a neural network our parameters are no longer vectors, but instead they are these matrices where for a full neural network we would have parameter matrices theta 1, theta 2, theta 3 that we might represent in Octave as these matrices theta 1, theta 2, theta 3.",
    "output": "ロジスティック回帰で使ってる時はこれで問題無かったのだが、今やニューラルネットワークなので、パラメータはもうベクトルでは無くなってしまった。今やパラメータはこれらの行列で四段のニューラルネットワークだとするとパラメータ行列シータ1、シータ2,シータ3を持ちOctaveではこれらは行列Theta1、Theta2、Theta3と表されるだろう。"
  },
  {
    "index": "F15501",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, in the previous video we showed how to compute these gradient matrices, which was capital D1, capital D2, capital D3, which we might represent an octave as matrices D1, D2, D3.",
    "output": "同様にこれらのgradientの項として返されると期待しているのは、前回のビデオでこれらのgradient項をどう計算するかを扱ったが、それらは大文字のD1、D2、D3で、それはOctaveでは行列D1、D2、D3として表される。"
  },
  {
    "index": "F15502",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I want to quickly tell you about the idea of how to take these matrices and unroll them into vectors.",
    "output": "このビデオでは、これらの行列をとってどうベクトルにアンロールするかをお話する。"
  },
  {
    "index": "F15503",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that they end up being in a format suitable for passing into as theta here off for getting out for a gradient there.",
    "output": "ここのシータとして渡すのに適切なフォーマットにしたりここのgradientから取り出す為に。"
  },
  {
    "index": "F15504",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, let's say we have a neural network with one input layer with ten units, hidden layer with ten units and one output layer with just one unit, so s1 is the number of units in layer one and s2 is the number of units in layer two, and s3 is a number of units in layer three.",
    "output": "具体的に、入力レイヤとして10個のユニットがあり、隠れレイヤとして10ユニット、そして出力レイヤとしてユニット一つとしよう。そしてs1はレイヤ1のユニット数、s2はレイヤ2のユニット数、そしてs3はレイヤ3のユニット数とする。"
  },
  {
    "index": "F15505",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this case, the dimension of your matrices theta and D are going to be given by these expressions.",
    "output": "この場合、行列シータの次元とDの次元はこれらの式で与えられる。"
  },
  {
    "index": "F15506",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For example, theta one is going to a 10 by 11 matrix and so on.",
    "output": "例えば、シータ1は10x11行列、などとなる。"
  },
  {
    "index": "F15507",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "vectors. What you can do is take your theta 1, theta 2, theta 3, and write this piece of code and this will take all the elements of your three theta matrices and take all the elements of theta one, all the elements of theta 2, all the elements of theta 3, and unroll them and put all the elements into a big long vector.",
    "output": "だからこれらの行列をベクトルと変換したければ、変換したければ、可能な手としてはTheta1、Theta2、Theta3に対してこんなコードを書くと、3つのシータ行列から全ての要素を取り出して、つまりシータ1の全要素、シータ2の全要素、シータ3の全要素を取り出して、それらをアンロール(展開)して、それら全要素を一つの長いベクトルに突っ込む。"
  },
  {
    "index": "F15508",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Which is thetaVec and similarly the second command would take all of your D matrices and unroll them into a big long vector and call them DVec.",
    "output": "それがthetaVecとなる。同様に二番目のコマンドはDの行列全てを大きな長いベクトル、DVecにアンロールする。"
  },
  {
    "index": "F15509",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally if you want to go back from the vector representations to the matrix representations. What you do to get back to theta one say is take thetaVec and pull out the first 110 elements.",
    "output": "そして最後にもしベクトルの表現から行列の表現に戻したくなったら、例えばシータ1を取り戻したいと思ったとすると、やるべき事はまずthetaVecから最初の110個の要素を取り出す。"
  },
  {
    "index": "F15510",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So theta 1 has 110 elements because it's a 10 by 11 matrix so that pulls out the first 110 elements and then you can use the reshape command to reshape those back into theta 1.",
    "output": "つまりシータ1は110個の要素があるという事、何故ならそれは10x11の行列だから。だから最初の110個の要素を取り出しそしてそこで、単に変形してtheta1に戻す事が出来る。"
  },
  {
    "index": "F15511",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, to get back theta 2 you pull out the next 110 elements and reshape it.",
    "output": "同様にシータ2を取り戻すには、次の110要素を取り出しreshapeすれば良い。"
  },
  {
    "index": "F15512",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for theta 3, you pull out the final eleven elements and run reshape to get back the theta 3.",
    "output": "そしてシータ3は、最後の11要素を取り出し、reshapeを実行してシータ3が取り戻せる。"
  },
  {
    "index": "F15513",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for this example let's set theta 1 equal to be ones of 10 by 11, so it's a matrix of all ones.",
    "output": "この例の為にシータ1を、onesの10x11にセットしよう、つまり全ての要素が1の行列になる。"
  },
  {
    "index": "F15514",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just to make this easier seen, let's set that to be 2 times ones, 10 by 11 and let's set theta 3 equals 3 times 1's of 1 by 11.",
    "output": "見やすいように、シータ2は2掛けるonesの10x11と、さらにシータ3は3掛けるonesの1x11としよう。"
  },
  {
    "index": "F15515",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is 3 separate matrices: theta 1, theta 2, theta 3.",
    "output": "つまり以上で3つの異なる行列シータ1、シータ2、シータ3が出来た。"
  },
  {
    "index": "F15516",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We want to put all of these as a vector.",
    "output": "これら全てをベクトルに突っ込みたい。"
  },
  {
    "index": "F15517",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "ThetaVec equals theta 1; theta 2 theta 3.",
    "output": "thetaVecは、イコールtheta1;theta2;theta3だ。"
  },
  {
    "index": "F15518",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, that's a colon in the middle and like so and now thetavec is going to be a very long vector.",
    "output": "真ん中にあるのはコロンだ。これでthetaVecはとても長いベクトルとなる。"
  },
  {
    "index": "F15519",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That's 231 elements.",
    "output": "231要素のベクトルだ。"
  },
  {
    "index": "F15520",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I display it, I find that this very long vector with all the elements of the first matrix, all the elements of the second matrix, then all the elements of the third matrix.",
    "output": "これを表示すると、このとても長いベクトルは最初の行列の要素全てと、二番目の行列の要素全てと三番目の行列の要素全てだ。"
  },
  {
    "index": "F15521",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if I want to get back my original matrices, I can do reshape thetaVec.",
    "output": "そして最初の行列を取り出したいとしたら、thetaVecをreshapeすれば良い。"
  },
  {
    "index": "F15522",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's pull out the first 110 elements and reshape them to a 10 by 11 matrix.",
    "output": "最初の110の要素を取り出しそれを10x11行列にreshapeしよう。"
  },
  {
    "index": "F15523",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if I then pull out the next 110 elements.",
    "output": "そして次に続く110要素を取り出す。"
  },
  {
    "index": "F15524",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's indices 111 to 220.",
    "output": "つまりインデックスで111から220まで。"
  },
  {
    "index": "F15525",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if I go from 221 up to the last element, which is element 231, and reshape to 1 by 11, I get back theta 3.",
    "output": "これで2を全て取り出し、そして残りの221から最後までの要素で、それは231番目の要素となるが、それを1x11にreshapeする。これでtheta3に戻せる。"
  },
  {
    "index": "F15526",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To make this process really concrete, here's how we use the unrolling idea to implement our learning algorithm.",
    "output": "このプロセスをもっともっと具体的にすべく学習アルゴリズムを実装する時にアンロールのアイデアをどう使うかをここに示す。"
  },
  {
    "index": "F15527",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that you have some initial value of the parameters theta 1, theta 2, theta 3.",
    "output": "シータ1、シータ2、シータ3の何らかの初期値があるとする。"
  },
  {
    "index": "F15528",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we're going to do is take these and unroll them into a long vector we're gonna call initial theta to pass in to fminunc as this initial setting of the parameters theta.",
    "output": "我らがやりたいのはこれら全部を取り出して一つの長いベクトルにアンロールしたい。これをinitialThetaと呼ぼう。"
  },
  {
    "index": "F15529",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The other thing we need to do is implement the cost function.",
    "output": "他にやらなきゃいけない事としては、コスト関数を実装する、という事。"
  },
  {
    "index": "F15530",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's my implementation of the cost function.",
    "output": "これが私のコスト関数の実装だ。"
  },
  {
    "index": "F15531",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The cost function is going to give us input, thetaVec, which is going to be all of my parameters vectors that in the form that's been unrolled into a vector.",
    "output": "コスト関数はthetaVecという入力を受け取る、これはパラメータのベクトルで、それはベクトルにアンロールされた形式で入っている。"
  },
  {
    "index": "F15532",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the first thing I'm going to do is I'm going to use thetaVec and I'm going to use the reshape functions. So I'll pull out elements from thetaVec and use reshape to get back my original parameter matrices, theta 1, theta 2, theta 3.",
    "output": "だから最初にやるべき事はthetaVecを使って、reshape関数を使い、thetaVecから要素を取り出しreshapeを使って元のパラメータ行列、シータ1、シータ2、シータ3を復元する。"
  },
  {
    "index": "F15533",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So these are going to be matrices that I'm going to get.",
    "output": "これらが得られるであろう行列だ。"
  },
  {
    "index": "F15534",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that gives me a more convenient form in which to use these matrices so that I can run forward propagation and back propagation to compute my derivatives, and to compute my cost function j of theta.",
    "output": "こうすることで、微分やコスト関数、Jのシータを計算する為にフォワードプロパゲーションやバックプロパゲーションを実行する為にこれらの行列を使う事が出来る。"
  },
  {
    "index": "F15535",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, I can then take my derivatives and unroll them, to keeping the elements in the same ordering as I did when I unroll my thetas.",
    "output": "最後に、微分をとって、シータをアンロールした時と要素が同じ順番になるようにアンロール出来る。"
  },
  {
    "index": "F15536",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But I'm gonna unroll D1, D2, D3, to get gradientVec which is now what my cost function can return.",
    "output": "だが、今度はD1、D2、D3をアンロールする、コスト関数が返す事ができるgradientVecを得る為に。"
  },
  {
    "index": "F15537",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It can return a vector of these derivatives.",
    "output": "これでこれらの微分のベクトルを返す事が出来る。"
  },
  {
    "index": "F15538",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, hopefully, you now have a good sense of how to convert back and forth between the matrix representation of the parameters versus the vector representation of the parameters.",
    "output": "以上で、ガウス分布が行列表現にしたりベクトル表現にしたり、の変換をどうやったらいいのかだいぶはっきり分かったんじゃないかな。"
  },
  {
    "index": "F15539",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The advantage of the matrix representation is that when your parameters are stored as matrices it's more convenient when you're doing forward propagation and back propagation and it's easier when your parameters are stored as matrices to take advantage of the, sort of, vectorized implementations.",
    "output": "行列表現の利点はパラメータを行列に保存しておけばフォワードプロパゲーションやバックワードプロパゲーションを行う時により便利で、しかも実装をいわゆるベクトル化する時にも行列の方がやりやすい、という利点がある。"
  },
  {
    "index": "F15540",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in contrast the advantage of the vector representation, when you have like thetaVec or DVec is that when you are using the advanced optimization algorithms.",
    "output": "一方対照的に、ベクトル表現の利点は、つまりthetaVecとかDVecにしておく利点はアドバンスドな最適化アルゴリズムを使う時だ。"
  },
  {
    "index": "F15541",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Those algorithms tend to assume that you have all of your parameters unrolled into a big long vector.",
    "output": "それらのアルゴリズムはパラメータを一つの大きなベクトルにアンロールしてある事を仮定している事が多い。"
  },
  {
    "index": "F15542",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so with what we just went through, hopefully you can now quickly convert between the two as needed.",
    "output": "さて、以上見てきた事で、2つの間を手早く変換出来るようになった事でしょう。"
  },
  {
    "index": "F15543",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last few videos we talked about how to do forward propagation and back propagation in a neural network in order to compute derivatives.",
    "output": "前回までの一連のビデオでニューラルネットワークにおいて微分を計算するために、フォワードプロパゲーションとバックワードプロパゲーションを行うやり方を見てきた。"
  },
  {
    "index": "F15544",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But back prop as an algorithm has a lot of details and can be a little bit tricky to implement.",
    "output": "だがバックプロパはたくさんの細かい部分のあるアルゴリズムで実装するのにちょっとトリッキーな所がある。"
  },
  {
    "index": "F15545",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And one unfortunate property is that there are many ways to have subtle bugs in back prop.",
    "output": "そして一つの不運な特徴としてバックプロパは色々と微妙にバグる、というのがある。"
  },
  {
    "index": "F15546",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that if you run it with gradient descent or some other optimizational algorithm, it could actually look like it's working.",
    "output": "だから最急降下法なり別の最適化アルゴリズムなりなどで実行すると、ぱっと見うまく行ってるように見えたりする。"
  },
  {
    "index": "F15547",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And your cost function, J of theta may end up decreasing on every iteration of gradient descent. But this could prove true even though there might be some bug in your implementation of back prop.",
    "output": "そしてあなたのコスト関数Jのシータが、結局は最急降下法の各イテレーションで減少していく場合があるが、それはあなたのバックプロパの実装にバグがあるのを見逃していても起こりうる。"
  },
  {
    "index": "F15548",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that it looks J of theta is decreasing, but you might just wind up with a neural network that has a higher level of error than you would with a bug free implementation.",
    "output": "つまり、Jのシータは減少しているように見えるが、バグ無しの実装に比べて結局はより高レベルのエラーに見舞われる可能性があり、そんなパフォーマンスになってしまっている微妙なバグに単に気づいていないだけ、という事にもなりかねない。"
  },
  {
    "index": "F15549",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what can we do about this?",
    "output": "ではこの事態にどう対処すれば良いか?"
  },
  {
    "index": "F15550",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's an idea called gradient checking that eliminates almost all of these problems.",
    "output": "グラディアントチェッキングと呼ばれるアイデアがあり、それはこれらの問題のほとんどを駆逐してくれる。"
  },
  {
    "index": "F15551",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, today every time I implement back propagation or a similar gradient to a on a neural network or any other reasonably complex model, I always implement gradient checking.",
    "output": "だからこんにちでは、バックプロパゲーションなりそれ以外でもそれなりに複雑なモデルの最急降下法を実装する時には、私は毎回、グラディアントチェッキングを実装するようにしている。"
  },
  {
    "index": "F15552",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you do this, it will help you make sure and sort of gain high confidence that your implementation of four prop and back prop or whatever is 100% correct.",
    "output": "そしてこれを行えば、あなたのフォワードプロパゲートやバックワードプロパゲートやそれ以外のなんでも、とにかく実装した物が、100%正しい、と確認したり、深く確信したりするのを助けてくれる。"
  },
  {
    "index": "F15553",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And from what I've seen this pretty much eliminates all the problems associated with a sort of a buggy implementation as a back prop.",
    "output": "私はこの手法がバックプロパゲートの実装に関したあらゆるバグを駆逐してくれるのを見て来た。"
  },
  {
    "index": "F15554",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in the previous videos I asked you to take on faith that the formulas I gave for computing the deltas and the vs and so on, I asked you to take on faith that those actually do compute the gradients of the cost function.",
    "output": "そして前回のビデオでは、私はあなたに、デルタやDたちを計算する式を単に信じてくれ、と頼んだ。私はあなたにそれらの公式が実際にコスト関数の微分を計算している事を単に信じてくれ、と頼んだ。"
  },
  {
    "index": "F15555",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But once you implement numerical gradient checking, which is the topic of this video, you'll be able to absolute verify for yourself that the code you're writing does indeed, is indeed computing the derivative of the cross function J.",
    "output": "だがひとたびあなたが数値的なグラディアントチェッキングを実装すれば、それこそがこのビデオのトピックだが、そうすればあなた自身があなたの書いたコードが確かにコスト関数Jの微分を計算している事を確認する事が出来る。"
  },
  {
    "index": "F15556",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's the idea, consider the following example.",
    "output": "そのアイデアはこうだ。以下のような例を考えてみよう。"
  },
  {
    "index": "F15557",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose that I have the function J of theta and I have some value theta and for this example gonna assume that theta is just a real number.",
    "output": "Jのシータがあるとして、そしてある値シータがあるとする。そしてこの例では、シータは単なる実数だと仮定しよう。"
  },
  {
    "index": "F15558",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say that I want to estimate the derivative of this function at this point and so the derivative is equal to the slope of that tangent one.",
    "output": "そしてこの関数の、例えばこの点の微分を推計したいとしよう。すると微分は、この接線の傾きに等しい。"
  },
  {
    "index": "F15559",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's how I'm going to numerically approximate the derivative, or rather here's a procedure for numerically approximating the derivative.",
    "output": "これが、数値的に微分を近似する方法、あるいはむしろ微分を数値的に近似する手続きはこうだ:シータ+エプシロンを計算する、つまりちょっとだけ右の値だ。そしてシータ-エプシロンも計算する。"
  },
  {
    "index": "F15560",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm gonna compute theta minus epsilon and I'm going to look at those two points, And connect them by a straight line And I'm gonna connect these two points by a straight line, and I'm gonna use the slope of that little red line as my approximation to the derivative.",
    "output": "これら二つの点を直線でつなげよう。"
  },
  {
    "index": "F15561",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Which is, the true derivative is the slope of that blue line over there.",
    "output": "ここで、真の微分の値はここの青い線の傾きだ。"
  },
  {
    "index": "F15562",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you know it seems like it would be a pretty good approximation.",
    "output": "つまり、ふむ、それはとても良い近似になりそうだ。"
  },
  {
    "index": "F15563",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Mathematically, the slope of this red line is this vertical height divided by this horizontal width.",
    "output": "数学的には、この赤い直線の傾きは垂直方向の高さ割る事のこの水平方向の幅だ。"
  },
  {
    "index": "F15564",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this point on top is the J of (Theta plus Epsilon).",
    "output": "この上の点はJのシータ+エプシロン。"
  },
  {
    "index": "F15565",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This point here is J (Theta minus Epsilon), so this vertical difference is J (Theta plus Epsilon) minus J of theta minus epsilon and this horizontal distance is just 2 epsilon.",
    "output": "つまりこの垂直の差はJのシータ+エプシロン引くことのJのシータ-エプシロン。そしてこの水平距離は2エプシロンだ。"
  },
  {
    "index": "F15566",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So my approximation is going to be that the derivative respect of theta of J of theta at this value of theta, that that's approximately J of theta plus epsilon minus J of theta minus epsilon over 2 epsilon.",
    "output": "つまり、私の近似は以下のようになる:Jのシータの、シータによる微分は、このシータの場所での微分は、だいたい近似的にはJのシータ+エプシロン引く事のJのシータ-エプシロン,割ることの2エプシロンだ。"
  },
  {
    "index": "F15567",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Usually, I use a pretty small value for epsilon, expect epsilon to be maybe on the order of 10 to the minus 4.",
    "output": "普通私は、エプシロンにはとても小さい数字を用いる。エプシロンにだいたい10の-4乗とかそういうオーダーの数をセットしてる。"
  },
  {
    "index": "F15568",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's usually a large range of different values for epsilon that work just fine.",
    "output": "だいたいにおいて、うまく行くようなエプシロンの範囲は結構大きな範囲に渡る。"
  },
  {
    "index": "F15569",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact, if you let epsilon become really small, then mathematically this term here, actually mathematically, it becomes the derivative.",
    "output": "そして実際に、エプシロンにとても小さい値を入れていくと数学的には、この項は実際に数学的に、微分となる。"
  },
  {
    "index": "F15570",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It becomes exactly the slope of the function at this point.",
    "output": "この点における関数の完全な傾きになる。"
  },
  {
    "index": "F15571",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's just that we don't want to use epsilon that's too, too small, because then you might run into numerical problems.",
    "output": "そんなに小さなエプシロンを使いたくない理由は、単に小さすぎるエプシロンは数値的な問題を引き起こすからというだけ。"
  },
  {
    "index": "F15572",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I usually use epsilon around ten to the minus four.",
    "output": "だから私はだいたい、エプシロンに10の-4乗あたりの値を使う。"
  },
  {
    "index": "F15573",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way some of you may have seen an alternative formula for s meeting the derivative which is this formula.",
    "output": "ところで、あなたがたの中には微分を推計する別の式、こんな式を見た事がある人もいるかもしれない。"
  },
  {
    "index": "F15574",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This one on the right is called a one-sided difference, whereas the formula on the left, that's called a two-sided difference.",
    "output": "この右側のは片側微分と呼ばれる物だ。一方で、左側の式は両側微分と呼ばれる。"
  },
  {
    "index": "F15575",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The two sided difference gives us a slightly more accurate estimate, so I usually use that, rather than this one sided difference estimate.",
    "output": "両側微分は通常はわずかだがより良い推計を与えるので、私は通常はこの片側微分の代わりに両側微分を用いている。つまり、具体的に言えば、Octaveであなたが実装するのは、以下のような物だ。"
  },
  {
    "index": "F15576",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, concretely, when you implement an octave, is you implemented the following, you implement call to compute gradApprox, which is going to be our approximation derivative as just here this formula, J of theta plus epsilon minus J of theta minus epsilon divided by 2 times epsilon.",
    "output": "gradApproxを計算するコードは、こうなる、これは微分を近似する。それはこんな式だ:Jのシータ+エプシロン引くことのJのシータ-エプシロン割ることの2掛けるエプシロン。"
  },
  {
    "index": "F15577",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this will give you a numerical estimate of the gradient at that point.",
    "output": "この式はこの点の微分の数値的な推計を与える。"
  },
  {
    "index": "F15578",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this example it seems like it's a pretty good estimate.",
    "output": "そしてこの例では、これは極めて良い推計になっているようだ。"
  },
  {
    "index": "F15579",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now on the previous slide, we considered the case of when theta was a rolled number.",
    "output": "ここで、前のスライドでは、シータが実数の場合を検討した。"
  },
  {
    "index": "F15580",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's look at a more general case of when theta is a vector parameter, so let's say theta is an R n.",
    "output": "ここでは、より一般的なケースとなる、シータがパラメータベクトルの場合を見てみよう。シータがRnとしよう。"
  },
  {
    "index": "F15581",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it might be an unrolled version of the parameters of our neural network.",
    "output": "これはニューラルネットワークのパラメータをアンロールしたバージョンと考えても良い。"
  },
  {
    "index": "F15582",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So theta is a vector that has n elements, theta 1 up to theta n. We can then use a similar idea to approximate all the partial derivative terms.",
    "output": "つまりシータはn個の要素を持つベクトルで、つまりシータ1からシータnまでで、これらそれぞれに関する偏微分の項についてさっきと似たような近似のアイデアを用いる事が出来る。"
  },
  {
    "index": "F15583",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely the partial derivative of a cost function with respect to the first parameter, theta one, that can be obtained by taking J and increasing theta one.",
    "output": "具体的には、コスト関数の最初のパラメータ、シータ1による偏微分は、以下のように求める事が出来る。"
  },
  {
    "index": "F15584",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you have J of theta one plus epsilon and so on.",
    "output": "それはJに対しシータ1を増加させて、つまりJのシータ1+エプシロンにして、そこから引く事のJのシータ1-エプシロンに、全体を2エプシロンで割る。"
  },
  {
    "index": "F15585",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Minus J of this theta one minus epsilon and divide it by two epsilon.",
    "output": "二番目のパラメータシータ2に関しての偏微分は、だいたいこれと同じ事をするが、唯一の違いは、エプシロンだけ増加させるのがシータ2だという所。"
  },
  {
    "index": "F15586",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The partial derivative respect to the second parameter theta two, is again this thing except that you would take J of here you're increasing theta two by epsilon, and here you're decreasing theta two by epsilon and so on down to the derivative.",
    "output": "そしてここは、シータ2をエプシロンだけ減少させる。"
  },
  {
    "index": "F15587",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "With respect of theta n would give you increase and decrease theta and by epsilon over there.",
    "output": "などと、シータnに関する微分まで降りていく。そこではここにあるシータnをエプシロンだけ増加させたり減少させたりする。"
  },
  {
    "index": "F15588",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, these equations give you a way to numerically approximate the partial derivative of J with respect to any one of your parameters theta i.",
    "output": "さて、これらの等式はJの、各パラメータに対する偏微分を数値的に近似する方法を与える。"
  },
  {
    "index": "F15589",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Completely, what you implement is therefore the following.",
    "output": "具体的にはあなたが実装するのは、以下のような物だ。"
  },
  {
    "index": "F15590",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We implement the following in octave to numerically compute the derivatives.",
    "output": "我らはOctaveで以下のように実装して数値的に微分を求める。"
  },
  {
    "index": "F15591",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We say, for i = 1:n, where n is the dimension of our parameter of vector theta.",
    "output": "fori=1からnまでの、、、ここでnはパラメータベクトル、シータの次元だ。"
  },
  {
    "index": "F15592",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I usually do this with the unrolled version of the parameter.",
    "output": "そして私は普通、これをアンロールしたバージョンのパラメータでやる。"
  },
  {
    "index": "F15593",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So theta is just a long list of all of my parameters in my neural network, say.",
    "output": "つまりシータは私のニューラルネットワークのパラメータの単なる長いリストに過ぎない。"
  },
  {
    "index": "F15594",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm gonna set thetaPlus = theta, then increase thetaPlus of the (i) element by epsilon.",
    "output": "thetaPlusにthetaをセットし、thetaPlusのi番目の要素をEPSILONだけ増加させる。"
  },
  {
    "index": "F15595",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is basically thetaPlus is equal to theta except for thetaPlus(i) which is now incremented by epsilon.",
    "output": "つまりこれは、thetaPlusは基本的にはthetaに等しい、thetaPlus(i)以外は。"
  },
  {
    "index": "F15596",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Epsilon, so theta plus is equal to, write theta 1, theta 2 and so on.",
    "output": "つまり、thetaPlusはtheta1,theta2,...などと等しくて、そしてtheta(i)の所では、EPSILONを足した物に等しい。"
  },
  {
    "index": "F15597",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then theta I has epsilon added to it and then we go down to theta N.",
    "output": "そしてさらにtheta(n)まで降りていく。"
  },
  {
    "index": "F15598",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similar these two lines set theta minus to something similar except that this instead of theta I plus Epsilon, this now becomes theta I minus Epsilon.",
    "output": "同様に、これら二つの行はthetaMinusに、上と似たような物を代入しているが、theta(i)+EPSILONの代わりにtheta(i)-EPSILONな所だけが違う。"
  },
  {
    "index": "F15599",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then finally you implement this gradApprox (i) and this would give you your approximation to the partial derivative respect of theta i of J of theta.",
    "output": "そして最後に、このgradApprox(i)を実装する。これがJのシータのシータiによる偏微分の近似を与える。"
  },
  {
    "index": "F15600",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the way we use this in our neural network implementation is, we would implement this four loop to compute the top partial derivative of the cost function for respect to every parameter in that network, and we can then take the gradient that we got from backprop.",
    "output": "そしてこれの使い方としては、ニューラルネットワークの実装において、ニューラルネットワークの各パラメータによるコスト関数の偏微分を求める為にこれを実装する、このforループを実装する。"
  },
  {
    "index": "F15601",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So DVec was the derivative we got from backprop.",
    "output": "そして次に、バックプロパからグラディアントを取得出来る。"
  },
  {
    "index": "F15602",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "All right, so backprop, backpropogation, was a relatively efficient way to compute a derivative or a partial derivative.",
    "output": "つまりDVecはバックプロパから得た微分だ。"
  },
  {
    "index": "F15603",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of a cost function with respect to all our parameters.",
    "output": "つまり、バックプロパ、バックプロパゲーションは微分を計算する比較的効率的な方法だ、より正確に言えばコスト関数の各パラメータによる偏微分を計算する為の。"
  },
  {
    "index": "F15604",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what I usually do is then, take my numerically computed derivative that is this gradApprox that we just had from up here.",
    "output": "そして私がよくやるのは、数値的に計算した微分に対して、それはこのここの上で得たgradApproxだが、これがbackpropで得た物と等しいかほとんど等しい事を確認する事だ。"
  },
  {
    "index": "F15605",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And make sure that that is equal or approximately equal up to small values of numerical round up, that it's pretty close.",
    "output": "小さい数値的な丸めの範囲に収まっているかを。backpropで得たDVecと極めて近いかを。"
  },
  {
    "index": "F15606",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if these two ways of computing the derivative give me the same answer, or give me any similar answers, up to a few decimal places, then I'm much more confident that my implementation of backprop is correct.",
    "output": "そしてこれら二つの方法で計算した微分の値が、同じ答えか、少なくともとても近い答えをはじきだしたなら、小数点以下数桁の範囲で近ければ、私のバックプロパの実装が正しい、という事によりしっかりと自信を持つ事が出来る。"
  },
  {
    "index": "F15607",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And when I plug these DVec vectors into gradient assent or some advanced optimization algorithm, I can then be much more confident that I'm computing the derivatives correctly, and therefore that hopefully my code will run correctly and do a good job optimizing J of theta.",
    "output": "そうしてから、これらのDVecベクトルを最急降下法なり何らかのアドバンスドな最適化アルゴリズムに食わせれば、その時には微分をちゃんと計算していると自信を持っているから、自分のコードが正しく走るとも期待出来て、Jのシータを最適化するのに良い仕事をしてくれると期待出来る。"
  },
  {
    "index": "F15608",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, I wanna put everything together and tell you how to implement this numerical gradient checking.",
    "output": "最後に、全部を合わせて数値的グラディアントチェッキングをどう実装するかをお話したい。"
  },
  {
    "index": "F15609",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's what I usually do.",
    "output": "私はいつも、こんな風にする。"
  },
  {
    "index": "F15610",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First thing I do is implement back propagation to compute DVec.",
    "output": "最初にやるのは、DVecを計算する為にバックプロパゲーションを実装する。"
  },
  {
    "index": "F15611",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there's a procedure we talked about in the earlier video to compute DVec which may be our unrolled version of these matrices.",
    "output": "これは以前のビデオで話したDVecを計算する手順となり、それはこれらの行列を展開したバージョンとなる。"
  },
  {
    "index": "F15612",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So then what I do, is implement a numerical gradient checking to compute gradApprox.",
    "output": "次に私がやるのは、gradApproxを計算する為に数値的なグラディアントチェッキングを実装する事だ。"
  },
  {
    "index": "F15613",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is what I described earlier in this video and in the previous slide.",
    "output": "これが私がこのビデオで話してきた所だ、前のスライドで話した奴。"
  },
  {
    "index": "F15614",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then should make sure that DVec and gradApprox give similar values, you know let's say up to a few decimal places.",
    "output": "そして次に、DVecとgradApproxが似た値かどうかを確認する、たとえば小数点第二位とか第三位までで一致するかを見る。"
  },
  {
    "index": "F15615",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally and this is the important step, before you start to use your code for learning, for seriously training your network, it's important to turn off gradient checking and to no longer compute this gradApprox thing using the numerical derivative formulas that we talked about earlier in this video.",
    "output": "そして最後に、そしてこれは大切なステップなのだが、あなたのコードを実際に学習させ始める前に、真面目にネットワークをトレーニングする前に、グラディアントチェッキングを切るのが大切だ。"
  },
  {
    "index": "F15616",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason for that is the numeric code gradient checking code, the stuff we talked about in this video, that's a very computationally expensive, that's a very slow way to try to approximate the derivative.",
    "output": "そしてそれ以降はこのビデオでやってきたgradApproxの数値的な微分の式での計算をしないように。"
  },
  {
    "index": "F15617",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas In contrast, the back propagation algorithm that we talked about earlier, that is the thing we talked about earlier for computing. You know, D1, D2, D3 for Dvec.",
    "output": "その理由は、数値的なグラディアントチェッキングのコードは、このビデオで議論してきた内容は、計算量的にとても高価で、微分を近似しようとするのには凄い遅いやり方だ。"
  },
  {
    "index": "F15618",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Backprop is much more computationally efficient way of computing for derivatives. So once you've verified that your implementation of back propagation is correct, you should turn off gradient checking and just stop using that.",
    "output": "一方で対照的に、以前に話したバックプロパゲーションのアルゴリズムはそれは前にD1とかD2とかD3とかDVecを計算するのに議論してきた物だが、そのバックプロパゲーションは微分を計算するのに、もっとずっと計算量的に効率的な方法だ。"
  },
  {
    "index": "F15619",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just to reiterate, you should be sure to disable your gradient checking code before running your algorithm for many iterations of gradient descent or for many iterations of the advanced optimization algorithms, in order to train your classifier.",
    "output": "だからひとたびあなたのバックプロパゲーションの実装が正しい、と確認した後には、グラディアントチェッキングは切るべきだ、単純に使うのをやめるべきだ。"
  },
  {
    "index": "F15620",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, if you were to run the numerical gradient checking on every single iteration of gradient descent.",
    "output": "あなたのアルゴリズムを最急降下法のたくさんの繰り返しで走らせる前には、あるいはアドバンスドな最適化アルゴリズムで分類器を訓練する為にたくさんの繰り返しを走らせる前には、グラディアントチェッキングのコードを切る事を忘れないようにしよう。"
  },
  {
    "index": "F15621",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or if you were in the inner loop of your costFunction, then your code would be very slow.",
    "output": "具体的には、もし万が一最急降下法の各イテレーションで毎回数値的グラディアントチェッキングを走らせてしまったら、またはcostFunctionの内側のループで走らせてしまったら、あなたのコードはとてものろくなってしまうだろう。"
  },
  {
    "index": "F15622",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because the numerical gradient checking code is much slower than the backpropagation algorithm, than the backpropagation method where, you remember, we were computing delta(4), delta(3), delta(2), and so on.",
    "output": "何故なら数値的なグラディアントチェッキングのコードはバックプロパゲーションのアルゴリズムに比べてずっと遅いからだ。つまりデルタ4、デルタ3、デルタ2などを計算する時に用いたバックプロパゲーションと比較するとだ。"
  },
  {
    "index": "F15623",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is a much faster way to compute derivates than gradient checking.",
    "output": "それはグラディアントチェッキングよりもずっと早い微分の計算方法だ。"
  },
  {
    "index": "F15624",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when you're ready, once you've verified the implementation of back propagation is correct, make sure you turn off or you disable your gradient checking code while you train your algorithm, or else you code could run very slowly.",
    "output": "だから準備が出来たら。一旦あなたのバックプロパゲーションの実装が正しいと確認出来たら、グラディアントチェッキングのコードを切るなりdisableするなりを確実に行おう、アルゴリズムをトレーニングする間は。"
  },
  {
    "index": "F15625",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's how you take gradients numericaly, and that's how you can verify tha implementation of back propagation is correct.",
    "output": "こうやって、あなたのバックプロパゲーションの実装が正しい、と検証する事が出来る。"
  },
  {
    "index": "F15626",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whenever I implement back propagation or similar gradient discerning algorithm for a complicated mode,l I always use gradient checking and this really helps me make sure that my code is correct.",
    "output": "私がバックプロパゲーションや、似たような複雑なモデルに対して最急降下法を実装する時にはいつでも、グラディアントチェッキングを使っている。これは自分のコードが正しいと確認する為の、本当に良い助けとなってくれるんだ。"
  },
  {
    "index": "F15627",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, we've put together almost all the pieces you need in order to implement and train in your network.",
    "output": "前回のビデオではニューラルネットワークを実装し訓練するのに必要なほとんど全てのピースをまとめた。"
  },
  {
    "index": "F15628",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's just one last idea I need to share with you, which is the idea of random initialization.",
    "output": "だがまだもう一つ、最後のピースをあなたにも伝えなくてはいけない。それはランダム初期化と呼ばれるアイデアだ。"
  },
  {
    "index": "F15629",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When you're running an algorithm of gradient descent, or also the advanced optimization algorithms, we need to pick some initial value for the parameters theta.",
    "output": "最急降下法なりアドバンスドな最適化アルゴリズムなりのようなアルゴリズムを走らせる時には、パラメータシータのある初期値を選ぶ必要がある。"
  },
  {
    "index": "F15630",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for the advanced optimization algorithm, it assumes you will pass it some initial value for the parameters theta.",
    "output": "アドバンスドな最適化アルゴリズムは、あなたがパラメータのシータのある初期値を渡すと想定している。"
  },
  {
    "index": "F15631",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's consider a gradient descent.",
    "output": "ここでは最急降下法を考えよう。"
  },
  {
    "index": "F15632",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For that, we'll also need to initialize theta to something, and then we can slowly take steps to go downhill using gradient descent.",
    "output": "その場合でも、シータを何かしらには初期化しなくてはいけない。"
  },
  {
    "index": "F15633",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To go downhill, to minimize the function j of theta.",
    "output": "そしてそこから、ゆっくりと丘を最急降下法を使って一歩一歩降りていき、Jのシータの最小値まで降りていく。"
  },
  {
    "index": "F15634",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what can we set the initial value of theta to?",
    "output": "ではシータの初期値をどうセットしたらいいだろうか?"
  },
  {
    "index": "F15635",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Is it possible to set the initial value of theta to the vector of all zeros?",
    "output": "シータの初期値に全部0を入れる、というのが考えられる。"
  },
  {
    "index": "F15636",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas this worked okay when we were using logistic regression, initializing all of your parameters to zero actually does not work when you are trading on your own network.",
    "output": "これはロジスティック回帰の時にはうまく行ったが、パラメータの初期値を全て0にするのはニューラルネットワークをトレーニングする時にはうまく行かない。"
  },
  {
    "index": "F15637",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Consider trading the follow Neural network, and let's say we initialize all the parameters of the network to 0.",
    "output": "以下のニューラルネットワークをトレーニングする事を考えてみよう。そして全てのネットワークのパラメータを0に初期化したとしよう。"
  },
  {
    "index": "F15638",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you do that, then what you, what that means is that at the initialization, this blue weight, colored in blue is gonna equal to that weight, so they're both 0.",
    "output": "そうすると、それの意味する事は、この青いウェイトを初期化する時に、、、ここでこのウェイトを青で色づけする。これらは共に0となる。"
  },
  {
    "index": "F15639",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this weight that I'm coloring in in red, is equal to that weight, colored in red, and also this weight, which I'm coloring in green is going to equal to the value of that weight.",
    "output": "そしてこのウェイト、赤でいろづけしておくと、このウェイトがこっちのウェイト、これも赤で色付けしておくが、これらが等しい。"
  },
  {
    "index": "F15640",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what that means is that both of your hidden units, A1 and A2, are going to be computing the same function of your inputs.",
    "output": "そしてこのウェイト、これは緑に色付けしよう、これはこっちのウェイトの値と等しくなる。"
  },
  {
    "index": "F15641",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And thus you end up with for every one of your training examples, you end up with A 2 1 equals A 2 2.",
    "output": "その意味する所は、隠れユニット、a1とa2はどちらも同じ入力の関数を計算する事になる、という事。"
  },
  {
    "index": "F15642",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And moreover because I'm not going to show this in too much detail, but because these outgoing weights are the same you can also show that the delta values are also gonna be the same.",
    "output": "さらに、あまり細かい話に首をつっこむ気は無いけれど、しかしこれらのウェイトが等しければ、デルタの値も等しくなる、という事を示す事が出来る。"
  },
  {
    "index": "F15643",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So concretely you end up with delta 1 1, delta 2 1 equals delta 2 2, and if you work through the map further, what you can show is that the partial derivatives with respect to your parameters will satisfy the following, that the partial derivative of the cost function with respected to breaking out the derivatives respect to these two blue waves in your network.",
    "output": "さらに数学を続けていくと、あなたのパラメータによる偏微分は以下を満たす事を示す事が出来る。コスト関数の偏微分、、、ここに書いている、ニューラルネットワークのこれら二つの青のウェイトによる偏微分。"
  },
  {
    "index": "F15644",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You find that these two partial derivatives are going to be equal to each other.",
    "output": "これら二つの偏微分はお互いに等しい事が分かる。"
  },
  {
    "index": "F15645",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what this means is that even after say one greater descent update, you're going to update, say, this first blue rate was learning rate times this, and you're gonna update the second blue rate with some learning rate times this.",
    "output": "この意味する所は、一回最急降下法のアップデートが行われた後も、この青いウェイトを学習率掛けるこれでアップデートし、二番目の青いウェイトも学習率掛けるこれ、でアップデートする。"
  },
  {
    "index": "F15646",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what this means is that even after one created the descent update, those two blue rates, those two blue color parameters will end up the same as each other.",
    "output": "だがその意味する所は、一回最急降下法のアップデートをかました後でも、これら二つの青のウェイトは、これら二つの青の色付けしたパラメータは結局相等しいままだ。"
  },
  {
    "index": "F15647",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there'll be some nonzero value, but this value would equal to that value.",
    "output": "なんらかの非0の値にはなるだろう、だがこの値はこっちの値と等しい。"
  },
  {
    "index": "F15648",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, even after one gradient descent update, this value would equal to that value.",
    "output": "そして同様に、一回最急降下法のアップデートを行った後でも、この値はこっちの値と等しい。"
  },
  {
    "index": "F15649",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There'll still be some non-zero values, just that the two red values are equal to each other.",
    "output": "なんらかの非0の値にはなる。この二つの赤の値もお互いに相等しい。"
  },
  {
    "index": "F15650",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, the two green ways. Well, they'll both change values, but they'll both end up with the same value as each other.",
    "output": "そして同様に、二つの緑のウェイトもそれらの値も共に変化するが、だが変化した結果はどちらも同じ値になる。"
  },
  {
    "index": "F15651",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So after each update, the parameters corresponding to the inputs going into each of the two hidden units are identical.",
    "output": "だから各アップデートの後でも、各入力から二つの隠れユニットへと至る二つのウェイトは、同一となる。"
  },
  {
    "index": "F15652",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That's just saying that the two green weights are still the same, the two red weights are still the same, the two blue weights are still the same, and what that means is that even after one iteration of say, gradient descent and descent.",
    "output": "それは単に、二つの緑のウェイトが等しくなり、二つの赤のウェイトも等しくなり、二つの青のウェイトも等しいままだ、と言っているだけだ。"
  },
  {
    "index": "F15653",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You find that your two headed units are still computing exactly the same functions of the inputs.",
    "output": "そしてその意味する所は、例えば最急降下法などの一回のイテレーションの後でも、あなたの二つの隠れユニットが全く同一の入力の関数を計算し続ける事となる。"
  },
  {
    "index": "F15654",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You still have the a1(2) = a2(2).",
    "output": "だから、a(1)2=a(2)2のままとなる。"
  },
  {
    "index": "F15655",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so you're back to this case.",
    "output": "そしてこのケースに戻る。"
  },
  {
    "index": "F15656",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as you keep running greater descent, the blue waves,, the two blue waves, will stay the same as each other.",
    "output": "そして最急降下法を走らせ続けても、この二つの青のウェイトはお互いに等しいままだ。"
  },
  {
    "index": "F15657",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The two red waves will stay the same as each other and the two green waves will stay the same as each other.",
    "output": "二つの赤いウェイトはお互いに等しいままだ。"
  },
  {
    "index": "F15658",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what this means is that your neural network really can compute very interesting functions, right?",
    "output": "この意味する所は、このニューラルネットワークはあまり面白い関数は計算出来ない、という事。"
  },
  {
    "index": "F15659",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Imagine that you had not only two hidden units, but imagine that you had many, many hidden units.",
    "output": "二つの隠れユニットだけじゃなくてもっともっとたくさんの隠れユニットがある場合を考えてみよう。"
  },
  {
    "index": "F15660",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then what this is saying is that all of your headed units are computing the exact same feature.",
    "output": "するとこれの意味する所は、隠れユニットが全て完全に同じフィーチャーを計算する、という事になる。"
  },
  {
    "index": "F15661",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "All of your hidden units are computing the exact same function of the input.",
    "output": "全ての隠れユニットが、全く同一の、入力の関数を計算する事になる。"
  },
  {
    "index": "F15662",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is a highly redundant representation because you find the logistic progression unit. It really has to see only one feature because all of these are the same.",
    "output": "何故ならそれはつまりは、最後のロジスティック回帰のユニットは、実際は一つの入力があるだけに見えるから。"
  },
  {
    "index": "F15663",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this prevents you and your network from doing something interesting.",
    "output": "何故ならこれらは全て同じで、このことがあなたのニューラルネットワークが何かしら面白い学習をする事を妨げているから。"
  },
  {
    "index": "F15664",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to get around this problem, the way we initialize the parameters of a neural network therefore is with random initialization.",
    "output": "この問題を回避する為に、ニューラルネットワークのパラメータを初期化する方法は、ランダム初期化の方法だ。"
  },
  {
    "index": "F15665",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, the problem was saw on the previous slide is something called the problem of symmetric ways, that's the ways are being the same.",
    "output": "具体的には、前のスライドで我らが見た問題は対称ウェイトの問題と呼ばれる事もあり、それはウェイトが全て同じという事だ。"
  },
  {
    "index": "F15666",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this random initialization is how we perform symmetry breaking.",
    "output": "だからこのランダム初期化で対称性を破る訳だ。"
  },
  {
    "index": "F15667",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what we do is we initialize each value of theta to a random number between minus epsilon and epsilon.",
    "output": "我らのやるべき事は、各シータの値を-エプシロンからエプシロンまでの間のランダムな値で初期化する。"
  },
  {
    "index": "F15668",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is a notation to b numbers between minus epsilon and plus epsilon.",
    "output": "これは-エプシロンと+エプシロンの間の値を表す記法だ。"
  },
  {
    "index": "F15669",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The way I write code to do this in octave is I've said Theta1 should be equal to this.",
    "output": "Octaveでこれをやるコードを書く方法は、Theta1をイコールこれ、とする。"
  },
  {
    "index": "F15670",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "All the values are between 0 and 1, so these are going to be raw numbers that take on any continuous values between 0 and 1.",
    "output": "つまりこれらは、連続的な実数の0と1の間のいかなる値でも取る事が出来る。"
  },
  {
    "index": "F15671",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if you take a number between zero and one, multiply it by two times INIT_EPSILON then minus INIT_EPSILON, then you end up with a number that's between minus epsilon and plus epsilon.",
    "output": "すると、0と1の間の数を取って、2*エプシロンを掛けて、そこからエプシロンを引く。"
  },
  {
    "index": "F15672",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the so that leads us, this epsilon here has nothing to do with the epsilon that we were using when we were doing gradient checking.",
    "output": "すると最終的には-エプシロンとエプシロンの間の数となる。"
  },
  {
    "index": "F15673",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when numerical gradient checking, there we were adding some values of epsilon and theta.",
    "output": "ついでに言っておくと、このエプシロンはグラディアントチェッキングで使ってたエプシロンとは全く関係が無い。"
  },
  {
    "index": "F15674",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is your unrelated value of epsilon.",
    "output": "数値的なグラディアントチェッキングを行っていた時は、シータにあるエプシロンという値を足していたが、この値は、そのエプシロンとは無関係だ。"
  },
  {
    "index": "F15675",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We just wanted to notate init epsilon just to distinguish it from the value of epsilon we were using in gradient checking.",
    "output": "そんな訳でこのエプシロンをINIT_EPSILONと書いている。これはグラディアントチェッキングで使ったエプシロンの値と区別する為だけの理由だ。"
  },
  {
    "index": "F15676",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly if you want to initialize theta2 to a random 1 by 11 matrix you can do so using this piece of code here.",
    "output": "同様にもしシータ2を1x11のランダムの行列で初期化したければ、このコード辺でそれが行える。"
  },
  {
    "index": "F15677",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So to summarize, to create a neural network what you should do is randomly initialize the waves to small values close to zero, between -epsilon and +epsilon say.",
    "output": "さて、まとめると、ニューラルネットワークをトレーニングする為には、ウェイトを小さな値、0のそばの-エプシロンから+エプシロンの間のどこかの値でランダムに初期化しなければならない。"
  },
  {
    "index": "F15678",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then implement back propagation, do great in checking, and use either great in descent or 1b advanced optimization algorithms to try to minimize j(theta) as a function of the parameters theta starting from just randomly chosen initial value for the parameters.",
    "output": "そしてバックプロパゲーションを実装し、グラディアントチェッキングをする。そして最急降下法なりアドバンスドな最適化アルゴリズムなりを用いて、Jのシータをパラメータシータの関数として、ランダムに初期化したパラメータから始めて最小化を試みる。"
  },
  {
    "index": "F15679",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by doing symmetry breaking, which is this process, hopefully great gradient descent or the advanced optimization algorithms will be able to find a good value of theta.",
    "output": "そして対称性の破れを行う事で、それはこのプロセスだが、最急降下法なりアドバンスドな最適化アルゴリズムなりが、シータの良い値を見つける事が期待出来るようになる。"
  },
  {
    "index": "F15680",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, it's taken us a lot of videos to get through the neural network learning algorithm.",
    "output": "ニューラルネットワークのアルゴリズムを見ていくのにたくさんのビデオを費やしてきた。"
  },
  {
    "index": "F15681",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, what I'd like to do is try to put all the pieces together, to give a overall summary or a bigger picture view, of how all the pieces fit together and of the overall process of how to implement a neural network learning algorithm.",
    "output": "このビデオでは、全てのピースを一つに合わせて全体的なサマリー、あるいはより大きな鳥瞰図を提供したい、全てのピースがどう組み合わさりニューラルネットワークを全体としてどう実装したらいいのかについて。"
  },
  {
    "index": "F15682",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When training a neural network, the first thing you need to do is pick some network architecture and by architecture I just mean connectivity pattern between the neurons.",
    "output": "ニューラルネットワークをトレーニングする時は最初にやらなくてはいけない事はなんらかのネットワークアーキテクチャを選ぶ事だ。ここでアーキテクチャという言葉でニューロン同士の接続のパターンを意味している。"
  },
  {
    "index": "F15683",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you know, we might choose between say, a neural network with three input units and five hidden units and four output units versus one of 3, 5 hidden, 5 hidden, 4 output and here are 3, 5, 5, 5 units in each of three hidden layers and four open units, and so these choices of how many hidden units in each layer and how many hidden layers, those are architecture choices.",
    "output": "だから例えば3つの入力ユニットに5つの隠れユニット、そして4つの出力を選ぶかもしれないし、対して3つ、5つの隠れ、5つの隠れ4つの出力を選ぶかもしれないし、そしてこれは3で、5,5,5ユニットが三つの隠れレイヤーにあって、そして4つの出力ユニット、これを選ぶかもしれない。これらが幾つの隠れユニットと幾つの隠れレイヤーを持つかの選択肢だ。"
  },
  {
    "index": "F15684",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how do you make these choices?",
    "output": "これらの選択肢をどうやって選ぶのか?"
  },
  {
    "index": "F15685",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well first, the number of input units well that's pretty well defined.",
    "output": "まず、入力ユニットの総数はかっちりと定義されている。"
  },
  {
    "index": "F15686",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And once you decides on the fix set of features x the number of input units will just be, you know, the dimension of your features x(i) would be determined by that.",
    "output": "そして一旦固定されたフィーチャーの集まりxを決定してしまえば、入力ユニットの数は、フィーチャーx(i)の次元によって決定される。"
  },
  {
    "index": "F15687",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you are doing multiclass classifications the number of output of this will be determined by the number of classes in your classification problem.",
    "output": "そしてもしマルチクラスの分類問題を扱っているなら、出力の数は、あなたの扱ってる分類問題のクラスの数によって決められる。"
  },
  {
    "index": "F15688",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just a reminder if you have a multiclass classification where y takes on say values between 1 and 10, so that you have ten possible classes.",
    "output": "思い出してもらう為に触れておくとマルチクラスの分類問題の時には、例えばyが1から10の間の数を取るとすると、つまり10個のとりうるクラスがある事になるが、その時は、覚えているだろうか、出力のyはこれらのベクトルとなるのだった。"
  },
  {
    "index": "F15689",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So instead of clause one, you recode it as a vector like that, or for the second class you recode it as a vector like that.",
    "output": "つまりこれがクラス1で、それをこのベクトルに再コーディングする。二番目のクラスはこんなベクトルに再コーディングする。"
  },
  {
    "index": "F15690",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if one of these apples takes on the fifth class, you know, y equals 5, then what you're showing to your neural network is not actually a value of y equals 5, instead here at the upper layer which would have ten output units, you will instead feed to the vector which you know with one in the fifth position and a bunch of zeros down here.",
    "output": "だから、これらのうちの一つを例として挙げると、5番目のクラスの場合、yイコール5の時は、ニューラルネットワークにおいて見る事になるのはy=5という値では無く、その代わりにここでは出力レイヤで、この場合は10個の出力ユニットがあり、そこに5番目に1があってそれ以外が0で埋まっているようなベクトルを食わせることになる。"
  },
  {
    "index": "F15691",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the choice of number of input units and number of output units is maybe somewhat reasonably straightforward.",
    "output": "つまり入力ユニットと出力ユニットの総数の選択は割とまっすぐ決まると思う。"
  },
  {
    "index": "F15692",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as for the number of hidden units and the number of hidden layers, a reasonable default is to use a single hidden layer and so this type of neural network shown on the left with just one hidden layer is probably the most common.",
    "output": "そして隠れユニットと隠れレイヤーの数の選択についてはもっとも普通の基本形は隠れレイヤは一層。つまりこの左側に示したような隠れレイヤ一つのニューラルネットワークが恐らくもっとも一般的な物と言える。"
  },
  {
    "index": "F15693",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or if you use more than one hidden layer, again the reasonable default will be to have the same number of hidden units in every single layer.",
    "output": "そしてもし一つ以上の隠れレイヤを使うならここでも普通の基本形は各レイヤの隠れユニットは同じ数にする、という物。"
  },
  {
    "index": "F15694",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here we have two hidden layers and each of these hidden layers have the same number five of hidden units and here we have, you know, three hidden layers and each of them has the same number, that is five hidden units.",
    "output": "つまりここでは2つの隠れレイヤがあり、これらの隠れレイヤはそれぞれ同じ隠れユニットの数である5つの隠れユニットを持ち、こちらは見ての通り3つの隠れレイヤがありそれらはそれぞれ、同じユニット数でそれぞれ5つの隠れユニットがある。"
  },
  {
    "index": "F15695",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Rather than doing this sort of network architecture on the left would be a perfect ably reasonable default.",
    "output": "だがこれらのネットワークアーキテクチャを使うまでもなくこの左側のアーキテクチャでも十分に悪くないデフォルトの選択肢だ。"
  },
  {
    "index": "F15696",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as for the number of hidden units - usually, the more hidden units the better; it's just that if you have a lot of hidden units, it can become more computationally expensive, but very often, having more hidden units is a good thing.",
    "output": "そして隠れユニットの数としては、通常は隠れユニットが多ければ多い程良いが隠れユニットがたくさんあると計算量的には高価になる。"
  },
  {
    "index": "F15697",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And usually the number of hidden units in each layer will be maybe comparable to the dimension of x, comparable to the number of features, or it could be any where from same number of hidden units of input features to maybe so that three or four times of that.",
    "output": "だがしばしば、隠れユニットは多ければ多い程良い。"
  },
  {
    "index": "F15698",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So having the number of hidden units is comparable.",
    "output": "そして通常は、各レイヤーの隠れユニットの総数はだいたいxの次元と同じ程度、フィーチャーの総数と同じ程度で、あるいは入力のフィーチャーの総数と同じ程度から2倍とか3倍とか4倍程度の適当な数が良い。"
  },
  {
    "index": "F15699",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, several times, or some what bigger than the number of input features is often a useful thing to do So, hopefully this gives you one reasonable set of default choices for neural architecture and and if you follow these guidelines, you will probably get something that works well, but in a later set of videos where I will talk specifically about advice for how to apply algorithms, I will actually say a lot more about how to choose a neural network architecture.",
    "output": "だから入力のフィーチャーと同じ程度の数か、その数倍程度の数でもやる価値のある事が多い。以上で、ニューラルネットワークのアーキテクチャのデフォルトの選択としてリーズナブルな選択肢を幾つか示せただろう。"
  },
  {
    "index": "F15700",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or actually have quite a lot I want to say later to make good choices for the number of hidden units, the number of hidden layers, and so on.",
    "output": "また、実際に隠れユニットや隠れレイヤーの総数の良い選び方について、などはたくさん話すつもりだ。"
  },
  {
    "index": "F15701",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, here's what we need to implement in order to trade in neural network, there are actually six steps that I have; I have four on this slide and two more steps on the next slide.",
    "output": "次に、ニューラルネットワークをトレーニングする為に実装しなくてはいけない事はこれだ。6つのステップがある。"
  },
  {
    "index": "F15702",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First step is to set up the neural network and to randomly initialize the values of the weights.",
    "output": "最初のステップはニューラルネットワークをセットアップして、ウェイトの値をランダムに初期化する。"
  },
  {
    "index": "F15703",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we usually initialize the weights to small values near zero.",
    "output": "ウェイトの初期化は普通、0付近の小さな値で初期化する。"
  },
  {
    "index": "F15704",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then we implement forward propagation so that we can input any excellent neural network and compute h of x which is this output vector of the y values.",
    "output": "次にフォワードプロパゲーションを実装する、入力をニューラルネットワークに入れて予測が計算出来るように。つまりhのxを計算し、この出力ベクトルのyの値を得る為に。"
  },
  {
    "index": "F15705",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We then also implement code to compute this cost function j of theta.",
    "output": "そして次にコスト関数であるJのシータを計算するコードを実装する。"
  },
  {
    "index": "F15706",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And next we implement back-prop, or the back-propagation algorithm, to compute these partial derivatives terms, partial derivatives of j of theta with respect to the parameters.",
    "output": "そして次にバックプロパゲーションをバックプロパゲーションアルゴリズムを実装する、これらの偏微分の項を計算する為に、、、Jのシータの、パラメータによる偏微分を計算する為に。"
  },
  {
    "index": "F15707",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, to implement back prop. Usually we will do that with a fore loop over the training examples.",
    "output": "だが最初にバックプロパゲーションを実装する時には、ほぼ確実にforループのある実装から始めるべきだろう。"
  },
  {
    "index": "F15708",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Some of you may have heard of advanced, and frankly very advanced factorization methods where you don't have a four-loop over the m-training examples, that the first time you're implementing back prop there should almost certainly the four loop in your code, where you're iterating over the examples, you know, x1, y1, then so you do forward prop and back prop on the first example, and then in the second iteration of the four-loop, you do forward propagation and back propagation on the second example, and so on.",
    "output": "そこでトレーニング手本に渡ってイテレーションを行い、つまり、まずx1とy1に対して最初の手本に対してフォワードプロパケーションとバックプロパゲーションを行い、そして次にforループの二回目のイテレーションで二番目の手本に対しフォワードプロパケーションとバックワードプロパゲーションを行う、などなど。"
  },
  {
    "index": "F15709",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Until you get through the final example.",
    "output": "これを最後の手本に至るまで進める。"
  },
  {
    "index": "F15710",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there should be a four-loop in your implementation of back prop, at least the first time implementing it.",
    "output": "つまり、少なくとも初めて実装する時にはあなたのバックプロパゲーションの実装にはforループがあるはずだ。"
  },
  {
    "index": "F15711",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then there are frankly somewhat complicated ways to do this without a four-loop, but I definitely do not recommend trying to do that much more complicated version the first time you try to implement back prop.",
    "output": "そして簡単に言うとなんか難しいやり方でforループ無しのもある、という事。だが私は心から最初にバックプロパケーションを実装する時にその複雑なバージョンにチャレンジするのは、おすすめしない。"
  },
  {
    "index": "F15712",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So concretely, we have a four-loop over my m-training examples and inside the four-loop we're going to perform fore prop and back prop using just this one example.",
    "output": "さて、では具体的には、mトレーニング手本に渡るfor文があり、そしてfor文の中では、フォワードプロパゲートとバックワードプロパゲートをこの一つの手本だけに対して行う。"
  },
  {
    "index": "F15713",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what that means is that we're going to take x(i), and feed that to my input layer, perform forward-prop, perform back-prop and that will if all of these activations and all of these delta terms for all of the layers of all my units in the neural network then still inside this four-loop, let me draw some curly braces just to show the scope with the four-loop, this is in octave code of course, but it's more a sequence Java code, and a four-loop encompasses all this.",
    "output": "するとニューラルネットワーク内の全てのレイヤーの全てのユニットのアクティベーション全部とデルタの項が全部得られるので、次に、まだforループの内部だが、、、よし、中括弧を書こう、forループのスコープを示す為に。これはもちろんOctaveのコードなんだが、だがC++とかJavaのコードのシーケンスみたいな意味でこれはfor文をここ全体に拡張する。"
  },
  {
    "index": "F15714",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to compute those delta terms, which are is the formula that we gave earlier.",
    "output": "我らはこれらのデルタの項を計算する訳だがその式は以前に既に得ている。"
  },
  {
    "index": "F15715",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Plus, you know, delta l plus one times a, l transpose of the code.",
    "output": "、、、足すことのデルタl+1掛けるaのlの転置に、さらにコードが続く。"
  },
  {
    "index": "F15716",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then finally, outside the having computed these delta terms, these accumulation terms, we would then have some other code and then that will allow us to compute these partial derivative terms.",
    "output": "そして最後に、これらのデルタ項がアキュームレーションの項が計算されたあとの外側ではさらに幾つかコードがあったあとで、これらの偏微分の項の計算が出来るようになる。"
  },
  {
    "index": "F15717",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right and these partial derivative terms have to take into account the regularization term lambda as well.",
    "output": "そしてこれらの偏微分の項は正規化項のラムダも同様に考慮に入れる必要がある。"
  },
  {
    "index": "F15718",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, those formulas were given in the earlier video.",
    "output": "それらの式は前半のビデオで与えてあった。"
  },
  {
    "index": "F15719",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how do you done that you now hopefully have code to compute these partial derivative terms.",
    "output": "だからそれを終えているなら、これらの偏微分項を計算するコードを既に手中に収めているはずだ。"
  },
  {
    "index": "F15720",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next is step five, what I do is then use gradient checking to compare these partial derivative terms that were computed.",
    "output": "次はステップ5。そこではグラディアントチェッキングを用いて計算した偏微分の項と比較する。"
  },
  {
    "index": "F15721",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I've compared the versions computed using back propagation versus the partial derivatives computed using the numerical estimates as using numerical estimates of the derivatives.",
    "output": "つまりバックプロパケーションを用いて計算したバージョンと、微分を数値計算的に推計した偏微分の数値推計を比較する。"
  },
  {
    "index": "F15722",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I do gradient checking to make sure that both of these give you very similar values.",
    "output": "つまりグラディアントチェッキングをこれらの値が両方とも似通っている事を確認する為に使う。"
  },
  {
    "index": "F15723",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having done gradient checking just now reassures us that our implementation of back propagation is correct, and is then very important that we disable gradient checking, because the gradient checking code is computationally very slow.",
    "output": "グラディアントチェッキングで我らのバックプロパゲーションの実装が正しいと確認した後には、次のとても大事な事だが、グラディアントチェッキングをdisableする。何故ならグラディアントチェッキングのコードは計算量的にとても遅いからだ。"
  },
  {
    "index": "F15724",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, we then use an optimization algorithm such as gradient descent, or one of the advanced optimization methods such as LB of GS, contract gradient has embodied into fminunc or other optimization methods.",
    "output": "そして最後に、最急降下法なりアドバンスドな最適化アルゴリズムのL-BFGSとかConjugateグラディアントとかfminuncで使われているアルゴリズムでもそれ以外のアルゴリズムでもとにかく何かしらを用いて、、、これらをバックプロパゲーションと共に用いる、つまりバックプロパゲーションがこれらの偏微分を我らに提供してくれる。"
  },
  {
    "index": "F15725",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We use these together with back propagation, so back propagation is the thing that computes these partial derivatives for us.",
    "output": "そしてコスト関数を計算する方法を知っていて、バックプロパゲーションを用いて偏微分を計算する方法も知っているので、これらの最適化手法のどれかを用いてJのシータをパラメータシータの関数として最小化する事を、試みる事が出来る。"
  },
  {
    "index": "F15726",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, we know how to compute the cost function, we know how to compute the partial derivatives using back propagation, so we can use one of these optimization methods to try to minimize j of theta as a function of the parameters theta.",
    "output": "ところで、ニューラルネットワークの場合、このコスト関数Jのシータは非凸関数、言い換えると凸関数では無いので、理論上はローカル最小に陥りうるし、実際、最急降下法やアドバンスドな最適化手法は、理論上はローカル最適に捕まってしまう事がありうる。だが実際には、通常はこれはそんなに大きな問題にならない。"
  },
  {
    "index": "F15727",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way, for neural networks, this cost function j of theta is non-convex, or is not convex and so it can theoretically be susceptible to local minima, and in fact algorithms like gradient descent and the advance optimization methods can, in theory, get stuck in local optima, but it turns out that in practice this is not usually a huge problem and even though we can't guarantee that these algorithms will find a global optimum, usually algorithms like gradient descent will do a very good job minimizing this cost function j of theta and get a very good local minimum, even if it doesn't get to the global optimum.",
    "output": "そしてこれらのアルゴリズムがグローバル最小を探す、とは保証出来なくても、普通は最急降下法のようなアルゴリズムはこのコスト関数Jのシータを小さくする、という仕事をとてもうまくこなす。"
  },
  {
    "index": "F15728",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, gradient descents for a neural network might still seem a little bit magical.",
    "output": "最後に、ニューラルネットワークに対する最急降下法は、まだこれでもちょっと魔法っぽく見えるかもしれない。"
  },
  {
    "index": "F15729",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let me just show one more figure to try to get that intuition about what gradient descent for a neural network is doing.",
    "output": "だからもう一つ最急降下法がニューラルネットワークに何をしているかを示す図をお見せしたい。"
  },
  {
    "index": "F15730",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This was actually similar to the figure that I was using earlier to explain gradient descent.",
    "output": "これは最急降下法を以前説明した時に用いた図に実は似た物だ。"
  },
  {
    "index": "F15731",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we have some cost function, and we have a number of parameters in our neural network.",
    "output": "あるコスト関数があり、ニューラルネットワークのパラメータがある。"
  },
  {
    "index": "F15732",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In reality, of course, in the neural network, we can have lots of parameters with these.",
    "output": "ここではパラメータの値を二つ書いた。"
  },
  {
    "index": "F15733",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Theta one, theta two--all of these are matrices, right?",
    "output": "実際には、もちろん、ニューラルネットワークの場合、もっとたくさんのパラメータを持ちうる。シータ1もシータ2も行列だ。"
  },
  {
    "index": "F15734",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we can have very high dimensional parameters but because of the limitations the source of parts we can draw.",
    "output": "だからパラメータの次元はとても高次になりうる。"
  },
  {
    "index": "F15735",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm pretending that we have only two parameters in this neural network. Although obviously we have a lot more in practice.",
    "output": "だがプロットして表示してみる側の制約の為に、ニューラルネットワークに二つのパラメータしか無いかのように話をする。"
  },
  {
    "index": "F15736",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, this cost function j of theta measures how well the neural network fits the training data.",
    "output": "いま、このコスト関数Jのシータはそのニューラルネットワークがトレーニングデータにどれだけフィットしているかを測っている物だ。"
  },
  {
    "index": "F15737",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you take a point like this one, down here, that's a point where j of theta is pretty low, and so this corresponds to a setting of the parameters.",
    "output": "つまりこんな点を取るなら、この下側の、そこはJのシータがとても低い点に対応し、つまりこれは以下のようなシチュエーションのパラメータに対応する、つまりだいたいのトレーニング手本に対して仮説の出力がy(i)に極めて近くなるようなそういうシチュエーションのパラメータに対応する。"
  },
  {
    "index": "F15738",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's a setting of the parameters theta, where, you know, for most of the training examples, the output of my hypothesis, that may be pretty close to y(i) and if this is true than that's what causes my cost function to be pretty low.",
    "output": "この条件が真の時が、コスト関数が極めて低い、という状態に対応する。"
  },
  {
    "index": "F15739",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in contrast, if you were to take a value like that, a point like that corresponds to, where for many training examples, the output of my neural network is far from the actual value y(i) that was observed in the training set.",
    "output": "他方、対照的に、こっちのような値を取る時には、この点はトレーニング手本の大多数が、ニューラルネットワークの出力が実際の値、実際の観測された値のy(i)から、大きく離れた値に対応した点となる。"
  },
  {
    "index": "F15740",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So points like this on the line correspond to where the hypothesis, where the neural network is outputting values on the training set that are far from y(i).",
    "output": "つまりこの右側の点のような点はトレーニングセットに対する仮説の出力、ニューラルネットワークの出力が、y(i)から遠く離れた値となる点に対応する。"
  },
  {
    "index": "F15741",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, it's not fitting the training set well, whereas points like this with low values of the cost function corresponds to where j of theta is low, and therefore corresponds to where the neural network happens to be fitting my training set well, because I mean this is what's needed to be true in order for j of theta to be small.",
    "output": "他方このような点は、コスト関数の値が低いというのはJのシータが低い所に対応しているので、つまりはニューラルネットワークがちょうど良く私のトレーニングセットにフィットする場所に対応する。何故ならこれこそがJのシータが小さくなる時に真である必要のある事だから。"
  },
  {
    "index": "F15742",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what gradient descent does is we'll start from some random initial point like that one over there, and it will repeatedly go downhill.",
    "output": "だから最急降下法のやる事は、なんらかのランダムな初期点から始めて、たとえばこことか、そこから繰り返し丘を降りていく。"
  },
  {
    "index": "F15743",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what back propagation is doing is computing the direction of the gradient, and what gradient descent is doing is it's taking little steps downhill until hopefully it gets to, in this case, a pretty good local optimum.",
    "output": "つまりバックプロパゲーションが行うのはグラディアントの方向を計算して、そして最急降下法がやる事は、一歩一歩ちょっとずつ丘を降りていき、期待するのは、そしてこの場合は実際にそうだが、かなり良いローカル最適に至るまで進む訳だ。"
  },
  {
    "index": "F15744",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, when you implement back propagation and use gradient descent or one of the advanced optimization methods, this picture sort of explains what the algorithm is doing.",
    "output": "つまり、バックプロパゲーションを実装して、最急降下法やアドバンスドな最適化法の一つを使う時には、この図はアルゴリズムが何をするかの、いくらかの説明となっている。"
  },
  {
    "index": "F15745",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's trying to find a value of the parameters where the output values in the neural network closely matches the values of the y(i)'s observed in your training set.",
    "output": "それはニューラルネットワークの出力する値が、トレーニングセットにおける観測値のy(i)になるべく近いようなニューラルネットのパラメータを探す事を試みる。"
  },
  {
    "index": "F15746",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, hopefully this gives you a better sense of how the many different pieces of neural network learning fit together.",
    "output": "これで、あなたもニューラルネットワークのそれぞれのピースが、どう組み合わさるのか分かったんじゃないかな。"
  },
  {
    "index": "F15747",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case even after this video, in case you still feel like there are, like, a lot of different pieces and it's not entirely clear what some of them do or how all of these pieces come together, that's actually okay.",
    "output": "でも、もしこのビデオが終わった後でも、これらの様々なピースの中にいまいちどうもしっくり来ない物があったり、あるいはそれらのうちの幾つかが完全にはっきりと分かったという訳で無くても、あるいはこれらのピースがどうくっつくのか全部は分からなくても、実際は問題無い。"
  },
  {
    "index": "F15748",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Neural network learning and back propagation is a complicated algorithm.",
    "output": "ニューラルネットワーク学習とバックプロパゲーションは複雑なアルゴリズムだ。"
  },
  {
    "index": "F15749",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And even though I've seen the math behind back propagation for many years and I've used back propagation, I think very successfully, for many years, even today I still feel like I don't always have a great grasp of exactly what back propagation is doing sometimes.",
    "output": "そして私ですら、バックプロパゲーションの背後にある数学を長年見てきて、さらに自分で思うにはバックプロパゲーションをとても成功裡に何年も使い続けてきた私ですら、こんにちでも、まだ時々バックプロパゲーションが正確に何やってるのかをいつもしっかりつかんでる、という訳では無いと感じる事がある。"
  },
  {
    "index": "F15750",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what the optimization process looks like of minimizing j if theta.",
    "output": "そしてJのシータを最小化する最適化がどう進んでいくかもしっかりとつかんでないと感じる事がある。"
  },
  {
    "index": "F15751",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Much this is a much harder algorithm to feel like I have a much less good handle on exactly what this is doing compared to say, linear regression or logistic regression.",
    "output": "これはより難しいアルゴリズムで、これはしっかり分かるのが、、、これが正確に何をやっているのかをしっかりと把握するのが、線形回帰とかロジスティック回帰に比べて難しい。"
  },
  {
    "index": "F15752",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Which were mathematically and conceptually much simpler and much cleaner algorithms.",
    "output": "線形回帰などの方が数学的にも概念的にもよりシンプルで、よりクリーンなアルゴリズムだ。"
  },
  {
    "index": "F15753",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But so in case if you feel the same way, you know, that's actually perfectly okay, but if you do implement back propagation, hopefully what you find is that this is one of the most powerful learning algorithms and if you implement this algorithm, implement back propagation, implement one of these optimization methods, you find that back propagation will be able to fit very complex, powerful, non-linear functions to your data, and this is one of the most effective learning algorithms we have today.",
    "output": "だがバックプロパゲーションを実装してみれば、きっとこれがもっとも強力な学習アルゴリズムの一つだと分かるだろう。そしてもしこのアルゴリズム、バックプロパゲーションをこれらの最適化手法の一つを実装すれば、バックプロパゲーションがとても複雑で強力で非線型な関数であなたのデータにフィッティング出来てそしてこれがこんにちある中でも最も効率的な学習アルゴリズムの一つである事が分かるだろう。"
  },
  {
    "index": "F15754",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to show you a fun and historically important example of neural networks learning of using a neural network for autonomous driving.",
    "output": "このビデオでは、楽しくて歴史的にも重要なニューラルネットワークの学習の例をお見せしたい。"
  },
  {
    "index": "F15755",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is getting a car to learn to drive itself.",
    "output": "ニューラルネットワークを用いて自動運転をする例を、つまり車が自分で運転するように学習させるという事。"
  },
  {
    "index": "F15756",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The video that I'll showed a minute was something that I'd gotten from Dean Pomerleau, who was a colleague who works out in Carnegie Mellon University out on the east coast of the United States.",
    "output": "私がこのちょっと後に見せるビデオはDeanPomilieuから頂いた物だ。彼はカーネギーメロンでアメリカの東海岸で働いている同僚だ。"
  },
  {
    "index": "F15757",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in part of the video you see visualizations like this.",
    "output": "ビデオの中では、このような可視化を目にする事になるので、ビデオを開始する前にこの可視化がどんな風になるのかを、ちょっと言っておきたい。"
  },
  {
    "index": "F15758",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I want to tell what a visualization looks like before starting the video.",
    "output": "この左下の部分は、車が見ている車の前に何があるかの映像で、つまり、そこに映るのは、道路で、例えばちょっとだけ左に向かってたりちょっとだけ右に向かってたりする訳だ。"
  },
  {
    "index": "F15759",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Down here on the lower left is the view seen by the car of what's in front of it.",
    "output": "そしてこの上のここにあるのは、この最初の水平バーは人間の運転手が選んだ方向。"
  },
  {
    "index": "F15760",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And up here on top, this first horizontal bar shows the direction selected by the human driver.",
    "output": "そしてこの白いバンドの場所は人間の運転手が選んだステアリングの方向だ。"
  },
  {
    "index": "F15761",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this location of this bright white band that shows the steering direction selected by the human driver where you know here far to the left corresponds to steering hard left, here corresponds to steering hard to the right.",
    "output": "この一番左の端のここの端は、ステアリングを一番左に切った状態に対応し、ここは、右側に強く切る事に対応する。"
  },
  {
    "index": "F15762",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this location which is a little bit to the left, a little bit left of center means that the human driver at this point was steering slightly to the left.",
    "output": "つまり、この位置は、ちょっとだけ左、真ん中のやや左のこの位置は、人間の運転手が、わずかに左にステアリングを切っている事を意味する。"
  },
  {
    "index": "F15763",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this second bot here corresponds to the steering direction selected by the learning algorithm and again the location of this sort of white band means that the neural network was here selecting a steering direction that's slightly to the left.",
    "output": "そしてこの二番目のバーは学習アルゴリズムによって選択されたステアリングの方向に対応している。これも、この種の白のバンドの位置が意味する事は、ニューラルネットワークはここでは、ステアリングの方向をちょっとだけ左と選んでいるという事。"
  },
  {
    "index": "F15764",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact before the neural network starts leaning initially, you see that the network outputs a grey band, like a grey, like a uniform grey band throughout this region and sort of a uniform gray fuzz corresponds to the neural network having been randomly initialized. And initially having no idea how to drive the car.",
    "output": "そして実際は、ニューラルネットワークが学習を始める前には、見ての通り、ネットワークはグレーのバンドを出力している、一様にグレーな、この範囲全体に渡ったグレーのバンド、つまりこのグレーで一様でぼんやりした物は、ニューラルネットワークがランダムに初期化されている事に対応していて、最初の時点では、車をどう運転したらいいか、さっぱり分からない。"
  },
  {
    "index": "F15765",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or initially having no idea of what direction to steer in.",
    "output": "あるいは最初の時点では、ステアリングをどちらに切ったらいいか分からない。"
  },
  {
    "index": "F15766",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And is only after it has learned for a while, that will then start to output like a solid white band in just a small part of the region corresponding to choosing a particular steering direction.",
    "output": "そしてしばらく学習した後になってはじめてしっかりとした白いバンドで、小さな領域のみを占有した物を出力するようになる、それは特定のステアリングの方向を選ぶ事に対応している。"
  },
  {
    "index": "F15767",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that corresponds to when the neural network becomes more confident in selecting a band in one particular location, rather than outputting a sort of light gray fuzz, but instead outputting a white band that's more constantly selecting one's steering direction.",
    "output": "そしてそれは、ニューラルネットワークが特定のバンド、特定の場所を選ぶ事に確信を持った、という事に対応する、グレーでぼやけた出力では無く、白いバンド、これは一つのステアリングの方向をしっかりと選び続けている事に対応しているが、それを出力するという。"
  },
  {
    "index": "F15768",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": ">> ALVINN is a system of artificial neural networks that learns to steer by watching a person drive.",
    "output": "Albanは人工知能ニューラルネットワークのシステムで、人が運転しているのを見る事でステアリングを学習する。"
  },
  {
    "index": "F15769",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "ALVINN is designed to control the NAVLAB 2, a modified Army Humvee who had put sensors, computers, and actuators for autonomous navigation experiments.",
    "output": "Albanはナトラブ2を制御するように設計されている、ナトラブ2は軍用ハンビーを改造した物で、センサーとコンピュータとアクチュエーターを、自動運転の実験の為に搭載した物だ。"
  },
  {
    "index": "F15770",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The initial step in configuring ALVINN is creating a network just here. During training, a person drives the vehicle while ALVINN watches.",
    "output": "Albanを設定する最初のステップは人間に運転させて、それをAlbanが観察する事でトレーニングする事だ。"
  },
  {
    "index": "F15771",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Once every two seconds, ALVINN digitizes a video image of the road ahead, and records the person's steering direction.",
    "output": "毎秒に12回、Albanは前方の道路のビデオ画像をデジタイズして、そして人間のステアリングの方向を記録する。"
  },
  {
    "index": "F15772",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This training image is reduced in resolution to 30 by 32 pixels and provided as input to ALVINN's three layered network.",
    "output": "このトレーニング画像は解像度で30x32pixelに縮小して、Albanの3層のネットワークの入力として供給される。"
  },
  {
    "index": "F15773",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Using the back propagation learning algorithm,ALVINN is training to output the same steering direction as the human driver for that image.",
    "output": "バックプロパゲーションの学習アルゴリズムを用いてAlbanはその画像に対して人間のドライバと同じステアリングの方向を出力するようにトレーニングされる。"
  },
  {
    "index": "F15774",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Initially the network steering response is random.",
    "output": "初期には、ネットワークのステアリングの答えは、ランダムだ。"
  },
  {
    "index": "F15775",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "After about two minutes of training the network learns to accurately imitate the steering reactions of the human driver.",
    "output": "だいたい2分くらいトレーニングした後には、ネットワークは人間の運転手のステアリングの反応を正確に模倣するようになる。"
  },
  {
    "index": "F15776",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This same training procedure is repeated for other road types.",
    "output": "これと同じトレーニングを他の種類の道路でも繰り返す。"
  },
  {
    "index": "F15777",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "After the networks have been trained the operator pushes the run switch and ALVINN begins driving.",
    "output": "ネットワークのトレーニングが終わったら、オペレータは実行スイッチを押して、そして運転を開始する。"
  },
  {
    "index": "F15778",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Twelve times per second, ALVINN digitizes the image and feeds it to its neural networks.",
    "output": "一秒間に12回、Albanは画像をデジタイズしてニューラルネットワークに食わせる。"
  },
  {
    "index": "F15779",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Each network, running in parallel, produces a steering direction, and a measure of its' confidence in its' response.",
    "output": "各ネットワークを、並行に走らせて、ステアリングの方向を生成させて、その応答の信頼度を測る。"
  },
  {
    "index": "F15780",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The steering direction, from the most confident network, in this network training for the one lane road, is used to control the vehicle.",
    "output": "もっとも信頼度が高いネットワークからのステアリングの方向、この場合は1レーンの道でトレーニングされたネットワークが、乗り物を制御する為に使われる。"
  },
  {
    "index": "F15781",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suddenly an intersection appears ahead of the vehicle.",
    "output": "突然、交差点が乗り物の前に現れた。"
  },
  {
    "index": "F15782",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As the vehicle approaches the intersection the confidence of the lone lane network decreases.",
    "output": "乗り物が交差点に近づくと1レーンのニューラルネットワークの信頼度が低下する。"
  },
  {
    "index": "F15783",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As it crosses the intersection and the two lane road ahead comes into view, the confidence of the two lane network rises.",
    "output": "交差点を渡ると、2レーンの道が前方に現れ、そして2レーンのネットワークの信頼度が上昇する。"
  },
  {
    "index": "F15784",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When its' confidence rises the two lane network is selected to steer. Safely guiding the vehicle into its lane onto the two lane road.",
    "output": "その信頼度が上昇すると、2レーンのネットワークがステアリングを選択し、安全に乗り物をレーン内に導く、2レーンの道の。"
  },
  {
    "index": "F15785",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of course there are more recently more modern attempts to do autonomous driving.",
    "output": "もちろん、最近だともっとモダンな、車の自動運転を目指すプロジェクトが、USとかヨーロッパに存在している。"
  },
  {
    "index": "F15786",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There are few projects in the US and Europe and so on, that are giving more robust driving controllers than this, but I think it's still pretty remarkable and pretty amazing how instant neural network trained with backpropagation can actually learn to drive a car somewhat well.",
    "output": "それらはここに示した物よりももっと頑強なコントローラーを用いているが、それでも私は、これはとても特筆すべきで、そしてとても驚きだと思う、シンプルなニューラルネットワークをバックプロパゲーションでトレーニングしただけの物が結構いい感じに車が運転出来る、という事が。"
  },
  {
    "index": "F15787",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By now you have seen a lot of different learning algorithms.",
    "output": "ここまでに、あなたは様々な学習のアルゴリズムを見てきた。"
  },
  {
    "index": "F15788",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you've been following along these videos you should consider yourself an expert on many state-of-the-art machine learning techniques.",
    "output": "そしてここまでのビデオと共に歩んできたあなたは、自分自身が既に、たくさんの芸術の域にまで達した機械学習のエキスパートである事を自覚すべき所まで来ている。"
  },
  {
    "index": "F15789",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But even among people that know a certain learning algorithm.",
    "output": "だがある学習アルゴリズムを知っている人の間でも、本当にそのアルゴリズムを強力かつ効率的に適用する方法を知っている人と、私が今から語る事に、そこまでは慣れ親しんでいなく、これらのアルゴリズムをどう適用したらいいか分からなくて、うまく行くはずも無い事にトライしてたくさんの時間を無駄にする人とでは本当に大きな違いが存在する。"
  },
  {
    "index": "F15790",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's often a huge difference between someone that really knows how to powerfully and effectively apply that algorithm, versus someone that's less familiar with some of the material that I'm about to teach and who doesn't really understand how to apply these algorithms and can end up wasting a lot of their time trying things out that don't really make sense.",
    "output": "私がお伝えしたいのは、もしあなたが機械学習のシステムを開発する時に、もっとも時間を有効に投資出来る道を確実に選べるようになる事です。"
  },
  {
    "index": "F15791",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I would like to do is make sure that if you are developing machine learning systems, that you know how to choose one of the most promising avenues to spend your time pursuing.",
    "output": "そしてこのビデオとつづくビデオでいくつかの実践的な提案、アドバイス、ガイドラインを、それが出来るように提供する。"
  },
  {
    "index": "F15792",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And on this and the next few videos I'm going to give a number of practical suggestions, advice, guidelines on how to do that.",
    "output": "具体的には、フォーカスしたい問題は、あなたは機械学習のシステムを開発するなり、パフォーマンスを改善したいなりするとする。"
  },
  {
    "index": "F15793",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And concretely what we'd focus on is the problem of, suppose you are developing a machine learning system or trying to improve the performance of a machine learning system, how do you go about deciding what are the proxy avenues to try next?",
    "output": "その時に、次に試すのに約束された道をどうやって決定するのか?という問題。"
  },
  {
    "index": "F15794",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To explain this, let's continue using our example of learning to predict housing prices.",
    "output": "これを説明する為に、家の価格を予言する例をまた使っていこう。"
  },
  {
    "index": "F15795",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say you've implement and regularize linear regression.",
    "output": "そして正規化した線形回帰を実装したとしよう。"
  },
  {
    "index": "F15796",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Thus minimizing that cost function j.",
    "output": "かくしてコスト関数Jを最小化した。"
  },
  {
    "index": "F15797",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now suppose that after you take your learn parameters, if you test your hypothesis on the new set of houses, suppose you find that this is making huge errors in this prediction of the housing prices.",
    "output": "さて、学習したパラメータを用いて新しい家の集合の価格を予言したとする。そこで予言された価格がとても巨大な誤差を生み出していたとします。"
  },
  {
    "index": "F15798",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The question is what should you then try mixing in order to improve the learning algorithm?",
    "output": "さて、ここで問題です。学習アルゴリズムを改善する為に、次に何をやりますか?"
  },
  {
    "index": "F15799",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There are many things that one can think of that could improve the performance of the learning algorithm.",
    "output": "学習アルゴリズムを改善しうる事はたくさん考えられる。"
  },
  {
    "index": "F15800",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One thing they could try, is to get more training examples.",
    "output": "一つには、より多くのトレーニング手本を使う、というのが考えられる。"
  },
  {
    "index": "F15801",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And concretely, you can imagine, maybe, you know, setting up phone surveys, going door to door, to try to get more data on how much different houses sell for.",
    "output": "具体的には、想像してみてくれ、電話による調査を立ち上げて、一軒一軒それぞれの家が幾らで売られているかより多くのデータを得ようとする事を。"
  },
  {
    "index": "F15802",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the sad thing is I've seen a lot of people spend a lot of time collecting more training examples, thinking oh, if we have twice as much or ten times as much training data, that is certainly going to help, right?",
    "output": "悲しい事に、私はこれまでたくさんの人々がより多くのトレーニング手本を集めるのに、たくさんの時間を使っているのを見てきた。「もし二倍とか10倍のデータがあれば、きっと良くなるに違い無いでしょ?"
  },
  {
    "index": "F15803",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But sometimes getting more training data doesn't actually help and in the next few videos we will see why, and we will see how you can avoid spending a lot of time collecting more training data in settings where it is just not going to help.",
    "output": "でもより多くのトレーニングデータを得る事が助けにならない事もある。次の一連のビデオでそれが何故なのか、そしてそれが役に立たない状況なのにたくさんのトレーニングデータを集めるのにたくさんの時間を費やしてしまうのをどうやって避けるのか、見ていく事にする。"
  },
  {
    "index": "F15804",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Other things you might try are to well maybe try a smaller set of features.",
    "output": "他に試す事としては、より少ない数のフィーチャーを使う、というのが考えられる。"
  },
  {
    "index": "F15805",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you have some set of features such as x1, x2, x3 and so on, maybe a large number of features.",
    "output": "つまり、もし幾つかのフィーチャー、例えばx1、x2、x3、、、というようなのがひょっとしたらたくさんあるとする。"
  },
  {
    "index": "F15806",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe you want to spend time carefully selecting some small subset of them to prevent overfitting.",
    "output": "その時は、それらのサブセットを時間をかけて慎重にオーバーフィッティングを避ける為に選ぶという事が考えられる。"
  },
  {
    "index": "F15807",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or maybe you need to get additional features.",
    "output": "または、追加のフィーチャーが必要かもしれない。"
  },
  {
    "index": "F15808",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe the current set of features aren't informative enough and you want to collect more data in the sense of getting more features.",
    "output": "ひょっとしたら現在のフィーチャーの集まりは十分な情報を持っていなくて、だからより多くの種類のフィーチャーを得るという意味でより多くのデータを集めるべきかもしれない。"
  },
  {
    "index": "F15809",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And once again this is the sort of project that can scale up the huge projects can you imagine getting phone surveys to find out more houses, or extra land surveys to find out more about the pieces of land and so on, so a huge project.",
    "output": "そしてこれはある種、とても巨大なスケールに拡張しうるプロジェクトだ。電話調査でよりたくさんの家や追加の土地測量でよりたくさんの地価を集めたりするのを想像出来るだろうか?"
  },
  {
    "index": "F15810",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And once again it would be nice to know in advance if this is going to help before we spend a lot of time doing something like this.",
    "output": "つまり巨大プロジェクトなんです。そしてここでも、こんな事を実際に試す前にそれが役に立つかを前もって知れたらいいと思わない?"
  },
  {
    "index": "F15811",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can also try adding polynomial features things like x2 square x2 square and product features x1, x2.",
    "output": "また、さらなるフィーチャーの多項式を追加する事も出来る。例えばx1の二乗とかx2の二乗とかフィーチャーの積、x1x2など。"
  },
  {
    "index": "F15812",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can still spend quite a lot of time thinking about that and we can also try other things like decreasing lambda, the regularization parameter or increasing lambda.",
    "output": "そんな事を考えるのにもたくさんの時間を使えるし、他にも試せる事としては、ラムダ、正規化のパラメータを減らしたり増やしたりも試せる。"
  },
  {
    "index": "F15813",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given a menu of options like these, some of which can easily scale up to six month or longer projects.",
    "output": "これらのような選択肢のメニューがあったとして、そのうちの幾つかは簡単に6ヶ月とかもっと長いプロジェクトにスケールアップ出来る。"
  },
  {
    "index": "F15814",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Unfortunately, the most common method that people use to pick one of these is to go by gut feeling.",
    "output": "残念なことに、これらから一つ選ぶのに人々がもっとも良く使うのはなんとなくのフィーリングだ。"
  },
  {
    "index": "F15815",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In which what many people will do is sort of randomly pick one of these options and maybe say, \"Oh, lets go and get more training data.\" And easily spend six months collecting more training data or maybe someone else would rather be saying, \"Well, let's go collect a lot more features on these houses in our data set.\" And I have a lot of times, sadly seen people spend, you know, literally 6 months doing one of these avenues that they have sort of at random only to discover six months later that that really wasn't a promising avenue to pursue.",
    "output": "そこで多くの人がやってるのは適当にランダムに一つこれらの選択肢の中から選んで「よーし、もっと多くのトレーニングデータを集めに行こう!"
  },
  {
    "index": "F15816",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Fortunately, there is a pretty simple technique that can let you very quickly rule out half of the things on this list as being potentially promising things to pursue.",
    "output": "」とか言ってる訳だ。そして簡単に六ヶ月とかより多くのトレーニングデータを集めるのに使っちゃう。"
  },
  {
    "index": "F15817",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And there is a very simple technique, that if you run, can easily rule out many of these options, and potentially save you a lot of time pursuing something that's just is not going to work.",
    "output": "または別の誰かが代わりに、「よし、我らのデータセットの家に対して、もっと追加のフィーチャーを集めに行こうっと」とか言ってたりする。そして私は何度も見た!"
  },
  {
    "index": "F15818",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next two videos after this, I'm going to first talk about how to evaluate learning algorithms.",
    "output": "可哀想に見える人々が文字通り六ヶ月とかの時間を使ってようするにランダムに選んだこれらの選択肢の一つを進み六ヶ月後にはこれは未来のある道では無かったという事だけが分かった、という出来事を!"
  },
  {
    "index": "F15819",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in the next few videos after that, I'm going to talk about these techniques, which are called the machine learning diagnostics.",
    "output": "幸運なことに、リストの半分を除外しつつ、将来のある手段は残すとてもシンプルなテクニックが存在する。"
  },
  {
    "index": "F15820",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what a diagnostic is, is a test you can run, to get insight into what is or isn't working with an algorithm, and which will often give you insight as to what are promising things to try to improve a learning algorithm's performance.",
    "output": "しかもそれは、とてもシンプルなテクニックでもし実行すれば、簡単にこれらの選択肢の多くを排除出来て、単に機能しない何かをし続けるのにたくさんの時間を浪費するのを、防ぐことが出来る。これに続く2つのビデオで、まず学習アルゴリズムをどうやって評価するかを議論する。"
  },
  {
    "index": "F15821",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We'll talk about specific diagnostics later in this video sequence.",
    "output": "そしてそれに続くビデオでこれらの機械学習診断と呼ばれるテクニックの議論に進む。"
  },
  {
    "index": "F15822",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But I should mention in advance that diagnostics can take time to implement and can sometimes, you know, take quite a lot of time to implement and understand but doing so can be a very good use of your time when you are developing learning algorithms because they can often save you from spending many months pursuing an avenue that you could have found out much earlier just was not going to be fruitful.",
    "output": "だが前もって言っておきたい事とてしては、診断は実装するのに時間もかかるし、時にはただかかるだけじゃなくて、実装したり理解したりするのに凄い時間がかかる場合もある。でもそうやって時間を使うのは学習アルゴリズム開発する時にはとても良い時間の使い方だ。"
  },
  {
    "index": "F15823",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in the next few videos, I'm going to first talk about how evaluate your learning algorithms and after that I'm going to talk about some of these diagnostics which will hopefully let you much more effectively select more of the useful things to try mixing if your goal to improve the machine learning system.",
    "output": "では次のビデオで最初にまず私は学習アルゴリズムをどう評価するかについて話し、そしてその後これらの診断の幾つかを議論することで、機械学習のシステムを改善するというゴールにより有益な物をより効率的に見つけられるようになる事を期待する。"
  },
  {
    "index": "F15824",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I would like to talk about how to evaluate a hypothesis that has been learned by your algorithm.",
    "output": "このビデオでは、あなたのアルゴリズムで学習させた仮説をどうやって評価するか?について話す。"
  },
  {
    "index": "F15825",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In later videos, we will build on this to talk about how to prevent in the problems of overfitting and underfitting as well.",
    "output": "後半のビデオで、これを用いてオーバーフィッティングとアンダーフィッティングをどう防止するかについて話す。"
  },
  {
    "index": "F15826",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When we fit the parameters of our learning algorithm we think about choosing the parameters to minimize the training error.",
    "output": "学習アルゴリズムのパラメータをフィットする時はトレーニングの誤差を最小にするようにパラメータを選ぼうとする。"
  },
  {
    "index": "F15827",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One might think that getting a really low value of training error might be a good thing, but we have already seen that just because a hypothesis has low training error, that doesn't mean it is necessarily a good hypothesis.",
    "output": "トレーニングでの誤差をとっても小さくするのが良い事だ、と思う人も居るかもしれない。でも既に見たように、ただ仮説のトレーニング誤差が低いという事だけでは、それが良い仮説だという事は、必ずしも意味しない。"
  },
  {
    "index": "F15828",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we've already seen the example of how a hypothesis can overfit.",
    "output": "そして既に、仮説がどうオーバーフィットしてしまうか、の例も見てきた。"
  },
  {
    "index": "F15829",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And therefore fail to generalize the new examples not in the training set.",
    "output": "そしてその結果、トレーニングセットに無い新しいサンプルに対して、一般化する事にどう失敗するかを。"
  },
  {
    "index": "F15830",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So how do you tell if the hypothesis might be overfitting.",
    "output": "ではどのように仮説がオーバーフィットしている事を知る事が出来るか?"
  },
  {
    "index": "F15831",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this simple example we could plot the hypothesis h of x and just see what was going on.",
    "output": "この単純な例では、仮説h(x)をプロットして何が起きているかを見てみる事が出来る。"
  },
  {
    "index": "F15832",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in general for problems with more features than just one feature, for problems with a large number of features like these it becomes hard or may be impossible to plot what the hypothesis looks like and so we need some other way to evaluate our hypothesis.",
    "output": "だが一般的には、フィーチャーが一つよりも多い問題に対しては、、、これらのようにたくさんのフィーチャーがある問題に対しては仮説をプロットするのが難しかったり、時には不可能だったりする。"
  },
  {
    "index": "F15833",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The standard way to evaluate a learned hypothesis is as follows.",
    "output": "だから仮説評価する他の手段が必要だ。学習の仮説を評価するスタンダードな方法は以下のようになる。"
  },
  {
    "index": "F15834",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose we have a data set like this.",
    "output": "こんなデータセットがあるとする。"
  },
  {
    "index": "F15835",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here I have just shown 10 training examples, but of course usually we may have dozens or hundreds or maybe thousands of training examples.",
    "output": "ここではトレーニング手本を10個しか示していないが通常は何十、何百、または何千ものトレーニング手本がある。"
  },
  {
    "index": "F15836",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to make sure we can evaluate our hypothesis, what we are going to do is split the data we have into two portions.",
    "output": "仮説を評価出来ている、という事を確認する為に、実行するのは、データを2つの部分に分ける、という事。"
  },
  {
    "index": "F15837",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first portion is going to be our usual training set and the second portion is going to be our test set, and a pretty typical split of this all the data we have into a training set and test set might be around say a 70%, 30% split.",
    "output": "最初の部分は通常のトレーニングセットで、二番目の部分はテストセットとなる。そしてこの分割の典型的なやり方は全体のデータを、トレーニングセットとテストセットがだいたい70%、30%になるように分ける。"
  },
  {
    "index": "F15838",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Worth more today to grade the training set and relatively less to the test set.",
    "output": "より多くのデータがトレーニングセットになり、相対的には少ない量がテストセットになるように。"
  },
  {
    "index": "F15839",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so now, if we have some data set, we run a sine of say 70% of the data to be our training set where here \"m\" is as usual our number of training examples and the remainder of our data might then be assigned to become our test set.",
    "output": "つまりデータセットがあったとすると、データの70%だけをトレーニングセットに振り分ける。ここで「m」はいつも通り、トレーニング手本の数。"
  },
  {
    "index": "F15840",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And here, I'm going to use the notation m subscript test to denote the number of test examples.",
    "output": "そしてここで、mの下付き添字のtestでテストのサンプルの数を示す。"
  },
  {
    "index": "F15841",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so in general, this subscript test is going to denote examples that come from a test set so that x1 subscript test, y1 subscript test is my first test example which I guess in this example might be this example over here.",
    "output": "今後は一般に、このtestという下付き添字でテストセットから来たサンプルを表す事にする。つまりx1の下付き添字test、y1の下付き添字のtestで最初のテストのサンプルを表し、この例だとここにあるサンプルとなる。"
  },
  {
    "index": "F15842",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, one last detail whereas here I've drawn this as though the first 70% goes to the training set and the last 30% to the test set.",
    "output": "最後に一つ細かい話を。"
  },
  {
    "index": "F15843",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If there is any sort of ordinary to the data.",
    "output": "ここではまるで最初の70%をトレーニングセットに、最後の30%をテストセットにするかのように線を引いたが、データにもし順番があるような物なら、ランダムに70%のデータをトレーニングセットに残りの30%をテストセットに選んだ方が良い。"
  },
  {
    "index": "F15844",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That should be better to send a random 70% of your data to the training set and a random 30% of your data to the test set.",
    "output": "もしあなたのデータが既にランダムに並んでいたらただ最初の70%と後ろの30%を取ればよろしい。"
  },
  {
    "index": "F15845",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if your data were already randomly sorted, you could just take the first 70% and last 30% that if your data were not randomly ordered, it would be better to randomly shuffle or to randomly reorder the examples in your training set.",
    "output": "もしデータがランダムに並んでる訳では無ければ、トレーニングセットをランダムにシャッフルする、またはランダムに並べ替える方が良い。"
  },
  {
    "index": "F15846",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Before you know sending the first 70% in the training set and the last 30% of the test set.",
    "output": "最初の70%をトレーニングセットに送り、後ろの30%をテストセットに送る前に。"
  },
  {
    "index": "F15847",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here then is a fairly typical procedure for how you would train and test the learning algorithm and the learning regression.",
    "output": "ここに、学習アルゴリズムと回帰の学習の訓練を実施する、きわめて典型的な手順を示す。"
  },
  {
    "index": "F15848",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First, you learn the parameters theta from the training set so you minimize the usual training error objective j of theta, where j of theta here was defined using that 70% of all the data you have.",
    "output": "まず、トレーニングセットからパラメータのシータを学習して、通常のトレーニングの目的関数であるトレーニングの誤差、Jのシータを最小化する。ここでJのシータは全データの70%を使って定義した物。"
  },
  {
    "index": "F15849",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There is only the training data.",
    "output": "トレーニングデータだけについて計算した物ね。"
  },
  {
    "index": "F15850",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then you would compute the test error.",
    "output": "そして次に、テストの誤差を計算する。"
  },
  {
    "index": "F15851",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I am going to denote the test error as j subscript test.",
    "output": "そしてテストの誤差をJの下付き添字testで示す。"
  },
  {
    "index": "F15852",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what you do is take your parameter theta that you have learned from the training set, and plug it in here and compute your test set error.",
    "output": "で、何をするかといえば、トレーニングセットで学習したパラメータのシータをここに入れて、テストセットの誤差を計算する。それは以下のようになる。"
  },
  {
    "index": "F15853",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is basically the average squared error as measured on your test set.",
    "output": "これは基本的にはテストセットに対して測った誤差の二乗の平均だ。"
  },
  {
    "index": "F15854",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's pretty much what you'd expect.",
    "output": "たぶんご想像の通りでしょう。"
  },
  {
    "index": "F15855",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if we run every test example through your hypothesis with parameter theta and just measure the squared error that your hypothesis has on your m subscript test, test examples.",
    "output": "つまりパラメータシータを入れた仮説でテストセットの要素一つずつを予言して、仮説の誤差をm下付き添字test個に対して計測する。"
  },
  {
    "index": "F15856",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And of course, this is the definition of the test set error if we are using linear regression and using the squared error metric.",
    "output": "もちろん、これは線形回帰を使っている時のテストセットの誤差の定義であって、二乗誤差の計量を用いている場合だ。"
  },
  {
    "index": "F15857",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How about if we were doing a classification problem and say using logistic regression instead.",
    "output": "ではもし分類問題を行なっていて、代わりに例えばロジスティック回帰を使っていた場合はどうだろう?"
  },
  {
    "index": "F15858",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case, the procedure for training and testing say logistic regression is pretty similar first we will do the parameters from the training data, that first 70% of the data.",
    "output": "その場合でも、ロジスティック回帰でのトレーニングとテストの手続きはとても似た物だ。まず先にトレーニングデータからパラメータを学習する、データの最初の70%からね。"
  },
  {
    "index": "F15859",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it will compute the test error as follows.",
    "output": "そして以下のようにテスト誤差を計算する。"
  },
  {
    "index": "F15860",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's the same objective function as we always use but we just logistic regression, except that now is define using our m subscript test, test examples.",
    "output": "これは普段ロジスティック回帰に使っているのと同じ目的関数だ。"
  },
  {
    "index": "F15861",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "While this definition of the test set error j subscript test is perfectly reasonable.",
    "output": "違いはm下付き添字testに対して定義されている。つまりテストセットに対して。"
  },
  {
    "index": "F15862",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Sometimes there is an alternative test sets metric that might be easier to interpret, and that's the misclassification error.",
    "output": "そちらの方が解釈が容易かも。それは誤判別の誤差だ。"
  },
  {
    "index": "F15863",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's also called the zero one misclassification error, with zero one denoting that you either get an example right or you get an example wrong.",
    "output": "それはまた、ゼロワン誤判別の誤差とも呼ばれる。ゼロワンはサンプルから正しい結果が得られたか、誤った結果が得られたかを表す。"
  },
  {
    "index": "F15864",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's what I mean.",
    "output": "それはこういう事だ。"
  },
  {
    "index": "F15865",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me define the error of a prediction.",
    "output": "予言の誤差を定義しよう。"
  },
  {
    "index": "F15866",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is h of x.",
    "output": "予言とはh(x)だ。"
  },
  {
    "index": "F15867",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And given the label y as equal to one if my hypothesis outputs the value greater than equal to five and Y is equal to zero or if my hypothesis outputs a value of less than 0.5 and y is equal to one, right, so both of these cases basic respond to if your hypothesis mislabeled the example assuming your threshold at an 0.5.",
    "output": "そしてラベルyをつけて、イコール1となるのは、仮説の出力が0.5以上の値で、なおかつy=0の時。または、仮説の出力値が0.5未満でy=1の時。"
  },
  {
    "index": "F15868",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So either thought it was more likely to be 1, but it was actually 0, or your hypothesis stored was more likely to be 0, but the label was actually 1.",
    "output": "つまりどちらのケースでも、基本的には仮説がサンプルを間違えてラベルづけした時となる、しきい値を0.5として。つまり、より1の可能性が高いと考えたが実際は0か仮説がより0の可能性が高いと考えたが実際は0という事。"
  },
  {
    "index": "F15869",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And otherwise, we define this error function to be zero.",
    "output": "そしてそれ以外の場合は、このエラー関数を0と定義する。"
  },
  {
    "index": "F15870",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If your hypothesis basically classified the example y correctly.",
    "output": "もし仮説が基本的には手本のyを正しく分類したら。"
  },
  {
    "index": "F15871",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We could then define the test error, using the misclassification error metric to be one of the m tests of sum from i equals one to m subscript test of the error of h of x(i) test comma y(i).",
    "output": "すると、テスト誤差を誤判別の誤差計量を用いて、それのi=1からm下付き添字testまでのerrのhのxi_test、yiの和として定義出来る。"
  },
  {
    "index": "F15872",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's just my way of writing out that this is exactly the fraction of the examples in my test set that my hypothesis has mislabeled.",
    "output": "つまり、これをずばり書き下す方法は正確に我らの仮説がテストセットの手本を誤判別した割合という事になる。"
  },
  {
    "index": "F15873",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's the definition of the test set error using the misclassification error of the 0 1 misclassification metric.",
    "output": "以上がゼロワン誤判別計量を用いたテストセットの誤判別の誤差の定義だ。"
  },
  {
    "index": "F15874",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the standard technique for evaluating how good a learned hypothesis is.",
    "output": "以上が学習した仮説がどれだけ良いかを評価する標準的なテクニックだ。"
  },
  {
    "index": "F15875",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video, we will adapt these ideas to helping us do things like choose what features like the degree polynomial to use with the learning algorithm or choose the regularization parameter for learning algorithm.",
    "output": "次のビデオでは、これらのアイデアを用いて、なんのフィーチャーを含めるべきか選んだり何次の多項式を学習アルゴリズムに含めるべきかを選んだり学習アルゴリズムの正規化パラメータを選ぶ助けとしていきます。"
  },
  {
    "index": "F15876",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose you're left to decide what degree of polynomial to fit to a data set.",
    "output": "データセットに何次の多項式まで含めてフィットさせたいか決めたい、としよう。"
  },
  {
    "index": "F15877",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that what features to include that gives you a learning algorithm.",
    "output": "つまり学習アルゴリズムになんのフィーチャーを含めるか、という話だ。"
  },
  {
    "index": "F15878",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or suppose you'd like to choose the regularization parameter longer for learning algorithm.",
    "output": "または学習アルゴリズムの正規化パラメータ、ラムダを選びたいとしよう。"
  },
  {
    "index": "F15879",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How do you do that?",
    "output": "どうやる?"
  },
  {
    "index": "F15880",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Browsers, and in our discussion of how to do this, we'll talk about not just how to split your data into the train and test sets, but how to switch data into what we discover is called the train, validation, and test sets.",
    "output": "これをどう行うかの議論をするにあたり、データをどうトレーニングセットとテストセットに分割するかだけでなく、データを、やがて見る事になる、トレインバリデーションと呼ばれる物とテストセットにデータをどう分割するかを扱う。"
  },
  {
    "index": "F15881",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We'll see in this video just what these things are, and how to use them to do model selection.",
    "output": "このビデオの中で以上の事がいったいなんなのか、そしてそれらを使ってどうモデル選択をするかを見ていく。"
  },
  {
    "index": "F15882",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We've already seen a lot of times the problem of overfitting, in which just because a learning algorithm fits a training set well, that doesn't mean it's a good hypothesis.",
    "output": "ここまでに何度もオーバーフィティングの問題を見てきた。そこでは学習アルゴリズムがトレーニングセットに良くフィットしているからといって必ずしも良い仮説だとは限らないのだった。"
  },
  {
    "index": "F15883",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "More generally, this is why the training set's error is not a good predictor for how well the hypothesis will do on new example.",
    "output": "より一般的には、これこそがトレーニングセットの誤差が新しいサンプルに対してどれだけ良い予測器かの良い指標にはならない理由だった。"
  },
  {
    "index": "F15884",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, if you fit some set of parameters. Theta0, theta1, theta2, and so on, to your training set.",
    "output": "具体的には、あるパラメータの集まり、シータ0、シータ1、シータ2など、をトレーニングセットにフィットしようとしているとしよう。"
  },
  {
    "index": "F15885",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, this doesn't mean much in terms of predicting how well your hypothesis will generalize to new examples not seen in the training set. And a more general principle is that once your parameter is what fit to some set of data.",
    "output": "その時に仮説がトレーニングセットに対して良いという事実は、仮説が新たな、トレーニングセットに無いサンプルをどれだけうまく予言するかという観点からは、多くは意味しない、という事。"
  },
  {
    "index": "F15886",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then the error of your hypothesis as measured on that same data set, such as the training error, that's unlikely to be a good estimate of your actual generalization error.",
    "output": "より一般的な原則としては、一旦パラメータをあるデータの集合にフィッティングしたら、それがトレーニングセットであれそれ以外であれ、その同じデータセットに対して測った仮説の誤差というのは、たとえばトレーニング誤差などはそれは恐らく実際の一般のケースの誤差を推計するには良い物では無い。"
  },
  {
    "index": "F15887",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is how well the hypothesis will generalize to new examples.",
    "output": "つまりどれだけ仮説が新しいサンプルに対してうまく一般化されているか、という事については。"
  },
  {
    "index": "F15888",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's consider the model selection problem.",
    "output": "では、モデル選択問題を検討してみよう。"
  },
  {
    "index": "F15889",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say you're trying to choose what degree polynomial to fit to data.",
    "output": "データをフィットするのに、何次の多項式を含めるかを選ぼうとしている、としよう。"
  },
  {
    "index": "F15890",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, should you choose a linear function, a quadratic function, a cubic function? All the way up to a 10th-order polynomial.",
    "output": "つまり、あなたは線形関数を選びたい、二次関数、三次関数、、、、と10乗の多項式まで。"
  },
  {
    "index": "F15891",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it's as if there's one extra parameter in this algorithm, which I'm going to denote d, which is, what degree of polynomial.",
    "output": "つまり、それはまるで、一つ追加のパラメータ、それをdで表すが、何次の多項式まで含めるか、を表すパラメータがアルゴリズムにあるみたいな物だ。"
  },
  {
    "index": "F15892",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it's as if, in addition to the theta parameters, it's as if there's one more parameter, d, that you're trying to determine using your data set.",
    "output": "つまりまるで、パラメータのシータに追加してさらにもう一つ、パラメータdという物が追加されていて、それをデータセットを用いて決めたい、と考える。"
  },
  {
    "index": "F15893",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, the first option is d equals one, if you fit a linear function.",
    "output": "最初の選択肢はd=1でそれは線形関数となる。"
  },
  {
    "index": "F15894",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can choose d equals two, d equals three, all the way up to d equals 10.",
    "output": "d=2も、d=3も、、、とd=10までの選択肢がある。"
  },
  {
    "index": "F15895",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we'd like to fit this extra sort of parameter which I'm denoting by d.",
    "output": "つまりこの、ある意味で追加のパラメータについてフィッティングしたい、という訳だ。"
  },
  {
    "index": "F15896",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And concretely let's say that you want to choose a model, that is choose a degree of polynomial, choose one of these 10 models.",
    "output": "それをdで示していて、具体的には、モデルを選びたいと、これら10個のモデルから一つの多項式の次数を選びたいとする。"
  },
  {
    "index": "F15897",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And fit that model and also get some estimate of how well your fitted hypothesis was generalize to new examples.",
    "output": "そしてそのモデルをフィッティングしてそのフィッティングした仮説がどれくらいうまく、新しいサンプルに対して一般化されているかを評価する。"
  },
  {
    "index": "F15898",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What you could, first take your first model and minimize the training error. And this would give you some parameter vector theta.",
    "output": "一つ、ありえる事としては、こんなのがある:最初のモデルを取ってトレーニング誤差を最小化し、それであるパラメータベクトルが得られる。"
  },
  {
    "index": "F15899",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you could then take your second model, the quadratic function, and fit that to your training set and this will give you some other. Parameter vector theta.",
    "output": "そして次に、二番目のモデル、二次関数をとってきて、トレーニングセットに対してーー別のパラメータベクトル、シータを得る事になる。"
  },
  {
    "index": "F15900",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to distinguish between these different parameter vectors, I'm going to use a superscript one superscript two there where theta superscript one just means the parameters I get by fitting this model to my training data.",
    "output": "これら別々のパラメータベクトルを区別する為に、下付き添字1、2、、、を使っていく。ここでシータの下付き添字1は、このモデルをトレーニングデータに対してフィッティングして得られたパラメータを意味するに過ぎない。"
  },
  {
    "index": "F15901",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And theta superscript two just means the parameters I get by fitting this quadratic function to my training data and so on.",
    "output": "シータ下付き添字2は単にこの二次関数をトレーニングデータに対してフィッティングして得られたパラメータを表すだけ、などなど。"
  },
  {
    "index": "F15902",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By fitting a cubic model I get parenthesis three up to, well, say theta 10.",
    "output": "そして三次のモデルをフィッティングする事で、パラメータ、シータ3を得て、これをシータ10まで続ける事が出来る。"
  },
  {
    "index": "F15903",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And one thing we ccould do is that take these parameters and look at test error.",
    "output": "そしてここまて来た後にやれる事としては、一つにはこれらのパラメータに対しテストセットの誤差を見ていく。"
  },
  {
    "index": "F15904",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I can compute on my test set J test of one, J test of theta two, and so on.",
    "output": "つまり、テストセットに対してJのtestのシータ1、Jのtestのシータ2、みたいに。"
  },
  {
    "index": "F15905",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "J test of theta three, and so on.",
    "output": "Jのtestのシータ3とか。"
  },
  {
    "index": "F15906",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I'm going to take each of my hypotheses with the corresponding parameters and just measure the performance of on the test set.",
    "output": "つまり、対応する仮説を一つずつ見ていってテストセットに対するパフォーマンスをただ計測していくだけ。"
  },
  {
    "index": "F15907",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, one thing I could do then is, in order to select one of these models, I could then see which model has the lowest test set error.",
    "output": "そこから出来る事としては、一つにはこれらのモデルを選ぶ為にどのモデルが一番低いテストセットの誤差となっているかを見る、というのがある。"
  },
  {
    "index": "F15908",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's just say for this example that I ended up choosing the fifth order polynomial.",
    "output": "そしてこの例では、仮に5次の多項式を選ぶ事になったとしよう。"
  },
  {
    "index": "F15909",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this seems reasonable so far.",
    "output": "ここまではリーズナブルっぽいね。"
  },
  {
    "index": "F15910",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But now let's say I want to take my fifth hypothesis, this, this, fifth order model, and let's say I want to ask, how well does this model generalize?",
    "output": "だがここで、フィットさせた仮説、この5次のモデルに対して、このモデルがどれだけうまく一般化されているか知りたいとする。"
  },
  {
    "index": "F15911",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One thing I could do is look at how well my fifth order polynomial hypothesis had done on my test set.",
    "output": "一つ考えられるのは、五次の多項式の仮説がどれだけテストセットに対してうまく機能するかを見る、という事だが、、、だがこれには問題がある。"
  },
  {
    "index": "F15912",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the problem is this will not be a fair estimate of how well my hypothesis generalizes.",
    "output": "これは我らの仮説がどれだけうまく一般化出来ているかを見積もるにはフェアなやり方じゃないって事だ。"
  },
  {
    "index": "F15913",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason is what we've done is we've fit this extra parameter d, that is this degree of polynomial.",
    "output": "その理由は、我らがやった事はそもそも、この追加のパラメータdをそれは多項式の次数だが、これをフィッティングたのだった。"
  },
  {
    "index": "F15914",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what fits that parameter d, using the test set, namely, we chose the value of d that gave us the best possible performance on the test set.",
    "output": "そしてこのdは、テストセットを使ってフィッティングしたのだった。"
  },
  {
    "index": "F15915",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, the performance of my parameter vector theta5, on the test set, that's likely to be an overly optimistic estimate of generalization error.",
    "output": "ようするに、我らはdの値をテストセットに対して最も高いパフォーマンスが出るように選んだのだった。"
  },
  {
    "index": "F15916",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, so, that because I had fit this parameter d to my test set is no longer fair to evaluate my hypothesis on this test set, because I fit my parameters to this test set, I've chose the degree d of polynomial using the test set.",
    "output": "だから、パラメータベクトルのシータ5のテストセットに対してのパフォーマンスはたぶん一般化の誤差を見積もるには過剰に楽観的になってしまうはず。でしょ?"
  },
  {
    "index": "F15917",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And specifically, what we did was, we fit this parameter d to the test set.",
    "output": "だってパラメータdはテストセットに対してフィッティングしたんだから。"
  },
  {
    "index": "F15918",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by having fit the parameter to the test set, this means that the performance of the hypothesis on that test set may not be a fair estimate of how well the hypothesis is, is likely to do on examples we haven't seen before.",
    "output": "だからもはや、仮説をテストセットに対して用いるのはフェアじゃ無くなってしまっているよ。"
  },
  {
    "index": "F15919",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when faced with a model selection problem like this, what we're going to do is, instead of using the test set to select the model, we're instead going to use the validation set, or the cross validation set, to select the model.",
    "output": "だってパラメータのフィッティングにテストセットを使っちゃったんだから。"
  },
  {
    "index": "F15920",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, what this means is that that parameter d, remember d was the degree of polynomial, right?",
    "output": "多項式の次数、dをテストセットを使って選んだんだった。"
  },
  {
    "index": "F15921",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we've done is we'll fit that parameter d and we'll say d equals four. And we did so using the cross-validation set.",
    "output": "だから我らの仮説はたぶんテストセットに対しての方がまだ見ぬ新しいサンプルに対してよりうまく振舞ってしまうだろう。"
  },
  {
    "index": "F15922",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The machine learning, as of this practice today, there aren't many people that will do that early thing that I talked about, and said that, you know, it isn't such a good idea, of selecting your model using this test set.",
    "output": "そしてその新しいサンプルに対してこそが知りたい事だ。"
  },
  {
    "index": "F15923",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then using the same test set to report the error as though selecting your degree of polynomial on the test set, and then reporting the error on the test set as though that were a good estimate of generalization error. That sort of practice is unfortunately many, many people do do it.",
    "output": "前のスライドを繰り返すと、もしあるパラメータの集合、例えばシータ0とかシータ1とかを、なんらかのトレーニングセットに対してフィッティングしたら、そのトレーニングセットにフィットさせたモデルの(そのトレーニングセットに対する)パフォーマンスは新しいサンプルに対してどれだけうまく一般化出来た仮説となっているかを予測するのには使えない。"
  },
  {
    "index": "F15924",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it's considered better practice to have separate train validation and test sets.",
    "output": "何故ならこれらのパラメータは当然このトレーニングセットにはフィットするだろうから。"
  },
  {
    "index": "F15925",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I just warned you to sometimes people to do, you know, use the same data for the purpose of the validation set, and for the purpose of the test set.",
    "output": "つまりそれらはトレーニングセットに対しては良いだろうと思われる。"
  },
  {
    "index": "F15926",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You need a training set and a test set, and that's good, that's practice, though you will see some people do it.",
    "output": "たとえそのパラメータが、他のサンプルには良くなくても。そしてこのスライドで記述した手順では、"
  },
  {
    "index": "F15927",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you run a learning algorithm and it doesn't do as long as you are hoping, almost all the time, it will be because you have either a high bias problem or a high variance problem, in other words, either an underfitting problem or an overfitting problem.",
    "output": "もし学習アルゴリズムを走らせて期待ほど良い結果で無ければ、だいたいいつもそれは高いバイアス問題か、高い分散問題のどちらかだ。言い換えると、それはアンダーフィット問題かオーバーフィット問題のどちらか、という事。"
  },
  {
    "index": "F15928",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this case, it's very important to figure out which of these two problems is bias or variance or a bit of both that you actually have.",
    "output": "そしてこの場合、これらの問題のどちらなのかを見分けるのは凄く重要。バイアスなのか分散なのか。"
  },
  {
    "index": "F15929",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because knowing which of these two things is happening would give a very strong indicator for whether the useful and promising ways to try to improve your algorithm.",
    "output": "何故かといえば、これらのどちらが起きているのかを知ると、アルゴリズムを改善するのに将来のありそうな道がどちらかのとても強力な指標となるから。"
  },
  {
    "index": "F15930",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to delve more deeply into this bias and variance issue and understand them better as was figure out how to look in a learning algorithm and evaluate or diagnose whether we might have a bias problem or a variance problem since this will be critical to figuring out how to improve the performance of a learning algorithm that you will implement.",
    "output": "このビデオでは、バイアスと分散の問題についてより深くつっこんで行き、それらをより深く理解すると同時に我らの問題がバイアス問題なのか分散問題なのかを見分ける為に学習アルゴリズムをどう見ていくか、どう評価して診断していくかも探求していきたい。何故ならあなたの実装した学習アルゴリズムのパフォーマンスを改善する方法を知るのに必須だから。"
  },
  {
    "index": "F15931",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you've already seen this figure a few times where if you fit two simple hypothesis like a straight line that underfits the data, if you fit a two complex hypothesis, then that might fit the training set perfectly but overfit the data and this may be hypothesis of some intermediate level of complexities of some maybe degree two polynomials or not too low and not too high degree that's like just right and gives you the best generalization error over these options.",
    "output": "あまりにも複雑過ぎる仮説にフィットさせるとそれはトレーニングセットに完璧に一致するかもしれないがデータにオーバーフィットするかもしれない。そしてこれは、中間の複雑さで、たとえば多項式の次数が高すぎもせず、低すぎもしない、という仮説の例。"
  },
  {
    "index": "F15932",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now that we're armed with the notion of chain training and validation in test sets, we can understand the concepts of bias and variance a little bit better.",
    "output": "今やトレーニング、バリデーション、テストセットという考え方で武装したので、バイアスと分散の概念をもうちょっと良く理解出来る。"
  },
  {
    "index": "F15933",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, let's let our training error and cross validation error be defined as in the previous videos.",
    "output": "具体的にいこう。トレーニング誤差とクロスバリデーション誤差を前回のビデオ同様に定義しよう。"
  },
  {
    "index": "F15934",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just say the squared error, the average squared error, as measured on the training sets or as measured on the cross validation set.",
    "output": "誤差の自乗、トレーニングセットかクロスバリデーションセットに対して計測した誤差の自乗の平均の値。"
  },
  {
    "index": "F15935",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, let's plot the following figure.",
    "output": "今、以下の図をプロットしよう。"
  },
  {
    "index": "F15936",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "On the horizontal axis I'm going to plot the degree of polynomial.",
    "output": "横軸には多項式の次数。"
  },
  {
    "index": "F15937",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, as I go to the right I'm going to be fitting higher and higher order polynomials.",
    "output": "つまり右に行くに連れてどんどん高い次元の多項式をフィッティングする事となる。"
  },
  {
    "index": "F15938",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So where the left of this figure where maybe d equals one, we're going to be fitting very simple functions whereas we're here on the right of the horizontal axis, I have much larger values of ds, of a much higher degree polynomial.",
    "output": "つまりこの図の右側ではd=1の時など、とても簡単な関数でフィッティングする事となる。他方、横軸の右側ではより大きな数字となり、とても複雑な高い次元の多項式にフィッティングする事となる。"
  },
  {
    "index": "F15939",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here, that's going to correspond to fitting much more complex functions to your training set.",
    "output": "つまりトレーニングセットとより複雑な関数でフィッティングする事となる。"
  },
  {
    "index": "F15940",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at the training error and the cross validation error and plot them on this figure.",
    "output": "ではトレーニング誤差とクロスバリデーション誤差を見てみて、この図にプロットしてみよう。"
  },
  {
    "index": "F15941",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's start with the training error.",
    "output": "トレーニング誤差から始めよう。"
  },
  {
    "index": "F15942",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As we increase the degree of the polynomial, we're going to be able to fit our training set better and better and so if d equals one, then there is high training error, if we have a very high degree of polynomial our training error is going to be really low, maybe even 0 because will fit the training set really well.",
    "output": "多項式の次元を上げるに連れてトレーニングセットへのフィッティングもますます良くなる。もしd=1なら高いトレーニング誤差となるし、もしとても高い次数の多項式ならば我らのトレーニング誤差は極めて低くなる。"
  },
  {
    "index": "F15943",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, as we increase the degree of polynomial, we find typically that the training error decreases.",
    "output": "つまり多項式の次数を高めれば高める程、典型的にはトレーニングの誤差は減少していく。"
  },
  {
    "index": "F15944",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I'm going to write J subscript train of theta there, because our training error tends to decrease with the degree of the polynomial that we fit to the data.",
    "output": "だからJ下付き添字trainのシータを書くとこんな感じ。何故ならトレーニング誤差はデータにフィットする多項式の次元の増加とともに減少していくから。"
  },
  {
    "index": "F15945",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, let's look at the cross-validation error or for that matter, if we look at the test set error, we'll get a pretty similar result as if we were to plot the cross validation error.",
    "output": "仮にテストセットの誤差を見るとすると、そちらも似たような結果となるだろう。まるでクロスバリデーション誤差をプロットしたかのように。"
  },
  {
    "index": "F15946",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we know that if d equals one, we're fitting a very simple function and so we may be underfitting the training set and so it's going to be very high cross-validation error.",
    "output": "もしd=1なら、とても簡単な関数にフィッティングする事になるのでトレーニングセットにアンダーフィットするかもしれない。"
  },
  {
    "index": "F15947",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we fit an intermediate degree polynomial, we had d equals two in our example in the previous slide, we're going to have a much lower cross-validation error because we're finding a much better fit to the data.",
    "output": "だからクロスバリデーション誤差はとても高いはず。"
  },
  {
    "index": "F15948",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if d took on say a value of four, then we're again overfitting, and so we end up with a high value for cross-validation error.",
    "output": "もし中間の次数の多項式をフィッティングさせたら前のスライドの例だとd=2のケースとなるが、この場合はより低いクロスバリデーション誤差となるだろう。何故なら我らはより良くデータにフィットするのを見つけたという事だから。"
  },
  {
    "index": "F15949",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you were to vary this smoothly and plot a curve, you might end up with a curve like that where that's JCV of theta.",
    "output": "そして、逆にdが大きすぎる時は例えばdが4の時はふたたびオーバーフィットしてしまう。だから結果としては高いクロスバリデーション誤差となるだろう。"
  },
  {
    "index": "F15950",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Again, if you plot J test of theta you get something very similar.",
    "output": "だからこれをなめらかにつなげて、曲線をプロットすると結局こんな曲線となる。"
  },
  {
    "index": "F15951",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this sort of plot also helps us to better understand the notions of bias and variance.",
    "output": "この種のプロットはまた、バイアスと分散という考え方をよりよく理解する助けともなる。"
  },
  {
    "index": "F15952",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, suppose you have applied a learning algorithm and it's not performing as well as you are hoping, so if your cross-validation set error or your test set error is high, how can we figure out if the learning algorithm is suffering from high bias or suffering from high variance?",
    "output": "どうやって学習アルゴリズムが何の被害を、、、具体的に、学習アルゴリズムを適用したとする、そして期待通りにはパフォーマンスが出ていないとする。つまり、クロスバリデーションセット誤差とテストセット誤差が高い。"
  },
  {
    "index": "F15953",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, the setting of a cross-validation error being high corresponds to either this regime or this regime.",
    "output": "クロスバリデーション誤差が高いという事なのでこのレジームかこのレジームに対応するはずだ。"
  },
  {
    "index": "F15954",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this regime on the left corresponds to a high bias problem.",
    "output": "この左のレジームは高バイアス問題に対応している、つまり、もし単純すぎる多項式、例えばd=1でフィッティングしていて、でも本当はもっと高い次数でフィッティングする必要があるようなデータの時。"
  },
  {
    "index": "F15955",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, if you are fitting a overly low order polynomial such as a d equals one when we really needed a higher order polynomial to fit to data, whereas in contrast this regime corresponds to a high variance problem.",
    "output": "他方、対照的に、このレジームは高分散の問題に対応する。"
  },
  {
    "index": "F15956",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, if d the degree of polynomial was too large for the data set that we have, and this figure gives us a clue for how to distinguish between these two cases.",
    "output": "そこではd、つまり多項式の次数が我らのデータセットに対しては大きすぎるという事。そしてこの図が、これら2つのケースをどうやって見分けるか、に関する手がかりを与えてくれる。"
  },
  {
    "index": "F15957",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, for the high bias case, that is the case of underfitting, what we find is that both the cross validation error and the training error are going to be high.",
    "output": "具体的に、高バイアスのケースではつまりアンダーフィットのケースでは、この図を見ると、クロスバリデーション誤差とトレーニング誤差の両方とも高くなっている事が分かる。"
  },
  {
    "index": "F15958",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if your algorithm is suffering from a bias problem, the training set error will be high and you might find that the cross validation error will also be high.",
    "output": "だからもしあなたのアルゴリズムがバイアスの問題を被っているなら、トレーニングセットの誤差も高くなるだろうし、またクロスバリデーションの誤差もまた高くなるはず。"
  },
  {
    "index": "F15959",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It might be close, maybe just slightly higher, than the training error.",
    "output": "それら2つは、近い。トレーニング誤差よりちょっと高いだけかも。"
  },
  {
    "index": "F15960",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you see this combination, that's a sign that your algorithm may be suffering from high bias.",
    "output": "だから、この組み合わせを観測したらあなたのアルゴリズムが高バイアスの被害を被っているサインだ。"
  },
  {
    "index": "F15961",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast, if your algorithm is suffering from high variance, then if you look here, we'll notice that J train, that is the training error, is going to be low.",
    "output": "対照的にアルゴリズムが高分散の被害を被ってるならここを見るとJtrain、つまりトレーニング誤差が低くなっている事に気付くだろう。"
  },
  {
    "index": "F15962",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, you're fitting the training set very well, whereas your cross validation error assuming that this is, say, the squared error which we're trying to minimize say, whereas in contrast your error on a cross validation set or your cross function or cross validation set will be much bigger than your training set error.",
    "output": "つまり、トレーニングセットにはとてもうまくフィッティング出来ている。一方、クロスバリデーション誤差はこれが最小化したい誤差の二乗だと仮定するとーーー他方で対照的に、クロスバリデーションセットに対する誤差はつまりクロスバリデーションセットに対するコスト関数はトレーニングセットの誤差よりも、ずっと大きくなるだろう。"
  },
  {
    "index": "F15963",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is a double greater than sign. That's the map symbol for much greater thans, denoted by two greater than signs.",
    "output": "つまりこれは「大なり大なり」の記号でそれは数学の記号でずっと大きい、を意味するもので2つの大なりの記号で示す。"
  },
  {
    "index": "F15964",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you see this combination of values, then that's a clue that your learning algorithm may be suffering from high variance and might be overfitting.",
    "output": "だからもしこの組み合わせを見たらそれはつまり、、、そしてつまりこの値の組み合わせ見たら、それは学習アルゴリズムが高分散を被っていて、オーバーフィットしてるという手がかりを得たという事。"
  },
  {
    "index": "F15965",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The key that distinguishes these two cases is, if you have a high bias problem, your training set error will also be high is your hypothesis just not fitting the training set well.",
    "output": "そしてこれら2つのケースを分けるキーとなるのはもし高バイアスの問題ならトレーニングセットの誤差も高くなるはずで、何故なら仮説がトレーニングセットにうまくフィッティング出来ていないだけだから。"
  },
  {
    "index": "F15966",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have a high variance problem, your training set error will usually be low, that is much lower than your cross-validation error.",
    "output": "そしてもし高分散の問題なら、トレーニングセットの誤差は通常は低く、クロスバリデーション誤差とくらべると大きく低いはず。"
  },
  {
    "index": "F15967",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully that gives you a somewhat better understanding of the two problems of bias and variance.",
    "output": "以上で、バイアスと分散の2つの問題が、いくらかでもより良く理解出来たら幸いです。"
  },
  {
    "index": "F15968",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I still have a lot more to say about bias and variance in the next few videos, but what we'll see later is that by diagnosing whether a learning algorithm may be suffering from high bias or high variance, I'll show you even more details on how to do that in later videos.",
    "output": "次の幾つかのビデオで言います。のち程見るのは、学習アルゴリズムが高バイアスか高分散を被っているかを診断する事です。"
  },
  {
    "index": "F15969",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But we'll see that by figuring out whether a learning algorithm may be suffering from high bias or high variance or combination of both, that that would give us much better guidance for what might be promising things to try in order to improve the performance of a learning algorithm.",
    "output": "学習アルゴリズムが被っているのが高バイアスか両方の組み合わせかを区別する事により、学習アルゴリズムを改善する為に何をやるのが成果が期待出来そうな道かを知る、より良いガイダンスを提供してくれる事を見ていきます。"
  },
  {
    "index": "F15970",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You've seen how regularization can help prevent over-fitting.",
    "output": "正規化がどうオーバーフィットを防止するかは、これまで見てきた。"
  },
  {
    "index": "F15971",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But how does it affect the bias and variances of a learning algorithm?",
    "output": "でもそれは、どんな風に学習アルゴリズムのバイアスと分散に影響を与えるか?"
  },
  {
    "index": "F15972",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I'd like to go deeper into the issue of bias and variances and talk about how it interacts with and is affected by the regularization of your learning algorithm.",
    "output": "このビデオでは、バイアスと分散の問題についてより深く見ていきたい。そして学習アルゴリズムの正規化とどう相互作用し、影響を与えるか議論していきたい。"
  },
  {
    "index": "F15973",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose we're fitting a high auto polynomial, like that showed here, but to prevent over fitting we need to use regularization, like that shown here.",
    "output": "このような高次の多項式をフィッティングするが、そこでオーバーフィットを避けるためにここに示したような正規化を行うとする。"
  },
  {
    "index": "F15974",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we have this regularization term to try to keep the values of the prem to small.",
    "output": "つまりこの正規化の項でパラメータを値を小さく保とうとするという訳。"
  },
  {
    "index": "F15975",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as usual, the regularizations comes from J = 1 to m, rather than j = 0 to m.",
    "output": "そしていつも通り、正規化の和はjが1からmまで取り、jが0からmでは無い。"
  },
  {
    "index": "F15976",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's consider three cases.",
    "output": "では3つのケースを考えてみよう。"
  },
  {
    "index": "F15977",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first is the case of the very large value of the regularization parameter lambda, such as if lambda were equal to 10,000.",
    "output": "最初のケースは正規化パラメータのラムダの値がとても大きな場合。例えばラムダ=10,000とか巨大な値の時。"
  },
  {
    "index": "F15978",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this case, all of these parameters, theta 1, theta 2, theta 3, and so on would be heavily penalized and so we end up with most of these parameter values being closer to zero.",
    "output": "この場合、これらのパラメータ全てシータ1、シータ2、シータ3などなど、が、とても重くペナルティを課される。"
  },
  {
    "index": "F15979",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the hypothesis will be roughly h of x, just equal or approximately equal to theta zero.",
    "output": "すると、これらのパラメータの値のほとんどがゼロになり、結果として、仮説はだいたいh(x)は単にだいたい近似としてはイコールシータ0となる。"
  },
  {
    "index": "F15980",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we end up with a hypothesis that more or less looks like that, more or less a flat, constant straight line.",
    "output": "だから仮説は多かれ少なかれそんな感じに見える。これは多かれ少なかれフラットで、定数の直線。"
  },
  {
    "index": "F15981",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this hypothesis has high bias and it badly under fits this data set, so the horizontal straight line is just not a very good model for this data set.",
    "output": "だから水平の直線は単にこれらのデータセットのとても良い仮説という訳では無い。"
  },
  {
    "index": "F15982",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "At the other extreme is if we have a very small value of lambda, such as if lambda were equal to zero.",
    "output": "反対側の極端では、とても小さい値ラムダの時、たとえばラムダがイコール0の時。"
  },
  {
    "index": "F15983",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case, given that we're fitting a high order polynomial, this is a usual over-fitting setting.",
    "output": "その場合、高次の多項式でフィッティングしているなら、これは良くあるオーバーフィットの状況だ。"
  },
  {
    "index": "F15984",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case, given that we're fitting a high-order polynomial, basically, without regularization or with very minimal regularization, we end up with our usual high-variance, over fitting setting.",
    "output": "その場合、高次の多項式でフィッティングしていて、基本的には正規化無しかとても少ない正規化しかしないとすると、通常の高分散と、オーバーフィットの状況となる。"
  },
  {
    "index": "F15985",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is basically if lambda is equal to zero, we're just fitting with our regularization, so that over fits the hypothesis.",
    "output": "何故ならもしラムダがイコール0ならそれは単に正規化無しでフィッティングしているのだから、仮説はオーバーフィットする事となる。"
  },
  {
    "index": "F15986",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it's only if we have some intermediate value of longer that is neither too large nor too small that we end up with parameters data that give us a reasonable fit to this data.",
    "output": "そしてもしラムダの値を中間に大きすぎず、小さすぎずにパラメータを設定出来た時だけ、データに対してリーズナブルにフィッティング出来る。"
  },
  {
    "index": "F15987",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how can we automatically choose a good value for the regularization parameter?",
    "output": "では、どうやったら正規化パラメータのラムダの良い値を自動的に選ぶ事が出来るだろうか?"
  },
  {
    "index": "F15988",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just to reiterate, here's our model, and here's our learning algorithm's objective.",
    "output": "ここに、我らのモデル、学習アルゴリズムの目的関数を再掲しておく。"
  },
  {
    "index": "F15989",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For the setting where we're using regularization, let me define J train(theta) to be something different, to be the optimization objective, but without the regularization term.",
    "output": "正規化を使っている状況として、Jtrainのシータを最適化の目的関数とは分けて定義しよう。正規化項を抜いて定義する。"
  },
  {
    "index": "F15990",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Previously, in an earlier video, when we were not using regularization I define J train of data to be the same as J of theta as the cause function but when we're using regularization when the six well under term we're going to define J train my training set to be just my sum of squared errors on the training set or my average squared error on the training set without taking into account that regularization.",
    "output": "以前は、前のビデオで正規化をしていない時はJtrainのシータをJのシータ、つまりコスト関数と同じ物として定義していた。だが正規化を用いるべく追加のラムダの項を足したら、Jtrain、つまりトレーニングセットの誤差をトレーニングセットの誤差の二乗の和だけで定義する、トレーニングセットの平均の誤差の二乗で正規化項を考慮に入れないで定義するのだ。"
  },
  {
    "index": "F15991",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly I'm then also going to define the cross validation sets error and to test that error as before to be the average sum of squared errors on the cross validation in the test sets so just to summarize my definitions of J train J CU and J test are just the average square there one half of the other square record on the training validation of the test set without the extra regularization term.",
    "output": "そして同様にクロスバリデーションセット誤差とテストセット誤差も以前と同様に定義する、つまりクロスバリデーションセットとテストセットの誤差の二乗の和の平均。まとめると、JtrainとJcvとJtestの定義は単なる誤差の二乗の平均、、、じゃなくてトレーニングセットとバリデーションセットとテストセットの誤差の二乗の平均で追加の正規化項が無しの物と定義する。"
  },
  {
    "index": "F15992",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is how we can automatically choose the regularization parameter lambda.",
    "output": "そしてこんな風に正規化パラメータのラムダを自動で選ぶ。"
  },
  {
    "index": "F15993",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what I usually do is maybe have some range of values of lambda I want to try out.",
    "output": "普通やるのは、試したいラムダの値の範囲をあらかじめ持っておく。"
  },
  {
    "index": "F15994",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I might be considering not using regularization or here are a few values I might try lambda considering lambda = 0.01, 0.02, 0.04, and so on.",
    "output": "例えば正規化しない、というのもあるかもしれないし、幾つかの試してみたい値も入れてあるかもしれない。例えばラムダを0.01、0.02、0.04、、、などに渡って構築するかもしれない。"
  },
  {
    "index": "F15995",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I usually set these up in multiples of two, until some maybe larger value if I were to do these in multiples of 2 I'd end up with a 10.24.",
    "output": "そして見ての通り、普通私は、これらのステップを2倍づつで進めて、ある程度の大きい数字までやる。これを二の倍数とすると、実際は10.24だ。"
  },
  {
    "index": "F15996",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's 10 exactly, but this is close enough.",
    "output": "ぴったり10じゃないけどだいたい同じだ。"
  },
  {
    "index": "F15997",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the three to four decimal places won't effect your result that much.",
    "output": "桁数で三桁目とか四桁目の数は結果にはそんなに影響を与えない。"
  },
  {
    "index": "F15998",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this gives me maybe 12 different models.",
    "output": "これは、えーと、12個の異なるモデルを与える。"
  },
  {
    "index": "F15999",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm trying to select a month corresponding to 12 different values of the regularization of the parameter lambda.",
    "output": "私が試す事になるモデルの選択肢を、12個の異なる正規化パラメータに対応した。"
  }
]