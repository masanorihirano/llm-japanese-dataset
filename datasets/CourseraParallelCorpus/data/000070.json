[
  {
    "index": "F16000",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And of course you can also go to values less than 0.01 or values larger than 10 but I've just truncated it here for convenience.",
    "output": "もちろん、0.01以下の値とか10より大きい値を試したっていい。でもここでは簡単のためにこんだけで切った。"
  },
  {
    "index": "F16001",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given the issue of these 12 models, what we can do is then the following, we can take this first model with lambda equals zero and minimize my cost function J of data and this will give me some parameter of active data.",
    "output": "これらの12のモデルを所与とすると我らが行うのは以下の事だ:この最初のモデルラムダ=0を取り、コスト関数Jのシータを最小化する。"
  },
  {
    "index": "F16002",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similar to the earlier video, let me just denote this as theta super script one.",
    "output": "この結果、あるパラメータベクトルのシータが得られ前回のビデオと同様にこれをシータの上付き添字1で示す事にしよう。"
  },
  {
    "index": "F16003",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then I can take my second model with lambda set to 0.01 and minimize my cost function now using lambda equals 0.01 of course.",
    "output": "そして次に、二番目のモデル、ラムダを0.01として取り、コスト関数を最小化する。"
  },
  {
    "index": "F16004",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To get some different parameter vector theta.",
    "output": "今回は当然ラムダ=0.01を使ってさっきとは別のパラメータベクトル、シータを得る。"
  },
  {
    "index": "F16005",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for that I end up with theta(3).",
    "output": "次は三番目のモデルからシータ3を得て、それを以下同様に最後のモデルであるラムダ=10まで、または10.24までやり、結局このシータ12を得る。"
  },
  {
    "index": "F16006",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if part for my third model. And so on until for my final model with lambda set to 10 or 10.24, I end up with this theta(12).",
    "output": "基本的にはこれらのパラメータ毎に、誤差の二乗の平均をクロスバリデーションセットに対して測る。"
  },
  {
    "index": "F16007",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, I can talk all of these hypotheses, all of these parameters and use my cross validation set to validate them so I can look at my first model, my second model, fit to these different values of the regularization parameter, and evaluate them with my cross validation set based in measure the average square error of each of these square vector parameters theta on my cross validation sets.",
    "output": "そしてこれら12個のモデルから一番低いクロスバリデーションセット誤差が得られる物を選びだす。"
  },
  {
    "index": "F16008",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I would then pick whichever one of these 12 models gives me the lowest error on the trans validation set.",
    "output": "そして、ここで話を進める為に結果としてシータ5、つまり5次の多項式のモデルを選んだとしよう。"
  },
  {
    "index": "F16009",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say, for the sake of this example, that I end up picking theta 5, the 5th order polynomial, because that has the lowest cause validation error.",
    "output": "何故ならこれが一番クロスバリデーションの誤差が小さかったから、とする。"
  },
  {
    "index": "F16010",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having done that, finally what I would do if I wanted to report each test set error, is to take the parameter theta 5 that I've selected, and look at how well it does on my test set.",
    "output": "それを終えて、最後にテストセットの誤差をレポートしたければ、やることは、パラメータのシータ5、これは私の選んだ物だが、それを取って、それがテストセットに対してとれくらい良いかを見る。"
  },
  {
    "index": "F16011",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So once again, here is as if we've fit this parameter, theta, to my cross-validation set, which is why I'm setting aside a separate test set that I'm going to use to get a better estimate of how well my parameter vector, theta, will generalize to previously unseen examples.",
    "output": "ここでも、このパラメータのシータはクロスバリデーションセットに対してフィッティングしたのだから、テストセットはそれとは別にとっておいて、初めて見る手本に対してどれくらいうまく一般化出来ているかを見積もるのをよりうまくやる為に使うのです。"
  },
  {
    "index": "F16012",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's model selection applied to selecting the regularization parameter lambda.",
    "output": "以上がモデル選択を正規化パラメータのラムダを選ぶのに適用した場合です。"
  },
  {
    "index": "F16013",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The last thing I'd like to do in this video is get a better understanding of how cross validation and training error vary as we vary the regularization parameter lambda.",
    "output": "このビデオで最後にやりたい事は、正規化パラメータのラムダを変化させていくと、クロスバリデーションとトレーニングの誤差がどう変化していくかをより良く理解する事です。"
  },
  {
    "index": "F16014",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so just a reminder right, that was our original cost on j of theta.",
    "output": "備忘録として、これが元のコスト関数、Jのシータでした。"
  },
  {
    "index": "F16015",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But for this purpose we're going to define training error without using a regularization parameter, and cross validation error without using the regularization parameter.",
    "output": "だが今回の目的では、正規化パラメータ無しでトレーニング誤差とクロスバリデーション誤差を定義する、正規化パラメータ無しで。"
  },
  {
    "index": "F16016",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what I'd like to do is plot this Jtrain and plot this Jcv, meaning just how well does my hypothesis do on the training set and how does my hypothesis do when it cross validation sets. As I vary my regularization parameter lambda.",
    "output": "意図する所は、仮説がどれくらいトレーニングセットとクロスバリデーションセットに対して良いか、が、正規化パラメータのラムダを変化させていくとどう変わっていくかを見ていきたい。"
  },
  {
    "index": "F16017",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So as we saw earlier if lambda is small then we're not using much regularization and we run a larger risk of over fitting whereas if lambda is large that is if we were on the right part of this horizontal axis then, with a large value of lambda, we run the higher risk of having a biased problem, so if you plot J train and J cv, what you find is that, for small values of lambda, you can fit the trading set relatively way cuz you're not regularizing.",
    "output": "で、以前に見たように、ラムダが小さい時は、あまり正規化をしない、という事なのでよりオーバーフィットの危険性にさらされる。他方、ラムダが大きくなると、つまり、この横軸の右側の部分だと、その時はより大きなラムダとなるので、バイアスの問題に遭遇するリスクが上がる。"
  },
  {
    "index": "F16018",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, for small values of lambda, the regularization term basically goes away, and you're just minimizing pretty much just gray arrows.",
    "output": "だから小さなラムダの値では正規化の項は基本的にはどっかに行ってしまうような物で、単に誤差の二乗を最小化しているのに、極めて近い事をしている。"
  },
  {
    "index": "F16019",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when lambda is small, you end up with a small value for Jtrain, whereas if lambda is large, then you have a high bias problem, and you might not feel your training that well, so you end up the value up there.",
    "output": "だからラムダが小さい時は結果としてはJtrainは小さくなるが、他方ラムダが大きいと、高バイアス問題となりトレーニングセットにはうまくフィットしなくなる。だから値は結局、上昇する。"
  },
  {
    "index": "F16020",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So Jtrain of theta will tend to increase when lambda increases, because a large value of lambda corresponds to high bias where you might not even fit your trainings that well, whereas a small value of lambda corresponds to, if you can really fit a very high degree polynomial to your data, let's say.",
    "output": "つまり、Jtrainのシータはラムダを増加させるとそれに連れて増加する傾向にある。というのは、ラムダの大きな値は高バイアスに対応していて、そこではトレーニングセットに対してすらうまくフィット出来ないだろう。"
  },
  {
    "index": "F16021",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "After the cost validation error we end up with a figure like this, where over here on the right, if we have a large value of lambda, we may end up under fitting, and so this is the bias regime.",
    "output": "他方で小さな値のラムダは考えてみれば分かるが自由にとても高い次数の多項式でデータにフィッティング出来る。"
  },
  {
    "index": "F16022",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the cross validation error will be high.",
    "output": "クロスバリデーション誤差については、結局こんな図となる。"
  },
  {
    "index": "F16023",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me just leave all of that to this Jcv (theta) because so, with high bias, we won't be fitting, we won't be doing well in cross validation sets, whereas here on the left, this is the high variance regime, where we have two smaller value with longer, then we may be over fitting the data.",
    "output": "えーと、だから、クロスバリデーション誤差は高くなりちょっとラベルをつけておこう、これはJcvのシータで、高バイアスではフィッティングしない、、、クロスバリデーションセットに対しては良く無いだろう。他方、ここ、左の方では、これは高分散のレジームで、そこではラムダの値が小さすぎて、だからデータにオーバーフィットしている可能性がある。"
  },
  {
    "index": "F16024",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so by over fitting the data, then the cross validation error will also be high.",
    "output": "だからデータにオーバーフィットしている事により、クロスバリデーション誤差も高くなるだろう。"
  },
  {
    "index": "F16025",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, this is what the cross validation error and what the trading error may look like on a trading stance as we vary the regularization parameter lambda.",
    "output": "以上で、これがクロスバリデーション誤差とトレーニング誤差がどんな見た目になるか、だ。パラメータのラムダを変更していった時に。"
  },
  {
    "index": "F16026",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so once again, it will often be some intermediate value of lambda that is just right or that works best In terms of having a small cross validation error or a small test theta.",
    "output": "繰り返しておくと、しばしば、ある中間のラムダの値がいわゆる「ちょうどぴったし」のまたはクロスバリデーション誤差かテストセット誤差がどれだけ小さいかという観点で最良となる。"
  },
  {
    "index": "F16027",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And whereas the curves I've drawn here are somewhat cartoonish and somewhat idealized so on the real data set the curves you get may end up looking a little bit more messy and just a little bit more noisy then this.",
    "output": "ところでここで描いた曲線はいくらか漫画的というか、理想化された物だ。だから実際のデータセットにおいては、得られるプロットの結果はもうちょっとごちゃごちやしていて、もっとノイジーかもしれない。"
  },
  {
    "index": "F16028",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For some data sets you will really see these for sorts of trends and by looking at a plot of the hold-out cross validation error you can either manual, automatically try to select a point that minimizes the cross validation error and select the value of lambda corresponding to low cross validation error.",
    "output": "データセットによっては、あまりトレンドらしき物が分からない事もある。全体やクロスバリデーション誤差のプロットを見る事で人力で、または自動でクロスバリデーション誤差を最小化する点を選び、そして低いクロスバリデーション誤差に対応するラムダの値を選ぶ。"
  },
  {
    "index": "F16029",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When I'm trying to pick the regularization parameter lambda for learning algorithm, often I find that plotting a figure like this one shown here helps me understand better what's going on and helps me verify that I am indeed picking a good value for the regularization parameter monitor.",
    "output": "学習アルゴリズムの正規化パラメータラムダを選ぶ時はこのような図をプロットすることは何が起きているのか理解しやすくしてくれて、実際に正しい正規化パラメータの値を選んでいる、という事を確認しやすくしてくれる事が多い。"
  },
  {
    "index": "F16030",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully that gives you more insight into regularization and it's effects on the bias and variance of a learning algorithm.",
    "output": "以上で、正規化とその学習アルゴリズムのバイアスや分散への影響に関する洞察を深めてくれてるといいな。"
  },
  {
    "index": "F16031",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By now you've seen bias and variance from a lot of different perspectives.",
    "output": "今や、バイアスと分散をたくさんの異なる視点から見てきた事になる。"
  },
  {
    "index": "F16032",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we like to do in the next video is take all the insights we've gone through and build on them to put together a diagnostic that's called learning curves, which is a tool that I often use to diagnose if the learning algorithm may be suffering from a bias problem or a variance problem, or a little bit of both.",
    "output": "次のビデオでやりたい事としては、ここまで見てきたたくさんの洞察を組み合わせてその上に学習曲線と言われる診断を作り上げたい、それは学習アルゴリズムがバイアス問題にあっているか、分散問題にあっているか、またはその両方かを診断する為に私が良く使うツールです。"
  },
  {
    "index": "F16033",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to tell you about learning curves.",
    "output": "このビデオでは、学習曲線について議論したい。"
  },
  {
    "index": "F16034",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If either you wanted to sanity check that your algorithm is working correctly, or if you want to improve the performance of the algorithm.",
    "output": "学習曲線はプロットするととても便利な物で、アルゴリズムがちゃんと機能している、というサニティチェック(簡単な正当性チェック)をしたい時やアルゴリズムのパフォーマンスを改善したい時に役に立つ物だ。"
  },
  {
    "index": "F16035",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And learning curves is a tool that I actually use very often to try to diagnose if a physical learning algorithm may be suffering from bias, sort of variance problem or a bit of both.",
    "output": "そして学習曲線は実際私がバイアスやバリアンスやその混合の問題が起きてないかを診断したい時にしょっちゅう使っている物だ。"
  },
  {
    "index": "F16036",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's what a learning curve is.",
    "output": "その学習曲線とはこんな物だ。"
  },
  {
    "index": "F16037",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To plot a learning curve, what I usually do is plot j train which is, say, average squared error on my training set or Jcv which is the average squared error on my cross validation set.",
    "output": "学習曲線をプロットするには普通私はJtrain、つまりトレーニングセットの二乗誤差の平均をプロットするかまたはJcv、つまりクロスバリデーションセットの平均の二乗誤差をプロットする。"
  },
  {
    "index": "F16038",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm going to plot that as a function of m, that is as a function of the number of training examples I have.",
    "output": "そしてそのプロットはmの関数としてプロットする、つまりトレーニング手本の数の関数という事だ。"
  },
  {
    "index": "F16039",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so m is usually a constant like maybe I just have, you know, a 100 training examples but what I'm going to do is artificially with use my training set exercise.",
    "output": "普通、mは定数で、例えば100トレーニング手本とかだ。だがここで私は、人工的にトレーニングセットのサイズを減らす、という事をやる訳だ。"
  },
  {
    "index": "F16040",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I deliberately limit myself to using only, say, 10 or 20 or 30 or 40 training examples and plot what the training error is and what the cross validation is for this smallest training set exercises.",
    "output": "つまり自分で、自分自身に10とか20とか30とか40のトレーニング手本だけを使う、という制限を課す訳だ。そしてそれらのトレーニング誤差がどうなってるかそしてクロスバリデーションの誤差がどうなっているかをプロットする。"
  },
  {
    "index": "F16041",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's see what these plots may look like.",
    "output": "ではそのプロットがどんな感じか、見てみよう。"
  },
  {
    "index": "F16042",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose I have only one training example like that shown in this this first example here and let's say I'm fitting a quadratic function.",
    "output": "トレーニング手本が一つしか無いとしよう、こんな風に最初の手本だけ、そして二次関数をフィッティングしてるとしよう。"
  },
  {
    "index": "F16043",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, I have only one training example. I'm going to be able to fit it perfectly right?",
    "output": "トレーニング手本が一つだけなので完全にフィットさせる事が出来る。"
  },
  {
    "index": "F16044",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, just fit the quadratic function. I'm going to have 0 error on the one training example.",
    "output": "二次関数をフィットさせるだけで一つのトレーニング手本に対しては誤差0に出来る。"
  },
  {
    "index": "F16045",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I have two training examples. Well the quadratic function can also fit that very well.",
    "output": "トレーニング手本が2つの時は、、、この場合も二次関数はよくフィットさせられる。"
  },
  {
    "index": "F16046",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, even if I am using regularization, I can probably fit this quite well.",
    "output": "正規化しててもたぶんかなり良くフィットさせられるだろう。"
  },
  {
    "index": "F16047",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if I am using no neural regularization, I'm going to fit this perfectly and if I have three training examples again.",
    "output": "そしてもし正規化してなければこれに完璧にフィットさせられる。そしてトレーニング手本が3つの時も二次関数を完全にフィッティング出来る。"
  },
  {
    "index": "F16048",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Yeah, I can fit a quadratic function perfectly so if m equals 1 or m equals 2 or m equals 3, my training error on my training set is going to be 0 assuming I'm not using regularization or it may slightly large in 0 if I'm using regularization and by the way if I have a large training set and I'm artificially restricting the size of my training set in order to J train.",
    "output": "つまり、もしm=1かm=2かm=3ならこれらのトレーニングセットに対するトレーニング誤差は正規化してなければ0になると予想され、正規化してたら0よりちょっとだけ大きい値が予想される。"
  },
  {
    "index": "F16049",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here if I set M equals 3, say, and I train on only three examples, then, for this figure I am going to measure my training error only on the three examples that actually fit my data too and so even I have to say a 100 training examples but if I want to plot what my training error is the m equals 3.",
    "output": "ところで、もしたくさんのトレーニングセットがあり、それをJtrain向けにトレーニングセットのサイズを制限したら、ここを、仮にm=3にしてみたら、そして3つの手本だけでトレーニングしてみたら、この図で3つの手本に対してだけの実際にフィッティングしてる対象に対してだけのトレーニング誤差を測る事になりだから100個のトレーニング手本がある訳だけど、m=3だけのトレーニング誤差をプロットしようという訳だ。"
  },
  {
    "index": "F16050",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I'm going to do is to measure the training error on the three examples that I've actually fit to my hypothesis 2.",
    "output": "つまり、3つの手本だけに対して仮説をフィッティングしてそのトレーニング誤差を測るわけ。"
  },
  {
    "index": "F16051",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And not all the other examples that I have deliberately omitted from the training process.",
    "output": "そしてその他のトレーニング手本をわざと学習プロセスから抜くわけ。"
  },
  {
    "index": "F16052",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just to summarize what we've seen is that if the training set size is small then the training error is going to be small as well.",
    "output": "まとめると、ここまで見てきた事から、トレーニングセットのサイズが小さい時はトレーニング誤差も小さくなる。"
  },
  {
    "index": "F16053",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because you know, we have a small training set is going to be very easy to fit your training set very well may be even perfectly now say we have m equals 4 for example.",
    "output": "何故なら、トレーニングセットが小さければとても簡単にフィッティング出来るから。トレーニングセットにとても良くフィットさせられる、時には完全に一致させられる事も。"
  },
  {
    "index": "F16054",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well then a quadratic function can be a longer fit this data set perfectly and if I have m equals 5 then you know, maybe quadratic function will fit to stay there so so, then as my training set gets larger.",
    "output": "さて、ここでm=4となると、二次関数はもはや完全にはデータセットにフィットさせられなくなる。そしてm=5となると、、、まぁこの位ならそこそこフィットしたままかもしれん。"
  },
  {
    "index": "F16055",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It becomes harder and harder to ensure that I can find the quadratic function that process through all my examples perfectly.",
    "output": "でもそこからトレーニングセットを大きくしていくと全てのトレーニング手本の上を完全に通る二次関数を見つけるのはどんどん難しくなる。"
  },
  {
    "index": "F16056",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in fact as the training set size grows what you find is that my average training error actually increases and so if you plot this figure what you find is that the training set error that is the average error on your hypothesis grows as m grows and just to repeat when the intuition is that when m is small when you have very few training examples.",
    "output": "だからこの図をプロットするとトレーニングセット誤差はそれは仮説の誤差の平均だが、それはmが増加するにつれて増加する。その直感的な理解を繰り返すと、mが小さい時は、トレーニング手本がとてもちょっとしか無い時には、トレーニング手本を一つ一つ完璧に通過するようにフィッティングするのはとても簡単だ。"
  },
  {
    "index": "F16057",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's pretty easy to fit every single one of your training examples perfectly and so your error is going to be small whereas when m is larger then gets harder all the training examples perfectly and so your training set error becomes more larger now, how about the cross validation error.",
    "output": "他方、mが大きくなると全てのトレーニング手本を完璧に通るのはどんどん難しくなる。だからトレーニングセットの誤差はより大きくなる。"
  },
  {
    "index": "F16058",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, the cross validation is my error on this cross validation set that I haven't seen and so, you know, when I have a very small training set, I'm not going to generalize well, just not going to do well on that.",
    "output": "クロスバリデーションの誤差とはこのクロスバリデーションセットでの誤差で、これはまだ見ていないデータだ。だからとても小さなトレーニングセットではしっかりと一般化は出来ない。"
  },
  {
    "index": "F16059",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, right, this hypothesis here doesn't look like a good one, and it's only when I get a larger training set that, you know, I'm starting to get hypotheses that maybe fit the data somewhat better.",
    "output": "つまりこの仮説はそんなにいい仮説じゃない。トレーニングセットをより大きくしていく事で初めて、、、初めていくらかデータにより良くフィットするような仮説が得られ始めるのだ。"
  },
  {
    "index": "F16060",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So your cross validation error and your test set error will tend to decrease as your training set size increases because the more data you have, the better you do at generalizing to new examples.",
    "output": "つまり、クロスバリデーション誤差とテストセットの誤差は、トレーニングセットのサイズを大きくするにつれて、減少する傾向にある、なぜならデータが多くあればある程、新しいサンプルに対して一般化するのもうまく出来るだろうからだ。"
  },
  {
    "index": "F16061",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just the more data you have, the better the hypothesis you fit.",
    "output": "ようするに、データを多く使えば使うほど、フィッティングで得られる仮説も良くなっていくはず。"
  },
  {
    "index": "F16062",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you plot j train, and Jcv this is the sort of thing that you get.",
    "output": "だからJtrainとJcvをプロットすると、こんな感じの物が得られるはず。"
  },
  {
    "index": "F16063",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's look at what the learning curves may look like if we have either high bias or high variance problems.",
    "output": "ではここで、もし高バイアスだったり高バリアンスだったりといった問題を被った時に、どうなるか見てみよう。"
  },
  {
    "index": "F16064",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose your hypothesis has high bias and to explain this I'm going to use a, set an example, of fitting a straight line to data that, you know, can't really be fit well by a straight line.",
    "output": "仮説が高バイアスだったとしよう、それを説明する為に、例として、あまり直線ではうまくフィット出来ないようなデータに対し直線をフィッティングさせる場合を考えてみよう。"
  },
  {
    "index": "F16065",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we end up with a hypotheses that maybe looks like that.",
    "output": "すると仮説は、こんな感じになる。"
  },
  {
    "index": "F16066",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's think what would happen if we were to increase the training set size.",
    "output": "ここでトレーニングセットのサイズを大きくしていくと、何が起こるか考えてみよう。"
  },
  {
    "index": "F16067",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if instead of five examples like what I've drawn there, imagine that we have a lot more training examples.",
    "output": "つまりここに書いた5つだけの手本の代わりに、もっとたくさんのトレーニングの手本があると想像してみよう。"
  },
  {
    "index": "F16068",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well what happens, if you fit a straight line to this.",
    "output": "うーん、これに直線をフィッティングしたら、どうなるだろう?"
  },
  {
    "index": "F16069",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What you find is that, you end up with you know, pretty much the same straight line.",
    "output": "結局の所得られる結果はほとんど同じような直線だろう。"
  },
  {
    "index": "F16070",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I mean a straight line that just cannot fit this data and getting a ton more data, well the straight line isn't going to change that much.",
    "output": "つまり、このデータにうまくフィットするのが不可能な直線はさらに大量のデータを増やしてもその直線はたいして変わらない、という事。"
  },
  {
    "index": "F16071",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is the best possible straight-line fit to this data, but the straight line just can't fit this data set that well.",
    "output": "これがこのデータにもっとも適合する直線だ。それでも、この直線はこのデータセットにそんなに良くフィットさせる事は出来ない。"
  },
  {
    "index": "F16072",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you plot across validation error, this is what it will look like.",
    "output": "もしクロスバリデーション誤差をプロットしたらこんな感じの結果となるだろう。"
  },
  {
    "index": "F16073",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Option on the left, if you have already a miniscule training set size like you know, maybe just one training example and is not going to do well.",
    "output": "グラフの左側では、トレーニングセットをすごく小さく、例えば1つのトレーニング手本に制限した場合で、そんなに良くは無いだろう。"
  },
  {
    "index": "F16074",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But by the time you have reached a certain number of training examples, you have almost fit the best possible straight line, and even if you end up with a much larger training set size, a much larger value of m, you know, you're basically getting the same straight line, and so, the cross-validation error - let me label that - or test set error or plateau out, or flatten out pretty soon, once you reached beyond a certain the number of training examples, unless you pretty much fit the best possible straight line.",
    "output": "だがある程度の数のトレーニング手本の数に到達する頃には、ほとんど確実に、可能な範囲でベストな直線を得る事になる。そしてそこからさらにトレーニングセットのサイズを増やしたところで、つまりよりmの値を大きくした所で、基本的には同じ直線を得る事になる。"
  },
  {
    "index": "F16075",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And how about training error?",
    "output": "ではトレーニング誤差はどうだろう?"
  },
  {
    "index": "F16076",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, the training error will again be small.",
    "output": "トレーニング誤差は今回も小さい所から始まるが、高バイアスの場合、トレーニング誤差は最終的にはクロスバリデーション誤差に近くなる。"
  },
  {
    "index": "F16077",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you find in the high bias case is that the training error will end up close to the cross validation error, because you have so few parameters and so much data, at least when m is large.",
    "output": "何故ならあまりにもちょっとのパラメータしか無くデータはたくさんある、、、少なくともmが大きい所ではそうなので、トレーニングセットとクロスバリデーションセットの誤差は似たりよったりとなる。だからこんな感じの学習曲線が得られる事になる。"
  },
  {
    "index": "F16078",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The performance on the training set and the cross validation set will be very similar.",
    "output": "最後になるが、高バイアスの問題はクロスバリデーション誤差とトレーニング誤差が両方とも高い、という形であらわれる。"
  },
  {
    "index": "F16079",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, this is what your learning curves will look like, if you have an algorithm that has high bias.",
    "output": "つまり最終的に比較的高いJcvとJtrainの値に落ち着く。これはまた、とても興味深い事を示唆している。"
  },
  {
    "index": "F16080",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, the problem with high bias is reflected in the fact that both the cross validation error and the training error are high, and so you end up with a relatively high value of both Jcv and the j train.",
    "output": "それはもし学習アルゴリズムが高バイアスだと、もっとトレーニング手本を増やしていったとしても、つまりこの図の右側に移動していっても、クロスバリデーション誤差はたいして下がらない事が分かる。"
  },
  {
    "index": "F16081",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This also implies something very interesting, which is that, if a learning algorithm has high bias, as we get more and more training examples, that is, as we move to the right of this figure, we'll notice that the cross validation error isn't going down much, it's basically fattened up, and so if learning algorithms are really suffering from high bias.",
    "output": "それは高い所でだいたい水平になってしまってる。だから学習アルゴリズムが実際に高バイアスの問題を被ってる時はトレーニングデータを増やす事それ自体はそんなには助けにならないだろう。"
  },
  {
    "index": "F16082",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Getting more training data by itself will actually not help that much,and as our figure example in the figure on the right, here we had only five training.",
    "output": "この図によれば、この右側の図の例ではここでは5つのトレーニング手本しか無い。"
  },
  {
    "index": "F16083",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "examples, and we fill certain straight line.",
    "output": "そして何らかの直線をフィットさせてる。"
  },
  {
    "index": "F16084",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And when we had a ton more training data, we still end up with roughly the same straight line.",
    "output": "そしてそこに大量のトレーニングデータを追加しても、ほとんど同じ直線のまま。"
  },
  {
    "index": "F16085",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if the learning algorithm has high bias give me a lot more training data.",
    "output": "だから学習アルゴリズムが高バイアスな所にもっとたくさんのトレーニングデータを追加しても、テストセット誤差やクロスバリデーション誤差をたいして低下させない。"
  },
  {
    "index": "F16086",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That doesn't actually help you get a much lower cross validation error or test set error.",
    "output": "だからあなたの学習アルゴリズムが高バイアスの問題を被っているかは知ると便利な事だと思う、何故ならそれを知る事で、役に立たない所でたくさんのトレーニングデータを集めてしまうという無駄を避ける事が出来るから。"
  },
  {
    "index": "F16087",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So knowing if your learning algorithm is suffering from high bias seems like a useful thing to know because this can prevent you from wasting a lot of time collecting more training data where it might just not end up being helpful.",
    "output": "次に学習アルゴリズムが高バリアンスの場合を見てみよう。とても小さなトレーニングセット、右の図だと5つしか無いような場合でとても高次の多項式、ここでは100次の多項式を描いた。"
  },
  {
    "index": "F16088",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next let us look at the setting of a learning algorithm that may have high variance.",
    "output": "それは誰も使わないような物だが、例示の為に。この高次の多項式でフィッティングしたらトレーニング誤差がどうなるかを見てみよう。"
  },
  {
    "index": "F16089",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let us just look at the training error in a around if you have very smart training set like five training examples shown on the figure on the right and if we're fitting say a very high order polynomial, and I've written a hundredth degree polynomial which really no one uses, but just an illustration.",
    "output": "そしてとても小さなラムダの値、0では無いがとても小さな値を使えば、最終的にはこのデータにすこぶる良くフィットする事が出来て、ようするにそれはオーバーフィットする事になる。"
  },
  {
    "index": "F16090",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if we're using a fairly small value of lambda, maybe not zero, but a fairly small value of lambda, then we'll end up, you know, fitting this data very well that with a function that overfits this.",
    "output": "すると、トレーニングセットのサイズが小さい時にはトレーニング誤差、つまりJtrainのシータは小さくなるだろう。そしてトレーニングセットのサイズを少し増やしてもたぶんまだこのデータにオーバーフィットしたままだがでも多少はデータセットに完全にフィットさせるのは難しくなる。"
  },
  {
    "index": "F16091",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if the training set size is small, our training error, that is, j train of theta will be small.",
    "output": "だからトレーニングセットのサイズを増加させると、Jtrainも増えていくのが見られるだろう、何故ならトレーニングセットに完全にフィットさせるのはちょっと難しくなるだろうから、もっとトレーニング手本が増えると。"
  },
  {
    "index": "F16092",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as this training set size increases a bit, you know, we may still be overfitting this data a little bit but it also becomes slightly harder to fit this data set perfectly, and so, as the training set size increases, we'll find that j train increases, because it is just a little harder to fit the training set perfectly when we have more examples, but the training set error will still be pretty low.",
    "output": "でも増えると言ってもトレーニングセット誤差はまだ極めて低いままだろう。"
  },
  {
    "index": "F16093",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, how about the cross validation error?",
    "output": "さて、クロスバリデーション誤差はどうなるだろう?"
  },
  {
    "index": "F16094",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, in high variance setting, a hypothesis is overfitting and so the cross validation error will remain high, even as we get you know, a moderate number of training examples and, so maybe, the cross validation error may look like that.",
    "output": "高バリアンスの設定では仮説はオーバーフィットしているのだからクロスバリデーション誤差は高いままに留まる、トレーニング手本の数をある程度増やしたとしても。だから、クロスバリデーション誤差はこんな感じとなる。"
  },
  {
    "index": "F16095",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the indicative diagnostic that we have a high variance problem, is the fact that there's this large gap between the training error and the cross validation error.",
    "output": "高バリアンスの問題が起こってる時に特徴的な診断ポイントとしてはトレーニング誤差とクロスバリデーション誤差との間にとても大きなギャップがある、というもの。"
  },
  {
    "index": "F16096",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we think about adding more training data, that is, taking this figure and extrapolating to the right, we can kind of tell that, you know the two curves, the blue curve and the magenta curve, are converging to each other.",
    "output": "そしてこの図を見ると、もっとトレーニングデータを追加する事を考えるとこの図を外挿して右に伸ばすと、この2つのカーブは青いカーブとマゼンダのカーブは、同じ所に収束していくだろう事が分かる。"
  },
  {
    "index": "F16097",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, if we were to extrapolate this figure to the right, then it seems it likely that the training error will keep on going up and the cross-validation error would keep on going down.",
    "output": "つまり、仮にこの図を右へと外挿し続けていくと、たぶん、トレーニング誤差は増加し続け、そしてクロスバリデーション誤差は減少し続ける。"
  },
  {
    "index": "F16098",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the thing we really care about is the cross-validation error or the test set error, right?",
    "output": "そして我らが本当に問題としてるのはクロスバリデーション誤差かテストセット誤差でしょ?"
  },
  {
    "index": "F16099",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in this sort of figure, we can tell that if we keep on adding training examples and extrapolate to the right, well our cross validation error will keep on coming down.",
    "output": "だからこの種の図では、トレーニング手本をさらに追加していく事で右に外挿してく事で、クロスバリデーション誤差は減少していく事が分かる。"
  },
  {
    "index": "F16100",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, so, in the high variance setting, getting more training data is, indeed, likely to help.",
    "output": "だから高バリアンスの状況では、トレーニングデータをさらに増やす事は実際に状況を改善する。"
  },
  {
    "index": "F16101",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so again, this seems like a useful thing to know if your learning algorithm is suffering from a high variance problem, because that tells you, for example that it may be be worth your while to see if you can go and get some more training data.",
    "output": "だからこの場合もまた、学習アルゴリズムが高バリアンスの問題を被っているかを知るのは有益な事だと思う、何故ならそれを知る事で例えばもっと多くのデータを取りに行く価値があるかが分かるからだ。"
  },
  {
    "index": "F16102",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, on the previous slide and this slide, I've drawn fairly clean fairly idealized curves.",
    "output": "前のスライドとこのスライドではかなりクリーンで理想化されたカーブを描いた。"
  },
  {
    "index": "F16103",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you plot these curves for an actual learning algorithm, sometimes you will actually see, you know, pretty much curves, like what I've drawn here.",
    "output": "実際の学習アルゴリズムに対してこれらの曲線をプロットしたら、時には私が描いたのにとても似たカーブを見る事もあると思うが、時にはもうちょっとノイズが入った、もっととっちらかったようなカーブを見る事もあるだろう。"
  },
  {
    "index": "F16104",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Although, sometimes you see curves that are a little bit noisier and a little bit messier than this.",
    "output": "だがこれらのような学習曲線をプロットする事はしばしばあなたの学習アルゴリズムがバイアスの問題を被ってるかバリアンスの問題を被ってるか、またはその両方がちょっとずつ混ざってるかを知る助けとなる。"
  },
  {
    "index": "F16105",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But plotting learning curves like these can often tell you, can often help you figure out if your learning algorithm is suffering from bias, or variance or even a little bit of both.",
    "output": "だから学習アルゴリズムのパフォーマンスを改善しようと試みる時に、ほぼ必ずやる事としては、これらの学習曲線をプロットする、というのがある。"
  },
  {
    "index": "F16106",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when I'm trying to improve the performance of a learning algorithm, one thing that I'll almost always do is plot these learning curves, and usually this will give you a better sense of whether there is a bias or variance problem.",
    "output": "そしてだいたいは、バイアスかバリアンスの問題があるかについて、より良い感覚を与えてくれる。"
  },
  {
    "index": "F16107",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in the next video we'll see how this can help suggest specific actions is to take, or to not take, in order to try to improve the performance of your learning algorithm.",
    "output": "次のビデオでは、これがどのように、次に取るべきアクション、または取るべきでないアクションを考える助けとなるかを見ていく事にする。学習アルゴリズムのパフォーマンスを改善しようとする時に。"
  },
  {
    "index": "F16108",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We've talked about how to evaluate learning algorithms, talked about model selection, talked a lot about bias and variance.",
    "output": "ここまで、学習アルゴリズムをどう評価するか、モデルの選択、バイアスとバリアンスについて議論してきた。"
  },
  {
    "index": "F16109",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So how does this help us figure out what are potentially fruitful, potentially not fruitful things to try to do to improve the performance of a learning algorithm.",
    "output": "ではこれらがどのように学習アルゴリズムのパフォーマンスを改善しようとする時に何が潜在的に実りが多そうで何はそんなに良く無さそうかを見分けるのに役立たせる事が出来るだろうか?"
  },
  {
    "index": "F16110",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's go back to our original motivating example and go for the result.",
    "output": "もとの動機づけ目的の例に戻り、その結果を見てみよう。"
  },
  {
    "index": "F16111",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here is our earlier example of maybe having fit regularized linear regression and finding that it doesn't work as well as we're hoping.",
    "output": "さて、これは前に見た例で正規化した線形回帰をフィッティングして期待通りには振る舞ってない、と判明した物だ。"
  },
  {
    "index": "F16112",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We said that we had this menu of options.",
    "output": "そしてこんな選択肢のメニューがあると言った。"
  },
  {
    "index": "F16113",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So is there some way to figure out which of these might be fruitful options?",
    "output": "ではこの選択肢のうちどれが実り多そうな選択肢かを見分けるは方法が無いものか?"
  },
  {
    "index": "F16114",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first thing all of this was getting more training examples.",
    "output": "これらのうちの最初の物はもっと多くのトレーニング手本を得る、という物だった。"
  },
  {
    "index": "F16115",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What this is good for, is this helps to fix high variance.",
    "output": "これは高バリアンスを直すのに良い物だった。"
  },
  {
    "index": "F16116",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And concretely, if you instead have a high bias problem and don't have any variance problem, then we saw in the previous video that getting more training examples, while maybe just isn't going to help much at all.",
    "output": "そして例えば、代わりに高バイアスの問題にかかっていて、そしてバリアンスの問題は無ければ、その場合は前のビデオで見たようにトレーニング手本の数を増やす事はそんなに役には立たないと思われる。"
  },
  {
    "index": "F16117",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the first option is useful only if you, say, plot the learning curves and figure out that you have at least a bit of a variance, meaning that the cross-validation error is, you know, quite a bit bigger than your training set error.",
    "output": "つまり、この最初の選択肢が有用なのは学習曲線をプロットしてみて最低でもちょっとはバリアンスがある、と確認出来た場合のとにだけだ。つまり、クロスバリデーション誤差はトレーニングセット誤差よりもかなり大きい時という事。"
  },
  {
    "index": "F16118",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How about trying a smaller set of features?",
    "output": "フィーチャーの数を減らして試す、という件についてはどうだろう?"
  },
  {
    "index": "F16119",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, trying a smaller set of features, that's again something that fixes high variance.",
    "output": "フィーチャーの数を減らして試す、というのはこれまた高バリアンスを治す為の物だ。"
  },
  {
    "index": "F16120",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in other words, if you figure out, by looking at learning curves or something else that you used, that have a high bias problem; then for goodness sakes, don't waste your time trying to carefully select out a smaller set of features to use.",
    "output": "言い換えると、学習曲線なりそれ以外の何かしらなりで高バイアスの問題だ、と分かったなら、おお、どうかより少ない数のフィーチャーにする為に何を残すか、を慎重に選びだす事に時間を使わないでください。"
  },
  {
    "index": "F16121",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because if you have a high bias problem, using fewer features is not going to help.",
    "output": "何故ならもし高バイアスの問題にかかっているならより少ない数のフィーチャーを使うというのは、きっと役には立たないから。"
  },
  {
    "index": "F16122",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in contrast, if you look at the learning curves or something else you figure out that you have a high variance problem, then, indeed trying to select out a smaller set of features, that might indeed be a very good use of your time.",
    "output": "一方で、学習曲線なりなんなりを見て高バリアンスの問題を見つけた時には、まさにフィーチャーを減らす事を試してみるべきだ。それはきっとあなたの時間の、とても有効な使い方に違いない。"
  },
  {
    "index": "F16123",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How about trying to get additional features, adding features, usually, not always, but usually we think of this as a solution for fixing high bias problems.",
    "output": "フィーチャーを追加する、という選択肢はどうだろうか?フィーチャーを追加する、というのは必ずという訳では無いにせよ普通は高バイアスの問題の修正方法だとみなしている。"
  },
  {
    "index": "F16124",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you are adding extra features it's usually because your current hypothesis is too simple, and so we want to try to get additional features to make our hypothesis better able to fit the training set.",
    "output": "つまりフィーチャーを追加する時は普通は現在の仮説があまりにもシンプル過ぎる為、さらなるフィーチャーを追加する事で仮説がトレーニングセットにもっと良くフィットするようにしようとしたいからだ。"
  },
  {
    "index": "F16125",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, adding polynomial features; this is another way of adding features and so there is another way to try to fix the high bias problem.",
    "output": "同様に、多項式のフィーチャーを追加するのも、これはフィーチャーを追加するもう一つの方法で、つまり高バイアスの問題を修正するもう一つの方法がある訳だ。"
  },
  {
    "index": "F16126",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, if concretely if your learning curves show you that you still have a high variance problem, then, you know, again this is maybe a less good use of your time.",
    "output": "そして例えば、学習曲線が高バリアンスの問題を示していたら、その時はこの場合もあなたの時間の有効な使い方とはならないだろう。"
  },
  {
    "index": "F16127",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, decreasing and increasing lambda.",
    "output": "そして最後にラムダを増減させる。"
  },
  {
    "index": "F16128",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This are quick and easy to try, I guess these are less likely to be a waste of, you know, many months of your life.",
    "output": "これは手早く、簡単に試す事が出来る。これらはあなたの人生の何ヶ月とかを無駄にする可能性は、まぁ無いだろう。"
  },
  {
    "index": "F16129",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But decreasing lambda, you already know fixes high bias.",
    "output": "だがラムダを現象させるのは高バイアスを修正するという事を、既に知っている。"
  },
  {
    "index": "F16130",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case this isn't clear to you, you know, I do encourage you to pause the video and think through this that convince yourself that decreasing lambda helps fix high bias, whereas increasing lambda fixes high variance.",
    "output": "もしこれが当然、と感じられなければ、ひとまずビデオを止めて、ラムダを減らすのが高バイアスの問題を修正するのに有効で、ラムダを増加させるのか高バリアンスの問題を修正する事をしっかり納得出来るまでよく考えてみて欲しい。"
  },
  {
    "index": "F16131",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you aren't sure why this is the case, do pause the video and make sure you can convince yourself that this is the case.",
    "output": "どうしてそうなるのかいまいち自信が持てなければ、ビデオを止めてそうだと納得出来るまで確認して欲しい。"
  },
  {
    "index": "F16132",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or take a look at the curves that we were plotting at the end of the previous video and try to make sure you understand why these are the case.",
    "output": "または前回のビデオの最後でプロットしたカーブを見てそしてこれらがそうだと言う事をしっかりと納得してくれ。"
  },
  {
    "index": "F16133",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, let us take everything we have learned and relate it back to neural networks and so, here is some practical advice for how I usually choose the architecture or the connectivity pattern of the neural networks I use.",
    "output": "最後に、学んだ事全てをあわせて、それをニューラルネットワークに関連づけると、ニューラルネットワークの接続のパターンを、普段どうやって選んでいるかの実践的なアドバイスをしておく。"
  },
  {
    "index": "F16134",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you're fitting a neural network, one option would be to fit a relatively small neural network with, say, relatively few, maybe only one hidden layer and maybe only a relatively few number of hidden units.",
    "output": "ニューラルネットワークをフィッティングする時は考えられる一つの選択肢としてはとても小さなニューラルネット、相対的に少しの隠れユニットしか無いような、例えば一つの隠れユニットだけのような物にフィッティングする、というのが考えられる、、、ニューラルネットワークをフィッティングする時には考えられる一つの選択肢としては相対的に小さめのニューラルネットワーク、相対的に少しだけの、たとえば隠れレイヤが一つだけの物で隠れユニットの数も相対的に少しだけのネットワークにフィッティングするというのが考えられる。"
  },
  {
    "index": "F16135",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, a network like this might have relatively few parameters and be more prone to underfitting.",
    "output": "さて、このようなネットワークは相対的にはより少しのパラメータしか持たず、アンダーフィッティングしがちだ。"
  },
  {
    "index": "F16136",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The main advantage of these small neural networks is that the computation will be cheaper.",
    "output": "これらの小さなネットワークの主な利点は計算量がより安上がり、という所。"
  },
  {
    "index": "F16137",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "An alternative would be to fit a, maybe relatively large neural network with either more hidden units--there's a lot of hidden in one there--or with more hidden layers.",
    "output": "これに対して代替的な方法としては相対的により大きなニューラルネットワークを用いる事で、もっと多くの隠れユニットなり、この場合は一層内にたくさんの隠れユニットがあるとか。またはもっと多くの隠れレイヤがあるような物も考えられる。"
  },
  {
    "index": "F16138",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so these neural networks tend to have more parameters and therefore be more prone to overfitting.",
    "output": "するとこれらのネットワークはより多くのパラメータを持つ傾向にあり、ゆえによりオーバーフィッティングしやすい。"
  },
  {
    "index": "F16139",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One disadvantage, often not a major one but something to think about, is that if you have a large number of neurons in your network, then it can be more computationally expensive.",
    "output": "一つの欠点としてはしばしばそれほど大きな問題とはならないがたまに考える必要がある事としては、ネットワークに多くのニューロンがあると、より計算量的に高くつく場合がある、という事。"
  },
  {
    "index": "F16140",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Although within reason, this is often hopefully not a huge problem.",
    "output": "だが様々な理由から、これはたいていの場合は大きな問題とはならない。"
  },
  {
    "index": "F16141",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The main potential problem of these much larger neural networks is that it could be more prone to overfitting and it turns out if you're applying neural network very often using a large neural network often it's actually the larger, the better but if it's overfitting, you can then use regularization to address overfitting, usually using a larger neural network by using regularization to address is overfitting that's often more effective than using a smaller neural network.",
    "output": "これらより大きなニューラルネットワークの主要な問題点たりえるのは、よりオーバーフィッティングしやすい、という物で、ニューラルネットワークを用いてみると、ネットワークは大きければ大きいほど良い事が分かる。だがもしオーバーフィットしてしまったら正規化を用いてオーバーフィッティングの問題に対処出来る、通常は大きめのネットワークに正規化を適用してオーバーフィットに対処する方が小さめのニューラルネットワークを使うよりも効果的な事が多い。"
  },
  {
    "index": "F16142",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the main possible disadvantage is that it can be more computationally expensive.",
    "output": "そして主要な想定される短所としてはそちらの方が計算量的により高価になりえる、という所。"
  },
  {
    "index": "F16143",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, one of the other decisions is, say, the number of hidden layers you want to have, right?",
    "output": "最後に、決断すべき別の問題として、隠れレイヤの数を幾つにすべきか、というのがある。"
  },
  {
    "index": "F16144",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, do you want one hidden layer or do you want three hidden layers, as we've shown here, or do you want two hidden layers?",
    "output": "つまり隠れレイヤは一つにすべきか、あるいはここに示したように3つの隠れレイヤにすべきか?"
  },
  {
    "index": "F16145",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And usually, as I think I said in the previous video, using a single hidden layer is a reasonable default, but if you want to choose the number of hidden layers, one other thing you can try is find yourself a training cross-validation, and test set split and try training neural networks with one hidden layer or two hidden layers or three hidden layers and see which of those neural networks performs best on the cross-validation sets.",
    "output": "たが隠れレイヤの数を選ぼうという時に、もう一つ試せる事としては、トレーニングセット、クロスバリデーションセット、そしてテストセットに分けて、そして隠れレイヤ一つ、または隠れレイヤ二つ、または隠れレイヤ三つでニューラルネットワークをトレーニングしてみて、それらのネットワークのどれがクロスバリデーションセットで最も良いパフォーマンスを出すかを見てみる。"
  },
  {
    "index": "F16146",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You take your three neural networks with one, two and three hidden layers, and compute the cross validation error at Jcv and all of them and use that to select which of these is you think the best neural network.",
    "output": "隠れレイヤを一つ、二つ、三つのニューラルネットワークに対してクロスバリデーション誤差のJcvをそれら全てに対して計算してみて、そしてそれを用いてこれらのうちのどのニューラルネットワークを用いるのがベストかを選ぶのに使う事が出来る。"
  },
  {
    "index": "F16147",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's it for bias and variance and ways like learning curves, who tried to diagnose these problems.",
    "output": "以上が、バイアスとバリアンス、そして学習曲線などを用いてこれらの問題を診断する方法だ。"
  },
  {
    "index": "F16148",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As far as what you think is implied, for one might be truthful or not truthful things to try to improve the performance of a learning algorithm.",
    "output": "それを用いて、学習アルゴリズムのパフォーマンスを改善するのに何は実り多そうで何はあまり実りが無さそうかを判断出来る。"
  },
  {
    "index": "F16149",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you understood the contents of the last few videos and if you apply them you actually be much more effective already and getting learning algorithms to work on problems and even a large fraction, maybe the majority of practitioners of machine learning here in Silicon Valley today doing these things as their full-time jobs.",
    "output": "あなたがもしここ数回のビデオの内容を理解出来ていたら、そしてそれを実践出来るのなら、あなたはすでに、、、学習アルゴリズムを問題に対して適用出来、しかもそれを効率的に行う事が出来る、しかもそれを、ここシリコンバレーでこんにちフルタイムの仕事としてやっているエンジニアのかなりの割合、ひょっとしたら大多数の実践家よりもうまく、だ。"
  },
  {
    "index": "F16150",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I hope that these pieces of advice on by experience in diagnostics will help you to much effectively and powerfully apply learning and get them to work very well.",
    "output": "さて、これらの診断の時の経験に対するアドバイスが、学習アルゴリズムを効率的に適用し、とてもうまく機能出来るようになる、助けとなるといいな。"
  },
  {
    "index": "F16151",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos I'd like to talk about machine learning system design.",
    "output": "続くいくつかのビデオで機械学習システムのデザインについて話す。"
  },
  {
    "index": "F16152",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "These videos will touch on the main issues that you may face when designing a complex machine learning system, and will actually try to give advice on how to strategize putting together a complex machine learning system.",
    "output": "これらのビデオは複雑な機械学習のシステムをあなたがデザインする時に直面するであろう問題に触れる、そしてどうやって複雑な機械学習のシステムを組み合わせるかについての戦略を提供する。"
  },
  {
    "index": "F16153",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case this next set of videos seems a little disjointed that's because these videos will touch on a range of the different issues that you may come across when designing complex learning systems.",
    "output": "この後に続く一連のビデオがいまいちまとまりが無く感じられたとしたら、それはこれらのビデオが複雑な機械学習のシステムをデザインする時に遭遇するであろう様々な問題に触れていくからだ。"
  },
  {
    "index": "F16154",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And even though the next set of videos may seem somewhat less mathematical, I think that this material may turn out to be very useful, and potentially huge time savers when you're building big machine learning systems.",
    "output": "そしてこの後の一連のビデオはまた、いくらか、数学的で無い、と感じるかもしれないが、それでもこれらの題材はとても役に立つ、と分かる日が来るだろうと思う。そしてあなたが大きな機械学習のシステムを構築する時には巨大な時間の節約になってくれると思う。"
  },
  {
    "index": "F16155",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, I'd like to begin with the issue of prioritizing how to spend your time on what to work on, and I'll begin with an example on spam classification.",
    "output": "具体的に言うと、次に何をするのに時間を費やすべきかのプライオリティ付けする、という問題について議論したい。スパム分類の例から始めよう。"
  },
  {
    "index": "F16156",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say you want to build a spam classifier.",
    "output": "スパム分類器を構築したいとする。"
  },
  {
    "index": "F16157",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here are a couple of examples of obvious spam and non-spam emails.",
    "output": "ここに、明らかにスパムのメールと明らかにスパムで無いメールが幾つかある。"
  },
  {
    "index": "F16158",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "if the one on the left tried to sell things.",
    "output": "左側のは、物を売ろうとするメールだ。"
  },
  {
    "index": "F16159",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And notice how spammers will deliberately misspell words, like Vincent with a 1 there, and mortgages.",
    "output": "スパマーはどう単語をミススペルしているか気づくだろう。たとえばMedicineの中には数字の1が混じってたり、M0rgagesとかだったり。"
  },
  {
    "index": "F16160",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And on the right as maybe an obvious example of non-stamp email, actually email from my younger brother.",
    "output": "そして右側は明らかにスパムで無い例。これは実際の、私の弟からのメールだ。"
  },
  {
    "index": "F16161",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we have a labeled training set of some number of spam emails and some non-spam emails denoted with labels y equals 1 or 0, how do we build a classifier using supervised learning to distinguish between spam and non-spam?",
    "output": "我らは何通かのスパムのメールとスパムでないメールからなるトレーニングセットがあり、それをラベルでy=1か0で示してあるとする。スパムと非スパムを区別する分類器を教師あり学習を用いてどうやって構築出来るだろうか?"
  },
  {
    "index": "F16162",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to apply supervised learning, the first decision we must make is how do we want to represent x, that is the features of the email.",
    "output": "教師あり学習を適用する為には最初に決めなくてはならない事はxをどう表現するか、これはe-mailのフィーチャーだが、それを決める事だ。"
  },
  {
    "index": "F16163",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given the features x and the labels y in our training set, we can then train a classifier, for example using logistic regression.",
    "output": "フィーチャーxとラベルyがトレーニングセットとして与えられたとすると、我らは分類器を、例えばロジスティック回帰を使うなどして、訓練する事が出来る。"
  },
  {
    "index": "F16164",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's one way to choose a set of features for our emails.",
    "output": "これは、我らのe-mailについてのフィーチャーを選ぶ一つの方法だ。"
  },
  {
    "index": "F16165",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We could come up with, say, a list of maybe a hundred words that we think are indicative of whether e-mail is spam or non-spam, for example, if a piece of e-mail contains the word 'deal' maybe it's more likely to be spam if it contains the word 'buy' maybe more likely to be spam, a word like 'discount' is more likely to be spam, whereas if a piece of email contains my name, Andrew, maybe that means the person actually knows who I am and that might mean it's less likely to be spam.",
    "output": "例えば我らはe-mailがスパムか非スパムかを判断出来ると思われる、100個の特徴的な単語を選び出せたとする。"
  },
  {
    "index": "F16166",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And maybe for some reason I think the word \"now\" may be indicative of non-spam because I get a lot of urgent emails, and so on, and maybe we choose a hundred words or so.",
    "output": "そしてある理由により、私はnowという単語が入っている物も非スパムを示すと考える。何故なら私はたくさんの緊急のe-mailを受け取るからだ。"
  },
  {
    "index": "F16167",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given a piece of email, we can then take this piece of email and encode it into a feature vector as follows.",
    "output": "e-mailの一部分を与えられた時に、このe-mailの一部分を以下のようにフィーチャーベクトルへとエンコードする。"
  },
  {
    "index": "F16168",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to take my list of a hundred words and sort them in alphabetical order say.",
    "output": "100個の単語のリストに対し、それをアルファベット順に並べる。単語のリストはソートされてないかもしれないから。"
  },
  {
    "index": "F16169",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, you know, here's a, here's my list of words, just count and so on, until eventually I'll get down to now, and so on and given a piece of e-mail like that shown on the right, I'm going to check and see whether or not each of these words appears in the e-mail and then I'm going to define a feature vector x where in this piece of an email on the right, my name doesn't appear so I'm gonna put a zero there.",
    "output": "discountとかそのまま下まで行ってnowとか、などなど。そして右に示したようなe-mailの一部分が与えられた時に、これらの単語がそれぞれe-mailに現れるかをチェックしていき、そしてフィーチャーベクトルxを以下のように定義する:この右側のメールの一部分において、私の名前は現れてない、だからそこに0を入れる。"
  },
  {
    "index": "F16170",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The word \"by\" does appear, so I'm gonna put a one there and I'm just gonna put one's or zeroes.",
    "output": "そうやって1か0を入れていく。Buyが二回現れても1を入れる。"
  },
  {
    "index": "F16171",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm not gonna recount how many times the word occurs.",
    "output": "単語が何回出てくるか、はカウントしない。"
  },
  {
    "index": "F16172",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The word \"due\" appears, I put a one there.",
    "output": "単語dealは現れる、だから1を入れる。"
  },
  {
    "index": "F16173",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The word \"discount\" doesn't appear, at least not in this this little short email, and so on.",
    "output": "discountは現れない。少なくともこのメールの範囲には。"
  },
  {
    "index": "F16174",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The word \"now\" does appear and so on.",
    "output": "という風に続けていく。単語nowは現れる、などなど。"
  },
  {
    "index": "F16175",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I put ones and zeroes in this feature vector depending on whether or not a particular word appears.",
    "output": "つまり、私はフィーチャーベクトルに0か1を、特定の単語が現れるかどうかに応じて入れていく。"
  },
  {
    "index": "F16176",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this example my feature vector would have to mention one hundred, if I have a hundred, if if I chose a hundred words to use for this representation and each of my features Xj will basically be 1 if you have a particular word that, we'll call this word j, appears in the email and Xj would be zero otherwise.",
    "output": "私が100を選んだからだが。もし100語の単語をこの表現に使う事としたとして、そして各フィーチャーxjは基本的には、ある単語、それを単語jと呼ぼう、それがe-mailにあったら1をとり、それ以外ではxjは0となる。"
  },
  {
    "index": "F16177",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Okay.",
    "output": "オーケー。"
  },
  {
    "index": "F16178",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that gives me a feature representation of a piece of email.",
    "output": "こうしてe-mail片のフィーチャーによる表現が得られる。"
  },
  {
    "index": "F16179",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By the way, even though I've described this process as manually picking a hundred words, in practice what's most commonly done is to look through a training set, and in the training set depict the most frequently occurring n words where n is usually between ten thousand and fifty thousand, and use those as your features.",
    "output": "ところで、このプロセスを手動で100個の単語を選ぶ、と記述したが、実際にはもっとも一般的な方法は、トレーニングセットを取り、それを見ていってトレーニングセットの中にもっとも頻繁に現れるn個の単語を選び出す、ここでnは通常は1万から5万程度の値で、それらをフィーチャーとして使う。"
  },
  {
    "index": "F16180",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So rather than manually picking a hundred words, here you look through the training examples and pick the most frequently occurring words like ten thousand to fifty thousand words, and those form the features that you are going to use to represent your email for spam classification.",
    "output": "つまり、手動で100個の単語を選ぶ代わりに、トレーニング手本を見ていって、もっとも良く見かける単語の上位1万から5万語くらいを選び、それらから構成されるフィーチャーをスパム分類器に対してe-mailを表現するフィーチャーとして用いる。"
  },
  {
    "index": "F16181",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, if you're building a spam classifier one question that you may face is, what's the best use of your time in order to make your spam classifier have higher accuracy, you have lower error.",
    "output": "今、もしあなたがスパム分類器を構築しているとすると、直面するであろう一つの問いとして、スパム分類器として高い正確性と低いエラーを得る為に何に時間を使うのが、もっとも賢い自分の時間の使い方か?"
  },
  {
    "index": "F16182",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One natural inclination is going to collect lots of data.",
    "output": "自然に思い浮かぶ事としては、もっと多くのデータを集めるという事。"
  },
  {
    "index": "F16183",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right?",
    "output": "でしょ?"
  },
  {
    "index": "F16184",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact there's this tendency to think that, well the more data we have the better the algorithm will do.",
    "output": "そして現実でも、より多くのデータがあれば、アルゴリズムはよりうまくやってくれるに違いない、と考える傾向にある。"
  },
  {
    "index": "F16185",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact, in the email spam domain, there are actually pretty serious projects called Honey Pot Projects, which create fake email addresses and try to get these fake email addresses into the hands of spammers and use that to try to collect tons of spam email, and therefore you know, get a lot of spam data to train learning algorithms.",
    "output": "そして実際、e-mailのスパムの分野では、HoneyPot(ハチミツ壺)プロジェクト、と呼ばれるとても真剣なプロジェクトが存在していて、それは偽のe-mailアドレスを作ってその偽のアドレスをスパマーにつかませて、それを使って大量のスパムのe-mailを集める、という物だ。つまり、学習アルゴリズムを訓練するスパムのデータをたくさん集める、という事。"
  },
  {
    "index": "F16186",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But we've already seen in the previous sets of videos that getting lots of data will often help, but not all the time.",
    "output": "だが我らは既にここまでの何本かのビデオで、データをもっと集める、というのは役に立つ事も確かにあるが、いつもそうだという訳じゃない、というのを見てきた。"
  },
  {
    "index": "F16187",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But for most machine learning problems, there are a lot of other things you could usually imagine doing to improve performance.",
    "output": "そして多くの機械学習の問題において、パフォーマンスを改善する為に考えられる選択肢は他にもたくさんある。"
  },
  {
    "index": "F16188",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For spam, one thing you might think of is to develop more sophisticated features on the email, maybe based on the email routing information.",
    "output": "スパムに関しては、一つ考えられる手としては、e-mailについてもっと洗練されたフィーチャーを開発する、というのが考えられる、例えばe-mailのルーティング情報に基づいたりとか。"
  },
  {
    "index": "F16189",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this would be information contained in the email header.",
    "output": "これはe-mailのヘッダに含まれている情報だ。"
  },
  {
    "index": "F16190",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, when spammers send email, very often they will try to obscure the origins of the email, and maybe use fake email headers.",
    "output": "スパマーがe-mailを送る時には、彼らはよく、どこから送ってるかをごまかそうとする。だから偽物のe-mailヘッダを使ったりする。"
  },
  {
    "index": "F16191",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Through very unusual routes, in order to get the spam to you.",
    "output": "またはとても普通でないコンピューターサービス群を通して、とても普通で無いルートを通してスパムが届くようにする。"
  },
  {
    "index": "F16192",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And some of this information will be reflected in the email header.",
    "output": "そしてこれらの情報の幾らかはe-mailのヘッダに反映される。"
  },
  {
    "index": "F16193",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so one can imagine, looking at the email headers and trying to develop more sophisticated features to capture this sort of email routing information to identify if something is spam.",
    "output": "だからこんな風に考えられる。e-mailヘッダを見て、この種のe-mailのルーティング情報をうまくとらえるような、もっと洗練されたフィーチャーを開発して、それがスパムかどうかを判定する、という事を試みるという事を。"
  },
  {
    "index": "F16194",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Something else you might consider doing is to look at the email message body, that is the email text, and try to develop more sophisticated features.",
    "output": "他に考えられる事としてはe-mailのメッセージのボディ、つまりはe-mailのテキストを見て、もっと洗練したフィーチャーを構築しようと試みる、というのがある。"
  },
  {
    "index": "F16195",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For example, should the word 'discount' and the word 'discounts' be treated as the same words or should we have treat the words 'deal' and 'dealer' as the same word?",
    "output": "例えば、discountとdiscountsは同じ単語とみなした方がいいだろうか?"
  },
  {
    "index": "F16196",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe even though one is lower case and one in capitalized in this example.",
    "output": "この例では片方は大文字で片方は小文字だけれども。"
  },
  {
    "index": "F16197",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or do we want more complex features about punctuation because maybe spam is using exclamation marks a lot more.",
    "output": "または句読点などの記号についてのもっと複雑なフィーチャーが良いかもしれない。何故ならスパムはびっくりマーク(!"
  },
  {
    "index": "F16198",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I don't know.",
    "output": "知らないけどね。"
  },
  {
    "index": "F16199",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And along the same lines, maybe we also want to develop more sophisticated algorithms to detect and maybe to correct to deliberate misspellings, like mortgage, medicine, watches.",
    "output": "同じような考えとして、故意のミススペルを検出する為のもっと洗練されたアルゴリズムを開発したい、と思うかもしれない。"
  },
  {
    "index": "F16200",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because spammers actually do this, because if you have watches with a 4 in there then well, with the simple technique that we talked about just now, the spam classifier might not equate this as the same thing as the word \"watches,\" and so it may have a harder time realizing that something is spam with these deliberate misspellings.",
    "output": "何故ならスパマーは実際にこれをやってるから。何故ならwatchesに4が入っていると、今話した単純なテクニックではスパム分類器はこれを\"watches\"と同じ物だとはみなせないかもしれない。"
  },
  {
    "index": "F16201",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is why spammers do it.",
    "output": "そしてだからこそスパマーはそれをするのだ。"
  },
  {
    "index": "F16202",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "While working on a machine learning problem, very often you can brainstorm lists of different things to try, like these.",
    "output": "機械学習の問題にとりくんでいる時には、あなたはしょっちゅうこれらのように試してみたい事のリストをブレインストーミングする事が出来る。"
  },
  {
    "index": "F16203",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By the way, I've actually worked on the spam problem myself for a while.",
    "output": "ところで、実のところ私はスパムの問題にしばらく従事していた事がある。"
  },
  {
    "index": "F16204",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I actually spent quite some time on it.",
    "output": "私はかなりの時間、それに費やした。"
  },
  {
    "index": "F16205",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And even though I kind of understand the spam problem, I actually know a bit about it, I would actually have a very hard time telling you of these four options which is the best use of your time so what happens, frankly what happens far too often is that a research group or product group will randomly fixate on one of these options.",
    "output": "そして私はスパムの問題をかなり理解しているのだけれども、それについてかなり知っているのだけれども、それでもあなたにこの四つの選択肢のうちどれにあなたの時間を使うのがベストな選択なのか、と言うのは、凄く難しい。"
  },
  {
    "index": "F16206",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And sometimes that turns out not to be the most fruitful way to spend your time depending, you know, on which of these options someone ends up randomly fixating on.",
    "output": "そして時には、誰かがランダムにこれらの選択肢のどれに固執し始めたかに応じてあなたがそれに時間を費やすのが実り多いかどうかが決まってしまうという事になる。"
  },
  {
    "index": "F16207",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By the way, in fact, if you even get to the stage where you brainstorm a list of different options to try, you're probably already ahead of the curve.",
    "output": "ところで、実のところ、試すべき様々な選択肢をブレインストーミングする所まで来ていたら、あなたはたぶん他人よりは前に進んでいる。"
  },
  {
    "index": "F16208",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Sadly, what most people do is instead of trying to list out the options of things you might try, what far too many people do is wake up one morning and, for some reason, just, you know, have a weird gut feeling that, \"Oh let's have a huge honeypot project to go and collect tons more data\" and for whatever strange reason just sort of wake up one morning and randomly fixate on one thing and just work on that for six months.",
    "output": "残念な事に、多くの人が代わりにやる事は、試すべき選択肢をリストに書き出そうとはせず、その代わりに多くの人々がやってしまう事は、ある朝起きて、何らかの理由で、例えば妙なガッツフィーリングで、「よし、巨大なハニーポットのプロジェクトを立ち上げて、もっと大量のデータを集めよう!」とか思い立ったりとか、とにかく何かしら変な理由で、ある朝起きてランダムに一つに決めてしまって、それに六ヶ月とか費やしてしまう。"
  },
  {
    "index": "F16209",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But I think we can do better.",
    "output": "だが、きっと我らはもっとマシなやり方が出来る。"
  },
  {
    "index": "F16210",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular what I'd like to do in the next video is tell you about the concept of error analysis and talk about the way where you can try to have a more systematic way to choose amongst the options of the many different things you might work, and therefore be more likely to select what is actually a good way to spend your time, you know for the next few weeks, or next few days or the next few months.",
    "output": "具体的には、次のビデオで私が話すのはエラー分析という概念で、そしてもっとシステマティックな方法でさまざまな異なる選択肢からうまく行きそうな方法を選ぶ事を試みようとする時に使える方法で、つまりは実際に続く数週間、数日、または数ヶ月もの時間をあなたが費やすのにより良さそうな選択を行う方法だ。"
  },
  {
    "index": "F16211",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, I talked about error analysis and the importance of having error metrics, that is of having a single real number evaluation metric for your learning algorithm to tell how well it's doing.",
    "output": "前回のビデオではエラー分析とエラーのメトリクスを持つ必要性を議論した。エラーのメトリクスとは学習アルゴリズムがどれだけうまくやっているかを語る単一の実数値による評価指標の事だった。"
  },
  {
    "index": "F16212",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the context of evaluation and of error metrics, there is one important case, where it's particularly tricky to come up with an appropriate error metric, or evaluation metric, for your learning algorithm.",
    "output": "評価とエラーのメトリクスという文脈においては、一つの重要なケースとしてあなたの学習アルゴリズムに対して適切なエラーの指標、適切な評価の指標を得るのがトリッキーになってまう場合というのがある。"
  },
  {
    "index": "F16213",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That case is the case of what's called skewed classes.",
    "output": "そのケースとは、いわゆるスキューした(歪んだ)クラス、と言われる物だ。"
  },
  {
    "index": "F16214",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me tell you what that means.",
    "output": "それがどういう事か、説明しよう。"
  },
  {
    "index": "F16215",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Consider the problem of cancer classification, where we have features of medical patients and we want to decide whether or not they have cancer.",
    "output": "ガンの分類の問題を考えよう。医療患者のフィーチャーがあって、そして彼らがガンかどうかを判断したいとする。"
  },
  {
    "index": "F16216",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is like the malignant versus benign tumor classification example that we had earlier.",
    "output": "つまりこれは以前に見た、腫瘍が悪性か良性かの分類の例に似ている。"
  },
  {
    "index": "F16217",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's say y equals 1 if the patient has cancer and y equals 0 if they do not.",
    "output": "ガンの患者をy=1と、そしてそれ以外の患者をy=0としよう。"
  },
  {
    "index": "F16218",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We have trained the progression classifier and let's say we test our classifier on a test set and find that we get 1 percent error.",
    "output": "ロジスティック回帰の分類器をトレーニングしてテストセットに対して分類器をテストしてみたら、1パーセントのエラーを得たとする。"
  },
  {
    "index": "F16219",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Seems like a really impressive result, right.",
    "output": "うーん、これはなかなか素晴らしい結果、、、かしら?"
  },
  {
    "index": "F16220",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're correct 99% percent of the time.",
    "output": "99パーセントの場合は正しいんだから。"
  },
  {
    "index": "F16221",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But now, let's say we find out that only 0.5 percent of patients in our training test sets actually have cancer.",
    "output": "でもここで、トレーニングとテストセットの0.5パーセントの患者しか実際にガンにかかってないとする。"
  },
  {
    "index": "F16222",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So only half a percent of the patients that come through our screening process have cancer.",
    "output": "つまりたった1パーセントの半分の患者しか、スクリーニングプロセスを越えて実際にガンとならない。"
  },
  {
    "index": "F16223",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this case, the 1% error no longer looks so impressive.",
    "output": "この場合、1パーセントのエラーはもはやそんなに素晴らしくは見えない。"
  },
  {
    "index": "F16224",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, here's a piece of code, here's actually a piece of non learning code that takes this input of features x and it ignores it.",
    "output": "特に、ここにあるコードなら、これは実の所学習すらしてないコードで、単に入力としてフィーチャーxを受け取りながらただそれを無視してy=0と言うだけのコード。"
  },
  {
    "index": "F16225",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It just sets y equals 0 and always predicts, you know, nobody has cancer and this algorithm would actually get 0.5 percent error.",
    "output": "そしてどんな時でも、誰もガンじゃない、と予測するという物。このアルゴリズムは実際に0.5パーセントのエラーを得る。"
  },
  {
    "index": "F16226",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is even better than the 1% error that we were getting just now and this is a non learning algorithm that you know, it is just predicting y equals 0 all the time.",
    "output": "つまりこんなのですらさっき得た1パーセントのエラーよりマシになってしまう。そしてこれは非学習アルゴリズムで見ての通り、単にどんな時でもy=0と予測するだけの物だ。"
  },
  {
    "index": "F16227",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this setting of when the ratio of positive to negative examples is very close to one of two extremes, where, in this case, the number of positive examples is much, much smaller than the number of negative examples because y equals one so rarely, this is what we call the case of skewed classes.",
    "output": "つまり、このような状況、陽性の手本と陰性の手本の割合がとても両極端となるような状況では、その場合、陽性の手本の総数が陰性の手本の総数よりもずっと、ずっと少ない、何故ならy=1はとてもレアだから。"
  },
  {
    "index": "F16228",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We just have a lot more of examples from one class than from the other class.",
    "output": "単にある一方のクラスが、他方のクラスよりもずっとたくさんあるような場合。"
  },
  {
    "index": "F16229",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by just predicting y equals 0 all the time, or maybe our predicting y equals 1 all the time, an algorithm can do pretty well.",
    "output": "そういう場合は単にいつでもy=0と予測しておけば、またはいつでもy=1と予測しておけば、アルゴリズムはとてもうまく振る舞う事が出来る。"
  },
  {
    "index": "F16230",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the problem with using classification error or classification accuracy as our evaluation metric is the following.",
    "output": "さて、分類エラー、または分類の正確さを評価指標として使う際の問題点は、以下のようになる。"
  },
  {
    "index": "F16231",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say you have one joining algorithm that's getting 99.2% accuracy.",
    "output": "あなたは手元に、99.2%の正確さが得られるアルゴリズムを持ってたとしよう。"
  },
  {
    "index": "F16232",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's a 0.8% error.",
    "output": "つまりエラーは0.8%だ。"
  },
  {
    "index": "F16233",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say you make a change to your algorithm and you now are getting 99.5% accuracy.",
    "output": "そしてあなたは自分のアルゴリズムに変更を加えて、そして今やあなたは99.5%の正確さを得たとする。"
  },
  {
    "index": "F16234",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is 0.5% error.",
    "output": "つまり0.5%のエラーだ。"
  },
  {
    "index": "F16235",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, is this an improvement to the algorithm or not?",
    "output": "ではこれは、アルゴリズムの改善と言えるだろうか?"
  },
  {
    "index": "F16236",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One of the nice things about having a single real number evaluation metric is this helps us to quickly decide if we just need a good change to the algorithm or not.",
    "output": "単一の実数評価指標を持っている事の利点の一つとしては、これが我らに、アルゴリズムに良い変更をしたかを素早く判断する助けとなるというのがある。"
  },
  {
    "index": "F16237",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, did we just do something useful or did we just replace our code with something that just predicts y equals zero more often?",
    "output": "それとも我らのコードを単にy=0を単により多く予測するだけのコードに置き換えたのだろうか?"
  },
  {
    "index": "F16238",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you have very skewed classes it becomes much harder to use just classification accuracy, because you can get very high classification accuracies or very low errors, and it's not always clear if doing so is really improving the quality of your classifier because predicting y equals 0 all the time doesn't seem like a particularly good classifier.",
    "output": "つまり、とてもスキューしたクラスの場合には、単に分類の正確さを使うというのはより難しくなる。何故ならとても高い分類の正確さを得る、またはとても低いエラーを得るという事は出来て、そしてそれがあなたの分類器を改善したかは、いつも明らかという訳では無い、何故ならどんな時でもy=0を予測する、というのは、そんなに良い分類器とは思えない。"
  },
  {
    "index": "F16239",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But just predicting y equals 0 more often can bring your error down to, you know, maybe as low as 0.5%.",
    "output": "だが、単にy=0と、より多く予測するだけで、エラーを減少させる事が出来てしまい、これは0.5%まで減らす事が出来る。"
  },
  {
    "index": "F16240",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When we're faced with such a skewed classes therefore we would want to come up with a different error metric or a different evaluation metric.",
    "output": "スキューしたクラスに直面している時には、もっと別のエラーの指標、あるいは別の評価指標が、欲しくなる。"
  },
  {
    "index": "F16241",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One such evaluation metric are what's called precision recall.",
    "output": "そんな評価指標の一つには、Preicision(精度)とRecall(再現率)という物がある。"
  },
  {
    "index": "F16242",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me explain what that is.",
    "output": "これが何なのかを説明していこう。"
  },
  {
    "index": "F16243",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we are evaluating a classifier on the test set.",
    "output": "テストセットに対して分類器を評価しているとしよう。"
  },
  {
    "index": "F16244",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For the examples in the test set the actual class of that example in the test set is going to be either one or zero, right, if there is a binary classification problem.",
    "output": "例えばテストセットにある手本の実際のクラスが1か0のどちらかとする。つまりバイナリ分類問題だ。"
  },
  {
    "index": "F16245",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what our learning algorithm will do is it will, you know, predict some value for the class and our learning algorithm will predict the value for each example in my test set and the predicted value will also be either one or zero.",
    "output": "そして我らの学習アルゴリズムが行う事は、そのクラスの何らかの値を予測する事で、だから我らの学習アルゴリズムはテストセットの各手本に対して値を予測し、その予測する値もまた1か0のどちらかだ。"
  },
  {
    "index": "F16246",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let me draw a two by two table as follows, depending on a full of these entries depending on what was the actual class and what was the predicted class.",
    "output": "そこで以下のように2x2のテーブルを書いてみよう。これらのエントリは、実際のクラスと予測されたクラスに基づいて埋める。"
  },
  {
    "index": "F16247",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we have an example where the actual class is one and the predicted class is one then that's called an example that's a true positive, meaning our algorithm predicted that it's positive and in reality the example is positive.",
    "output": "もし我らが、実際のクラスが1で予測されたクラスが1の手本を持つ時には、それはtruepositive(真陽性)と呼ばれる手本である。その意味する所は、我らのアルゴリズムはそれが陽性だと予測して、しかも実際にも陽性の手本だった場合。"
  },
  {
    "index": "F16248",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If our learning algorithm predicted that something is negative, class zero, and the actual class is also class zero then that's what's called a true negative.",
    "output": "もし我らのアルゴリズムがある物を陰性、クラス0と予測して、そして実際のクラスもまたクラス0だった時には、それはtruenegative(真陰性)と呼ばれる。"
  },
  {
    "index": "F16249",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We predicted zero and it actually is zero.",
    "output": "我らは0と予測し、実際に0だ。"
  },
  {
    "index": "F16250",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To find the other two boxes, if our learning algorithm predicts that the class is one but the actual class is zero, then that's called a false positive.",
    "output": "その他の二つの箱は、我らの学習アルゴリズムがクラス1だと予測したが、実際のクラスが0の時には、これはfalsepositive(偽陽性)と呼ばれる。"
  },
  {
    "index": "F16251",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that means our algorithm for the patient is cancelled out in reality if the patient does not.",
    "output": "その意味する所は、我らのアルゴリズムは、ある患者がガンを持っていると予測しておきながら、実際には患者はガンを持っていなかった。"
  },
  {
    "index": "F16252",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, the last box is a zero, one.",
    "output": "そして最後の箱は、0、1。"
  },
  {
    "index": "F16253",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That's called a false negative because our algorithm predicted zero, but the actual class was one.",
    "output": "これはfalsenegative(偽陰性)と呼ぶ。何故なら我らのアルゴリズムは0を予測し、しかし実際のクラスは1だから。"
  },
  {
    "index": "F16254",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, we have this little sort of two by two table based on what was the actual class and what was the predicted class.",
    "output": "こうして、我らは小さなある種の2x2のテーブルを得た、それは実際のクラスと予測されたクラスに基づいた物だ。"
  },
  {
    "index": "F16255",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's a different way of evaluating the performance of our algorithm.",
    "output": "そしてここに、我らのアルゴリズムのパフォーマンスを評価する別の方法がある。"
  },
  {
    "index": "F16256",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to compute two numbers.",
    "output": "我らは二つの数を計算する事にする。"
  },
  {
    "index": "F16257",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first is called precision - and what that says is, of all the patients where we've predicted that they have cancer, what fraction of them actually have cancer?",
    "output": "最初の物は、Precision(精度)と呼ばれる物。それが伝える事は、我らがガンだと予測した全ての患者のうち、どれだけの割合の人が実際にガンだったのか?"
  },
  {
    "index": "F16258",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let me write this down, the precision of a classifier is the number of true positives divided by the number that we predicted as positive, right?",
    "output": "これを書き下してみよう。分類器のPrecisionとは、truepositiveの総数を陽性と予測した総数で割った物だ。"
  },
  {
    "index": "F16259",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So of all the patients that we went to those patients and we told them, \"We think you have cancer.\" Of all those patients, what fraction of them actually have cancer?",
    "output": "つまり、我らが実際に赴いて、「あなたはガンだと、我らは思ってる」と告げた患者のうち、それらの患者全員の中で、実際にガンを持ってる割合はどれだけか?"
  },
  {
    "index": "F16260",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's called precision.",
    "output": "それがPrecision(精度)と言われる物だ。"
  },
  {
    "index": "F16261",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And another way to write this would be true positives and then in the denominator is the number of predicted positives, and so that would be the sum of the, you know, entries in this first row of the table.",
    "output": "そしてこれの別の書き方としては、truepositiveとそして分母は、陽性と予測された総数、それはつまりテーブルの最初の行のエントリの和だ。"
  },
  {
    "index": "F16262",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to abbreviate positive as POS and then plus false positives, again abbreviating positive using POS.",
    "output": "つまりそれは、truepositiveを割る事のtruepositiveに...ここでpositiveをPOS、と略す事にしよう、そしてそこに足す事のfalsepositive、再びpositiveをPOSと省略して書く。"
  },
  {
    "index": "F16263",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's called precision, and as you can tell high precision would be good.",
    "output": "これがPrecision(精度)と呼ばれる物だ。見て分かるように、高いPrecisionは良い事だ。"
  },
  {
    "index": "F16264",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That means that all the patients that we went to and we said, \"You know, we're very sorry. We think you have cancer,\" high precision means that of that group of patients most of them we had actually made accurate predictions on them and they do have cancer.",
    "output": "その意味する所は、我らが赴き、「大変残念ですが、あなたはガンだと思います」と告げた患者全てに対して、高いPrecisionが意味する事は、その患者のグループの大多数が、我らが正しく予測を行った事になり、つまり実際にガンを持っている。"
  },
  {
    "index": "F16265",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The second number we're going to compute is called recall, and what recall say is, if all the patients in, let's say, in the test set or the cross-validation set, but if all the patients in the data set that actually have cancer, what fraction of them that we correctly detect as having cancer.",
    "output": "我らが計算する二番目の数値はRecall(再現率)と呼ばれる物で、Recallが言う事は、テストセットなりクロスバリデーションセットなり、とにかくデータセットのうち全てのガンの患者に対して、彼らのうちどれだけの割合を正しくガンだと、検出出来たか?"
  },
  {
    "index": "F16266",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if all the patients have cancer, how many of them did we actually go to them and you know, correctly told them that we think they need treatment.",
    "output": "という事。つまり、全ての患者がガンだったとして、それらのうち、何人の元に我らは実際におもむき、正しくも「あなた方には治療が必要だと我らは思っている」と告げる事が出来るだろうか?"
  },
  {
    "index": "F16267",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, writing this down, recall is defined as the number of positives, the number of true positives, meaning the number of people that have cancer and that we correctly predicted have cancer and we take that and divide that by, divide that by the number of actual positives, so this is the right number of actual positives of all the people that do have cancer.",
    "output": "つまり、これを書き下すと、Recall(再現率)は以下のように定義出来る:陽性の、、、truepositiveの総数、つまり、実際にガンを持ってる患者の中から、我らが正しくガンを持っていると予測出来た総数だが、それを割る事の、実際に陽性である患者の総数で割る。"
  },
  {
    "index": "F16268",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What fraction do we directly flag and you know, send the treatment.",
    "output": "どれだけの割合にフラグ立てして処置に送れるか?"
  },
  {
    "index": "F16269",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, to rewrite this in a different form, the denominator would be the number of actual positives as you know, is the sum of the entries in this first column over here.",
    "output": "つまり、これを違う形に書き直すと、分母は実際の陽性の数だから、つまりそれはここの最初の列のエントリの和だ。"
  },
  {
    "index": "F16270",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so writing things out differently, this is therefore, the number of true positives, divided by the number of true positives plus the number of false negatives.",
    "output": "つまり、別の書き方で書くと、これはつまり、truepositiveの総数をtruepositiveの総数足す事のfalsenegativeで割った物、という事になる。"
  },
  {
    "index": "F16271",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so once again, having a high recall would be a good thing.",
    "output": "そしてここでも、高いRecall(再現率)になるのは、良い事だ。"
  },
  {
    "index": "F16272",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So by computing precision and recall this will usually give us a better sense of how well our classifier is doing.",
    "output": "そしてPrecision(精度)とRecall(再現率)を計算する事により、我らは分類器がどれだけうまく振舞っているかについて、より良い感覚を得られる。"
  },
  {
    "index": "F16273",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular if we have a learning algorithm that predicts y equals zero all the time, if it predicts no one has cancer, then this classifier will have a recall equal to zero, because there won't be any true positives and so that's a quick way for us to recognize that, you know, a classifier that predicts y equals 0 all the time, just isn't a very good classifier.",
    "output": "そして例えば、いつもy=0を予測するような学習アルゴリズムだとすると、それが一回もガンだと予測しないとすると、その場合、この分類器はRecall(再現率)=0となる。何故ならそこには、truepositiveは全く無いから。"
  },
  {
    "index": "F16274",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And more generally, even for settings where we have very skewed classes, it's not possible for an algorithm to sort of \"cheat\" and somehow get a very high precision and a very high recall by doing some simple thing like predicting y equals 0 all the time or predicting y equals 1 all the time.",
    "output": "それはとても良い分類器、とは言えなかろう。そしてより一般的に言って、とてもスキューしたクラスのセッティングにおいても、アルゴリズムがある種のチート(ずる)をして、とても高いPrecision(精度)ととても高いRecall(再現率)をいつもy=0と予測する、というような何かしら簡単な方法で達成するのは不可能だ。"
  },
  {
    "index": "F16275",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so we're much more sure that a classifier of a high precision or high recall actually is a good classifier, and this gives us a more useful evaluation metric that is a more direct way to actually understand whether, you know, our algorithm may be doing well.",
    "output": "だから、高いPrecisionまたは高いRecallの分類器というのは、実際に良い分類器だという事により確信を持てる。そしてこれは我らに、より有用な評価指標で我らのアルゴリズムがうまい事やっているかをより直接的に理解させてくれるような物を、与えてくれる。"
  },
  {
    "index": "F16276",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So one final note in the definition of precision and recall, that we would define precision and recall, usually we use the convention that y is equal to 1, in the presence of the more rare class.",
    "output": "さて、Precision(精度)とRecall(再現率)の定義の最後のコメントとして、我らはPrecisionとRecallを普通はy=1が普通、より稀なクラスが存在している、という方をコンベンションを用いる。"
  },
  {
    "index": "F16277",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if we are trying to detect.",
    "output": "つまり我らが検出しようとするそのレアなクラスが存在する、という場合に基づいて。"
  },
  {
    "index": "F16278",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "rare conditions such as cancer, hopefully that's a rare condition, precision and recall are defined setting y equals 1, rather than y equals 0, to be sort of that the presence of that rare class that we're trying to detect.",
    "output": "そしてPrecision(精度)とRecall(再現率)を使う事で、起こる事というと、とてもスキューしたクラスの場合だとしても、アルゴリズムが「チート」(ずる)して、いかなる時もy=1を予測したり、あるいはいかなる時でもy=0を予測する事で、高いPrecision(精度)とかRecall(再現率)を得る事は不可能である、という事だ。"
  },
  {
    "index": "F16279",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by using precision and recall, we find, what happens is that even if we have very skewed classes, it's not possible for an algorithm to you know, \"cheat\" and predict y equals 1 all the time, or predict y equals 0 all the time, and get high precision and recall.",
    "output": "そして特に、もし分類器で高いPrecisionとRecallが得られたなら、我らはアルゴリズムがとてもうまく機能しているだろう事に実際にしっかりと確信が持てる、たとえスキューしたクラスだったとしても。"
  },
  {
    "index": "F16280",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, if a classifier is getting high precision and high recall, then we are actually confident that the algorithm has to be doing well, even if we have very skewed classes.",
    "output": "つまり、スキューしたクラスの問題に関しては、PrecisionとRecallは我らにアルゴリズムが実際にどうなってるか、についてのより直接的な洞察を与えてくれて、そしてこれはしばしば、我らの学習アルゴリズムを評価する、より良い方法だ。単に分類エラーや分類の正確さを見るだけに比べると。"
  },
  {
    "index": "F16281",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for the problem of skewed classes precision recall gives us more direct insight into how the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms, than looking at classification error or classification accuracy, when the classes are very skewed.",
    "output": "クラスがとてもスキューしている時には。"
  },
  {
    "index": "F16282",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, we talked about evaluation metrics.",
    "output": "前回のビデオで、我らは評価メトリクス(指標)に関して議論してきた。"
  },
  {
    "index": "F16283",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to switch tracks a bit and touch on another important aspect of machine learning system design, which will often come up, which is the issue of how much data to train on.",
    "output": "このビデオではトラックをちょっと変更して機械学習のシステムのデザインにおいてしばしば表面化するもう一つの重要な一面であるところの、どれだけのデータを試すか?"
  },
  {
    "index": "F16284",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, in some earlier videos, I had cautioned against blindly going out and just spending lots of time collecting lots of data, because it's only sometimes that that would actually help.",
    "output": "以前のビデオでは、盲目的に外に出て大量のデータを集めるという事を戒めてきた。何故ならそれは役に立つ場合と立たない場合があるからだ。"
  },
  {
    "index": "F16285",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But it turns out that under certain conditions, and I will say in this video what those conditions are, getting a lot of data and training on a certain type of learning algorithm, can be a very effective way to get a learning algorithm to do very good performance.",
    "output": "しかし、ある条件下では、そしてその条件が何なのかはこのビデオで伝えるが、ある種の学習アルゴリズムに対してなら大量のデータを得る事は、学習アルゴリズムにとても良いパフォーマンスで動かす為のとても効率的な方法たりえる。"
  },
  {
    "index": "F16286",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this arises often enough that if those conditions hold true for your problem and if you're able to get a lot of data, this could be a very good way to get a very high performance learning algorithm.",
    "output": "そしてこの事から、とてもしばしばあなたの問題においてこれらの条件を真のまま維持出来てそしてもしあなたが大量のデータを得る事が出来れば、これはとても高いパフォーマンスの学習アルゴリズムを得るとても良い方法になり得る。"
  },
  {
    "index": "F16287",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in this video, let's talk more about that.",
    "output": "だからこのビデオで、この事についてもっと議論していこう。"
  },
  {
    "index": "F16288",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me start with a story.",
    "output": "まずこんなストーリーから始めよう。"
  },
  {
    "index": "F16289",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Many, many years ago, two researchers that I know, Michelle Banko and Eric Broule ran the following fascinating study.",
    "output": "何年も前の事、二人の研究者、MichelloBankoとEricBrillは以下のような魅力的な研究を行った。"
  },
  {
    "index": "F16290",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "They were interested in studying the effect of using different learning algorithms versus trying them out on different training set sciences, they were considering the problem of classifying between confusable words, so for example, in the sentence: for breakfast I ate, should it be to, two or too?",
    "output": "彼らはややこしい単語間での分類問題を検討した。例えばこんな文の中では:ForbreakfastIateなら、入る単語はtwoだろうか?"
  },
  {
    "index": "F16291",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, for this example, for breakfast I ate two, 2 eggs.",
    "output": "この例の場合、ForbreakfastIateで「two」つまり「2」個の玉子を食べる。"
  },
  {
    "index": "F16292",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is one example of a set of confusable words and that's a different set.",
    "output": "つまり、これがややこしい単語の集合の一例で、これもまた別の集合の例だ。"
  },
  {
    "index": "F16293",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "They took a few different learning algorithms which were, you know, sort of considered state of the art back in the day, when they ran the study in 2001, so they took a variance, roughly a variance on logistic regression called the Perceptron.",
    "output": "そして彼らはこのような機械学習の問題、ある種の教師あり学習の問題であるところの、英語の文の中のある位置に来る単語はどの単語が適切かを分類しようとする問題に対して、彼らは幾つか異なる学習アルゴリズムを用いてーーそれらはその当時、彼らが研究を行った2001年だが、その当時最先端とみなされていた物達で、彼らはだいたいロジスティック回帰の変種のような物である所のPerceptronと、その当時は結構良く使われてたが昨今はあまり使われてないようなWinnowアルゴリズムと、これまたロジスティック回帰に類似した物たが多少違う物でたくさん使われていたが今はあまり使われていないMemory-Basedと呼ばれる学習アルゴリズムで、最近はあまり使われてないがあとでちょっとだけこれについては触れる。"
  },
  {
    "index": "F16294",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And they used a naive based algorithm, which is something they'll actually talk about in this course.",
    "output": "そしてナイーブなベイズアルゴリズム、これは彼らがこのコースで実際に語ってくれる。"
  },
  {
    "index": "F16295",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The exact algorithms of these details aren't important.",
    "output": "これらのアルゴリズムの正確な詳細は、重要では無い。"
  },
  {
    "index": "F16296",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Think of this as, you know, just picking four different classification algorithms and really the exact algorithms aren't important.",
    "output": "この事は、単に4つの異なる分類アルゴリズムを選んだ、と考えるべきだ。アルゴリズムが実際になんなのかは重要では無い。"
  },
  {
    "index": "F16297",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But what they did was they varied the training set size and tried out these learning algorithms on the range of training set sizes and that's the result they got.",
    "output": "彼らがやった事は、トレーニングセットのサイズを変えて、これらの学習アルゴリズムをそのトレーニングセットサイズの範囲のトレーニングセット群に対して試してみた。そしてその結果がこれだ。"
  },
  {
    "index": "F16298",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the trends are very clear right first most of these outer rooms give remarkably similar performance.",
    "output": "その傾向ははっきりしてる。まず、これらのアルゴリズムはほとんど、驚くほど似たようなパフォーマンスを与える。"
  },
  {
    "index": "F16299",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And second, as the training set size increases, on the horizontal axis is the training set size in millions go from you know a hundred thousand up to a thousand million that is a billion training examples.",
    "output": "二番目に、トレーニングセットのサイズが増加していくにつれて、ここで横軸が100万を単位としたトレーニングセットのサイズで、見ての通り10万から1000の100万、つまり10億のトレーニング手本までの範囲をとっている。"
  },
  {
    "index": "F16300",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The performance of the algorithms all pretty much monotonically increase and the fact that if you pick any algorithm may be pick a \"inferior algorithm\" but if you give that \"inferior algorithm\" more data, then from these examples, it looks like it will most likely beat even a \"superior algorithm\".",
    "output": "アルゴリズムのパフォーマンスは全てだいたい単調に増加していて、そしてあなたがどんなアルゴリズムを選ぼうと、たとえ、いわゆる「劣ったアルゴリズム」を選ぼうと、その「劣ったアルゴリズム」により多くのデータを与えさえすれば、これらの例から分かる事は、より「優れたアルゴリズム」を打ち負かすだろう。"
  },
  {
    "index": "F16301",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So since this original study which is very influential, there's been a range of many different studies showing similar results that show that many different learning algorithms you know tend to, can sometimes, depending on details, can give pretty similar ranges of performance, but what can really drive performance is you can give the algorithm a ton of training data.",
    "output": "つまり、多くの異なる学習アルゴリズムは時には詳細に依存する事もあるが、傾向としてはだいたい似たような範囲のパフォーマンスを示し、本当にパフォーマンスを先導するのはアルゴリズムに大量のトレーニングデータを与える事が出来るかどうかだ、という結果を示している。"
  },
  {
    "index": "F16302",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is, results like these has led to a saying in machine learning that often in machine learning it's not who has the best algorithm that wins, it's who has the most data So when is this true and when is this not true?",
    "output": "そこでこれらの結果は機械学習においてはこう言われる事になる:機械学習においてしばしば勝者になるのは、もっとも良いアルゴリズムを持つ者では無く、もっとも多くのデータを持つ者だ、と。ではそれが事実であるのはどういう時で、それが事実で無いのはどういう時だろう?"
  },
  {
    "index": "F16303",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's try to lay out a set of assumptions under which having a massive training set we think will be able to help.",
    "output": "では大量のトレーニングセットを持つ事が役に立ちそうと思えるような一連の条件を並べてみよう。"
  },
  {
    "index": "F16304",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's assume that in our machine learning problem, the features x have sufficient information with which we can use to predict y accurately.",
    "output": "我らの機械学習の問題においては、フィーチャーxがyを正確に予測するのに十分な情報を持っている事を仮定しよう。"
  },
  {
    "index": "F16305",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For example, if we take the confusable words all of them that we had on the previous slide.",
    "output": "例えば、もし前のスライドのややこしい単語の例の場合を考えるとすると、フィーチャーxが我らが埋めようとしている空白の回りの単語を捕捉しているとすると、つまりフィーチャーは「Forbreakfast,Ihave空白eggs」という文を捉えているとすると、その場合は、うん、私がこの間に欲しい単語はtwoである、と分かるだけのそしてtoでもtooでも無いと分かるだけの十分な情報がある。"
  },
  {
    "index": "F16306",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that it features x capture what are the surrounding words around the blank that we're trying to fill in.",
    "output": "だから、フィーチャーがこれらの回りの単語を捉えているなら、ラベルyが何なのか?という事をかなり曖昧さ無しで決める為に十分な情報を持っている。"
  },
  {
    "index": "F16307",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the features capture, you know, one of these surrounding words then that gives me enough information to pretty unambiguously decide what is the label y or in other words what is the word that I should be using to fill in that blank out of this set of three confusable words.",
    "output": "ラベルyが何かという事を言い換えると、これら三つのややこしい単語のどれを使って空白を埋めるべきか?という事だ。"
  },
  {
    "index": "F16308",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's an example what the future ex has sufficient information for specific y. For a counter example.",
    "output": "つまりこれは、フィーチャーxが特定のyについて十分な情報を持っている例だ。"
  },
  {
    "index": "F16309",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Consider a problem of predicting the price of a house from only the size of the house and from no other features.",
    "output": "これの反対の例としては、住宅の価格を予測する時に住宅のサイズだけで他のフィーチャーが何も無いような予測の問題を考えてみよう。"
  },
  {
    "index": "F16310",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you imagine I tell you that a house is, you know, 500 square feet but I don't give you any other features. I don't tell you that the house is in an expensive part of the city.",
    "output": "私があなたに住宅の価格は500平方フィートだ、とだけ伝えてそれ以外の情報を何も伝えなかったとする。"
  },
  {
    "index": "F16311",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or if I don't tell you that the house, the number of rooms in the house, or how nicely furnished the house is, or whether the house is new or old.",
    "output": "その住居が町の高い地区にあるとも言わないし、あるいは私はあなたにその住居の部屋の数も言わないし、あるいはどれくらい素晴らしい家具を備えているかも言わないし、その住居が新しいか古いかも言わない。"
  },
  {
    "index": "F16312",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I don't tell you anything other than that this is a 500 square foot house, well there's so many other factors that would affect the price of a house other than just the size of a house that if all you know is the size, it's actually very difficult to predict the price accurately.",
    "output": "もし私が、この家が500平方フィートという情報以外を何も言わなければ、住居の価格に影響を与える要因は住居のサイズ以外にもあまりにもたくさんあるので、あなたがサイズしか知らなければ、その価格を正確に予測するのはとても困難だ。"
  },
  {
    "index": "F16313",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that would be a counter example to this assumption that the features have sufficient information to predict the price to the desired level of accuracy.",
    "output": "だから以上は、この仮定、フィーチャーが望む水準の正確さで価格を推測するのに十分な情報を持っている、という仮定の反例と言えると思う。"
  },
  {
    "index": "F16314",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The way I think about testing this assumption, one way I often think about it is, how often I ask myself.",
    "output": "私がこの仮定をテストする方法は、私が良くやる方法の一つに、良く自分自身に問うのは、入力のフィーチャーxが与えられた時にこのフィーチャーが与えられた時、学習アルゴリズムと同様の情報が入手可能だった時に、仮にこのドメインの人間のエキスパートの元に赴いたとすると、実際に人間のエキスパートが予想をする事が、あるいは実際に人間のエキスパートが確信を持ってyの値を予想する事が出来るだろうか?"
  },
  {
    "index": "F16315",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For this first example if we go to, you know an expert human English speaker.",
    "output": "この最初の例の場合、人間の英語話者の専門家の所に行って、、、英語をうまく話せる人の所に行けば、英語の専門家の人間なら、単に読むだけで、あなたとか私みたいな多くの人々なら、ここに入るのが何であるのかを予測出来るだろう、英語の得意な話者なら、これをうまく予測出来る。"
  },
  {
    "index": "F16316",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You go to someone that speaks English well, right, then a human expert in English just read most people like you and me will probably we would probably be able to predict what word should go in here, to a good English speaker can predict this well, and so this gives me confidence that x allows us to predict y accurately, but in contrast if we go to an expert in human prices.",
    "output": "つまりこれで私は、xでyを正しく予測出来る、ということに、確信が持てる。"
  },
  {
    "index": "F16317",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Like maybe an expert realtor, right, someone who sells houses for a living.",
    "output": "だがこれに対して我らがもし住居の価格の専門家、例えば住宅を売る不動産屋とかとにかく住居を売る事で生計を立ててる人の所に行き、そしれ彼らに家のサイズを伝えて、そして彼らに価格は幾らか?"
  },
  {
    "index": "F16318",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I just tell them the size of a house and I tell them what the price is well even an expert in pricing or selling houses wouldn't be able to tell me and so this is fine that for the housing price example knowing only the size doesn't give me enough information to predict the price of the house.",
    "output": "と聞けば、たとえ住宅の価格や販売のエキスパートであっても、私に言う事は出来ないだろう、つまりこれは、住宅の価格の例で、サイズを知るだけでは住宅の価格を予測するのに十分な情報では無い、というサインである。"
  },
  {
    "index": "F16319",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's see then, when having a lot of data could help.",
    "output": "そこで、この前提を維持したままで、大量のデータを得る事が助けとなるかを見てみよう。"
  },
  {
    "index": "F16320",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose the features have enough information to predict the value of y.",
    "output": "yの値を予測するのに十分な情報を持つフィーチャーを得ていたとしよう。"
  },
  {
    "index": "F16321",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's suppose we use a learning algorithm with a large number of parameters so maybe logistic regression or linear regression with a large number of features.",
    "output": "そしてたくさんのパラメータの学習アルゴリズムを用いる事にしよう。それはロジスティック回帰かもしれないし、たくさんのフィーチャーの線形回帰かもしれない。"
  },
  {
    "index": "F16322",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or one thing that I sometimes do, one thing that I often do actually is using neural network with many hidden units.",
    "output": "あるいは、私が時々やる事として、、、私が実際に良くやる事としては、たくさんの隠れユニットを持ったニューラルネットワークを使う、というやり方もある。"
  },
  {
    "index": "F16323",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That would be another learning algorithm with a lot of parameters.",
    "output": "これもまたたくさんのパラメータを持つ学習アルゴリズムと言える。"
  },
  {
    "index": "F16324",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So these are all powerful learning algorithms with a lot of parameters that can fit very complex functions.",
    "output": "つまり、これらは全て、たくさんのパラメータを持つ強力なアルゴリズムであり、とても複雑な関数にフィッティング出来る。"
  },
  {
    "index": "F16325",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I'm going to call these, I'm going to think of these as low-bias algorithms because you know we can fit very complex functions and because we have a very powerful learning algorithm, they can fit very complex functions.",
    "output": "そこで私はこれらを、低バイアスのアルゴリズムと呼び、そうみなしていく事にする。何故ならとても複雑な関数にフィッティング出来るから。"
  },
  {
    "index": "F16326",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Chances are, if we run these algorithms on the data sets, it will be able to fit the training set well, and so hopefully the training error will be slow.",
    "output": "たぶん、これらのアルゴリズムをデータセットに対して実行すると、トレーニングセットに良くフィットするように出来るだろう。つまり、トレーニング誤差は小さくなる事が期待出来る。"
  },
  {
    "index": "F16327",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's say, we use a massive, massive training set, in that case, if we have a huge training set, then hopefully even though we have a lot of parameters but if the training set is sort of even much larger than the number of parameters then hopefully these albums will be unlikely to overfit.",
    "output": "ここで、大量の、本当に大量のトレーニングセットを用いる事にしよう。その場合、我らに巨大なトレーニングセットがあれば、たとえたくさんのパラメータがあっても、パラメータの数に対してでさえ十分に大量のトレーニングセットであれば、これらのアルゴリズムはオーバーフィットしそうには無い。"
  },
  {
    "index": "F16328",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right because we have such a massive training set and by unlikely to overfit what that means is that the training error will hopefully be close to the test error.",
    "output": "何故なら我らはそんなにも巨大なトレーニングセットを持っているのだから。そしてオーバーフィットしなさそう、という事はトレーニング誤差はテスト誤差と近い事が期待される、という事を意味する。"
  },
  {
    "index": "F16329",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally putting these two together that the train set error is small and the test set error is close to the training error what this two together imply is that hopefully the test set error will also be small.",
    "output": "最後に、これら二つをあわせると、トレーニングセット誤差は小さくて、テストセット誤差はトレーニング誤差と近くなる、これら二つをあわせると、テストセット誤差も小さくなるだろう事が期待される。"
  },
  {
    "index": "F16330",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Another way to think about this is that in order to have a high performance learning algorithm we want it not to have high bias and not to have high variance.",
    "output": "これのもう一つ別の考え方としては、高いパフォーマンスの学習アルゴリズムを得る為に、それが高バイアスでも高バリアンスでも無い事を望む。"
  },
  {
    "index": "F16331",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the bias problem we're going to address by making sure we have a learning algorithm with many parameters and so that gives us a low bias alorithm and by using a very large training set, this ensures that we don't have a variance problem here.",
    "output": "そこでバイアスの問題に対して、我らは学習アルゴリズムがたくさんのパラメータを持つ事で低バイアスのアルゴリズムとなるようにしつつ、一方でとても大量のトレーニングセットを用いる事で、これはバリアンスの問題が無い事を保証してくれる。"
  },
  {
    "index": "F16332",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully our algorithm will have no variance and so is by pulling these two together, that we end up with a low bias and a low variance learning algorithm and this allows us to do well on the test set.",
    "output": "つまり我らのアルゴリズムにバリアンスの問題が無い事が期待出来て、そしてこれら二つを合わせる事で、結局は低バイアス、低バリアンスの学習アルゴリズムとなる。そしてこれは、テストセットにおいてとても良く振舞ってくれる。"
  },
  {
    "index": "F16333",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And fundamentally it's a key ingredients of assuming that the features have enough information and we have a rich class of functions that's why it guarantees low bias, and then it having a massive training set that that's what guarantees more variance.",
    "output": "それは本質的には、鍵となる想定は、フィーチャーが十分な情報を持っている事、そしてリッチなクラスの関数である事で低バイアスである事を保証し、そして次に大量のトレーニングセットを持つ事で低バリアンスである事を保証する訳だ。つまり、これが我らに以下のような一連の条件を持った問題:より多くのデータを持ってたくさんのパラメータの学習アルゴリズムを訓練するような物。"
  },
  {
    "index": "F16334",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this gives us a set of conditions rather hopefully some understanding of what's the sort of problem where if you have a lot of data and you train a learning algorithm with lot of parameters, that might be a good way to give a high performance learning algorithm and really, I think the key test that I often ask myself are first, can a human experts look at the features x and confidently predict the value of y.",
    "output": "それは高いパフォーマンスの学習アルゴリズムを得る為の良い方法たりえる。そして実際に、私がキーだと思うテストとして、自分自身にも良く問う物としては、まず一つ目の問いは、人間のエキスパートがフィーチャーxを見た時に、yの値を確信を持って予測出来るか、という事。"
  },
  {
    "index": "F16335",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because that's sort of a certification that y can be predicted accurately from the features x and second, can we actually get a large training set, and train the learning algorithm with a lot of parameters in the training set and if you can't do both then that's more often give you a very kind performance learning algorithm.",
    "output": "何故ならそれは、フィーチャーxからyが正確に予測出来る、という保証となるからだ。そして二番目の問いは、我らは実際に大量のトレーニングセットを得る事が出来て、たくさんのパラメータの学習アルゴリズムをそのトレーニングセットでトレーニング出来るか?"
  },
  {
    "index": "F16336",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By now, you've seen a range of difference learning algorithms.",
    "output": "ここまでで、様々な学習アルゴリズムを見てきた。"
  },
  {
    "index": "F16337",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "With supervised learning, the performance of many supervised learning algorithms will be pretty similar, and what matters less often will be whether you use learning algorithm a or learning algorithm b, but what matters more will often be things like the amount of data you create these algorithms on, as well as your skill in applying these algorithms.",
    "output": "教師有り学習の中では、それぞれの学習アルゴリズム同士のパフォーマンスはとても似通っていて、学習アルゴリズムAを使うか学習アルゴリズムBを使うかの違いは重要で無い事が多い。"
  },
  {
    "index": "F16338",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Things like your choice of the features you design to give to the learning algorithms, and how you choose the colorization parameter, and things like that.",
    "output": "それよりも重要になる事が多いのはこれらのアルゴリズムを適用する対象のデータの量とか、または学習アルゴリズムを適用するスキル、例えば学習アルゴリズムに与えるフィーチャーの選択とか正規化パラメータをどう選ぶかとかそういった物の方が重要な事が多い。"
  },
  {
    "index": "F16339",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, there's one more algorithm that is very powerful and is very widely used both within industry and academia, and that's called the support vector machine.",
    "output": "だが、それでももう一つ、とても強力で、業界でもアカデミアでも良く使われているアルゴリズムがある。"
  },
  {
    "index": "F16340",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And compared to both logistic regression and neural networks, the Support Vector Machine, or SVM sometimes gives a cleaner, and sometimes more powerful way of learning complex non-linear functions.",
    "output": "それはサポートベクターマシーンと呼ばれていて、ロジスティック回帰やニューラルネットワークと比べて、サポートベクターマシーン、またの名をSVMは、複雑な非線形の関数を学習する方法として、場合によってはより明解で、よりパワフルな事がある。"
  },
  {
    "index": "F16341",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so let's take the next videos to talk about that.",
    "output": "だから次のビデオで、それについて話したい。"
  },
  {
    "index": "F16342",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Later in this course, I will do a quick survey of a range of different supervisory algorithms just as a very briefly describe them.",
    "output": "このコースの後半で、様々な教師有り学習アルゴリズムの簡単なサーベイを行い、とても簡潔にそれらを紹介するつもりだ。"
  },
  {
    "index": "F16343",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the support vector machine, given its popularity and how powerful it is, this will be the last of the supervisory algorithms that I'll spend a significant amount of time on in this course as with our development other learning algorithms, we're gonna start by talking about the optimization objective.",
    "output": "だがサポートベクターマシーンはその人気があまりにも大きいので、教師有り学習アルゴリズムの最後として、このコースの中のそれなりの時間を費やしたいと思う。ここまでの学習アルゴリズムの開発と同様、最適化の目的関数から始めたいと思う。"
  },
  {
    "index": "F16344",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's get started on this algorithm.",
    "output": "ではこのアルゴリズムを始めよう。"
  },
  {
    "index": "F16345",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to describe the support vector machine, I'm actually going to start with logistic regression, and show how we can modify it a bit, and get what is essentially the support vector machine.",
    "output": "サポートベクターマシーンを記述する為に、まずはロジスティック回帰から始めて、それをちょこっと変更して本質的にはサポートベクターマシーンが得られるやり方をお見せしたい。"
  },
  {
    "index": "F16346",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in logistic regression, we have our familiar form of the hypothesis there and the sigmoid activation function shown on the right.",
    "output": "ではロジスティック回帰において、今や見慣れた仮説の形がこれで、そしてsigmoidアクティベーション関数が右に示してある。"
  },
  {
    "index": "F16347",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in order to explain some of the math, I'm going to use z to denote theta transpose axiom.",
    "output": "そして数学をいくつか説明する為に、zをシータ転置のxを表すのに使う。"
  },
  {
    "index": "F16348",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's think about what we would like logistic regression to do.",
    "output": "では、ロジスティック回帰において我らが何をするかを見てみよう。"
  },
  {
    "index": "F16349",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we have an example with y equals one and by this I mean an example in either the training set or the test set or the cross-validation set, but when y is equal to one then we're sort of hoping that h of x will be close to one.",
    "output": "手本があって、y=1とする、これの意味は、トレーニングセットなりテストセットなりクロスバリデーションセットにおいて、y=1という事で、それはようするに、h(x)が1に近い事を期待する、という事を意味する。"
  },
  {
    "index": "F16350",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, we're hoping to correctly classify that example. And what having x subscript 1, what that means is that theta transpose x must be must larger than 0.",
    "output": "つまり、手本を正しく分類する事を望んでいて、h(x)が1に近いことは、シータ転置のxが0よりもずっと大きくなければならない事を意味する。"
  },
  {
    "index": "F16351",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there's greater than, greater than sign that means much, much greater than 0.",
    "output": "これは大なり大なりの記号で、0よりも、とってもとっても大きい事を意味する。"
  },
  {
    "index": "F16352",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's because it is z, the theta of transpose x is when z is much bigger than 0 is far to the right of the sphere.",
    "output": "そしてそれはとりもなさず、z、つまりシータ転置のxが0よりもずっと大きい時この図で遥か右に位置するという事で、ロジスティック回帰の出力は1に近くなるという事を意味する。"
  },
  {
    "index": "F16353",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That the outputs of logistic progression becomes close to one.",
    "output": "何故ならそれに対応する仮説の出力の値は0に近いから。"
  },
  {
    "index": "F16354",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Conversely, if we have an example where y is equal to zero, then what we're hoping for is that the hypothesis will output a value close to zero.",
    "output": "ここでロジスティック回帰のコスト関数を見てみると、見られる結果は各手本、x、yがこのような項として全体のコストに貢献している。"
  },
  {
    "index": "F16355",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that corresponds to theta transpose x of z being much less than zero because that corresponds to a hypothesis of putting a value close to zero.",
    "output": "つまりコスト関数の全体としては、普通はトレーニング手本全体に渡る和があり、さらに1/mの項もある。"
  },
  {
    "index": "F16356",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you look at the cost function of logistic regression, what you'll find is that each example (x,y) contributes a term like this to the overall cost function, right?",
    "output": "だがここのこの式は、これこそが、一つのトレーニング手本の寄与の項だ、ロジスティック回帰の目的関数全体への。"
  },
  {
    "index": "F16357",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for the overall cost function, we will also have a sum over all the chain examples and the 1 over m term, that this expression here, that's the term that a single training example contributes to the overall objective function so we can just rush them.",
    "output": "今、この仮説の定義の式を取り、ここに代入する。得られた物は、各トレーニング手本の寄与はこの項だ。"
  },
  {
    "index": "F16358",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now if I take the definition for the fall of my hypothesis and plug it in over here, then what I get is that each training example contributes this term, ignoring the one over M but it contributes that term to my overall cost function for logistic regression.",
    "output": "1/mは無視してるが、この項がロジスティック回帰の全体のコスト関数への寄与だ。"
  },
  {
    "index": "F16359",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's consider two cases of when y is equal to one and when y is equal to zero.",
    "output": "今、2つの場合を考えてみよう:y=1の時とy=0の時。"
  },
  {
    "index": "F16360",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the first case, let's suppose that y is equal to 1.",
    "output": "最初のケースとして、y=1の時を考えよう。"
  },
  {
    "index": "F16361",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case, only this first term in the objective matters, because this one minus y term would be equal to zero if y is equal to one.",
    "output": "この場合は、この目的関数の最初の項だけが重要だ、何故ならこの1-yの項は0になるから、y=1の時は。"
  },
  {
    "index": "F16362",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when y is equal to one, when in our example x comma y, when y is equal to 1 what we get is this term..",
    "output": "つまりy=1の時は手本x,yの、yが1の時には、我らが得るのはこの項、-logの1+eの-z乗分の一。"
  },
  {
    "index": "F16363",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Minus log one over one, plus E to the negative Z where as similar to the last line I'm using Z to denote data transposed X and of course in a cost I should have this minus line that we just had if Y is equal to one so that's equal to one I just simplify in a way in the expression that I have written down here.",
    "output": "ここで一つ前のスライドと同様、zをシータ転置xを表すのに使っている。もちろん、コストでは実際はこの-yがあるはずだが、今言ったように、y=1の場合だ。"
  },
  {
    "index": "F16364",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if we plot this function as a function of z, what you find is that you get this curve shown on the lower left of the slide.",
    "output": "そしてこの関数をzの関数として、プロットすると、こんな曲線がこの左下に描いたこの線が見られる。"
  },
  {
    "index": "F16365",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And thus, we also see that when z is equal to large, that is, when theta transpose x is large, that corresponds to a value of z that gives us a fairly small value, a very, very small contribution to the consumption.",
    "output": "こうして、zがとても大きい時はつまりシータ転置xが大きい場合は、とても小さい値、コスト関数にちょっとしか寄与しないzに対応する。"
  },
  {
    "index": "F16366",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this kinda explains why, when logistic regression sees a positive example, with y=1, it tries to set theta transport x to be very large because that corresponds to this term, in the cross function, being small.",
    "output": "これは、何故ロジスティック回帰において、陽性の手本でy=1を見たらシータ転置xにとても大きな値を入れたがる、ある種の説明になっている。何故ならそれは対応するコスト関数の中のこの項が、とても小さくなる事を意味するから。"
  },
  {
    "index": "F16367",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, to fill the support vec machine, here's what we're going to do.",
    "output": "ここで、サポートベクターマシンを構築する為に、これがやるべき事だ。"
  },
  {
    "index": "F16368",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're gonna take this cross function, this minus log 1 over 1 plus e to negative z, and modify it a little bit.",
    "output": "このコスト関数取って、この-logの1足すeの-z乗分の一を、ちょびっと変更する。"
  },
  {
    "index": "F16369",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me take this point 1 over here, and let me draw the cross functions you're going to use. The new pass functions can be flat from here on out, and then we draw something that grows as a straight line, similar to logistic regression.",
    "output": "この点、ここにある1を取り、今後使うコスト関数を書いてみよう、新しいコスト関数はここからフラットになり、そして成長の仕方は直線で描く、ロジスティック回帰に似ているが、しかしこれは直線。"
  },
  {
    "index": "F16370",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But this is going to be a straight line at this portion.",
    "output": "つまり、マゼンタで今描いた曲線。"
  },
  {
    "index": "F16371",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the curve that I just drew in magenta, and the curve I just drew purple and magenta, so if it's pretty close approximation to the cross function used by logistic regression.",
    "output": "紫というかマゼンダで描いた曲線。つまりこれは、ロジスティック回帰で使っていたコスト関数に極めて近い近似となっている。"
  },
  {
    "index": "F16372",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Except it is now made up of two line segments, there's this flat portion on the right, and then there's this straight line portion on the left.",
    "output": "2つの線分から構成されている所が違うが。右側にはこのフラットな部分があり、そしてこの左側には直線の部分がある。"
  },
  {
    "index": "F16373",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And don't worry too much about the slope of the straight line portion.",
    "output": "そして直線部分の傾きについてはあんま気にしないでくれ。"
  },
  {
    "index": "F16374",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But that's the new cost function we're going to use for when y is equal to one, and you can imagine it should do something pretty similar to logistic regression.",
    "output": "以上がy=1のときに使う事になる新しいコスト関数だ。そして想像できると思うがロジスティック回帰と極めて似た事をやっていく事になる。"
  },
  {
    "index": "F16375",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But turns out, that this will give the support vector machine computational advantages and give us, later on, an easier optimization problem that would be easier for software to solve.",
    "output": "だがやがて明らかになるが、これはサポートベクターマシンの計算的な優位である、より簡単な最適化問題をあとで与えてくれる事となる。"
  },
  {
    "index": "F16376",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We just talked about the case of y equals one.",
    "output": "ここまではy=1の場合だけを話してきた。"
  },
  {
    "index": "F16377",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The other case is if y is equal to zero. In that case, if you look at the cost, then only the second term will apply because the first term goes away, right?",
    "output": "もう一方のケース、y=0、この場合は、コスト関数を見てみると、この二番目の項だけが適用される、何故なら最初の項は、y=0の時は消え去るから。"
  },
  {
    "index": "F16378",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If y is equal to zero, then you have a zero here, so you're left only with the second term of the expression above.",
    "output": "つまりここは0となる。だから上の式で二番目の項だけが残る。"
  },
  {
    "index": "F16379",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the cost of an example, or the contribution of the cost function, is going to be given by this term over here.",
    "output": "だから手本のコスト、つまりコスト関数へと寄与はここの、この項で与えられる。"
  },
  {
    "index": "F16380",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you plot that as a function of z, to have pure z on the horizontal axis, you end up with this one.",
    "output": "そしてそれをzの関数としてプロットすると、、、だからここで横軸にzを取って、最終的にはこのカーブとなる。"
  },
  {
    "index": "F16381",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for the support vector machine, once again, we're going to replace this blue line with something similar and at the same time we replace it with a new cost, this flat out here, this 0 out here.",
    "output": "そしてサポートベクターマシンの為、ふたたびこの青い線を似たような物で置き換える。そして新しいコストでそれを置き換えると、ここは平坦となる。"
  },
  {
    "index": "F16382",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that then grows as a straight line, like so.",
    "output": "ここは0で、その後は直線で増加していく。こんな感じ。"
  },
  {
    "index": "F16383",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let me give these two functions names.",
    "output": "では、これら2つの関数に名前をつけよう。"
  },
  {
    "index": "F16384",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This function on the left I'm going to call cost subscript 1 of z, and this function of the right I'm gonna call cost subscript 0 of z.",
    "output": "この左の関数をcost下付き添字1のzと呼ぶ。そしてこの右側の関数を、cost下付き添字0のzと呼ぶ。"
  },
  {
    "index": "F16385",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the subscript just refers to the cost corresponding to when y is equal to 1, versus when y Is equal to zero.",
    "output": "ここで下付き添字は単にy=1に対応しているコストか、またはy=0に対応しているコストかを示しているに過ぎない。"
  },
  {
    "index": "F16386",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Armed with these definitions, we're now ready to build a support vector machine.",
    "output": "これらの定義で武装したので、サポートベクターマシンを構築する準備は整った!"
  },
  {
    "index": "F16387",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's the cost function, j of theta, that we have for logistic regression.",
    "output": "これはロジスティック回帰のコスト関数、Jのシータだ。"
  },
  {
    "index": "F16388",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case this equation looks a bit unfamiliar, it's because previously we had a minus sign outside, but here what I did was I instead moved the minus signs inside these expressions, so it just makes it look a little different.",
    "output": "この方程式がちょっと見慣れない、と感じたとしたら、それは前回はマイナスの符号を外に置いていた。だがここでは、マイナスの符号をこの式の中に移動した。"
  },
  {
    "index": "F16389",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For the support vector machine what we're going to do is essentially take this and replace this with cost1 of z, that is cost1 of theta transpose x.",
    "output": "サポートベクターマシンの為に我らがやる事は本質的にはここを、cost1のzで置き換える。"
  },
  {
    "index": "F16390",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we're going to take this and replace it with cost0 of z, that is cost0 of theta transpose x.",
    "output": "これはcost1のシータ転置x。そしてこれをcost0のzで置き換える。"
  },
  {
    "index": "F16391",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where the cost one function is what we had on the previous slide that looks like this.",
    "output": "これはcost0のシータ転置xで、ここでcost1関数は前のスライドで見た奴で、こんな感じで、cost0関数もまた、前のスライドで見た奴で、こんな感じの奴。"
  },
  {
    "index": "F16392",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what we have for the support vector machine is a minimization problem of one over M, the sum of Y I times cost one, theta transpose X I, plus one minus Y I times cause zero of theta transpose X I, and then plus my usual regularization parameter.",
    "output": "サポートベクターマシンにおいて、我らがやるのは、1/mの和をトレーニング手本に渡って取ることの、y(i)掛けるcost1のシータ転置x(i)足すことの1-y(i)掛けるcost0のシータ転置x(i)。そしてさらに、いつもの正規化パラメータ、こんな感じ。"
  },
  {
    "index": "F16393",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, by convention, for the support of vector machine, we're actually write things slightly different.",
    "output": "ここでサポートベクターマシーンの慣例により、実際にはちょっと違った書き方をする。"
  },
  {
    "index": "F16394",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First, we're going to get rid of the 1 over m terms, and this just this happens to be a slightly different convention that people use for support vector machines compared to or just a progression.",
    "output": "まず、1/mの項を取り除く。これは単にこれは単にロジスティック回帰とはちょっとだけ異なるコンベンションをサポートベクターマシンでは人々が偶然使っていた、というだけ。"
  },
  {
    "index": "F16395",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But here's what I mean.",
    "output": "それはこういう事だ。"
  },
  {
    "index": "F16396",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You're one way to do this, we're just gonna get rid of these one over m terms and this should give you me the same optimal value of beta right?",
    "output": "つまり単純に1/mの項を取り除く。これは最適なシータの値には違いをうまないはず。"
  },
  {
    "index": "F16397",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because one over m is just as constant so whether I solved this minimization problem with one over n in front or not.",
    "output": "何故なら1/mは単に定数だから。"
  },
  {
    "index": "F16398",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I should end up with the same optimal value for theta.",
    "output": "だからこの最小化問題を前に1/mを置いて解こうが置かないで解こうが得られる結果は同じ最適なシータの値となる。"
  },
  {
    "index": "F16399",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's what I mean, to give you an example, suppose I had a minimization problem.",
    "output": "それはこういう事だ。具体例を見よう。"
  },
  {
    "index": "F16400",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Minimize over a long number U of U minus five squared plus one.",
    "output": "実数のuを、(u-5)の二乗+1を最小化するように選ぶ。"
  },
  {
    "index": "F16401",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, the minimum of this happens to be U equals five.",
    "output": "この場合、最小になるのはこれが最小になるのはu=5の時だ。"
  },
  {
    "index": "F16402",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now if I were to take this objective function and multiply it by 10.",
    "output": "今、この目的関数に対し、これを10掛けるとするとこの場合、最小化問題は10掛ける(u-5)の二乗足すことの10を最小にするuだ。"
  },
  {
    "index": "F16403",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well the value of U that minimizes this is still U equals five right?",
    "output": "これを最小にするuはu=5のままだ。"
  },
  {
    "index": "F16404",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So multiply something that you're minimizing over, by some constant, 10 in this case, it does not change the value of U that gives us, that minimizes this function.",
    "output": "つまり、最小化したい物に、何か定数を掛けても、この場合は10を掛けた訳だが、その事はこの関数を最小にするuの値を、変える事は無い。"
  },
  {
    "index": "F16405",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the same way, what I've done is by crossing out the M is all I'm doing is multiplying my objective function by some constant M and it doesn't change the value of theta.",
    "output": "つまり同様に、このmを取り除く為にやったのは、私がやったのは、目的関数にある定数、mを掛けただけだ。だからそれは、最小になるシータを変化させる事は無い。"
  },
  {
    "index": "F16406",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The second bit of notational change, which is just, again, the more standard convention when using SVMs instead of logistic regression, is the following.",
    "output": "2つ目のちょっとしたノーテーションの変更はロジスティック回帰の代わりにSVMを使う時に、もっとも一般的なコンベンションだが、それは以下のような物だ。"
  },
  {
    "index": "F16407",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for logistic regression, we add two terms to the objective function.",
    "output": "ロジスティック回帰の時は、目的関数に2つの項があった。"
  },
  {
    "index": "F16408",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first is this term, which is the cost that comes from the training set and the second is this row, which is the regularization term.",
    "output": "一つ目の項はトレーニングセットから来るコストだった。二番目のこの項は、正規化の項。"
  },
  {
    "index": "F16409",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we had was we had a, we control the trade-off between these by saying, what we want is A plus, and then my regularization parameter lambda.",
    "output": "そして我らがやらなくてはいけなかったのは、これらの間のトレードオフを制御する事だった。つまり、最小化したいのはA足すことの、正規化のパラメータ、ラムダに掛けるなんかの項、Bだ。"
  },
  {
    "index": "F16410",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then times some other term B, where I guess I'm using your A to denote this first term, and I'm using B to denote the second term, maybe without the lambda.",
    "output": "ここでAをこの最初の項を指すのに使い、そしてBをこの二番目の項を指すのに使う。ラムダは抜きで。"
  },
  {
    "index": "F16411",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And instead of prioritizing this as A plus lambda B, and so what we did was by setting different values for this regularization parameter lambda, we could trade off the relative weight between how much we wanted the training set well, that is, minimizing A, versus how much we care about keeping the values of the parameter small, so that will be, the parameter is B for the support vector machine, just by convention, we're going to use a different parameter.",
    "output": "そしてこのAとBの優先度を考える代わりに、我らがやったのは、この正規化のパラメータ、ラムダに異なる値をセットしていったのだった。我らは相対的にどれだけトレーニングセットに良くフィットさせるか、つまりどれだけAを最小化するかと、とれだけパラメータの値を小さく保つ事を気にするか、それがパラメータBだが、それらの間のトレードオフを取る。"
  },
  {
    "index": "F16412",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So instead of using lambda here to control the relative waiting between the first and second terms.",
    "output": "一番目の項と二番目の項の間の重みをコントロールする為にラムダを使う代わりに、そこでもまだパラメータを使う事になるのだが、それはコンベンションでCと呼ばれる。"
  },
  {
    "index": "F16413",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're instead going to use a different parameter which by convention is called C and is set to minimize C times a + B.",
    "output": "そして、代わりにC掛けるA足すBを最小化する。"
  },
  {
    "index": "F16414",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for logistic regression, if we set a very large value of lambda, that means you will give B a very high weight.",
    "output": "ロジスティック回帰の時はとても大きなラムダの値を用いると、それの意味する所はBにとても大きな重みを付与する、という事だ。"
  },
  {
    "index": "F16415",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here is that if we set C to be a very small value, then that responds to giving B a much larger rate than C, than A.",
    "output": "今回は、もしCにとても小さな値をセットすると、それがBに、Aとくらべてとても大きな重みを付与する事になる。"
  },
  {
    "index": "F16416",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is just a different way of controlling the trade off, it's just a different way of prioritizing how much we care about optimizing the first term, versus how much we care about optimizing the second term.",
    "output": "つまり、これはトレードオフをコントロールする単なる別のやり方、またはどれだけ最初の項を最適化するか、vsどれだけ二番目の項の最適化を重視するか、をパラメトライズする異なるやり方に過ぎない。"
  },
  {
    "index": "F16417",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you want you can think of this as the parameter C playing a role similar to 1 over lambda.",
    "output": "必要に応じてこれをパラメータCが、1/ラムダと似たよう役割の物とみなすことができる。"
  },
  {
    "index": "F16418",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This equals 1 over lambda, that's not the case.",
    "output": "それはこれら2つの方程式、2つの式が同じになるという訳では無く、C=1/ラムダというのはそういう場合という訳では無く、もしCが1/ラムダと等しいとこれら2つの最適化の目的関数は同じ結果の値を与える、という事。"
  },
  {
    "index": "F16419",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's rather that if C is equal to 1 over lambda, then these two optimization objectives should give you the same value the same optimal value for theta so we just filling that in I'm gonna cross out lambda here and write in the constant C there.",
    "output": "それを踏まえると、ラムダを消して、定数Cをここに書く。"
  },
  {
    "index": "F16420",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that gives us our overall optimization objective function for the support vector machine.",
    "output": "以上の操作で、サポートベクターマシンの全体の最適化の目的関数が得られる。"
  },
  {
    "index": "F16421",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you minimize that function, then what you have is the parameters learned by the SVM.",
    "output": "そしてこの関数を最適化すれば、SVMにより学習したパラメータを得られる。"
  },
  {
    "index": "F16422",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally unlike logistic regression, the support vector machine doesn't output the probability is that what we have is we have this cost function, that we minimize to get the parameter's data, and what a support vector machine does is it just makes a prediction of y being equal to one or zero, directly.",
    "output": "最後に、ロジスティック回帰と異なり、サポートベクターマシンは確率を出力する訳では無い。その代わりに、我らが得るのはこのコスト関数を最小化するパラメータ、シータで、サポートベクターマシンがやるのは、yが1か0かの予言を直接行う。"
  },
  {
    "index": "F16423",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the hypothesis will predict one if theta transpose x is greater or equal to zero, and it will predict zero otherwise and so having learned the parameters theta, this is the form of the hypothesis for the support vector machine.",
    "output": "つまり、仮説は、シータ転置のxが0より大きければ1を予言し、それ以外なら0を予言する。つまり、学習したパラメータのシータを得た後は、これがサポートベクターマシンの仮説の形だ。"
  },
  {
    "index": "F16424",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that was a mathematical definition of what a support vector machine does.",
    "output": "以上が、サポートベクターマシンが何をするかの数学的な定義だ。"
  },
  {
    "index": "F16425",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos, let's try to get back to intuition about what this optimization objective leads to and whether the source of the hypotheses SVM will learn and we'll also talk about how to modify this just a little bit to the complex nonlinear functions.",
    "output": "次に続く幾つかのビデオて、この最適化の目的関数が何を意味するかを直感的に把握する事を目指します。そしてSVMが学習する仮説の元とどう修正したら、より複雑な、非線形の関数が学習出来るかも話します。"
  },
  {
    "index": "F16426",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Sometimes people talk about support vector machines, as large margin classifiers, in this video I'd like to tell you what that means, and this will also give us a useful picture of what an SVM hypothesis may look like.",
    "output": "ときどき皆さんはSVMを大きなマージンの分類器だといいますが、今回、みなさんにはこの意味と、皆さんに役立つであろうSVMの仮定がどんなものかという全体像についてお話します。これがサポートベクターマシンでのコスト関数です。"
  },
  {
    "index": "F16427",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's my cost function for the support vector machine where here on the left I've plotted my cost 1 of z function that I used for positive examples and on the right I've plotted my zero of 'Z' function, where I have 'Z' here on the horizontal axis.",
    "output": "ここで左にcost1(z)の関数をプロットした、これは陽性の手本に対して使う物だ。そして右にはcost0(z)をプロットした、ここでzは横軸に取った。"
  },
  {
    "index": "F16428",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, let's think about what it takes to make these cost functions small.",
    "output": "ここで、これらのコスト関数を小さくするとどうなるかを考えてみよう。"
  },
  {
    "index": "F16429",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have a positive example, so if y is equal to 1, then cost 1 of Z is zero only when Z is greater than or equal to 1.",
    "output": "もし陽性の手本があったとして、つまりy=1の時、cost1のzはzが1以上の時にだけ0となる。"
  },
  {
    "index": "F16430",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in other words, if you have a positive example, we really want theta transpose x to be greater than or equal to 1 and conversely if y is equal to zero, look this cost zero of z function, then it's only in this region where z is less than equal to 1 we have the cost is zero as z is equals to zero, and this is an interesting property of the support vector machine right, which is that, if you have a positive example so if y is equal to one, then all we really need is that theta transpose x is greater than equal to zero.",
    "output": "言い換えると、陽性の手本に対しては、シータ転置xが1以上であって欲しい、という事。そして反対に、もしy=0の時はこのcost0(z)関数を見ると、この領域の時だけつまりzが1以下の時だけ(訳注:-1の間違いか)cost0(z)はゼロとなる。"
  },
  {
    "index": "F16431",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that would mean that we classify correctly because if theta transpose x is greater than zero our hypothesis will predict zero.",
    "output": "もし陽性の手本、つまりy=1の時は本当に必要なのはシータ転置xが0以上であれば十分なはずだ。"
  },
  {
    "index": "F16432",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, if you have a negative example, then really all you want is that theta transpose x is less than zero and that will make sure we got the example right.",
    "output": "だってシータ転置xが0より大きければ仮説は0を予言するんだから(訳注:1の間違いか)同様に、陰性の手本があったらあなたが望むのはただシータ転置xがゼロより小さければ本当は良くて、それだけで手本で正解出来ている。"
  },
  {
    "index": "F16433",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the support vector machine wants a bit more than that.",
    "output": "だが、サポートベクターマシンは、それよりももうちょっと多くを要求する。"
  },
  {
    "index": "F16434",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It says, you know, don't just barely get the example right.",
    "output": "それはぎりぎり手本が正しければ良いだけにとどまらず、つまり単にゼロよりちょっとでも大きければ良いというのではなく、それが要求するのは、ゼロよりもかなり大きいという事。"
  },
  {
    "index": "F16435",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So then don't just have it just a little bit bigger than zero.",
    "output": "1よりもちょっと大きい、と言っている。"
  },
  {
    "index": "F16436",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What i really want is for this to be quite a lot bigger than zero say maybe bit greater or equal to one and I want this to be much less than zero.",
    "output": "そしてこれは0よりもずっと小さくしたい。"
  },
  {
    "index": "F16437",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe I want it less than or equal to -1.",
    "output": "たとえば-1以下とかにしたい。"
  },
  {
    "index": "F16438",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this builds in an extra safety factor or safety margin factor into the support vector machine.",
    "output": "つまりこれは、追加のセーフティーファクターをまたはセーフティーマージンをサポートベクターマシンに組み込むと言える。"
  },
  {
    "index": "F16439",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Logistic regression does something similar too of course, but let's see what happens or let's see what the consequences of this are, in the context of the support vector machine.",
    "output": "ロジスティック回帰ももちろん似たような事をしていたが、何が起こるか見てみよう。またはサポートベクターマシンの文脈ではこれはとういう結果になるのかを見てみよう。"
  },
  {
    "index": "F16440",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, what I'd like to do next is consider a case case where we set this constant C to be a very large value, so let's imagine we set C to a very large value, may be a hundred thousand, some huge number.",
    "output": "具体的には、次に私がやりたいのは、この定数Cにとても大きな値をセットしてみたい、というもの。ではCにとてもおおきな値、例えば何十万もの値、なんらかの巨大な値をセットしのを想像してみよう。"
  },
  {
    "index": "F16441",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's see what the support vector machine will do.",
    "output": "サポートベクターマシンが何をするか、見てみよう。"
  },
  {
    "index": "F16442",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If C is very, very large, then when minimizing this optimization objective, we're going to be highly motivated to choose a value, so that this first term is equal to zero.",
    "output": "もしCがとっても、とっても大きいと、その場合はこの最適化の目的関数を最小化する時に、値を選ぶ時にこの項がゼロになるように、とても高く動機づけされる。"
  },
  {
    "index": "F16443",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's try to understand the optimization problem in the context of, what would it take to make this first term in the objective equal to zero, because you know, maybe we'll set C to some huge constant, and this will hope, this should give us additional intuition about what sort of hypotheses a support vector machine learns.",
    "output": "では目的関数のこの最初の項を0にしよう、というコンテキストで最適化問題はどうなるかを理解していこう。その理由は、我らはCをとても大きな定数に設定すると言った。"
  },
  {
    "index": "F16444",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we saw already that whenever you have a training example with a label of y=1 if you want to make that first term zero, what you need is is to find a value of theta so that theta transpose x i is greater than or equal to 1.",
    "output": "これが、サポートベクターマシンの仮説がどんな感じか、さらなる直感を与えてくれる事を期待している。既に見たように、トレーニング手本のラベルy=1の時はいつでも最初の項を0にしたいならやるべき事はシータ転置xが1以上になるようなシータを探すという事。"
  },
  {
    "index": "F16445",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, whenever we have an example, with label zero, in order to make sure that the cost, cost zero of Z, in order to make sure that cost is zero we need that theta transpose x i is less than or equal to -1.",
    "output": "同様に、ラベル0の手本の時はいつでもcost0が、、、cost0(z)がそのコストが0に確実になるには、シータ転置xを-1以下にしなくてはならない。つまり、我らの最適化問題を実際にパラメータを選んでこの最初の項をゼロとしたら、その後に残るのは以下の最適化問題だ。"
  },
  {
    "index": "F16446",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if we think of our optimization problem as now, really choosing parameters and show that this first term is equal to zero, what we're left with is the following optimization problem.",
    "output": "我らが最小化するのは、最初の項が0なので、C掛ける0だ。"
  },
  {
    "index": "F16447",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to minimize that first term zero, so C times zero, because we're going to choose parameters so that's equal to zero, plus one half and then you know that second term and this first term is 'C' times zero, so let's just cross that out because I know that's going to be zero.",
    "output": "何故ならそれが0になるようにパラメータを選ぶのだから。"
  },
  {
    "index": "F16448",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this will be subject to the constraint that theta transpose x(i) is greater than or equal to one, if y(i) Is equal to one and theta transpose x(i) is less than or equal to minus one whenever you have a negative example and it turns out that when you solve this optimization problem, when you minimize this as a function of the parameters theta you get a very interesting decision boundary.",
    "output": "そしてこれは、シータ転置x(i)が1以上という制約条件に従う。"
  },
  {
    "index": "F16449",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, if you look at a data set like this with positive and negative examples, this data is linearly separable and by that, I mean that there exists, you know, a straight line, altough there is many a different straight lines, they can separate the positive and negative examples perfectly.",
    "output": "陰性の手本の時にはいつでも。"
  },
  {
    "index": "F16450",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For example, here is one decision boundary that separates the positive and negative examples, but somehow that doesn't look like a very natural one, right?",
    "output": "そして結局、この最適化問題を解くと、パラメータシータの関数としてこれを最小化すると、とても興味深い決定境界が得られる。"
  },
  {
    "index": "F16451",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or by drawing an even worse one, you know here's another decision boundary that separates the positive and negative examples but just barely.",
    "output": "具体的に、このようなデータセットを見た時、そこには陽性と陰性のサンプルがある訳だが、このデータは線形で分離可能。"
  },
  {
    "index": "F16452",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But neither of those seem like particularly good choices.",
    "output": "それの意味するところは、ある直線ーーたくさんの異なる直線が有り得るが、それらが陽性と陰性のサンプルを完璧に分離する、という事。"
  },
  {
    "index": "F16453",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The Support Vector Machines will instead choose this decision boundary, which I'm drawing in black.",
    "output": "サポートベクターマシンはそうではなく、この決定境界を選ぶ、この黒で描いた奴。"
  },
  {
    "index": "F16454",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that seems like a much better decision boundary than either of the ones that I drew in magenta or in green.",
    "output": "そしてそれは、マゼンタや緑で描いた決定境界のどちらよりも、ずっとマシっぽい。"
  },
  {
    "index": "F16455",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The black line seems like a more robust separator, it does a better job of separating the positive and negative examples.",
    "output": "黒い線の方が、よりロバストな分離器に見える。こっちの方が陽性と陰性のサンプルを分けるという仕事をうまくこなしてる。"
  },
  {
    "index": "F16456",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And mathematically, what that does is, this black decision boundary has a larger distance.",
    "output": "数学的には、それの意味する所はこの黒い決定境界の方が大きな距離を持っているという事。"
  },
  {
    "index": "F16457",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That distance is called the margin, when I draw up this two extra blue lines, we see that the black decision boundary has some larger minimum distance from any of my training examples, whereas the magenta and the green lines they come awfully close to the training examples.",
    "output": "この2つの追加の線を描いてみると分かるように、黒の決定境界は、トレーニング手本の中のサンプルへの最小距離がより大きい。他方マゼンタや緑の線はトレーニング手本に恐ろしいほど近い。"
  },
  {
    "index": "F16458",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "and then that seems to do a less a good job separating the positive and negative classes than my black line.",
    "output": "だからそちらの方が、陽性と陰性を分離するには黒の線に比べるといまいちな仕事しかしていない感じがする。"
  },
  {
    "index": "F16459",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this distance is called the margin of the support vector machine and this gives the SVM a certain robustness, because it tries to separate the data with as a large a margin as possible.",
    "output": "そしてこれがSVMに、ある程度のロバストさを与えている。何故ならそれは、データを出来るだけ大きなマージンになるように分離しようとするから。"
  },
  {
    "index": "F16460",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the support vector machine is sometimes also called a large margin classifier and this is actually a consequence of the optimization problem we wrote down on the previous slide.",
    "output": "だからサポートベクターマシンはたまに大きなマージン分類器とも呼ばれている。そしてこれは実は、前のスライドで書いた最適化問題の帰結だ。"
  },
  {
    "index": "F16461",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I know that you might be wondering how is it that the optimization problem I wrote down in the previous slide, how does that lead to this large margin classifier.",
    "output": "おっと分かってるって。前のスライドに書いた最適化問題がどうなってこの大きなマージン分類器になってるかって思ってるんでしょ?"
  },
  {
    "index": "F16462",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I know I haven't explained that yet.",
    "output": "それをまだ説明してないってのは分かってるよ。"
  },
  {
    "index": "F16463",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in the next video I'm going to sketch a little bit of the intuition about why that optimization problem gives us this large margin classifier.",
    "output": "それは次のビデオでさっきの最適化問題がなんで大きなマージン分類器になるのかの、ちょっとした直感の説明をするよ。"
  },
  {
    "index": "F16464",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But this is a useful feature to keep in mind if you are trying to understand what are the sorts of hypothesis that an SVM will choose.",
    "output": "だがこれは、SVMはどんな仮説を選ぶのか?を理解したければ、心にとめておく価値のある特徴だ。"
  },
  {
    "index": "F16465",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, trying to separate the positive and negative examples with as big a margin as possible.",
    "output": "つまり、陽性と陰性のサンプルを、できるだけ大きなマージンになるように分離しようとする、という事。"
  },
  {
    "index": "F16466",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I want to say one last thing about large margin classifiers in this intuition, so we wrote out this large margin classification setting in the case of when C, that regularization concept, was very large, I think I set that to a hundred thousand or something.",
    "output": "この直感的な考え方だと、我らが書き下した大きなマージン分類器は、C、つまり正規化の定数が凄く大きな場合の話だった。たぶん何十万とかその辺をセットした気がする。"
  },
  {
    "index": "F16467",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So given a dataset like this, maybe we'll choose that decision boundary that separate the positive and negative examples on large margin.",
    "output": "こんなデータセットが与えられたら、陽性と陰性のサンプルを大きなマージンになるようの分離する決定境界選ぶかもしれない。"
  },
  {
    "index": "F16468",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, the SVM is actually sligthly more sophisticated than this large margin view might suggest.",
    "output": "SVMは実際はこの大きなマージンという見方から考えられる物よりはもうちょっと洗練されている。"
  },
  {
    "index": "F16469",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, if all you're doing is use a large margin classifier then your learning algorithms can be sensitive to outliers, so lets just add an extra positive example like that shown on the screen.",
    "output": "とりわけ、大きなマージンという特徴だけの分類器を使っている場合は、ハズレ値にその学習アルゴリズムは、より敏感となる。つまり、画面に示したような陽性のサンプルを追加してみましょう。"
  },
  {
    "index": "F16470",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If he had one example then it seems as if to separate data with a large margin, maybe I'll end up learning a decision boundary like that, right?",
    "output": "もしサンプルを一つ追加したら、データを大きなマージンで分離しようとすると、こんな決定境界を学習する事になる。"
  },
  {
    "index": "F16471",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "that is the magenta line and it's really not clear that based on the single outlier based on a single example and it's really not clear that it's actually a good idea to change my decision boundary from the black one over to the magenta one.",
    "output": "このマゼンタの線。でも一つの外れ値、一つのサンプルに基づいて、決定境界を黒の物からマゼンダの物へと変更するのが本当に良い事なのかは結構怪しい。"
  },
  {
    "index": "F16472",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if C, if the regularization parameter C were very large, then this is actually what SVM will do, it will change the decision boundary from the black to the magenta one but if C were reasonably small if you were to use the C, not too large then you still end up with this black decision boundary.",
    "output": "だからもしCが、、、正規化パラメータのCがとても大きければ、その場合はこれがSVMが実際に行う事となる。それは決定境界を黒の物からマゼンタの物へ変更する。"
  },
  {
    "index": "F16473",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And of course if the data were not linearly separable so if you had some positive examples in here, or if you had some negative examples in here then the SVM will also do the right thing.",
    "output": "そして、もちろん、もしデータが直線で分離出来ない場合、例えばある陽性のサンプルがここにある場合とか、または陰性のサンプルがここにある場合とか、そういう場合もSVMは正しい事をする。"
  },
  {
    "index": "F16474",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this picture of a large margin classifier that's really, that's really the picture that gives better intuition only for the case of when the regulations parameter C is very large, and just to remind you this corresponds C plays a role similar to one over Lambda, where Lambda is the regularization parameter we had previously.",
    "output": "そしてこの大きなマージン分類器の絵は正規化パラメータCがとても大きい場合にだけ正しい直感を与えてくれる絵だ。そして繰り返すと、この、Cは1/ラムダに対応していて、このラムダは以前にあった正規化パラメータだ。"
  },
  {
    "index": "F16475",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so it's only of one over Lambda is very large or equivalently if Lambda is very small that you end up with things like this Magenta decision boundary, but in practice when applying support vector machines, when C is not very very large like that, it can do a better job ignoring the few outliers like here.",
    "output": "つまり、この絵は1/ラムダがとても大きい時、つまりラムダがとても小さい時にだけこのマゼンタの決定境界を得る結果となる。"
  },
  {
    "index": "F16476",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And also do fine and do reasonable things even if your data is not linearly separable.",
    "output": "でも実際にサポートベクターマシンを適用する時はCはそんなに凄く凄く大きい値をセットしたりはしないので、このようなちょっとの外れ値を無視するにはもっと良い仕事をしてくれます。"
  },
  {
    "index": "F16477",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But when we talk about bias and variance in the context of support vector machines which will do a little bit later, hopefully all of of this trade-offs involving the regularization parameter will become clearer at that time.",
    "output": "だがサポートベクターマシンの文脈でバイアスと分散の話をする時に、それはちょっと後でやる予定ですが、そのあかつきには、正規化パラメータにまつわるトレードオフはもっとクリアになってるといいな。"
  },
  {
    "index": "F16478",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I hope that gives some intuition about how this support vector machine functions as a large margin classifier that tries to separate the data with a large margin, technically this picture of this view is true only when the parameter C is very large, which is a useful way to think about support vector machines.",
    "output": "以上がサポートベクターマシンの関数が大きなマージン分類器として与えられたデータを大きなマージンでどのように分離しようとするかについて、ある程度の直感を与えてくれるといいな。技術的にはこの見方はパラメータのCがとても大きな時にしか成り立たないけど、その考え方はサポートベクターマシンを考える上でとても有用な考え方だ。"
  },
  {
    "index": "F16479",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There was one missing step in this video which is, why is it that the optimization problem we wrote down on these slides, how does that actually lead to the large margin classifier, I didn't do that in this video, in the next video I will sketch a little bit more of the math behind that to explain that separate reasoning of how the optimization problem we wrote out results in a large margin classifier.",
    "output": "このビデオではそこはやってない。次のビデオでは、背後にある数学をもう少しスケッチしてみる事で我らの書き下した最適化の問題がどうやって大きなマージン分類器となっているかを説明したいと思います。"
  },
  {
    "index": "F16480",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to tell you a bit about the math behind large margin classification.",
    "output": "このビデオでは大きなマージンによる分類の背後にある数学について、ちょっと話しておきたい。"
  },
  {
    "index": "F16481",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This video is optional, so please feel free to skip it.",
    "output": "このビデオはオプショナルです。だからどうぞご自由にスキップしちゃって下さい。"
  },
  {
    "index": "F16482",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It may also give you better intuition about how the optimization problem of the support vex machine, how that leads to large margin classifiers.",
    "output": "このビデオでは、また、サポートベクターマシンの最適化問題が、どのように大きなマージン分類器を導くのかについてのより良い直感も与えたいと思う。"
  },
  {
    "index": "F16483",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to get started, let me first remind you of a couple of properties of what vector inner products look like.",
    "output": "始めるにあたってまずベクトルの内積について幾つかの性質を思い出しておこう。"
  },
  {
    "index": "F16484",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say I have two vectors U and V, that look like this.",
    "output": "2つのベクトル、uとvがあったとする。こんな感じの。"
  },
  {
    "index": "F16485",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So both two dimensional vectors.",
    "output": "つまりどちらも二次元ベクトルだ。"
  },
  {
    "index": "F16486",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then let's see what U transpose V looks like.",
    "output": "そしてu転置のvがどんな風になるか見てみよう。"
  },
  {
    "index": "F16487",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And U transpose V is also called the inner products between the vectors U and V.",
    "output": "u転置vはまた、ベクトルuとvとの内積、とも言われる。"
  },
  {
    "index": "F16488",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Use a two dimensional vector, so I can on plot it on this figure.",
    "output": "二次元のベクトルを使う事でこの図にプロットする事が出来る。"
  },
  {
    "index": "F16489",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's say that's the vector U.",
    "output": "さて、これがベクトルuとしよう。"
  },
  {
    "index": "F16490",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what I mean by that is if on the horizontal axis that value takes whatever value U1 is and on the vertical axis the height of that is whatever U2 is the second component of the vector U.",
    "output": "これの意味する所は、水平軸上で、何かしらの値u1を取り、そして垂直軸上ではその高さは何かしらの値u2を取る、これがベクトルuの第二成分である、という事。"
  },
  {
    "index": "F16491",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, one quantity that will be nice to have is the norm of the vector U.",
    "output": "ここで、得ておくと良い一つの量として、ベクトルuのノルムがある。"
  },
  {
    "index": "F16492",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, these are, you know, double bars on the left and right that denotes the norm or length of U.",
    "output": "これらの、左と右の両方にある二重線が、uのノルム、言い換えると長さを表す。"
  },
  {
    "index": "F16493",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this just means; really the euclidean length of the vector U.",
    "output": "つまりこれは単に、ベクトルuのユークリッド距離を表すに過ぎない。"
  },
  {
    "index": "F16494",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is Pythagoras theorem is just equal to U1 squared plus U2 squared square root, right?",
    "output": "そして、ピタゴラスの定理により、イコールu1の二乗に足すことのu2の二乗、これをルートを取る。"
  },
  {
    "index": "F16495",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is the length of the vector U.",
    "output": "そしてこれはベクトルuの長さで、それは実数だ。"
  },
  {
    "index": "F16496",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just say you know, what is the length of this, what is the length of this vector down here.",
    "output": "この長さ、ここのこのベクトルの長さ。"
  },
  {
    "index": "F16497",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What is the length of this arrow that I just drew, is the normal view?",
    "output": "ここに今書いた矢印の長さ、これがuのノルムだ。"
  },
  {
    "index": "F16498",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's go back and look at the vector V because we want to compute the inner product.",
    "output": "さてここで、戻って、ベクトルvを見てみよう、何故なら内積を計算したいのだから。"
  },
  {
    "index": "F16499",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So V will be some other vector with, you know, some value V1, V2.",
    "output": "さて、vもまた何らかの別のベクトルで、何らかの値、v1とv2がある。"
  },
  {
    "index": "F16500",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, the vector V will look like that, towards V like so.",
    "output": "だからベクトルvもこんな感じに、vベクトル。"
  },
  {
    "index": "F16501",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's go back and look at how to compute the inner product between U and V.",
    "output": "ここで、戻って、uとvの内積を計算する方法を見てみよう。"
  },
  {
    "index": "F16502",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's how you can do it.",
    "output": "それはこんなやり方で出来る。"
  },
  {
    "index": "F16503",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me take the vector V and project it down onto the vector U.",
    "output": "ベクトルvに対してそれをベクトルu上に射影しよう。"
  },
  {
    "index": "F16504",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I'm going to take a orthogonal projection or a 90 degree projection, and project it down onto U like so.",
    "output": "つまり、直角に射影する、あるいは90度に射影する、そしてそれをu上に下ろす。"
  },
  {
    "index": "F16505",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what I'm going to do measure length of this red line that I just drew here.",
    "output": "そしてやるべき事は、この赤い線、たった今引いたこれの長さを測る。"
  },
  {
    "index": "F16506",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I'm going to call the length of that red line P.",
    "output": "この赤い線の長さをpと呼ぶ事にしよう。"
  },
  {
    "index": "F16507",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, P is the length or is the magnitude of the projection of the vector V onto the vector U.",
    "output": "するとpはベクトルvをベクトルuに射影した時の長さ、大きさである。"
  },
  {
    "index": "F16508",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me just write that down.",
    "output": "ここに書き記しておこう。"
  },
  {
    "index": "F16509",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, P is the length of the projection of the vector V onto the vector U.",
    "output": "つまりpはベクトルvをベクトルuに射影した時の長さである。"
  },
  {
    "index": "F16510",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it is possible to show that unit product U transpose V, that this is going to be equal to P times the norm or the length of the vector U.",
    "output": "そして内積であるu転置vはp掛けるuのノルム、あるいは長さに等しくなる、という事を示す事が出来る。"
  },
  {
    "index": "F16511",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is one way to compute the inner product.",
    "output": "つまり、これは内積を計算する一つの方法である。"
  },
  {
    "index": "F16512",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you actually do the geometry figure out what P is and figure out what the norm of U is.",
    "output": "そして実際に幾何学を用いるとpが何なのか、uのノルムが何なのかを知る事が出来る。"
  },
  {
    "index": "F16513",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This should give you the same way, the same answer as the other way of computing unit product.",
    "output": "この結果は内積の別の計算方法と同じ答えとなるべきだ。"
  },
  {
    "index": "F16514",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Which is if you take U transpose V then U transposes this U1 U2, its a one by two matrix, 1 times V.",
    "output": "それはuの転置vで、u転置はu1,u2。これは1x2行列。"
  },
  {
    "index": "F16515",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the theorem of linear algebra that these two formulas give you the same answer.",
    "output": "つまり、線形代数の定理としてこれら二つの式は同じ答えとなる。"
  },
  {
    "index": "F16516",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way, U transpose V is also equal to V transpose U.",
    "output": "ところで、uの転置vは、v転置uとも等しい。"
  },
  {
    "index": "F16517",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then, you know, do the same process, but with the rows of U and V reversed. And you would actually, you should actually get the same number whatever that number is.",
    "output": "だから同様のプロセスを反対に行えば、vをuに射影する代わりにuをvに射影出来て、同様のプロセスだがuとvの役割を反転させると、実際にも同じ数字を得る、その数字がなんであれ。"
  },
  {
    "index": "F16518",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just to clarify what's going on in this equation the norm of U is a real number and P is also a real number.",
    "output": "この等式で何が起こっているのかをもう少し明らかにしておく。uのノルムは実数でpもまた実数だ。"
  },
  {
    "index": "F16519",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so U transpose V is the regular multiplication as two real numbers of the length of P times the normal view.",
    "output": "だからu転置vは、単なる二つの実数の、普通の掛け算で、pの長さ掛けるuのノルム。"
  },
  {
    "index": "F16520",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just one last detail, which is if you look at the norm of P, P is actually signed so to the right.",
    "output": "最後に一つ細かい話を。pのノルムを見る時には、pは実際は符合つきだ。"
  },
  {
    "index": "F16521",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it can either be positive or negative.",
    "output": "それはプラスにもマイナスにもなりうる。"
  },
  {
    "index": "F16522",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then if I project V onto U, what I get is a projection it looks like this and so that length P.",
    "output": "ようするに、ベクトルuがこのようなベクトルだとして、vがこんなベクトルだとすると、つまりuとvのなす角が90度より大きければ、vをu上に射影すると、得られる射影はこんな感じとなり、これがpの長さとなる。"
  },
  {
    "index": "F16523",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this case, I will still have that U transpose V is equal to P times the norm of U.",
    "output": "そしてこの場合、u転置vは基本的には、依然としてp掛けるuのノルム、となるが、しかしこの場合は、pが負になる所だけが違う。"
  },
  {
    "index": "F16524",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you know, in inner products if the angle between U and V is less than ninety degrees, then P is the positive length for that red line whereas if the angle of this angle of here is greater than 90 degrees then P here will be negative of the length of the super line of that little line segment right over there.",
    "output": "つまり、内積においては、uとvのなす角が90度未満の時は、pは赤い線の正の長さとなり、一方でここの角度が90度以上の時は、ここのpはマイナスの長さ、マイナスの、ここの線分の長さとなる。"
  },
  {
    "index": "F16525",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the inner product between two vectors can also be negative if the angle between them is greater than 90 degrees.",
    "output": "つまり、二つのベクトルの内積はマイナスになる事がある、もしなす角が90度より大きければ。"
  },
  {
    "index": "F16526",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's how vector inner products work.",
    "output": "以上がベクトルの内積がどう機能するかだ。"
  },
  {
    "index": "F16527",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to use these properties of vector inner product to try to understand the support vector machine optimization objective over there.",
    "output": "これらのベクトルの内積の性質を用いて、ここのサポートベクターマシンの最適化の目的関数を理解しよう。"
  },
  {
    "index": "F16528",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here is the optimization objective for the support vector machine that we worked out earlier.",
    "output": "これが以前に見た、サポートベクターマシンの最適化の目的関数だ。"
  },
  {
    "index": "F16529",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just for the purpose of this slide I am going to make one simplification or once just to make the objective easy to analyze and what I'm going to do is ignore the indeceptrums.",
    "output": "このスライドの目的の為に一つ単純化しておこう、つまり、目的関数を分析しやすくする為に切片項を無視する事にする。"
  },
  {
    "index": "F16530",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we'll just ignore theta 0 and set that to be equal to 0.",
    "output": "つまりシータ0を無視する、つまりイコール0とする。"
  },
  {
    "index": "F16531",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To make things easier to plot, I'm also going to set N the number of features to be equal to 2.",
    "output": "またプロットしやすいように、n、つまりフィーチャーの数を2とする。"
  },
  {
    "index": "F16532",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we have only 2 features, X1 and X2.",
    "output": "つまり、x1とx2だけとする。"
  },
  {
    "index": "F16533",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, let's look at the objective function.",
    "output": "ここで、目的関数を見てみよう。"
  },
  {
    "index": "F16534",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The optimization objective of the SVM.",
    "output": "SVMの目的関数を。"
  },
  {
    "index": "F16535",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This can be written, one half of theta one squared plus theta two squared.",
    "output": "フィーチャーは二つだけなので、何故ならn=2だから、これは以下のように書ける:1/2のシータ1の二乗足すことのシータ2の二乗、と。"
  },
  {
    "index": "F16536",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because we only have two parameters, theta one and thetaa two.",
    "output": "何故なら我らは、二つのパラメータだけ、シータ1とシータ2だけしか持たないから。"
  },
  {
    "index": "F16537",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I'm going to do is rewrite this a bit.",
    "output": "そしてこれをちょっと書き換える。"
  },
  {
    "index": "F16538",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to write this as one half of theta one squared plus theta two squared and the square root squared.",
    "output": "これを、1/2のシータ1の二乗に足す事のシータ2の二乗、そしてこれらのルートを取る。"
  },
  {
    "index": "F16539",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason I can do that, is because for any number, you know, W, right, the square roots of W and then squared, that's just equal to W.",
    "output": "こんな事が出来る理由というのは、任意の数wに対して、wを二乗して、それ全体のルートを取ると、それはちょうどwと等しくなるから。"
  },
  {
    "index": "F16540",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So square roots and squared should give you the same thing.",
    "output": "つまり、ルートして二乗は同じ物を返す。"
  },
  {
    "index": "F16541",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What you may notice is that this term inside is that's equal to the norm or the length of the vector theta and what I mean by that is that if we write out the vector theta like this, as you know theta one, theta two.",
    "output": "ここで気づく事として、この中の項は、シータのノルムと言い換えるとベクトルシータの長さと等しい。"
  },
  {
    "index": "F16542",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then this term that I've just underlined in red, that's exactly the length, or the norm, of the vector theta.",
    "output": "そしてそれによって意味する事は、ベクトルシータをこんな風に書き下すと、つまり、シータ1,シータ2,と書くと、するとこの今赤で下線を引いたこの項は、これは正確にベクトルシータの長さ、ノルムである。"
  },
  {
    "index": "F16543",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We are calling the definition of the norm of the vector that we have on the previous line.",
    "output": "前のスライドにあった、ベクトルのノルムの定義に等しい。"
  },
  {
    "index": "F16544",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact this is actually equal to the length of the vector theta, whether you write it as theta zero, theta 1, theta 2.",
    "output": "そして実際に、これはベクトルシータの長さに等しい。"
  },
  {
    "index": "F16545",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or just the length of theta 1, theta 2; but for this line I am going to ignore theta 0. So let me just, you know, treat theta as this, let me just write theta, the normal theta as this theta 1, theta 2 only, but the math works out either way, whether we include theta zero here or not.",
    "output": "たとえあなたがシータ0,シータ1,シータ2で書いても、ここで仮定したようにシータ0が0だから、または単にこれはシータ1,シータ2の長さでもあるが、このスライドではシータ0を無視するのだったから、シータをこんな風にシータの、このシータ1,シータ2のノルムをこんな風に書いてみよう。"
  },
  {
    "index": "F16546",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it's not going to matter for the rest of our derivation.",
    "output": "だからこれは以下の導出には、影響しない。"
  },
  {
    "index": "F16547",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so finally this means that my optimization objective is equal to one half of the norm of theta squared.",
    "output": "そして最終的に、これは私の最適化の目的関数が1/2のシータのノルムの二乗に等しい事を意味する。"
  },
  {
    "index": "F16548",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So all the support vector machine is doing in the optimization objective is it's minimizing the squared norm of the square length of the parameter vector theta.",
    "output": "つまりサポートベクターマシンがやる事の全ては、最適化の目的関数として、パラメータベクトルのシータのノルムの二乗、つまり長さの二乗を最小化しようと試みる。"
  },
  {
    "index": "F16549",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now what I'd like to do is look at these terms, theta transpose X and understand better what they're doing.",
    "output": "ここで私はこれらの項を見て、シータ転置xを見て、それらが何をやっているのかをもっと良く理解していこう。"
  },
  {
    "index": "F16550",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So given the parameter vector theta and given and example x, what is this is equal to?",
    "output": "さて、パラメータベクトルのシータが与えられたとして、手本のxが与えられたとすると、これは何に等しいだろうか?"
  },
  {
    "index": "F16551",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And on the previous slide, we figured out what U transpose V looks like, with different vectors U and V.",
    "output": "前のスライドでは、u転置vとはどんな物なのかを見ていった、uとvは別のベクトル。"
  },
  {
    "index": "F16552",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so we're going to take those definitions, you know, with theta and X(i) playing the roles of U and V.",
    "output": "そこで我らはそれらの定義を持ってきて、シータとx(i)にuとvの役割をさせてみよう。"
  },
  {
    "index": "F16553",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's see what that picture looks like.",
    "output": "するとどんな絵となるかを見てみよう。"
  },
  {
    "index": "F16554",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's say I plot.",
    "output": "さて、プロットし、、、さて、たった一つのトレーニング手本を見てみるとする。"
  },
  {
    "index": "F16555",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say I look at just a single training example.",
    "output": "陽性の手本が一つあるとして、その場所に十字を描いておくとする。"
  },
  {
    "index": "F16556",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say I have a positive example the drawing was across there and let's say that is my example X(i), what that really means is plotted on the horizontal axis some value X(i) 1 and on the vertical axis X(i) 2.",
    "output": "これを手本x(i)としよう。それが実際に意味する事は、横軸にある値、x(i)1、そして縦軸にある値x(i)2をプロットした、という事。"
  },
  {
    "index": "F16557",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And although we haven't been really thinking of this as a vector, what this really is, this is a vector from the origin from 0, 0 out to the location of this training example.",
    "output": "そして、ここまでの所、これが実際にはベクトルであるとは考えてこなかったが、これは本当は原点、0,0からこのトレーニング手本の位置までのベクトルだ。"
  },
  {
    "index": "F16558",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And now let's say we have a parameter vector and I'm going to plot that as vector, as well.",
    "output": "そして今、パラメータベクトルがあるとして、それも同様にベクトルとしてプロットする。"
  },
  {
    "index": "F16559",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I mean by that is if I plot theta 1 here and theta 2 there so what is the inner product theta transpose X(i).",
    "output": "どういう事かというと、シータ1をここに、シータ2をここにプロットすると、内積であるシータ転置x(i)はどうなるだろうか?"
  },
  {
    "index": "F16560",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "While using our earlier method, the way we compute that is we take my example and project it onto my parameter vector theta.",
    "output": "以前の手法を用いると、それを計算する方法は、手本に対して、それをパラメータベクトルのシータの上に射影する。"
  },
  {
    "index": "F16561",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then I'm going to look at the length of this segment that I'm coloring in, in red.",
    "output": "そして次にこの線分の長さを見る、今赤で描いているこの長さだ。"
  },
  {
    "index": "F16562",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm going to call that P superscript I to denote that this is a projection of the i-th training example onto the parameter vector theta.",
    "output": "そしてこれを、p上付き添字iと呼ぼう、これはi番目のトレーニング手本のパラメータベクトル、シータに対する射影を表す。"
  },
  {
    "index": "F16563",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what we have is that theta transpose X(i) is equal to following what we have on the previous slide, this is going to be equal to P times the length of the norm of the vector theta.",
    "output": "すると我らが得るのは、シータ転置x(i)は前のスライドにあった、以下に等しい。これはp掛けるベクトルシータのノルム、長さに等しくなる。"
  },
  {
    "index": "F16564",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is of course also equal to theta 1 x1 plus theta 2 x2.",
    "output": "そしてこれはもちろん、シータ1x1足すことのシータ2x2に等しくなる。"
  },
  {
    "index": "F16565",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So each of these is, you know, an equally valid way of computing the inner product between theta and X(i).",
    "output": "つまりこれらはそれぞれどれも、シータとx(i)の内積を計算する同じように正統な方法である。"
  },
  {
    "index": "F16566",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Okay.",
    "output": "オーケー。"
  },
  {
    "index": "F16567",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What this means is that, this constrains that theta transpose X(i) be greater than or equal to one or less than minus one.",
    "output": "これの意味する所は、この制約条件、シータ転置x(i)が1以上か-1以下という条件、この意味する所は、制約に使われている式をp(i)掛けるxが1以上、に置き換える事が出来る。"
  },
  {
    "index": "F16568",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because theta transpose X(i) is equal to P(i) times the norm of theta.",
    "output": "何故ならシータ転置x(i)はp(i)掛けることのシータのノルムに等しいから。"
  },
  {
    "index": "F16569",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So writing that into our optimization objective. This is what we get where I have, instead of theta transpose X(i), I now have this P(i) times the norm of theta.",
    "output": "つまり、以上を最適化の目的関数に書きこむと、これが得られる、ここではシータ転置x(i)の代わりに、今度はp(i)掛けるシータのノルムとなっている。"
  },
  {
    "index": "F16570",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just to remind you we worked out earlier too that this optimization objective can be written as one half times the norm of theta squared.",
    "output": "そして、思い出そう、前に見てみたように、この最適化の目的関数は、1/2掛けるシータのノルムの二乗、とも書けるのだった。"
  },
  {
    "index": "F16571",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, now let's consider the training example that we have at the bottom and for now, continuing to use the simplification that theta 0 is equal to 0.",
    "output": "では下にあるようなトレーニング手本を考えてみよう、そしてここでも、シータ0が0と等しいという単純化を継続する。"
  },
  {
    "index": "F16572",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's see what decision boundary the support vector machine will choose.",
    "output": "サポートベクターマシンがどんな決定境界を選ぶかを見てみよう。"
  },
  {
    "index": "F16573",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's one option, let's say the support vector machine were to choose this decision boundary.",
    "output": "ここにそんな選択肢の一つがある。サポートベクターマシンが仮にこの決定境界を選んだ、と仮定してみよう。"
  },
  {
    "index": "F16574",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is not a very good choice because it has very small margins.",
    "output": "これはとても良い選択という訳では無さそうだ。何故ならマージンがとても小さいから。"
  },
  {
    "index": "F16575",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This decision boundary comes very close to the training examples.",
    "output": "この決定境界はトレーニング手本のとても近くを通ってる。"
  },
  {
    "index": "F16576",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's see why the support vector machine will not do this.",
    "output": "何故サポートベクターマシンがこれを行わないかを見ていこう。"
  },
  {
    "index": "F16577",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For this choice of parameters it's possible to show that the parameter vector theta is actually at 90 degrees to the decision boundary.",
    "output": "このパラメータの選択において、パラメータベクトルは実際に決定境界と90度の角度をなす事が証明出来る。"
  },
  {
    "index": "F16578",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, that green decision boundary corresponds to a parameter vector theta that points in that direction.",
    "output": "つまり、この緑の決定境界はこの方向を向いたパラメートルベクトルのシータに対応している。"
  },
  {
    "index": "F16579",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way, the simplification that theta 0 equals 0 that just means that the decision boundary must pass through the origin, (0,0) over there.",
    "output": "ところで、シータ0がイコール0だという単純化は、単に決定境界が原点、ここの(0,0)を通らないといけない、という事を意味しているに過ぎない。"
  },
  {
    "index": "F16580",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So now, let's look at what this implies for the optimization objective.",
    "output": "ではここで、これが最適化の目的関数に対して何を意味しているかを見ていこう。"
  },
  {
    "index": "F16581",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that this example here.",
    "output": "この手本がここにあったとする。"
  },
  {
    "index": "F16582",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that's my first example, you know, X1.",
    "output": "これが私の最初の手本、x(1)だとしよう。"
  },
  {
    "index": "F16583",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we look at the projection of this example onto my parameters theta.",
    "output": "この手本のパラメータシータに対する射影を見てみると、これが射影となる。"
  },
  {
    "index": "F16584",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that little red line segment.",
    "output": "つまりこの赤い線分。"
  },
  {
    "index": "F16585",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is equal to P1.",
    "output": "これはイコールp1だ。"
  },
  {
    "index": "F16586",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that is going to be pretty small, right.",
    "output": "そしてこれは極めて小さい。でしょ?"
  },
  {
    "index": "F16587",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, if this example here, if this happens to be X2, that's my second example.",
    "output": "そして同様に、このここの手本を見ると、これをx2とすると、これが二番目の手本という事だが、この手本のシータに対する射影を見てみよう。"
  },
  {
    "index": "F16588",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then, let me draw this one in magenta. This little magenta line segment, that's going to be P2.",
    "output": "お分かりの通り、これをマゼンタで描く事にすると、この小さなマゼンタの線分、これがp(2)となる。"
  },
  {
    "index": "F16589",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That's the projection of the second example onto my, onto the direction of my parameter vector theta which goes like this.",
    "output": "これが二番目の手本をパラメータベクトルシータに射影した物で、それはこんな感じになる。"
  },
  {
    "index": "F16590",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, this little projection line segment is getting pretty small.",
    "output": "つまり、この小さな射影の線分は、極めて小さくなってしまう。"
  },
  {
    "index": "F16591",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "P2 will actually be a negative number, right so P2 is in the opposite direction.",
    "output": "p(2)は実際には負の数だから、p(2)は反対の向きになる。"
  },
  {
    "index": "F16592",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This vector has greater than 90 degree angle with my parameter vector theta, it's going to be less than 0.",
    "output": "このベクトルはパラメータベクトルから90度より大きな角度となるので、0以下となる。"
  },
  {
    "index": "F16593",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what we're finding is that these terms P(i) are going to be pretty small numbers.",
    "output": "すると分かる事は、これらの項、p(i)は極めて小さな数となるという事だ。"
  },
  {
    "index": "F16594",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if we look at the optimization objective and see, well, for positive examples we need P(i) times the norm of theta to be bigger than either one.",
    "output": "つまり最適化の目的関数を見ると、陽性の手本に対してはp(i)掛けるシータのノルムが1以上である必要があるが、ここのp(i)、ここのp(1)がとても小さければ、シータのノルムはとても大きくならないといけない。"
  },
  {
    "index": "F16595",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if P(i) over here, if P1 over here is pretty small, that means that we need the norm of theta to be pretty large, right?",
    "output": "p(1)が小さくて、p(1)掛けるシータのノルムを1以上にしたいのだから、この条件を真にする唯一の方法は、この二つの数が大きくなるには、p(1)が小さいとするなら、シータのノルムが大きくなる必要がある。"
  },
  {
    "index": "F16596",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If P1 of theta is small and we want P1 you know times in all of theta to be bigger than either one, well the only way for that to be true for the profit that these two numbers to be large if P1 is small, as we said we want the norm of theta to be large.",
    "output": "同様に陰性の手本に対しては、p(2)掛けることのシータのノルムが、-1未満でなくてはならない。"
  },
  {
    "index": "F16597",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly for our negative example, we need P2 times the norm of theta to be less than or equal to minus one.",
    "output": "そしてこの手本に対してはp(2)は極めて小さい負の数である事を既に見た。だからその条件が満たされる唯一の方法も、同様に、シータのノルムが大きくなる、という事だ。"
  },
  {
    "index": "F16598",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we saw in this example already that P2 is going pretty small negative number, and so the only way for that to happen as well is for the norm of theta to be large, but what we are doing in the optimization objective is we are trying to find a setting of parameters where the norm of theta is small, and so you know, so this doesn't seem like such a good direction for the parameter vector and theta.",
    "output": "だが我らが行っている最適化の目的関数は、以下のような条件を満たしたシータの組を探すという事だった:その条件とはシータのノルムが小さくなる、という物、つまり、これはパラメータベクトルのシータにとって良い方向では無さそうだ。"
  },
  {
    "index": "F16599",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast, just look at a different decision boundary.",
    "output": "これと比較して、別の決定境界を見てみよう。"
  },
  {
    "index": "F16600",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here, let's say, this SVM chooses that decision boundary.",
    "output": "今度は、このSVMはこんな決定境界を選んだとしよう。"
  },
  {
    "index": "F16601",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now the is going to be very different.",
    "output": "こんどは随分と違う絵となる。"
  },
  {
    "index": "F16602",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If that is the decision boundary, here is the corresponding direction for theta.",
    "output": "これが決定境界だとすると、それに対応したシータの方向はこうなる。"
  },
  {
    "index": "F16603",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, with the direction boundary you know, that vertical line that corresponds to it is possible to show using linear algebra that the way to get that green decision boundary is have the vector of theta be at 90 degrees to it, and now if you look at the projection of your data onto the vector x, lets say its before this example is my example of x1.",
    "output": "さて、この決定境界、垂直な直線に、対応した決定境界は、線形代数を用いて証明する事が出来るのだが、この緑の決定境界を得る方法は、それと直角のシータベクトルを持てば良い。"
  },
  {
    "index": "F16604",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when I project this on to x, or onto theta, what I find is that this is P1.",
    "output": "そしてあなたのデータをベクトルシータに射影した物を見ると、前と同様にこの手本を手本x(1)とすると、これをシータに射影すると、これがp(1)となる。"
  },
  {
    "index": "F16605",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That length there is P1.",
    "output": "この長さがp(1)。"
  },
  {
    "index": "F16606",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The other example, that example is and I do the same projection and what I find is that this length here is a P2 really that is going to be less than 0.",
    "output": "別の手本、この手本にも同様の射影を行って、この長さ、ここが、p(2)となり、これは0未満となる。"
  },
  {
    "index": "F16607",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you notice that now P1 and P2, these lengths of the projections are going to be much bigger, and so if we still need to enforce these constraints that P1 of the norm of theta is phase number one because P1 is so much bigger now.",
    "output": "そして見て分かるように、ここでのp(1)とp(2)、これら射影の長さは、もっと大きくなっている。"
  },
  {
    "index": "F16608",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The normal can be smaller.",
    "output": "だから、この制約、p(1)シータのノルムが1以上、という制約がここでも満たされている必要があるとすると、p(1)はいまやもっと大きくなったのでノルムは小さくなれる。"
  },
  {
    "index": "F16609",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, what this means is that by choosing the decision boundary shown on the right instead of on the left, the SVM can make the norm of the parameters theta much smaller.",
    "output": "すると、この意味する所は、決定境界を、左に示したような物では無く右側に示したような物を選ぶ事によって、SVMはパラメータシータのノルムをもっと小さくする事が出来る。"
  },
  {
    "index": "F16610",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if we can make the norm of theta smaller and therefore make the squared norm of theta smaller, which is why the SVM would choose this hypothesis on the right instead.",
    "output": "すると、シータのノルムを小さくする事が出来ると、それはシータのノルムの二乗も小さくなるので、それこそが、SVMが右側の仮説の方を選ぶであろう理由となる。"
  },
  {
    "index": "F16611",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is how the SVM gives rise to this large margin certification effect.",
    "output": "そしてこのようにして、SVMは大きなマージンの分類の効果を生み出す。"
  },
  {
    "index": "F16612",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Mainly, if you look at this green line, if you look at this green hypothesis we want the projections of my positive and negative examples onto theta to be large, and the only way for that to hold true this is if surrounding the green line.",
    "output": "例えば、この緑の直線を見ると、この緑の仮説を見ると、我らは、陽性と陰性の手本のシータに対する射影を大きくしたい。"
  },
  {
    "index": "F16613",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's this large margin, there's this large gap that separates positive and negative examples is really the magnitude of this gap.",
    "output": "これを真に保つ、唯一の方法は、緑の線を囲むとすると、この大きなマージンが存在する事、この陽性と陰性の手本を分離する時に大きなギャップが存在する事、実際にこのギャップの大きさ、このマージンの大きさは、実際にp(1)、p(2)、p(3)などの値である。"
  },
  {
    "index": "F16614",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The magnitude of this margin is exactly the values of P1, P2, P3 and so on.",
    "output": "つまりこのマージンを大きくする事で、これらp(1)、p(2)、p(3)などを大きくしようとする事で、SVMは結果としてシータのノルムを小さくする事が出来る、そしてそれこそが目的関数でやろうとしていた事だった。"
  },
  {
    "index": "F16615",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so by making the margin large, by these tyros P1, P2, P3 and so on that's the SVM can end up with a smaller value for the norm of theta which is what it is trying to do in the objective.",
    "output": "こんな訳で、サポートベクターマシンは分類器のマージンを広げる事になる、何故なら、それはp(1)のノルム、それは決定境界から手本までの距離だが、それを最大化しようとするからだ。"
  },
  {
    "index": "F16616",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is why this machine ends up with enlarge margin classifiers because itss trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary.",
    "output": "最後に、我らはこれらの導出を全てパラメータのシータ0が、イコール0という単純化の元で行なってきた。その効果を簡単に伝えておく。"
  },
  {
    "index": "F16617",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, we did this whole derivation using this simplification that the parameter theta 0 must be equal to 0.",
    "output": "シータ0が0と等しい時というのは、それの意味する所は、決定境界をいつも、原点を通る物だけで決めるという事に対応する。"
  },
  {
    "index": "F16618",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The effect of that as I mentioned briefly, is that if theta 0 is equal to 0 what that means is that we are entertaining decision boundaries that pass through the origins of decision boundaries pass through the origin like that, if you allow theta zero to be non 0 then what that means is that you entertain the decision boundaries that did not cross through the origin, like that one I just drew.",
    "output": "こんな風に原点を通る。もしシータ0に、0以外の値を許すとすると、その意味する所は、原点を通らない決定境界も試していくという事を意味する。"
  },
  {
    "index": "F16619",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm not going to do the full derivation that.",
    "output": "今ここに描いたような物も、だ。"
  },
  {
    "index": "F16620",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that this same large margin proof works in pretty much in exactly the same way.",
    "output": "これの完全な証明をする気は無いが、まさにそっくりの大きなマージンの証明がほとんど同様に成立する。"
  },
  {
    "index": "F16621",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And there's a generalization of this argument that we just went through them long ago through that shows that even when theta 0 is non 0, what the SVM is trying to do when you have this optimization objective. Which again corresponds to the case of when C is very large.",
    "output": "そして、ここまで長々とやってきた議論を一般化出来て、シータ0が非ゼロでも、SVMが試みるのはーーこの最適化の目的関数を持つ時に、これもふたたび、Cがとても大きい場合に対応するが、ここでも、シータ0が0で無くてもこのサポートベクターマシンは同様に陽性と陰性の手本を分離するような、大きなマージンの分離を探そうとする、という事を示す事が出来る。"
  },
  {
    "index": "F16622",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that explains how this support vector machine is a large margin classifier.",
    "output": "以上が、サポートベクターマシンがどのように大きなマージンの分類器であるのか、の説明だ。"
  },
  {
    "index": "F16623",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video we will start to talk about how to take some of these SVM ideas and start to apply them to build a complex nonlinear classifiers.",
    "output": "次のビデオでは、これらのSVMの考え方の幾つかを用いて、それを複雑な非線型の分類器を構築する為に適用していく方法を見ていこう。"
  },
  {
    "index": "F16624",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to start adapting support vector machines in order to develop complex nonlinear classifiers.",
    "output": "このビデオでは、複雑な非線形の分類器を構築する為にサポートベクタマシンを用いる事を開始しよう。"
  },
  {
    "index": "F16625",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The main technique for doing that is something called kernels.",
    "output": "これを行う主要なテクニックは、カーネルと呼ばれる物だ。"
  },
  {
    "index": "F16626",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's see what this kernels are and how to use them.",
    "output": "カーネルとは何で、それをどう使うのかを見ていこう。"
  },
  {
    "index": "F16627",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have a training set that looks like this, and you want to find a nonlinear decision boundary to distinguish the positive and negative examples, maybe a decision boundary that looks like that.",
    "output": "こんなトレーニングセットがあったとして、陽性と陰性の手本を区別する非線型な決定境界を見つけたいとする。たとえばこんな感じの決定境界だ。"
  },
  {
    "index": "F16628",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One way to do so is to come up with a set of complex polynomial features, right?",
    "output": "これを行う方法として一つ考えられるのは、複雑な多項式のフィーチャーを用いる事だ。"
  },
  {
    "index": "F16629",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And another way of writing this, to introduce a level of new notation that I'll use later, is that we can think of a hypothesis as computing a decision boundary using this.",
    "output": "これを書くもう一つの方法として、後で使う事になるちょっとしたノーテーションを導入しておくと、仮説を、これを用いて決定境界を計算する物と考える事が出来る、つまり、シータ0+シータ1f1+シータ2f2+シータ3f3、などなど。"
  },
  {
    "index": "F16630",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where I'm going to use this new denotation f1, f2, f3 and so on to denote these new sort of features that I'm computing, so f1 is just X1, f2 is equal to X2, f3 is equal to this one here.",
    "output": "ここで私はこの新しいノーテーションf1,f2,f3などを、今計算している、ある種の新しいフィーチャーを示すのに用いていて、つまりf1は単にx1で、f2はイコールx2で、f3はこれに等しい。"
  },
  {
    "index": "F16631",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, f4 is equal to X1 squared where f5 is to be x2 squared and so on and we seen previously that coming up with these high order polynomials is one way to come up with lots more features, the question is, is there a different choice of features or is there better sort of features than this high order polynomials because you know it's not clear that this high order polynomial is what we want, and what we talked about computer vision talk about when the input is an image with lots of pixels.",
    "output": "f4はイコールx1の二乗で、f5はx2の二乗、などとなり、そして以前に見たように、これらの高次の多項式を含めていくのは、もっと多くのフィーチャーにする為の一つの方法だ。ここで疑問に思うのは、この高次の多項式以外のフィーチャーの選択が無いものか?"
  },
  {
    "index": "F16632",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We also saw how using high order polynomials becomes very computationally expensive because there are a lot of these higher order polynomial terms.",
    "output": "何故なら、これらの高次の多項式は、我らの欲しい物かいまいち分からないし、コンピュータビジョンの、入力が画像のピクセルの場合などを議論したが、そこでは高次の多項式を用いるのは計算量的にとても高くつく事を見た、何故ならその場合は、大量の高次の多項式が存在する事になるから。"
  },
  {
    "index": "F16633",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, is there a different or a better choice of the features that we can use to plug into this sort of hypothesis form.",
    "output": "だから、もっと別の、もっと良いフィーチャーの選択肢で、この種の仮説の形に代入出来るような物は無いものか?"
  },
  {
    "index": "F16634",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here is one idea for how to define new features f1, f2, f3.",
    "output": "そこでこれが、新しいフィーチャーf1,f2,f3を定義する一つのやり方だ。"
  },
  {
    "index": "F16635",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "On this line I am going to define only three new features, but for real problems we can get to define a much larger number.",
    "output": "このスライドでは、三つのフィーチャーだけを定義するが、現実の問題では、もっと多くの数のフィーチャーを定義する事になる。"
  },
  {
    "index": "F16636",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But here's what I'm going to do in this phase of features X1, X2, and I'm going to leave X0 out of this, the interceptor X0, but in this phase X1 X2, I'm going to just, you know, manually pick a few points, and then call these points l1, we are going to pick a different point, let's call that l2 and let's pick the third one and call this one l3, and for now let's just say that I'm going to choose these three points manually.",
    "output": "だがここでは、フィーチャーx1とx2の張る空間に対し、ここでx0、切片項のx0は省略する事にするが、このx1x2の空間の中で、幾つかの点を手動で選ぶ、そしてこれらの点を、これをl(1)と呼び、さらに別の点を選んで、これをl(2)と呼ぶ、そして三番目を選んで、これをl(3)と呼ぶ事にする。ここでは、これら三つの点は手動で選ぶ事にする。"
  },
  {
    "index": "F16637",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to call these three points line ups, so line up one, two, three.",
    "output": "そしてこれらの点をランドマークと呼ぶ事にする。つまりランドマークの1,2,3。"
  },
  {
    "index": "F16638",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I'm going to do is define my new features as follows, given an example X, let me define my first feature f1 to be some measure of the similarity between my training example X and my first landmark and this specific formula that I'm going to use to measure similarity is going to be this is E to the minus the length of X minus l1, squared, divided by two sigma squared.",
    "output": "ある手本xが与えられた時に、最初のフィーチャーf1をなんらかの類似度の指標、トレーニング手本xと最初のランドマークとの類似度を定義する、そしてここでは具体的に以下のような類似度の指標を用いる、eの指数乗のマイナスのx-l1の距離を二乗して、2シグマ二乗で割る。"
  },
  {
    "index": "F16639",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, depending on whether or not you watched the previous optional video, this notation, you know, this is the length of the vector W.",
    "output": "前回のオプショナルのビデオをあなたが見ているかどうかは分からないが、このノーテーション、これはベクトルwの長さを表す。"
  },
  {
    "index": "F16640",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, this thing here, this X minus l1, this is actually just the euclidean distance squared, is the euclidean distance between the point x and the landmark l1.",
    "output": "だからこの、ここのこれは、このx-l(1)は、これは実体は単なるユークリッド距離の二乗で、点xとランドマークl1とのユークリッド距離だ。"
  },
  {
    "index": "F16641",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We will see more about this later.",
    "output": "この事は後でもうちょっと詳しく見る事にする。"
  },
  {
    "index": "F16642",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But that's my first feature, and my second feature f2 is going to be, you know, similarity function that measures how similar X is to l2 and the game is going to be defined as the following function.",
    "output": "だがとにかく、これが最初のフィーチャーだ。二番目のフィーチャーf2はl(2)とxがどれだけ類似しているかを測るsimilarity関数で、これは以下のような関数で定義される。"
  },
  {
    "index": "F16643",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is E to the minus of the square of the euclidean distance between X and the second landmark, that is what the enumerator is and then divided by 2 sigma squared and similarly f3 is, you know, similarity between X and l3, which is equal to, again, similar formula.",
    "output": "それはeの、以下による指数乗で、その指数はマイナスのxと二番目のランドマークとの間の距離の二乗を分子として、割ることの2シグマ二乗が指数となる。同様にf3はxとl3の間の類似度で、それはイコールまた同様の式となる。"
  },
  {
    "index": "F16644",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what this similarity function is, the mathematical term for this, is that this is going to be a kernel function.",
    "output": "そしてこのsimilarity関数とは数学的な用語では、カーネル関数と呼ばれる物だ。"
  },
  {
    "index": "F16645",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the specific kernel I'm using here, this is actually called a Gaussian kernel.",
    "output": "そしてここで具体的にカーネルとして使っている関数は実際にはガウスカーネルと呼ばれる。"
  },
  {
    "index": "F16646",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this formula, this particular choice of similarity function is called a Gaussian kernel.",
    "output": "つまり、この式、この具体的なsimilarity関数の選択を、ガウスカーネルと呼ぶ。"
  },
  {
    "index": "F16647",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the way the terminology goes is that, you know, in the abstract these different similarity functions are called kernels and we can have different similarity functions and the specific example I'm giving here is called the Gaussian kernel.",
    "output": "だが用語的には、これらの様々なsimilarity関数を抽象的にカーネルと呼び、similarity関数には様々な物がありうる。そしてこの例で私が与えた物は、ガウスカーネルと呼ばれる。"
  },
  {
    "index": "F16648",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We'll see other examples of other kernels.",
    "output": "我らは後に、別のカーネルの例を見る事になるだろう。"
  },
  {
    "index": "F16649",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But for now just think of these as similarity functions.",
    "output": "だが現時点では、これらは単なる類似度の関数と思っておけば良い。"
  },
  {
    "index": "F16650",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, instead of writing similarity between X and l, sometimes we also write this a kernel denoted you know, lower case k between x and one of my landmarks all right.",
    "output": "こんな事情により、xとlの間のsimilarity(類似度)と書く代わりに、時々これを、カーネルを表す小文字のkを用いて、xとランドマークの間のカーネル、と記述する事もある。"
  },
  {
    "index": "F16651",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's see what a criminals actually do and why these sorts of similarity functions, why these expressions might make sense.",
    "output": "ではカーネルが実際に何をやるかを見てみよう、そして何故この種の類似度関数が、これらの式が筋が通っているのかを見てみよう。"
  },
  {
    "index": "F16652",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "My landmark l1, which is one of those points I chose on my figure just now. So the similarity of the kernel between x and l1 is given by this expression.",
    "output": "そこでまず最初のランドマーク、ランドマークl(1)に対して、これはさっきの図で私が選んだ点の一つだが、xとl(1)の間のカーネルの類似度は、この式で与えられる。"
  },
  {
    "index": "F16653",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just to make sure, you know, we are on the same page about what the numerator term is, the numerator can also be written as a sum from J equals 1 through N on sort of the distance.",
    "output": "この分子が実際にどうなるのかを一応書いておくと、分子はこのように、j=1からnまでの、ある種の距離の和となる。"
  },
  {
    "index": "F16654",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is the component wise distance between the vector X and the vector l.",
    "output": "つまりこれは、xとベクトルlの各要素に渡って取った距離だ。"
  },
  {
    "index": "F16655",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And again for the purpose of these slides I'm ignoring X0.",
    "output": "そしてここでも、これらのスライド上では、x0を無視する。"
  },
  {
    "index": "F16656",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just ignoring the intercept term X0, which is always equal to 1.",
    "output": "つまり切片項のx0を、これはいつも=1だが、単に無視する事にする。"
  },
  {
    "index": "F16657",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you know, this is how you compute the kernel with similarity between X and a landmark.",
    "output": "すると、これがxとランドマークの類似度を用いてカーネルを計算する方法だ。"
  },
  {
    "index": "F16658",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's see what this function does.",
    "output": "ではこの関数が何をするかを見ていこう。"
  },
  {
    "index": "F16659",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose X is close to one of the landmarks.",
    "output": "xがランドマークの一つと近いとしてみよう。"
  },
  {
    "index": "F16660",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then this euclidean distance formula and the numerator will be close to 0, right.",
    "output": "すると、この分子にあるユークリッド距離の式は0に近い値となる。"
  },
  {
    "index": "F16661",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that is this term here, the distance was great, the distance using X and 0 will be close to zero, and so f1, this is a simple feature, will be approximately E to the minus 0 and then the numerator squared over 2 is equal to squared so that E to the 0, E to minus 0, E to 0 is going to be close to one.",
    "output": "つまり、この項、このxとl(1)の距離、この距離は0に近い値となる。するとf1、フィーチャーは、だいたい近似的にeの指数乗である所の-0の二乗が分子で、2シグマ二乗が分母。"
  },
  {
    "index": "F16662",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'll put the approximation symbol here because the distance may not be exactly 0, but if X is closer to landmark this term will be close to 0 and so f1 would be close 1.",
    "output": "そしてここに近似の記号を用いたのは距離は厳密に0では無いかもしれないからだが、しかしxがランドマークに近ければ、この項は0に近くなり、f1は1に近くなる。"
  },
  {
    "index": "F16663",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Conversely, if X is far from 01 then this first feature f1 will be E to the minus of some large number squared, divided divided by two sigma squared and E to the minus of a large number is going to be close to 0.",
    "output": "逆に、xがl(1)から遠く離れている時には、この最初のフィーチャーf1はeの指数乗である所の何らかの大きな数字の二乗に、割ることの2シグマ二乗で、つまりeのマイナスの、大きな数による累乗は0に近い値となる。"
  },
  {
    "index": "F16664",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what these features do is they measure how similar X is from one of your landmarks and the feature f is going to be close to one when X is close to your landmark and is going to be 0 or close to zero when X is far from your landmark.",
    "output": "つまりこれらのフィーチャーが行う事は、xとランドマークの一つとがどれだけ似ているかを測っているのだ。そしてフィーチャーfはxがランドマークに近い時に1に近い値となり、xがランドマークから遥か離れていると、0に近い値となる。"
  },
  {
    "index": "F16665",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "On the previous line, I drew three landmarks, l1, l2,l3. Each of these landmarks, defines a new feature f1, f2 and f3.",
    "output": "前のスライドにあったこれらのランドマークそれぞれに対し、私は三つのランドマーク、l(1)、l(2)、l(3)を描いたのだが、これらのランドマークそれぞれに対し、新しいフィーチャーf1,f2,f3を定義する。"
  },
  {
    "index": "F16666",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, given the the training example X, we can now compute three new features: f1, f2, and f3, given, you know, the three landmarks that I wrote just now.",
    "output": "つまり、ある所与のトレーニング手本xに対し三つのフィーチャーf1,f2,f3を計算出来る、さっき書いた三つのランドマークが所与の時には。"
  },
  {
    "index": "F16667",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But first, let's look at this exponentiation function, let's look at this similarity function and plot in some figures and just, you know, understand better what this really looks like.",
    "output": "まずこの類似度(similarity)関数から見ていこう。そしてそれをプロットしてみて、それが実際にどんな形かをもっと良く理解していこう。"
  },
  {
    "index": "F16668",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For this example, let's say I have two features X1 and X2.",
    "output": "この例では、二つのフィーチャーx1とx2があるとする。"
  },
  {
    "index": "F16669",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say my first landmark, l1 is at a location, 3 5.",
    "output": "そして最初のランドマークのl(1)は、地点3,5にあるとする。"
  },
  {
    "index": "F16670",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So and let's say I set sigma squared equals one for now.",
    "output": "そしてシグマ二乗はここでは1としよう。"
  },
  {
    "index": "F16671",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I plot what this feature looks like, what I get is this figure.",
    "output": "もしこのフィーチャーをプロットすると、この図が得られる。"
  },
  {
    "index": "F16672",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the vertical axis, the height of the surface is the value of f1 and down here on the horizontal axis are, if I have some training example, and there is x1 and there is x2.",
    "output": "この垂直の軸、表面の高さは、f1の値だ。"
  },
  {
    "index": "F16673",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given a certain training example, the training example here which shows the value of x1 and x2 at a height above the surface, shows the corresponding value of f1 and down below this is the same figure I had showed, using a quantifiable plot, with x1 on horizontal axis, x2 on horizontal axis and so, this figure on the bottom is just a contour plot of the 3D surface.",
    "output": "この下の、水平軸達は、あるトレーニング手本があった時に、それはx1とx2を持っている訳だが、あるトレーニング手本が与えられた時に、このx1とx2の値のトレーニング手本、この点の上の曲面までの高さが、対応するf1の値を示している、そしてこの下に、同じ図を等高線プロットを用いて描いた物を示しておいた。x1が横軸でx2が縦軸。"
  },
  {
    "index": "F16674",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You notice that when X is equal to 3 5 exactly, then we the f1 takes on the value 1, because that's at the maximum and X moves away as X goes further away then this feature takes on values that are close to 0.",
    "output": "見て分かるように、xがぴったり3,5の時には、f1の値は1を取る、何故ならそこが最大となる点だからだ。そしてxが離れていくと、xが離れていくにつれて、このフィーチャーは0に近い値を取る。"
  },
  {
    "index": "F16675",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, this is really a feature, f1 measures, you know, how close X is to the first landmark and if varies between 0 and one depending on how close X is to the first landmark l1.",
    "output": "だからこれは実際に、f1はxが最初のランドマークとどれだけ近いかを測る指標、フィーチャーで、それは0と1の間の値をxが最初のランドマークl(1)がどれだけ近いかに応じて取る。"
  },
  {
    "index": "F16676",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now the other was due on this slide is show the effects of varying this parameter sigma squared.",
    "output": "さて、もう一つこのスライドで見せたい事は、このパラメータ、シグマ二乗を変えた時の効果だ。"
  },
  {
    "index": "F16677",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, sigma squared is the parameter of the Gaussian kernel and as you vary it, you get slightly different effects.",
    "output": "シグマ二乗はガウスカーネルのパラメータで、それを変えていく事で、ちょっと違った効果が得られる。"
  },
  {
    "index": "F16678",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's set sigma squared to be equal to 0.5 and see what we get.",
    "output": "シグマ二乗をイコール0.5にセットしよう。そしてどうなるか見てみよう。"
  },
  {
    "index": "F16679",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We set sigma square to 0.5, what you find is that the kernel looks similar, except for the width of the bump becomes narrower.",
    "output": "シグマ二乗に0.5をセットすると、カーネルは基本的には似たような形だが、こぶの幅が狭くなる。"
  },
  {
    "index": "F16680",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The contours shrink a bit too.",
    "output": "等高線もちょっと縮む。"
  },
  {
    "index": "F16681",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if sigma squared equals to 0.5 then as you start from X equals 3 5 and as you move away, then the feature f1 falls to zero much more rapidly and conversely, if you has increase since where three in that case and as I move away from, you know l.",
    "output": "つまりシグマ二乗がイコール0.5なら、xイコール35から始まって、そこから離れていくにつれて、フィーチャーf1は0に、より急速に落ちていく。"
  },
  {
    "index": "F16682",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this point here is really l, right, that's l1 is at location 3 5, right.",
    "output": "逆に、シグマ2乗を3に増加させると、その場合は、点lから離れていくと、この点がlだが、これはl(1)で、これは座標35だ。"
  },
  {
    "index": "F16683",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it's shown up here.",
    "output": "それはここ。"
  },
  {
    "index": "F16684",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if sigma squared is large, then as you move away from l1, the value of the feature falls away much more slowly.",
    "output": "そしてシグマ二乗が大きくなると、するとl(1)から離れていくに連れてフィーチャーの値はよりゆっくりと、低下していく。"
  },
  {
    "index": "F16685",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, given this definition of the features, let's see what source of hypothesis we can learn.",
    "output": "このフィーチャーの定義が与えられたとして、どんな仮説が学習出来るか見てみよう。"
  },
  {
    "index": "F16686",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given the training example X, we are going to compute these features f1, f2, f3 and a hypothesis is going to predict one when theta 0 plus theta 1 f1 plus theta 2 f2, and so on is greater than or equal to 0.",
    "output": "ある手本xが与えられたとして、これらのフィーチャーf1,f2,f3を計算する、そして仮説はシータ0+シータ1f1+シータ2f2...が0以上の時に1を予測する。"
  },
  {
    "index": "F16687",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For this particular example, let's say that I've already found a learning algorithm and let's say that, you know, somehow I ended up with these values of the parameter.",
    "output": "この具体例だと、既に学習アルゴリズムを見つけていて、どうにかして既にこれらのパラメータの値を得ていたとする。"
  },
  {
    "index": "F16688",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if theta 0 equals minus 0.5, theta 1 equals 1, theta 2 equals 1, and theta 3 equals 0 And what I want to do is consider what happens if we have a training example that takes has location at this magenta dot, right where I just drew this dot over here.",
    "output": "そしてシータ0=-0.5、シータ1=1,シータ2=1,シータ3=0だったとする。そして私がやりたい事は、以下のようなマゼンダの点の位置の手本の時には、何が起こるか?"
  },
  {
    "index": "F16689",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's say I have a training example X, what would my hypothesis predict?",
    "output": "たった今描いたここの点。トレーニング手本xがあったら、仮説は何を予測するだろうか?"
  },
  {
    "index": "F16690",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because my training example X is close to l1, we have that f1 is going to be close to 1 the because my training example X is far from l2 and l3 I have that, you know, f2 would be close to 0 and f3 will be close to 0.",
    "output": "この式を見てみると、トレーニング手本xはl(1)の近くなので、f1は1に近い値となる、トレーニング手本xはl(2)とl(3)からは遠く離れているから、f2は0に近い値で、f3も0に近い値だから、この式を見ると、シータ0+シータ1掛ける1+シータ2掛ける何かしらの値で、その値は完全に0では無いが、0に近いとしよう。"
  },
  {
    "index": "F16691",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then plus theta 3 times something close to 0.",
    "output": "そして足すことのシータ3掛ける何かしら0に近い値。"
  },
  {
    "index": "F16692",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that gives minus 0.5 plus 1 times 1 which is 1, and so on. Which is equal to 0.5 which is greater than or equal to 0.",
    "output": "これはイコール、これらの値を代入すると、すると-0.5+1*1でこれは1、などと以下同様に、これはイコール0.5となり、これは0以上だ。"
  },
  {
    "index": "F16693",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, at this point, we're going to predict Y equals 1, because that's greater than or equal to zero.",
    "output": "つまり、この点はy=1と予測する事になる、何故ならこれが0以上となったから。"
  },
  {
    "index": "F16694",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's take a different point.",
    "output": "ここで、別の点をとってみよう。"
  },
  {
    "index": "F16695",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now lets' say I take a different point, I'm going to draw this one in a different color, in cyan say, for a point out there, if that were my training example X, then if you make a similar computation, you find that f1, f2, Ff3 are all going to be close to 0.",
    "output": "今度は別の点、これを別の色でシアン色で描く事にしよう、それをここの点とする。これがトレーニング手本xだとすると、同様の計算を行っていくと、f1,f2,f3は全て0に近い値となる、すると、シータ0+シータ1f1+...と続いていき、これはイコール-0.5となる。"
  },
  {
    "index": "F16696",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, we have theta 0 plus theta 1, f1, plus so on and this will be about equal to minus 0.5, because theta 0 is minus 0.5 and f1, f2, f3 are all zero.",
    "output": "何故ならシータ0は-0.5で、f1,f2,f3は全てゼロだから。"
  },
  {
    "index": "F16697",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this will be minus 0.5, this is less than zero.",
    "output": "だからこれは0.5となり、これは0未満だ。"
  },
  {
    "index": "F16698",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, at this point out there, we're going to predict Y equals zero.",
    "output": "つまり、ここの点は、y=0と予測する事になる。"
  },
  {
    "index": "F16699",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you do this yourself for a range of different points, be sure to convince yourself that if you have a training example that's close to L2, say, then at this point we'll also predict Y equals one.",
    "output": "そしてもしあなたが自分で、さまざまな点で計算してみると、l(2)に近い点のトレーニング手本は、これもまたy=1を予測する事になる、と納得出来ると思う。"
  },
  {
    "index": "F16700",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact, what you end up doing is, you know, if you look around this boundary, this space, what we'll find is that for points near l1 and l2 we end up predicting positive.",
    "output": "そして結局、最終的にあなたがやってるのは、この空間を見回して、l(1)とl(2)に近い点なら、陽性と予測する事になる。"
  },
  {
    "index": "F16701",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for points far away from l1 and l2, that's for points far away from these two landmarks, we end up predicting that the class is equal to 0.",
    "output": "そしてl(1)とl(2)から遠い点は、これら二つのランドマークの両方から遠い点は、そのクラスはイコール0だと予測する事になる。"
  },
  {
    "index": "F16702",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As so, what we end up doing,is that the decision boundary of this hypothesis would end up looking something like this where inside this red decision boundary would predict Y equals 1 and outside we predict Y equals 0.",
    "output": "だから最終的にやってる事は、この仮説の決定境界はこんな感じの物となり、この赤い決定境界の内側をy=1と予測し、外側をy=0と予測する。"
  },
  {
    "index": "F16703",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can learn pretty complex non-linear decision boundary, like what I just drew where we predict positive when we're close to either one of the two landmarks. And we predict negative when we're very far away from any of the landmarks.",
    "output": "つまり以上が、ランドマークとカーネル関数を定義する事で、我らは極めて複雑な、非線型の決定境界を学習させる事が出来る、例えば私がさっき描いたような、二つのランドマークに近い時に陽性と予想し、どのランドマークからも遠く離れている時には陰性と予想するような。"
  },
  {
    "index": "F16704",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is part of the idea of kernels of and how we use them with the support vector machine, which is that we define these extra features using landmarks and similarity functions to learn more complex nonlinear classifiers.",
    "output": "以上がカーネルという考え方と、それをサポートベクターマシンで使う方法だ、それはランドマークを用いて、これらの新しいフィーチャーを定義し、類似度関数を用いてより複雑な非線型の分類器を学習する。"
  },
  {
    "index": "F16705",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully that gives you a sense of the idea of kernels and how we could use it to define new features for the Support Vector Machine.",
    "output": "以上でカーネルとそれを用いてサポートベクターマシンで新しいフィーチャーを定義する方法が分かっただろうか。"
  },
  {
    "index": "F16706",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But there are a couple of questions that we haven't answered yet.",
    "output": "だが、まだ幾つか答えてない疑問がある。"
  },
  {
    "index": "F16707",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is, how do we get these landmarks?",
    "output": "一つ目は、どうやってこれらのランドマークを得たらいいだろうか?"
  },
  {
    "index": "F16708",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How do we choose these landmarks?",
    "output": "これらのランドマークを、どうやって選んだらいいだろう?"
  },
  {
    "index": "F16709",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And another is, what other similarity functions, if any, can we use other than the one we talked about, which is called the Gaussian kernel.",
    "output": "そしてもう一つは、別の類似度関数として、我らが話してきた物、ガウスカーネル以外にはどんな物が使えるだろうか?"
  },
  {
    "index": "F16710",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video we give answers to these questions and put everything together to show how support vector machines with kernels can be a powerful way to learn complex nonlinear functions.",
    "output": "次のビデオではこれらの問いに対する解答を与えていく。そして全てを組み合わせてサポートベクターマシンがカーネルとあわせてどのように複雑な非線形の関数を学習する事が出来るかを見ていく。"
  },
  {
    "index": "F16711",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So far we've been talking about SVMs in a fairly abstract level.",
    "output": "ここまでの所、我らはSVMについてかなり抽象的なレベルで議論してきた。"
  },
  {
    "index": "F16712",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I'd like to talk about what you actually need to do in order to run or to use an SVM.",
    "output": "このビデオでは、SVMを実際に走らせるにあたり、あるいは使うにあたり実際にやらなくてはいけない事について議論したい。"
  },
  {
    "index": "F16713",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The support vector machine algorithm poses a particular optimization problem.",
    "output": "サポートベクタマシンアルゴリズムは特定の最適化問題を導く。"
  },
  {
    "index": "F16714",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But as I briefly mentioned in an earlier video, I really do not recommend writing your own software to solve for the parameter's theta yourself.",
    "output": "だが以前のビデオで簡単に述べた通り、パラメータシータを解くソフトウェアを自力で書く事は、全く推奨しない。"
  },
  {
    "index": "F16715",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just as today, very few of us, or maybe almost essentially none of us would think of writing code ourselves to invert a matrix or take a square root of a number, and so on.",
    "output": "こんにちでは、我々の中には、自分で行列の逆行列を求めるコードや数のルートを計算するコードを書こう、とする人はかなり少数派、いやほとんど居ないと言っても良かろう。"
  },
  {
    "index": "F16716",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We just, you know, call some library function to do that.",
    "output": "我らは、これを行うライブラリ関数を呼ぶ。"
  },
  {
    "index": "F16717",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the same way, the software for solving the SVM optimization problem is very complex, and there have been researchers that have been doing essentially numerical optimization research for many years.",
    "output": "同様に、SVMの最適化問題を解くソフトウェアは、とても複雑だ。そして、本質的には数値計算の最適化だけを何年もやってるような研究者達、というのも存在している。"
  },
  {
    "index": "F16718",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you come up with good software libraries and good software packages to do this.",
    "output": "だから、あなたはこれを行ってくれる素晴らしいソフトウェアライブラリやソフトウェアパッケージを見つける事が出来る。"
  },
  {
    "index": "F16719",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then strongly recommend just using one of the highly optimized software libraries rather than trying to implement something yourself.",
    "output": "であるから、自分で実装しよう、なんて考えないでそれらの高度に最適化されたソフトウェアライブラリのどれかを使う事を強く推奨する。"
  },
  {
    "index": "F16720",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And there are lots of good software libraries out there.",
    "output": "良いソフトウェアライブラリは、たくさんあるから。"
  },
  {
    "index": "F16721",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The two that I happen to use the most often are the linear SVM but there are really lots of good software libraries for doing this that you know, you can link to many of the major programming languages that you may be using to code up learning algorithm.",
    "output": "私がもっとも良く使う物を二つ上げておくと、liblinearとlibsvmだが、これを行うソフトウェアライブラリはこれ以外にもたくさんあって、あなたが学習アルゴリズムを実装している言語にリンク出来る物もたくさんあるはずだ。"
  },
  {
    "index": "F16722",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Even though you shouldn't be writing your own SVM optimization software, there are a few things you need to do, though.",
    "output": "あなたはSVMの最適化ソフトウェアを独自実装すべきでは無いのだけれども、あたながやるべき事も、ちょっとは存在する。"
  },
  {
    "index": "F16723",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First is to come up with with some choice of the parameter's C.",
    "output": "まず最初に、パラメータCを選ばなくてはいけない。"
  },
  {
    "index": "F16724",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We talked a little bit of the bias/variance properties of this in the earlier video.",
    "output": "この事については前回のビデオの中で、バイアス/バリアンスの性質の話の時に言及した。"
  },
  {
    "index": "F16725",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Second, you also need to choose the kernel or the similarity function that you want to use.",
    "output": "二番目に、あなたが使うカーネル、あるいは類似度関数を選ぶ必要がある。"
  },
  {
    "index": "F16726",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So one choice might be if we decide not to use any kernel.",
    "output": "選択の一例としては、カーネルを使わない、という決断もあり得る。"
  },
  {
    "index": "F16727",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the idea of no kernel is also called a linear kernel.",
    "output": "そしてカーネルを使わない、というアイデアは線形(リニア)カーネルとも呼ばれる。"
  },
  {
    "index": "F16728",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if someone says, I use an SVM with a linear kernel, what that means is you know, they use an SVM without using without using a kernel and it was a version of the SVM that just uses theta transpose X, right, that predicts 1 theta 0 plus theta 1 X1 plus so on plus theta N, X N is greater than equals 0.",
    "output": "だから、誰かが「俺はSVMをリニアカーネルで使うぜ」と言ったら、それの意味する事は、SVMを、カーネル無しで使う、という事。そしてそれは、SVMを単にシータ転置xで使うというSVMのバージョンという事になる。"
  },
  {
    "index": "F16729",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This term linear kernel, you can think of this as you know this is the version of the SVM that just gives you a standard linear classifier.",
    "output": "この「線形」カーネルという言葉は、これは標準的な「線形」の分類器を与えるSVMのバージョンだ、と考える事が出来る。"
  },
  {
    "index": "F16730",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that would be one reasonable choice for some problems, and you know, there would be many software libraries, like linear, was one example, out of many, one example of a software library that can train an SVM without using a kernel, also called a linear kernel.",
    "output": "さて、これはある種の問題にはリーズナブルな選択となる。そして世の中にはたくさんのソフトウェアのライブラリ、たくさんあるそんなソフトウェアライブラリの中の一つの例としては、liblinearのような物とかが挙げられるが、とにかく、SVMをカーネル無しで、またの名を線形カーネルでトレーニング出来るソフトウェアライブラリはたくさんある。"
  },
  {
    "index": "F16731",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, why would you want to do this?",
    "output": "では、何故これを使いたい、と思うのだろうか?"
  },
  {
    "index": "F16732",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have a large number of features, if N is large, and M the number of training examples is small, then you know you have a huge number of features that if X, this is an X is an Rn, Rn +1.",
    "output": "もしあなたが、大量のフィーチャーを持っていて、つまりnが大きくて、そしてm、トレーニング手本の数が、小さければ、その時はつまり、あなたは大量のフィーチャーを持っているという事で、xはRnとかRn+1という事だ。"
  },
  {
    "index": "F16733",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you have a huge number of features already, with a small training set, you know, maybe you want to just fit a linear decision boundary and not try to fit a very complicated nonlinear function, because might not have enough data.",
    "output": "だから既にたくさんのフィーチャーがあって、トレーニングセットのサイズは小さい、この場合は単に線形の決定境界へのフィッティングを望むかもしれない。そしてあまり複雑な非線型の関数にフィッティングしたいとは思わないだろう。"
  },
  {
    "index": "F16734",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you might risk overfitting, if you're trying to fit a very complicated function in a very high dimensional feature space, but if your training set sample is small.",
    "output": "何故なら、十分なデータが無いと、オーバーフィッティングのリスクがあるからだ。とても次数の高いフィーチャー空間上のとても複雑な関数にフィッティングしようとしていて、しかもトレーニングセットのサイズが小さい時には。"
  },
  {
    "index": "F16735",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this would be one reasonable setting where you might decide to just not use a kernel, or equivalents to use what's called a linear kernel.",
    "output": "だからこそ、この状況はカーネルを使わない、または同じ事だが線形カーネルと呼ばれる物を使う、という選択をするのが合理的となる状況だ。"
  },
  {
    "index": "F16736",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "A second choice for the kernel that you might make, is this Gaussian kernel, and this is what we had previously.",
    "output": "あなたが行いそうな二番目の選択肢はこのガウスカーネルだ。これは以前に得た物だ。"
  },
  {
    "index": "F16737",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you do this, then the other choice you need to make is to choose this parameter sigma squared when we also talk a little bit about the bias variance tradeoffs of how, if sigma squared is large, then you tend to have a higher bias, lower variance classifier, but if sigma squared is small, then you have a higher variance, lower bias classifier.",
    "output": "そしてもしあなたがこれを選ぶなら、あなたが行わなくてはいけないさらなる選択としては、パラメータであるシグマ二乗を選ぶ事だ。これはバイアス-バリアンスのトレードオフの話でちょっと触れたが、シグマ二乗が大きければ、高バイアス、低バリアンスの傾向の分類器となり、逆にシグマ二乗が小さければ、高バリアンス、低バイアスの傾向の分類器となる。"
  },
  {
    "index": "F16738",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when would you choose a Gaussian kernel?",
    "output": "では、どんな時にガウスカーネルを使う事になるだろう?"
  },
  {
    "index": "F16739",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, if your omission of features X, I mean Rn, and if N is small, and, ideally, you know, if n is large, right, so that's if, you know, we have say, a two-dimensional training set, like the example I drew earlier.",
    "output": "それはだね。あなたの元々のフィーチャーxが、Rnとして、nが小さい時、そして理想的には、mが大きい時。"
  },
  {
    "index": "F16740",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So n is equal to 2, but we have a pretty large training set.",
    "output": "つまり、例えば二次元のトレーニングセットだとして、これは以前に書いた物と同様という事だが、つまりn=2だとして、だがトレーニングセットの総数は多いという場合。"
  },
  {
    "index": "F16741",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you know, I've drawn in a fairly large number of training examples, then maybe you want to use a kernel to fit a more complex nonlinear decision boundary, and the Gaussian kernel would be a fine way to do this.",
    "output": "つまり、かなりたくさんのトレーニング手本を描いた。するとその場合、もっと複雑な非線形の決定境界にフィットするようなカーネルを使いたくなる場合がある。"
  },
  {
    "index": "F16742",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'll say more towards the end of the video, a little bit more about when you might choose a linear kernel, a Gaussian kernel and so on.",
    "output": "そしてガウスカーネルはこれを行う良い方法だ。このビデオの最後の方で線形カーネルを使う場合はどういう時かガウスカーネルを選ぶのはどういう場合か、などについて、話す機会がある。"
  },
  {
    "index": "F16743",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if concretely, if you decide to use a Gaussian kernel, then here's what you need to do.",
    "output": "だが、もし具体的にガウスカーネルを使う、と決めたとすると、あなたがやるべき事はこれだ。"
  },
  {
    "index": "F16744",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Depending on what support vector machine software package you use, it may ask you to implement a kernel function, or to implement the similarity function.",
    "output": "あなたの使うサポートベクタマシンのソフトウェアパッケージによっては、カーネル関数を、あるいは類似度関数をあなたが実装する必要があるものもある。"
  },
  {
    "index": "F16745",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you're using an octave or MATLAB implementation of an SVM, it may ask you to provide a function to compute a particular feature of the kernel.",
    "output": "もしあなたがoctaveやMATLABのSVMの実装を用いるなら、カーネルの具体的なフィーチャーを計算する関数をあなたが提供する必要があるかもしれない。"
  },
  {
    "index": "F16746",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is really computing f subscript i for one particular value of i, where f here is just a single real number, so maybe I should move this better written f(i), but what you need to do is to write a kernel function that takes this input, you know, a training example or a test example whatever it takes in some vector X and takes as input one of the landmarks and but only I've come down X1 and X2 here, because the landmarks are really training examples as well.",
    "output": "これは本当は、fの下付き添字iをある特定の値に対して計算する、ここでこのfは単一の実数値にすぎない、だからここでは、f(i)と書くべきかもしれない。"
  },
  {
    "index": "F16747",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But what you need to do is write software that takes this input, you know, X1, X2 and computes this sort of similarity function between them and return a real number.",
    "output": "だがあなたがしなくてはいけない事は、これ、トレーニング手本やテスト手本や、とにかくなんであれあるベクトルxを入力として受け取り、そしてランドマークの一つを入力として受け取るが、ここでは私は単にこれらをx1,x2と呼ぶ、何故ならランドマークも実際はトレーニング手本だから。"
  },
  {
    "index": "F16748",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what some support vector machine packages do is expect you to provide this kernel function that take this input you know, X1, X2 and returns a real number.",
    "output": "あなたがやらなくてはいけない事は、この入力、x1とx2をうけとりそれらの間の、この種の類似度関数を計算して一つの実数を返すソフトウェアを書く事だ。"
  },
  {
    "index": "F16749",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then it will take it from there and it will automatically generate all the features, and so automatically take X and map it to f1, f2, down to f(m) using this function that you write, and generate all the features and train the support vector machine from there.",
    "output": "そこで幾つかのサポートベクタマシンのパッケージはあなたがこの種のカーネル関数、x1とx2を入力に取って実数を返すような関数を提供する事を期待していて、そしてそこから、自動的に全てのフィーチャーを生成して、つまり自動的に、xに対しあなたの書いたこの関数を用いてf1,f2,...とf(m)まで、マッピングする。そして全てのフィーチャーを生成して、そこからサポートベクタマシンを訓練する。"
  },
  {
    "index": "F16750",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But sometimes you do need to provide this function yourself.",
    "output": "だが、あなたは時々、この関数を自分で提供してやる必要がある。"
  },
  {
    "index": "F16751",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Other if you are using the Gaussian kernel, some SVM implementations will also include the Gaussian kernel and a few other kernels as well, since the Gaussian kernel is probably the most common kernel.",
    "output": "ガウスカーネルを使う時でも、幾つかのSVM実装はガウスカーネルも含んでいて、そしてそれ以外にも幾つかのカーネルを含んでいる物だ。"
  },
  {
    "index": "F16752",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Gaussian and linear kernels are really the two most popular kernels by far. Just one implementational note.",
    "output": "ガウスカーネルは恐らくもっとも一般的なカーネルなので、ガウスカーネルと線形カーネルは本当にぶっちぎりで大人気の二大カーネルなので、たぶんあるだろう。"
  },
  {
    "index": "F16753",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have features of very different scales, it is important to perform feature scaling before using the Gaussian kernel.",
    "output": "もしあなたのフィーチャーが、とても異なるスケールだったら、ガウスカーネルを使う前にフィーチャースケーリングをするのが大切だ。"
  },
  {
    "index": "F16754",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you imagine the computing the norm between X and l, right, so this term here, and the numerator term over there.",
    "output": "あなたがxとlの間のノルムを計算している所を、想像してみよう。このここの項は、こっちの分子の項だ。"
  },
  {
    "index": "F16755",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What this is doing, the norm between X and l, that's really saying, you know, let's compute the vector V, which is equal to X minus l.",
    "output": "これが行う事は、xとlの間のノルムで、それは実際には、、、、ベクトルvを計算してみよう。vはイコールx-lだとする。"
  },
  {
    "index": "F16756",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then let's compute the norm does vector V, which is the difference between X.",
    "output": "そして次に、このベクトルvのノルムを計算する。それはxとlとの間の差だ。"
  },
  {
    "index": "F16757",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because here X is in Rn, or Rn plus 1, but I'm going to ignore, you know, X0.",
    "output": "ここでxはRn、いや、Rn+1だが、x0は無視する事にする。"
  },
  {
    "index": "F16758",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's pretend X is an Rn, square on the left side is what makes this correct.",
    "output": "そこでxはRnだとしよう、左側を二乗するとこれは正しくなる。"
  },
  {
    "index": "F16759",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is equal to that, right? And so written differently, this is going to be X1 minus l1 squared, plus x2 minus l2 squared, plus dot dot dot plus Xn minus ln squared.",
    "output": "別の書き方で書くと、これはx1-l(1)の二乗に、足すことのx2-l(2)の二乗に、足すことの点点点、、、と足すことのxn-l(n)の二乗。"
  },
  {
    "index": "F16760",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And now if your features take on very different ranges of value.",
    "output": "そして今、あなたのフィーチャーがとても異なったレンジの値を取るとしよう。"
  },
  {
    "index": "F16761",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if your second feature, X2 is the number of bedrooms.",
    "output": "住宅の価格を予測する例で考えてみると、データは住宅に関するデータで、xは、最初のフィーチャーx1に関しては何千平方フィートの範囲を取り、しかし二番目のフィーチャーx2は寝室の数とすると、寝室の数は範囲としては1部屋から5部屋程度だろう、するとx1-l(1)は巨大になりえて、1000平方フィートとかに成り得るが、一方でx2-l(2)はもっと小さくなり、その場合には、この項、これらの距離は、本質的にはほとんど住居のサイズで支配されてしまい、寝室の数はほとんど無視されてしまう。"
  },
  {
    "index": "F16762",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if this is in the range of one to five bedrooms, then X1 minus l1 is going to be huge.",
    "output": "そうする事で、SVMが様々なフィーチャーについて同じような程度の関心を払うようにし、この例の住居のサイズのように他のフィーチャーを塗りつぶしてしまうような事が無い事を、保証出来る。"
  },
  {
    "index": "F16763",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This could be like a thousand squared, whereas X2 minus l2 is going to be much smaller and if that's the case, then in this term, those distances will be almost essentially dominated by the sizes of the houses and the number of bathrooms would be largely ignored. As so as, to avoid this in order to make a machine work well, do perform future scaling.",
    "output": "あなたがサポートベクタマシンを試みる時に、おそらくもっとも一般的であろう二つのカーネルは、線形(linear)カーネル、つまりカーネル無しか、または既に話したガウスカーネルだろう。"
  },
  {
    "index": "F16764",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that will sure that the SVM gives, you know, comparable amount of attention to all of your different features, and not just to in this example to size of houses were big movement here the features.",
    "output": "そしてちょっと注意を。"
  },
  {
    "index": "F16765",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When you try a support vector machines chances are by far the two most common kernels you use will be the linear kernel, meaning no kernel, or the Gaussian kernel that we talked about.",
    "output": "類似度関数ならば、どんな物でもカーネルとして使える、という訳では無い。"
  },
  {
    "index": "F16766",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just one note of warning which is that not all similarity functions you might come up with are valid kernels.",
    "output": "そしてガウスカーネル、線形カーネル、そしてたまに他の人が使ってるのを見かけるその他のカーネルでも、それらはある技術的条件を満たす必要がある。"
  },
  {
    "index": "F16767",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the Gaussian kernel and the linear kernel and other kernels that you sometimes others will use, all of them need to satisfy a technical condition.",
    "output": "それはMercerの定理と言われる物だ。これが必要となる理由は、サポートベクタマシンのアルゴリズム、全てのSVMの実装は、大量の賢い数値計算的な最適化のトリックを使ってる、パラメータのシータについて効率的に解く為に。"
  },
  {
    "index": "F16768",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's called Mercer's Theorem and the reason you need to this is because support vector machine algorithms or implementations of the SVM have lots of clever numerical optimization tricks.",
    "output": "そしてオリジナルのSVMの設計において、主な関心を、Mercerの定理と呼ばれる技術的条件を満たすカーネルだけに集中する、という決定がなされた。"
  },
  {
    "index": "F16769",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to solve for the parameter's theta efficiently and in the original design envisaged, those are decision made to restrict our attention only to kernels that satisfy this technical condition called Mercer's Theorem.",
    "output": "そうする事で、それらSVMパッケージの全てで、それら全てのSVMソフトウェアパッケージで大きなクラスの最適化を用いる事が出来るようになり、パラメータのシータをとても早く得られるようになる。"
  },
  {
    "index": "F16770",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what that does is, that makes sure that all of these SVM packages, all of these SVM software packages can use the large class of optimizations and get the parameter theta very quickly.",
    "output": "さて、結局の所、ほとんどの人が使うのは線形カーネルかガウスカーネルだ。"
  },
  {
    "index": "F16771",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what most people end up doing is using either the linear or Gaussian kernel, but there are a few other kernels that also satisfy Mercer's theorem and that you may run across other people using, although I personally end up using other kernels you know, very, very rarely, if at all.",
    "output": "だがそれ以外にも幾つかMercerの定理を満たすカーネルが存在する。そして他の人々がそれらを使ってる所をあなたがみかける事もあるかもしれない。"
  },
  {
    "index": "F16772",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just to mention some of the other kernels that you may run across.",
    "output": "私個人としては、それ以外のカーネルを使う事ってほんとにほんとーに稀で、全く無いって事は無いにせよほとんど無いけど。"
  },
  {
    "index": "F16773",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for that the similarity between X and l is defined as, there are a lot of options, you can take X transpose l squared.",
    "output": "その場合、xとlの間の類似度は、どう定義されるかというと、たくさんの選択肢があるが、xの転置lの二乗という定義がありうる。"
  },
  {
    "index": "F16774",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here's one measure of how similar X and l are.",
    "output": "これは、xとlがどれだけ類似してるかを測る、一つの指標である。"
  },
  {
    "index": "F16775",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If X and l are very close with each other, then the inner product will tend to be large.",
    "output": "xとlがお互いにとても近いと、内積は大きくなる傾向にある。"
  },
  {
    "index": "F16776",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, you know, this is a slightly unusual kernel.",
    "output": "このカーネルはちょっと珍しいカーネルで、そんなには使われていない。"
  },
  {
    "index": "F16777",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is not used that often, but you may run across some people using it.",
    "output": "だがたまには、使ってる人に出くわす事もあるかもしれない。"
  },
  {
    "index": "F16778",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is one version of a polynomial kernel.",
    "output": "これは多項式カーネルの一つのバージョンだ。"
  },
  {
    "index": "F16779",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Another is X transpose l cubed.",
    "output": "もう一つ別の物としては、x転置lの三乗。"
  },
  {
    "index": "F16780",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "These are all examples of the polynomial kernel.",
    "output": "これらは全て、多項式カーネルの例だ。"
  },
  {
    "index": "F16781",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "X transpose l plus maybe a number different then one 5 and, you know, to the power of 4 and so the polynomial kernel actually has two parameters.",
    "output": "そして4乗。つまり、多項式カーネルは実際には二つのパラメータがある。"
  },
  {
    "index": "F16782",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is, what number do you add over here?",
    "output": "一つはどの数をここに足すか?"
  },
  {
    "index": "F16783",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It could be 0.",
    "output": "これは0でも良い。"
  },
  {
    "index": "F16784",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is really plus 0 over there, as well as what's the degree of the polynomial over there.",
    "output": "同様に、ここの、多項式の次数(degree)がある。つまり累乗の指数。"
  },
  {
    "index": "F16785",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the degree power and these numbers.",
    "output": "そしてこれらの数字。"
  },
  {
    "index": "F16786",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the more general form of the polynomial kernel is X transpose l, plus some constant and then to some degree in the X1 and so both of these are parameters for the polynomial kernel.",
    "output": "多項式カーネルのより一般的な形としては、x転置l足すことのある定数(const)そして指数のある次数(degree)。つまりこれら二つの両方が多項式カーネルのパラメータだ。"
  },
  {
    "index": "F16787",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the polynomial kernel almost always or usually performs worse.",
    "output": "多項式カーネルは、ほとんどいつでもガウスカーネルよりもより劣ったパフォーマンスを示す。だからそんなに使われない。"
  },
  {
    "index": "F16788",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the Gaussian kernel does not use that much, but this is just something that you may run across.",
    "output": "でもたまに出くわす事がある物ではある。通常、これはxとlが全て0より大きい、非負のデータに対してのみ使われる。"
  },
  {
    "index": "F16789",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Usually it is used only for data where X and l are all strictly non negative, and so that ensures that these inner products are never negative.",
    "output": "そうする事で、これらの内積が決して負にならない事を担保している。"
  },
  {
    "index": "F16790",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this captures the intuition that X and l are very similar to each other, then maybe the inter product between them will be large.",
    "output": "そしてこれは、xとlがとてもお互いに近いとそれら同士の内積が大きくなるだろう、という直感を捕捉している。"
  },
  {
    "index": "F16791",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "They have some other properties as well but people tend not to use it much.",
    "output": "他の性質もあるんだが、何にせよ人々はあまり使ってない。"
  },
  {
    "index": "F16792",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then, depending on what you're doing, there are other, sort of more esoteric kernels as well, that you may come across.",
    "output": "そしてさらに、あなたがやっている事に応じて、ある種より難解なカーネルも存在して、あなたがそれらに遭遇する事もあるかもしれない。"
  },
  {
    "index": "F16793",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, there's a string kernel, this is sometimes used if your input data is text strings or other types of strings.",
    "output": "文字列カーネルというのがある、これはあなたの入力データがテキスト文字列なり、それ以外でも文字列の時には、時々使われる事がある。"
  },
  {
    "index": "F16794",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There are things like the chi-square kernel, the histogram intersection kernel, and so on.",
    "output": "カイ二乗カーネルという物もあり、ヒストグラム交差カーネルという物などもある。"
  },
  {
    "index": "F16795",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There are sort of more esoteric kernels that you can use to measure similarity between different objects.",
    "output": "これらは異なるオブジェクト同士の類似度を測る事が出来るある種より難解なカーネルだ。"
  },
  {
    "index": "F16796",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for example, if you're trying to do some sort of text classification problem, where the input x is a string then maybe we want to find the similarity between two strings using the string kernel, but I personally you know end up very rarely, if at all, using these more esoteric kernels.",
    "output": "例えば、あなたがある種のテキスト分類の問題に挑んでいる時には、入力xが文字列となるので、我らは二つの文字列の間の類似度を、文字列カーネルを用いる事で知りたいかもしれない。だが私個人としては、これらのより難解なカーネルは全く見ないとは言わないが、とても稀にしか見ない。"
  },
  {
    "index": "F16797",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I think I might have use the chi-square kernel, may be once in my life and the histogram kernel, may be once or twice in my life.",
    "output": "カイ2乗カーネルを、たぶん私は生涯で、一回しか使ったことが無いと思うし、ヒストグラムカーネルは人生で一回か二回だったと思う。"
  },
  {
    "index": "F16798",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I've actually never used the string kernel myself.",
    "output": "私は実は、文字列カーネルを自分で使った事は無い。"
  },
  {
    "index": "F16799",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in case you've run across this in other applications. You know, if you do a quick web search we do a quick Google search or quick Bing search you should have found definitions that these are the kernels as well.",
    "output": "しかしもしあなたが別のアプリケーションでこれらに遭遇したら、ちょろっとwebを検索して、Googleで軽く検索するなり、Bingで軽く検索すれば、これらのカーネルの定義を見つける事が出来るはずだ。"
  },
  {
    "index": "F16800",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just two last details I want to talk about in this video.",
    "output": "このビデオで話しておきたい細かい話が最後に二つ。"
  },
  {
    "index": "F16801",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One in multiclass classification.",
    "output": "一つはマルチクラスの分類問題について。"
  },
  {
    "index": "F16802",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you have four classes or more generally 3 classes output some appropriate decision bounday between your multiple classes.",
    "output": "4つのクラスがあったとしよう、あるいはもっと一般的にk個のクラスの出力があったとしよう。"
  },
  {
    "index": "F16803",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Most SVM, many SVM packages already have built-in multiclass classification functionality.",
    "output": "ほとんどのSVM、とは言い過ぎだが多くのSVMパッケージは、既にマルチクラスの分類の機能がビルドインされている。"
  },
  {
    "index": "F16804",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if your using a pattern like that, you just use the both that functionality and that should work fine.",
    "output": "だからもしあなたがそのようなパッケージを使っているなら、あなたは単に、ビルドインの機能を使うだけで、うまくいくはずだ。"
  },
  {
    "index": "F16805",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Otherwise, one way to do this is to use the one versus all method that we talked about when we are developing logistic regression.",
    "output": "そうで無ければ、これを行う方法の一つには、1vsallの手法を使う事だ、これについてはロジスティック回帰を作ってる時に議論した。"
  },
  {
    "index": "F16806",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what you do is you trade kSVM's if you have k classes, one to distinguish each of the classes from the rest.",
    "output": "あなたがやる事は、K個のSVMをトレーニングする、もしあなたがKクラスあったとしてだが、一度に一つのクラスをそれ以外のクラスと区別するように。"
  },
  {
    "index": "F16807",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this would give you k parameter vectors, so this will give you, upi lmpw.",
    "output": "つまりこれは、あなたにパラメータベクトルのシータ1を与え、これはクラスy=1をそれ以外のクラスから区別しようと試みる物で、そして次に二番目のパラメータベクトル、シータ2を与え、これはy=2を陽性のクラスとした時にそしてそれ以外全てを陰性のクラスとした時に得られるパラメータで、そうやってパラメータベクトルシータKまで、これは最後のクラスKをそれ以外と区別するパラメータベクトルだ。"
  },
  {
    "index": "F16808",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "theta 1, which is trying to distinguish class y equals one from all of the other classes, then you get the second parameter, vector theta 2, which is what you get when you, you know, have y equals 2 as the positive class and all the others as negative class and so on up to a parameter vector theta k, which is the parameter vector for distinguishing the final class key from anything else, and then lastly, this is exactly the same as the one versus all method we have for logistic regression.",
    "output": "最後に、これはまさにロジスティック回帰でやった1vsALL法だ。"
  },
  {
    "index": "F16809",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where we you just predict the class i with the largest theta transpose X.",
    "output": "そこではあなたはシータ転置xが最大になったクラスiを予想とするのだった。"
  },
  {
    "index": "F16810",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's multiclass classification designate.",
    "output": "以上がマルチクラスの分類の方法だ。"
  },
  {
    "index": "F16811",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For the more common cases that there is a good chance that whatever software package you use, you know, there will be a reasonable chance that are already have built in multiclass classification functionality, and so you don't need to worry about this result.",
    "output": "だがもっと一般的なケースとしては、だいたいは、かなり良い確率で、あなたが何のソフトウェアパッケージを使ってるにせよ、だいたいは、かなりの確率で、そこには既にマルチクラスの機能がビルドインされている。だからこの事について思い悩む必要は無い。"
  },
  {
    "index": "F16812",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, we developed support vector machines starting off with logistic regression and then modifying the cost function a little bit.",
    "output": "最後に、我らはサポートベクタマシンをロジスティック回帰から始めて、コスト関数をちょっと変更していく事で開発してきた。"
  },
  {
    "index": "F16813",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The last thing we want to do in this video is, just say a little bit about.",
    "output": "このビデオで最後にやりたい事は、これらの二つのアルゴリズムのどちらをいつ使うのか、についてちょっと話しておきたい。"
  },
  {
    "index": "F16814",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "when you will use one of these two algorithms, so let's say n is the number of features and m is the number of training examples.",
    "output": "nをフィーチャーの数として、mをトレーニング手本の数とする。"
  },
  {
    "index": "F16815",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, when should we use one algorithm versus the other?",
    "output": "さて、いつどちらのアルゴリズムを使い、いつもう一つのアルゴリズムを使うべきだろう?"
  },
  {
    "index": "F16816",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, if n is larger relative to your training set size, so for example, if you take a business with a number of features this is much larger than m and this might be, for example, if you have a text classification problem, where you know, the dimension of the feature vector is I don't know, maybe, 10 thousand.",
    "output": "もしnがトレーニングセットサイズとの相対的に大きければ、例を挙げると、フィーチャーの数がこれがmよりずっと大きくて、そしてこれが、例えば、テキスト分類の問題だとすると、フィーチャーのベクトルの次元は、知らんけど例えば1万とかになりがちで、そしてトレーニングセットのサイズが10から、せいぜい1000くらいまでの間。"
  },
  {
    "index": "F16817",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, imagine a spam classification problem, where email spam, where you have 10,000 features corresponding to 10,000 words but you have, you know, maybe 10 training examples or maybe up to 1,000 examples.",
    "output": "スパム分類の問題を想像してみよう。e-mailスパムでは、1万の単語に対応した1万個のフィーチャーがあるとする、でも例えば10通からせいぜい1000通とかのトレーニング手本しか無いとする。"
  },
  {
    "index": "F16818",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if n is large relative to m, then what I would usually do is use logistic regression or use it as the m without a kernel or use it with a linear kernel.",
    "output": "つまり、nはmとの相対で考えると大きい。その場合に私が普段やるのは、ロジスティック回帰を使うか、カーネル無しの、あるいは線形カーネルのSVMを使う。"
  },
  {
    "index": "F16819",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because, if you have so many features with smaller training sets, you know, a linear function will probably do fine, and you don't have really enough data to fit a very complicated nonlinear function.",
    "output": "何故なら、そんなにたくさんのフィーチャーで、トレーニングセットが小さいと、線形関数はたぶんいい感じだと思う、そしてとても複雑な非線形の関数をフィッティングするには十分なデータを持ってない。"
  },
  {
    "index": "F16820",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now if is n is small and m is intermediate what I mean by this is n is maybe anywhere from 1 - 1000, 1 would be very small.",
    "output": "今、もしnが小さくて、mが中くらいの大きさだと、ここでnとしては1から1000くらいをイメージしてて、1はとても小さい場合だが、せいぜい1000フィーチャーくらいまで、そしてトレーニング手本の総数が10から1万手本くらいの間のどこか位。"
  },
  {
    "index": "F16821",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe up to 50,000 examples.",
    "output": "5万くらいまでの間でもいいかもしれない。"
  },
  {
    "index": "F16822",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If m is pretty big like maybe 10,000 but not a million.",
    "output": "mがとても大きければ、1万くらい。だが100万は行かない。"
  },
  {
    "index": "F16823",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if m is an intermediate size then often an SVM with a linear kernel will work well.",
    "output": "さて、mがもし中くらいのサイズの時は、しばしばSVMに線形カーネルが、うまく機能するだろう。"
  },
  {
    "index": "F16824",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We talked about this early as well, with the one concrete example, this would be if you have a two dimensional training set.",
    "output": "この場合については以前にも話した、一つの具体例で。"
  },
  {
    "index": "F16825",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if n is equal to 2 where you have, you know, drawing in a pretty large number of training examples.",
    "output": "それは、もしあなたが二次元のトレーニングセットの時に、つまり、もしn=2の時で、たくさんのトレーニング手本が描いてある時などは、ガウスカーネルは、陽性と陰性のクラスを分離するのにきわめて良い働きをするだろう。"
  },
  {
    "index": "F16826",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One third setting that's of interest is if n is small but m is large.",
    "output": "興味のある三番目の状況としては、nが小さくてmが大きい時。"
  },
  {
    "index": "F16827",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You have very very large training set sizes, right. So if this is the case, then a SVM of the Gaussian Kernel will be somewhat slow to run.",
    "output": "例えばnは1から1000までとか、もうちょっと大きくてもいいかもしれないが、しかしmは5万くらいから100とかまでとか、5万から10万、100万、200万、と、とても大きなトレーニングセットのサイズの時、この場合は、ガウスカーネルのSVMは走らせるといくらか遅い。"
  },
  {
    "index": "F16828",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Today's SVM packages, if you're using a Gaussian Kernel, tend to struggle a bit.",
    "output": "こんにちのSVMパッケージでは、ガウスカーネルを使うと、5万くらいなら、たぶん問題無い。"
  },
  {
    "index": "F16829",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have, you know, maybe 50 thousands okay, but if you have a million training examples, maybe or even a 100,000 with a massive value of m.",
    "output": "だが、100万個のトレーニング手本だと、または10万個のトレーニング手本で、かつnが大きい値の時には、こんにちのSVMパッケージはとても良い物だが、それでも大量の、本当に大量のトレーニングセットの時にはガウスカーネルを使うとちょっとだけ苦戦するかもしれない。"
  },
  {
    "index": "F16830",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Today's SVM packages are very good, but they can still struggle a little bit when you have a massive, massive trainings that size when using a Gaussian Kernel.",
    "output": "その場合には、私が普段やるのは、手動でフィーチャーを増やしてロジスティック回帰を使うか、カーネル無しのSVMを試す。"
  },
  {
    "index": "F16831",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in that case, what I would usually do is try to just manually create have more features and then use logistic regression or an SVM without the Kernel.",
    "output": "そしてもしあなたがこのスライドを見てロジスティック回帰とカーネル無しのSVMがこれらの場所がいつもペアで一緒に出てきてると思ったなら、それには理由があるのだ。"
  },
  {
    "index": "F16832",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in case you look at this slide and you see logistic regression or SVM without a kernel.",
    "output": "ロジスティック回帰もカーネル無しのSVMも通常は極めて似た振る舞いをする。そして極めて似たパフォーマンスを与える。"
  },
  {
    "index": "F16833",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's a reason for that, is that logistic regression and SVM without the kernel, those are really pretty similar algorithms and, you know, either logistic regression or SVM without a kernel will usually do pretty similar things and give pretty similar performance, but depending on your implementational details, one may be more efficient than the other.",
    "output": "だが実装の詳細によっては、片方がもう片方よりも効率的だったりはするかもしれない。"
  },
  {
    "index": "F16834",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, where one of these algorithms applies, logistic regression where SVM without a kernel, the other one is to likely to work pretty well as well.",
    "output": "だが、これらのアルゴリズムの一つを適用する時、ロジスティック回帰とカーネル無しのSVMのどちらも、だいたい同様に機能する。"
  },
  {
    "index": "F16835",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But along with the power of the SVM is when you use different kernels to learn complex nonlinear functions.",
    "output": "だがSVMの威力は、複雑な非線型の関数を学習する為に別のカーネルを使う時に発揮される。"
  },
  {
    "index": "F16836",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this regime, you know, when you have maybe up to 10,000 examples, maybe up to 50,000. And your number of features, this is reasonably large.",
    "output": "そしてこの形態では、1万手本くらいまでとか、5万手本くらいまでとかで、そしてフィーチャーの数は、ちょうど良い程度に多い、これはとても良くある形態だ。"
  },
  {
    "index": "F16837",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That's a very common regime and maybe that's a regime where a support vector machine with a kernel kernel will shine.",
    "output": "そしてこの状況こそ、カーネルと共にサポートベクタマシンを用いる事が、光り輝く形態だ。"
  },
  {
    "index": "F16838",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You can do things that are much harder to do that will need logistic regression.",
    "output": "ロジスティック回帰を使ってではもっと大変になるような事をやる事が出来る。"
  },
  {
    "index": "F16839",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, where do neural networks fit in?",
    "output": "そして最後に、ニューラルネットワークが適合するのはどういう形態か?"
  },
  {
    "index": "F16840",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well for all of these problems, for all of these different regimes, a well designed neural network is likely to work well as well.",
    "output": "うーん、これら全ての問題に対して、これらの別々の形態全てに対して、良く設計されたニューラルネットワークは、うまく機能するだろう。"
  },
  {
    "index": "F16841",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The one disadvantage, or the one reason that might not sometimes use the neural network is that, for some of these problems, the neural network might be slow to train.",
    "output": "一つの欠点としては、あるいはニューラルネットワークを使わない事がある理由の一つには、これらの問題の中には、ニューラルネットワークは訓練するのに遅いという場合がある。"
  },
  {
    "index": "F16842",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you have a very good SVM implementation package, that could run faster, quite a bit faster than your neural network.",
    "output": "だがもしとても良いSVM実装パッケージがあるなら、そちらの方が速く走りうる、ニューラルネットワークよりずっと速く走る。"
  },
  {
    "index": "F16843",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, although we didn't show this earlier, it turns out that the optimization problem that the SVM has is a convex optimization problem and so the good SVM optimization software packages will always find the global minimum or something close to it.",
    "output": "そして、これは証明しなかったが、SVMの持つ最適化の問題は、凸最適化問題である事が知られている。だから良いSVM最適化ソフトウェアパッケージは必ずグローバル最小か、それに近い値を探してくれる。"
  },
  {
    "index": "F16844",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so for the SVM you don't need to worry about local optima.",
    "output": "だからSVMでは、ローカル最適の問題を心配する必要は無い。"
  },
  {
    "index": "F16845",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In practice local optima aren't a huge problem for neural networks but they all solve, so this is one less thing to worry about if you're using an SVM.",
    "output": "現実問題としては、ニューラルネットワークでもローカル最適はそんなに大きな問題では無い。だけど、、、あー、とにかくSVMを使えば、心配事が一つ減ると言えば減る。"
  },
  {
    "index": "F16846",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And depending on your problem, the neural network may be slower, especially in this sort of regime than the SVM.",
    "output": "そして問題によっては、ニューラルネットワークの方が遅いかもしれない、特にこの形態の問題では、SVMよりも遅いかもしれない。"
  },
  {
    "index": "F16847",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case the guidelines they gave here, seem a little bit vague and if you're looking at some problems, you know, the guidelines are a bit vague, I'm still not entirely sure, should I use this algorithm or that algorithm, that's actually okay.",
    "output": "ここに示したガイドラインがちょっと曖昧だなぁ、と思っても、もしあなたが何かの問題に際して、ガイドラインがいまいち曖昧だなぁ、と思っても、私もいまだに完全には確信が持てる訳では無い、私はこっちのアルゴリズムを使うべきか?"
  },
  {
    "index": "F16848",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When I face a machine learning problem, you know, sometimes its actually just not clear whether that's the best algorithm to use, but as you saw in the earlier videos, really, you know, the algorithm does matter, but what often matters even more is things like, how much data do you have.",
    "output": "それは実際には問題無い。私が機械学習の問題に直面する時には、時々、どのアルゴリズムを使うのがベストなのか、はっきりしない事がある。"
  },
  {
    "index": "F16849",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And how skilled are you, how good are you at doing error analysis and debugging learning algorithms, figuring out how to design new features and figuring out what other features to give you learning algorithms and so on.",
    "output": "だが以前のビデオで見た通り、アルゴリズムは確かに重要だけど、しかししばしばもっと重要なのは、どれだけの量のデータを持ってるか、そしてどれだけあなたの技術力が高いか、エラー分析や学習アルゴリズムをデバッグするのをどれだけうまく行えるか、新しいフィーチャーをどうデザインする方法をどれだけうまく見いだせるか、学習アルゴリズムに渡す別のフィーチャーをどれだけ上手く見いだせるか、などだったりする。"
  },
  {
    "index": "F16850",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And often those things will matter more than what you are using logistic regression or an SVM.",
    "output": "そしてしばしば、これらの事項はあなたがロジスティック回帰を使うかSVMを使うか、よりももっと重要な事だろう。"
  },
  {
    "index": "F16851",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But having said that, the SVM is still widely perceived as one of the most powerful learning algorithms, and there is this regime of when there's a very effective way to learn complex non linear functions.",
    "output": "だが、言ってきたように、SVMはいまだに、もっとも強力な学習アルゴリズムの一つだと広く受け止められている。そしてSVMが複雑な非線型の関数をとても良く学習出来るような事態がある。"
  },
  {
    "index": "F16852",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so I actually, together with logistic regressions, neural networks, SVM's, using those to speed learning algorithms you're I think very well positioned to build state of the art you know, machine learning systems for a wide region for applications and this is another very powerful tool to have in your arsenal.",
    "output": "そして私も実際、、、ロジスティック回帰、ニューラルネットワーク、SVM、これら三つの学習アルゴリズムを使えるなら、私が思うにあなたはとても広範な応用に対し最先端の機械学習システムを構築するのにとても良い位置に居る。そしてこれはもう一つ、あなたの武器庫に備えておくのに良い、とてもパワフルなツールという訳だ。"
  },
  {
    "index": "F16853",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One that is used all over the place in Silicon Valley, or in industry and in the Academia, to build many high performance machine learning system.",
    "output": "この一つは、シリコンバレー中で業界中で、アカデミックな世界で、多くの高パフォーマンスの機械学習システムを作るのに使われている物だ。"
  },
  {
    "index": "F16854",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to start to talk about clustering.",
    "output": "このビデオでは、クラスタリングについて議論しはじめたい。"
  },
  {
    "index": "F16855",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This will be exciting, because this is our first unsupervised learning algorithm, where we learn from unlabeled data instead from labelled data.",
    "output": "これはエキサイティングな話だ、というのはこれが我らの初めての教師なし学習の例だからだ。そこではラベル付けされたデータではなく、ラベル付けされていないデータから学習する。"
  },
  {
    "index": "F16856",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what is unsupervised learning?",
    "output": "では、教師なし学習ってなんだろう?"
  },
  {
    "index": "F16857",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I briefly talked about unsupervised learning at the beginning of the class but it's useful to contrast it with supervised learning.",
    "output": "この講義の始めの所で簡単に述べたけど、ここで再び教師有り学習と対比してみるのは有益に思う。"
  },
  {
    "index": "F16858",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here's a typical supervised learning problem where we're given a labeled training set and the goal is to find the decision boundary that separates the positive label examples and the negative label examples.",
    "output": "その為に、これは典型的な教師有り問題だ、そこではラベルづけされたトレーニングセットが与えられてゴールは陽性の手本と陰性の手本を分離する決定境界を探す事。"
  },
  {
    "index": "F16859",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, the supervised learning problem in this case is given a set of labels to fit a hypothesis to it.",
    "output": "この場合、教師有り学習の問題はラベルの集まりを与えられて、それに仮説をフィットさせる、という事になる。"
  },
  {
    "index": "F16860",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast, in the unsupervised learning problem we're given data that does not have any labels associated with it.",
    "output": "対して、教師なし学習では、関連したラベルが無いデータが与えられる。"
  },
  {
    "index": "F16861",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we're given data that looks like this.",
    "output": "つまりこんな感じのデータを与えられる。"
  },
  {
    "index": "F16862",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's a set of points add in no labels, and so, our training set is written just x1, x2, and so on up to x m and we don't get any labels y.",
    "output": "ここに点の集まりがあり、ラベルが無い。つまり、トレーニングセットは単にx(1)、x(2)、、、とx(m)まで書かれているだけで、いかなるラベルyも存在しない。"
  },
  {
    "index": "F16863",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's why the points plotted up on the figure don't have any labels with them.",
    "output": "だから上の図にプロットされた点はラベルが無いのだ。"
  },
  {
    "index": "F16864",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in unsupervised learning what we do is we give this sort of unlabeled training set to an algorithm and we just ask the algorithm find some structure in the data for us.",
    "output": "つまり教師なし学習においては、こんな感じのラベル付けされていないトレーニングセットがある時に、アルゴリズムにこう尋ねるのだ、データのある構造を探し出してちょうだいな、と。"
  },
  {
    "index": "F16865",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given this data set one type of structure we might have an algorithm find is that it looks like this data set has points grouped into two separate clusters and so an algorithm that finds clusters like the ones I've just circled is called a clustering algorithm.",
    "output": "このデータセットがあった時に、アルゴリズムに探させる構造として一つ考えられるのは、データセットは点を2つのクラスタのグループに分けられそう。"
  },
  {
    "index": "F16866",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this would be our first type of unsupervised learning, although there will be other types of unsupervised learning algorithms that we'll talk about later that finds other types of structure or other types of patterns in the data other than clusters.",
    "output": "そしてこれは、我らの最初の教師なし学習の最初の例となるだろう。他の種類の教師なし学習、それはあとで扱うが、つまり別の種類の構造またはパターンをデータから見つける物だ、クラスタ以外の。"
  },
  {
    "index": "F16867",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what is clustering good for?",
    "output": "では、クラスタリングは何に使える?"
  },
  {
    "index": "F16868",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Early in this class I already mentioned a few applications.",
    "output": "このクラスの最初の方で、幾つかの応用例に触れた。"
  },
  {
    "index": "F16869",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is market segmentation where you may have a database of customers and want to group them into different marker segments so you can sell to them separately or serve your different market segments better.",
    "output": "そこでは顧客のデータベースがあって、彼らを異なるマーケットセグメントごとにグルーピングしたい。異なるセグメントごとに別々に売ったり、セグメントに合わせて対応する事でより良いサービスを提供したり出来るように。"
  },
  {
    "index": "F16870",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There are actually groups have done this things like looking at a group of people's social networks.",
    "output": "ソーシャル・ネットワーク分析、というのをやっている人々もいて、たとえば人々のグループを調べる、SNSつまりFacebookやGoogle+やあなたがもっともたくさんメールを送っているか、もっともemailを送り合っているのは誰か?"
  },
  {
    "index": "F16871",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, things like Facebook, Google+, or maybe information about who other people that you email the most frequently and who are the people that they email the most frequently and to find coherence in groups of people.",
    "output": "という情報から人々の同質なグループを見つけ出す。"
  },
  {
    "index": "F16872",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this would be another maybe clustering algorithm where you know want to find who are the coherent groups of friends in the social network?",
    "output": "これもある種のクラスタリングのアルゴリズムと言えるかもしれない、そこではソーシャル・ネットワークの中の友達の同質なグループを見つけ出したい、という。"
  },
  {
    "index": "F16873",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's something that one of my friends actually worked on which is, use clustering to organize computer clusters or to organize data centers better.",
    "output": "これは私の友人が実際にやってる例で、コンピュータのクラスタやデータセンターをより良く構成する為にクラスタリングを使う。"
  },
  {
    "index": "F16874",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because if you know which computers in the data center in the cluster tend to work together, you can use that to reorganize your resources and how you layout the network and how you design your data center communications.",
    "output": "というのはどのコンピュータがデータセンターにある他のクラスタと一緒に仕事をする傾向にある、と分かれば、それを用いてリソースやネットワークのレイアウトやデータセンターやそのコミュニケーションのデザインをするのに使えるからだ。"
  },
  {
    "index": "F16875",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And lastly, something that actually another friend worked on using clustering algorithms to understand galaxy formation and using that to understand astronomical data.",
    "output": "最後にもう一つ、実際に私がやってた仕事で、銀河の形成を理解するのにクラスタリングアルゴリズムを使う、というのがある。それを用いて、天文学的な詳細を理解する方法。"
  },
  {
    "index": "F16876",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's clustering which is our first example of an unsupervised learning algorithm.",
    "output": "以上がクラスタリング、我らの最初の教師なし学習の例となるアルゴリズムだ。"
  },
  {
    "index": "F16877",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video we'll start to talk about a specific clustering algorithm.",
    "output": "次のビデオでは、具体的なクラスタリングアルゴリズムについて扱っていく。"
  },
  {
    "index": "F16878",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the clustering problem we are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us.",
    "output": "クラスタリングの問題では、ラベル付けされていないデータセットが渡されて、アルゴリズムに自動で互いに密接なサブセット、または互いに密接なクラスタにグループ分けして欲しい。"
  },
  {
    "index": "F16879",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The K Means algorithm is by far the most popular, by far the most widely used clustering algorithm, and in this video I would like to tell you what the K Means Algorithm is and how it works.",
    "output": "K-Meansアルゴリズムはずば抜けて人気のある、ずば抜けて広く使われているクラスタリングアルゴリズムだ。このビデオでは、K-Meansアルゴリズムとは何か、それがどう機能するかを話していきたい。"
  },
  {
    "index": "F16880",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The K means clustering algorithm is best illustrated in pictures.",
    "output": "K-Meansクラスタリングアルゴリズムは絵で表すのが一番。"
  },
  {
    "index": "F16881",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say I want to take an unlabeled data set like the one shown here, and I want to group the data into two clusters.",
    "output": "ここに見せたようなラベルの無いデータセットがあるとしよう。そしてこのデータを2つのクラスタにグループ分けしたい。"
  },
  {
    "index": "F16882",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I run the K Means clustering algorithm, here is what I'm going to do.",
    "output": "K-Meansアルゴリズムを実行するとしたらこれがやるべきことだ。"
  },
  {
    "index": "F16883",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first step is to randomly initialize two points, called the cluster centroids.",
    "output": "最初の一歩はランダムに2つの点を選ぶ、これはクラスタの重心と呼ばれる。"
  },
  {
    "index": "F16884",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, these two crosses here, these are called the Cluster Centroids and I have two of them because I want to group my data into two clusters.",
    "output": "そう、これら2つのバッテンが、クラスタ重心と呼ばれる物だ。そしてそれが2つなのは、データを2つのクラスタにグループ分けしたいから。"
  },
  {
    "index": "F16885",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "K Means is an iterative algorithm and it does two things.",
    "output": "K-Meansはイテレーティブなアルゴリズムで、2つの事をする。"
  },
  {
    "index": "F16886",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First is a cluster assignment step, and second is a move centroid step.",
    "output": "最初はクラスタの割り付けステップ。二番目は重心移動ステップ。"
  },
  {
    "index": "F16887",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let me tell you what those things mean.",
    "output": "それらが何を意味するか解説していこう。"
  },
  {
    "index": "F16888",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first of the two steps in the loop of K means, is this cluster assignment step.",
    "output": "K-Meansのループの中の2つのステップの内、最初の方は、クラスタ割り付けのステップだ。"
  },
  {
    "index": "F16889",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What that means is that, it's going through each of the examples, each of these green dots shown here and depending on whether it's closer to the red cluster centroid or the blue cluster centroid, it is going to assign each of the data points to one of the two cluster centroids.",
    "output": "その意味する所は各手本を見ていって、ここで示したのだと、この緑のドットを見ていき、赤のクラスタ重心と青のクラスタ重心のどちらと近いかによって、各データポイントを2つのクラスタのうちのどちらかに割り振る。"
  },
  {
    "index": "F16890",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Specifically, what I mean by that, is to go through your data set and color each of the points either red or blue, depending on whether it is closer to the red cluster centroid or the blue cluster centroid, and I've done that in this diagram here.",
    "output": "具体的にその意味する所を見ると、データセットを見ていって、各点を赤か青に色付けしていく、赤のクラスタ重心に近いか青のクラスタ重心に近いかによって。この図ではそれを実際にやってみた。"
  },
  {
    "index": "F16891",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that was the cluster assignment step.",
    "output": "以上がクラスタ割り付けステップ。"
  },
  {
    "index": "F16892",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The other part of K means, in the loop of K means, is the move centroid step, and what we are going to do is, we are going to take the two cluster centroids, that is, the red cross and the blue cross, and we are going to move them to the average of the points colored the same colour.",
    "output": "K-Meansのループ内を構成するもう一方は重心の移動ステップだ。我らがやるべきことは、2つのクラスタの重心を、つまり、赤のバッテンと青のバッテンを、同じ色で色付けされた点の平均へと移動する。"
  },
  {
    "index": "F16893",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what we are going to do is look at all the red points and compute the average, really the mean of the location of all the red points, and we are going to move the red cluster centroid there.",
    "output": "つまり我らがやる事は、全ての赤の点を見て、平均を計算し、それは真に全ての赤い点の平均だが、赤のクラスタの重心をそこへ移動する。"
  },
  {
    "index": "F16894",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the same things for the blue cluster centroid, look at all the blue dots and compute their mean, and then move the blue cluster centroid there.",
    "output": "同じ事を青のクラスタ重心にも行う。青い点を全て見て、平均を計算し、青のクラスタ重心をそこへ移動する。"
  },
  {
    "index": "F16895",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let me do that now.",
    "output": "ではやってみよう。"
  },
  {
    "index": "F16896",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to move the cluster centroids as follows and I've now moved them to their new means.",
    "output": "クラスタ重心を以下のように動かし、それらは新しい平均へと移動した。赤いのはこんな感じで動き、青いのはこんな感じで動いた。"
  },
  {
    "index": "F16897",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The red one moved like that and the blue one moved like that and the red one moved like that.",
    "output": "そして赤いのはこんな感じで動いた。"
  },
  {
    "index": "F16898",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we go back to another cluster assignment step, so we're again going to look at all of my unlabeled examples and depending on whether it's closer the red or the blue cluster centroid, I'm going to color them either red or blue.",
    "output": "そして次にあらたなクラスタ割り当てステップに戻る。つまりまたラベルづけされていない手本を全て見ていって、青と赤のどちらの重心に近いかによって、それらを赤か青に色付けする。"
  },
  {
    "index": "F16899",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to assign each point to one of the two cluster centroids, so let me do that now.",
    "output": "各点に2つのクラスタ重心のどちらかを割り振る、という事なので、やってみよう。"
  },
  {
    "index": "F16900",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the colors of some of the points just changed.",
    "output": "幾つかの点の色は変わった。"
  },
  {
    "index": "F16901",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then I'm going to do another move centroid step.",
    "output": "そしてまた、重心移動のステップに進む。"
  },
  {
    "index": "F16902",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I'm going to compute the average of all the blue points, compute the average of all the red points and move my cluster centroids like this, and so, let's do that again.",
    "output": "つまり全ての青の点の平均を計算し、全ての赤の点の平均を計算し、クラスタ重心をこんな感じで移動する。やってみよう。"
  },
  {
    "index": "F16903",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me do one more cluster assignment step.",
    "output": "クラスタ割り振りステップをもう一回やってみよう。"
  },
  {
    "index": "F16904",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So colour each point red or blue, based on what it's closer to and then do another move centroid step and we're done.",
    "output": "各点を赤か青に色分けする、とちらに近いかに基づいてそして次に重心移動のステップを行う。"
  },
  {
    "index": "F16905",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further.",
    "output": "そして実の所、ここからさらにK-Meansのイテレーションを走らせ続けても、クラスタ重心はこれ以上移動しない。そして点の色もこれ以上は変わらない。"
  },
  {
    "index": "F16906",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, this is the, at this point, K means has converged and it's done a pretty good job finding the two clusters in this data.",
    "output": "つまり、これがこここそが、K-Meansが収束する点だ。そしてそれは、このデータの2つのクラスタを探すには、かなり良い仕事をしている。"
  },
  {
    "index": "F16907",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's write out the K means algorithm more formally.",
    "output": "ではK-Meansのアルゴリズムをよりフォーマルに記述しよう。"
  },
  {
    "index": "F16908",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The K means algorithm takes two inputs.",
    "output": "K-Meansアルゴリズムは2つの入力を取る。"
  },
  {
    "index": "F16909",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is a parameter K, which is the number of clusters you want to find in the data.",
    "output": "一つ目はパラメータK、それはデータの中から見つけたいクラスタの数。"
  },
  {
    "index": "F16910",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'll later say how we might go about trying to choose k, but for now let's just say that we've decided we want a certain number of clusters and we're going to tell the algorithm how many clusters we think there are in the data set.",
    "output": "あとでどうやってこのKをどうやって選んだらいいかの話をするつもりだが、今のところはあるクラスタの数を既に決めた、としておこう。そして幾つのクラスタがデータセットにあるかをアルゴリズムに伝えるとする。"
  },
  {
    "index": "F16911",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then K means also takes as input this sort of unlabeled training set of just the Xs and because this is unsupervised learning, we don't have the labels Y anymore.",
    "output": "単なるxだけ。これは教師なし学習だから、もうラベルyは無い。"
  },
  {
    "index": "F16912",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for unsupervised learning of the K means I'm going to use the convention that XI is an RN dimensional vector.",
    "output": "そしてK-Meansの教師なし学習に対してはxiをRnのベクトルを表すというコンベンションを用いることにする。"
  },
  {
    "index": "F16913",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's why my training examples are now N dimensional rather N plus one dimensional vectors. This is what the K means algorithm does.",
    "output": "そんな訳でトレーニング手本はいまや、n+1次元では無くn次元のベクトルとなる。"
  },
  {
    "index": "F16914",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so in the earlier diagram, the cluster centroids corresponded to the location of the red cross and the location of the blue cross.",
    "output": "これがK-Meansアルゴリズムがやる事だ:最初のステップはランダムにK個の重心を選ぶ、それをミュー1、ミュー2、、、ミューkと名付けるつまり、前の図だと、クラスタ重心は赤のバッテンと青のバッテンの場所に対応してた。"
  },
  {
    "index": "F16915",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there we had two cluster centroids, so maybe the red cross was mu 1 and the blue cross was mu 2, and more generally we would have k cluster centroids rather than just 2.",
    "output": "つまりそれは、2つのクラスタ重心があったという事で、たとえば赤のバッテンがミュー1で、青のバッテンがミュー2だった、という事。"
  },
  {
    "index": "F16916",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then the inner loop of k means does the following, we're going to repeatedly do the following.",
    "output": "そして今度はより一般的に2つだけじゃなくて、K個のクラスタ重心を考えていく。"
  },
  {
    "index": "F16917",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First for each of my training examples, I'm going to set this variable CI to be the index 1 through K of the cluster centroid closest to XI.",
    "output": "するとK-Meansの内側のループは以下を実行する、つまり以下を繰り返し実行する事になる:まず、各トレーニング手本に対してこの変数c(i)をxiに一番近いクラスタ重心のインデックスをセットする。"
  },
  {
    "index": "F16918",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this was my cluster assignment step, where we took each of my examples and coloured it either red or blue, depending on which cluster centroid it was closest to.",
    "output": "つまりこれは、クラスタ割り付けステップにあたる。そこでは各サンプルに対してそれを赤か青か、そのどちらの重心に近いかに基づいて色分けしていく。"
  },
  {
    "index": "F16919",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So CI is going to be a number from 1 to K that tells us, you know, is it closer to the red cross or is it closer to the blue cross, and another way of writing this is I'm going to, to compute Ci, I'm going to take my Ith example Xi and and I'm going to measure it's distance to each of my cluster centroids, this is mu and then lower-case k, right, so capital K is the total number centroids and I'm going to use lower case k here to index into the different centroids.",
    "output": "つまりc(i)は1からKまでの数で、その値は我らにそれが赤のバッテンか青のバッテンか、どちらに近いかを教えてくれる。これの他の書き方としては、ciを計算するのに、、、i番目の手本xiを取ってきてそれと個々のクラスタ重心との距離を測る。"
  },
  {
    "index": "F16920",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But so, Ci is going to, I'm going to minimize over my values of k and find the value of K that minimizes this distance between Xi and the cluster centroid, and then, you know, the value of k that minimizes this, that's what gets set in Ci.",
    "output": "そしてciをkの値に関して最小化する、そしてこのxiとクラスタ重心の距離を最小化するkを探す。そして、あー、これを最小化する値kを、そのkをciに代入する。"
  },
  {
    "index": "F16921",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here's another way of writing out what Ci is.",
    "output": "以上がciとは何なのか?を記述するもう一つの書き方。"
  },
  {
    "index": "F16922",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I write the norm between Xi minus Mu-k, then this is the distance between my ith training example Xi and the cluster centroid Mu subscript K, this is--this here, that's a lowercase K.",
    "output": "xiマイナスミューkのノルム、と書いた時は、これはi番目のトレーニング手本とクラスター重心のミュー下付き添字kとの距離を表す。これ、このこれは小文字のk。"
  },
  {
    "index": "F16923",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So uppercase K is going to be used to denote the total number of cluster centroids, and this lowercase K's a number between one and capital K.",
    "output": "つまり大文字のKはクラスタ重心の総数を表すのに使う。"
  },
  {
    "index": "F16924",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm just using lower case K to index into my different cluster centroids.",
    "output": "そしてこの小文字のkは1から大文字のKまでの間の数字で異なるクラスタ重心同士を識別するのに小文字のkを使う。"
  },
  {
    "index": "F16925",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next is lower case k.",
    "output": "それが小文字のk。"
  },
  {
    "index": "F16926",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the distance between the example and the cluster centroid and so what I'm going to do is find the value of K, of lower case k that minimizes this, and so the value of k that minimizes you know, that's what I'm going to set as Ci, and by convention here I've written the distance between Xi and the cluster centroid, by convention people actually tend to write this as the squared distance.",
    "output": "これがサンプルとクラスタ重心の距離で、我らがやるべき事はこれを最小化するk、このkは小文字のkだが、それを探す。そしてその最小化するkの値をciにセットする。"
  },
  {
    "index": "F16927",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we think of Ci as picking the cluster centroid with the smallest squared distance to my training example Xi.",
    "output": "つまりciを、トレーニング手本xiとの二乗距離が最小になるクラスタ重心を選びとった物と考える事が出来る。"
  },
  {
    "index": "F16928",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But of course minimizing squared distance, and minimizing distance that should give you the same value of Ci, but we usually put in the square there, just as the convention that people use for K means.",
    "output": "だがもちろん、距離の二乗を最小化しようと、距離を最小化しようと、同じciの値になるはず。でも普通は二乗をつける。"
  },
  {
    "index": "F16929",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that was the cluster assignment step.",
    "output": "以上がクラスタ割り付けステップ。"
  },
  {
    "index": "F16930",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The other in the loop of K means does the move centroid step.",
    "output": "K-Meansのループの中の他の仕事は重心移動のステップだ。"
  },
  {
    "index": "F16931",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what that does is for each of my cluster centroids, so for lower case k equals 1 through K, it sets Mu-k equals to the average of the points assigned to cluster.",
    "output": "そこで行うのは、クラスタ重心に対し、つまり小文字のkを1から大文字のKまでの範囲で、ミューkにその重心に関連付けられた点たちの平均を代入する。"
  },
  {
    "index": "F16932",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So as a concrete example, let's say that one of my cluster centroids, let's say cluster centroid two, has training examples, you know, 1, 5, 6, and 10 assigned to it.",
    "output": "具体例としては、クラスタ重心の一つ、クラスタ重心2としよう、それがトレーニング手本を持ってるとして、それは1、5、6、10に、それが割り振られているとする。"
  },
  {
    "index": "F16933",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what this means is, really this means that C1 equals to C5 equals to C6 equals to and similarly well c10 equals, too, right?",
    "output": "その意味するところは、c1=c5=c6=...と、c10も同様にイコールだ。"
  },
  {
    "index": "F16934",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we got that from the cluster assignment step, then that means examples 1,5,6 and 10 were assigned to the cluster centroid two.",
    "output": "クラスタ割り振りのステップでそれを得たとすると、それはつまり、手本1、5、6、10はクラスタ重心2が割り付けられている。"
  },
  {
    "index": "F16935",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then in this move centroid step, what I'm going to do is just compute the average of these four things.",
    "output": "次にこの重心移動のステップでは、そこでやるべきは単にこれら4つの平均を取るという事。"
  },
  {
    "index": "F16936",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And now I'm going to average them so here I have four points assigned to this cluster centroid, just take one quarter of that.",
    "output": "そして今、それらを平均したいのだから、このクラスタには点が4つ割り振られているのだから、1/4を取る。"
  },
  {
    "index": "F16937",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And now Mu2 is going to be an n-dimensional vector.",
    "output": "するとミュー2はn次元ベクトルとなる。"
  },
  {
    "index": "F16938",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because each of these example x1, x5, x6, x10 each of them were an n-dimensional vector, and I'm going to add up these things and, you know, divide by four because I have four points assigned to this cluster centroid, I end up with my move centroid step, for my cluster centroid mu-2.",
    "output": "何故なら各手本、x1、x5、x6、x10は、どれもn次元ベクトルだったから。そしてこれらを足し合わせて、4で割ってる、だってこのクラスタ重心には4つの点が割り振られているから。"
  },
  {
    "index": "F16939",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This has the effect of moving mu-2 to the average of the four points listed here.",
    "output": "これはミュー2をここに列挙した4つの点の平均となる。"
  },
  {
    "index": "F16940",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One thing that I've asked is, well here we said, let's let mu-k be the average of the points assigned to the cluster.",
    "output": "よく質問される事として、今、ミューkをクラスタに割り振られた点の平均にしよう、と言ったが、もし点を割り振られないクラスタ重心、点が0個しか割り振られないクラスタ重心があったら、どうしたらいい?"
  },
  {
    "index": "F16941",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case the more common thing to do is to just eliminate that cluster centroid.",
    "output": "その場合、一番普通の対応はたんにそのクラスタ重心を取り除く。"
  },
  {
    "index": "F16942",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you do that, you end up with K minus one clusters instead of k clusters.",
    "output": "そうすると、最終結果はK個のクラスタではなくてK-1個のクラスタとなる。時々、ほんとうにK個のクラスタが必要な場合もある。"
  },
  {
    "index": "F16943",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Sometimes if you really need k clusters, then the other thing you can do if you have a cluster centroid with no points assigned to it is you can just randomly reinitialize that cluster centroid, but it's more common to just eliminate a cluster if somewhere during K means it with no points assigned to that cluster centroid, and that can happen, altthough in practice it happens not that often.",
    "output": "その場合にやる別の手段としては、もし点が割り振られないクラスタ重心があったら、単にそのクラスタ重心をランダムに再初期化する。でも単に取り除く方が普通だね。"
  },
  {
    "index": "F16944",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the K means Algorithm.",
    "output": "以上がK-Meansアルゴリズム。"
  },
  {
    "index": "F16945",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Before wrapping up this video I just want to tell you about one other common application of K Means and that's to the problems with non well separated clusters.",
    "output": "このビデオのまとめに入る前に、もう一つの良くあるK-Meansの応用を話しておきたい。それは、あまり綺麗に分かれていないクラスタの問題だ。"
  },
  {
    "index": "F16946",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's what I mean.",
    "output": "それはこんな意味だ。"
  },
  {
    "index": "F16947",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So far we've been picturing K Means and applying it to data sets like that shown here where we have three pretty well separated clusters, and we'd like an algorithm to find maybe the 3 clusters for us.",
    "output": "そしてアルゴリズムに3つのクラスタを探させてきた。"
  },
  {
    "index": "F16948",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But it turns out that very often K Means is also applied to data sets that look like this where there may not be several very well separated clusters.",
    "output": "だがK-Meansはこんな感じのデータセットに対してもとても良く適用されている、そこでは幾つかのクラスタに綺麗に分けられるようには見えない。"
  },
  {
    "index": "F16949",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here is an example application, to t-shirt sizing.",
    "output": "これはTシャツのサイズに関する適用の例だ。"
  },
  {
    "index": "F16950",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say you are a t-shirt manufacturer you've done is you've gone to the population that you want to sell t-shirts to, and you've collected a number of examples of the height and weight of these people in your population and so, well I guess height and weight tend to be positively highlighted so maybe you end up with a data set like this, you know, with a sample or set of examples of different peoples heights and weight.",
    "output": "あなたはTシャツ作ってる会社だとしよう、あなたは自分たちがTシャツを売りたい、と思っている母集団に対して、たくさんのサンプルの身長と体重のデータを集めた、つまり、えー、たぶん身長と体重は正の相関があるだろうから、こんな感じのデータセットになるだろう。"
  },
  {
    "index": "F16951",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say you want to size your t shirts.",
    "output": "Tシャツのサイズを決めたいと思ってるとしよう。"
  },
  {
    "index": "F16952",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say I want to design and sell t shirts of three sizes, small, medium and large.",
    "output": "3つのサイズ、S、M、LのTシャツのデザインをして売りたい、としよう。"
  },
  {
    "index": "F16953",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So how big should I make my small one?",
    "output": "ではその時、Sはどのくらいの大きさにすべきだろう?"
  },
  {
    "index": "F16954",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One way to do that would to be to run my k means clustering logarithm on this data set that I have shown on the right and maybe what K Means will do is group all of these points into one cluster and group all of these points into a second cluster and group all of those points into a third cluster.",
    "output": "それを決める一つの方法としては、右に示したこのデータセットに対しK-Meansクラスタリングアルゴリズムを適用する、というのがある。するとたぶんK-Meansが行う事は、これら全部の点を一つのクラスタに、これらの点全部を二番目のクラスタに、そしてこれらの点全部を三番目のクラスタにグループ分けする、という事だ。"
  },
  {
    "index": "F16955",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, even though the data, you know, before hand it didn't seem like we had 3 well separated clusters, K Means will kind of separate out the data into multiple pluses for you.",
    "output": "つまり、もともとのデータセットが3つの異なるクラスタに分かれているようには見えないのにも関わらず、K-Meansは複数のクラスタに分けてくれるのだ。"
  },
  {
    "index": "F16956",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you can do is then look at this first population of people and look at them and, you know, look at the height and weight, and try to design a small t-shirt so that it kind of fits this first population of people well and then design a medium t-shirt and design a large t-shirt.",
    "output": "そこから可能な事としては、この最初の人々を彼らを見て、彼らの身長と体重を見て、そしてSのTシャツをデザインする、という事。"
  },
  {
    "index": "F16957",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is in fact kind of an example of market segmentation where you're using K Means to separate your market into 3 different segments.",
    "output": "これはつまり、マーケットセグメンテーションの例となっている、そこではK-Meansを使って、マーケットを3つのセグメントに分けている。"
  },
  {
    "index": "F16958",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you can design a product separately that is a small, medium, and large t-shirts, that tries to suit the needs of each of your 3 separate sub-populations well.",
    "output": "つまりこれで、S、M、Lに分けてTシャツをデザイン出来る、3つのサブ集団のニーズに良く合うように。"
  },
  {
    "index": "F16959",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the K Means algorithm.",
    "output": "以上がK-Meansアルゴリズムです。"
  },
  {
    "index": "F16960",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by now you should know how to implement the K Means Algorithm and kind of get it to work for some problems.",
    "output": "ここまでで、もうK-Meansをどうやって実装するか、そしてどんな問題に適用出来るかを理解したはずだ。"
  },
  {
    "index": "F16961",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in the next few videos what I want to do is really get more deeply into the nuts and bolts of K means and to talk a bit about how to actually get this to work really well.",
    "output": "だが次に続くいくつかのビデオでK-Meansの要点をより深く話していくのと実際にとてもうまくやる為に必要な事をちょろっと話していきたい。"
  },
  {
    "index": "F16962",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Most of the supervised learning algorithms we've seen, things like linear regression, logistic regression, and so on, all of those algorithms have an optimization objective or some cost function that the algorithm was trying to minimize.",
    "output": "これまで見て来た教師あり学習アルゴリズム、線形回帰やロジスティック回帰などは、それらは全て、最適化の為の目的関数、またの名をコスト関数を持っていて、それを最小化しようとしていた。"
  },
  {
    "index": "F16963",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that k-means also has an optimization objective or a cost function that it's trying to minimize.",
    "output": "K-meansもまた、最適化の目的関数、またはコスト関数を持っていて、それを最小化しようとする。"
  },
  {
    "index": "F16964",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this video I'd like to tell you what that optimization objective is.",
    "output": "そしてこの動画では、最適化の目的関数が何かを説明する。"
  },
  {
    "index": "F16965",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason I want to do so is because this will be useful to us for two purposes.",
    "output": "これをやりたい理由としては、二つの目的にこれは有用だからだ。"
  },
  {
    "index": "F16966",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First, knowing what is the optimization objective of k-means will help us to debug the learning algorithm and just make sure that k-means is running correctly.",
    "output": "まず、K-meansの最適化の目的関数がなんなのかを知る事は、学習アルゴリズムをデバッグする助けとなる。K-meansがちゃんと走ってるか確認も出来る。"
  },
  {
    "index": "F16967",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And second, and perhaps more importantly, in a later video we'll talk about how we can use this to help k-means find better costs for this and avoid the local ultima.",
    "output": "二番目に、そしてたぶんこっちの方が重要だが、後半のビデオで、これをどう用いてK-meansがより良いクラスタを見つける助けに出来るか、そしてどう局所最適を避ける事が出来るかを議論する。"
  },
  {
    "index": "F16968",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But we do that in a later video that follows this one.",
    "output": "でもそれはこのビデオの後に続くビデオでね。"
  },
  {
    "index": "F16969",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just as a quick reminder while k-means is running we're going to be keeping track of two sets of variables.",
    "output": "思い出してもらう為に言っておくと、K-meansを実行している間、我らは二つの種類の変数を管理していく。"
  },
  {
    "index": "F16970",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First is the ci's and that keeps track of the index or the number of the cluster, to which an example xi is currently assigned.",
    "output": "これはx(i)が現在どのクラスタに割り振られているかのインデックスをトラックする為の変数だ。"
  },
  {
    "index": "F16971",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then the other set of variables we use is mu subscript k, which is the location of cluster centroid k.",
    "output": "そしてもう一方の管理する変数はミューの下付き添字kだ。それはクラスター重心Kの場所を表す。"
  },
  {
    "index": "F16972",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Again, for k-means we use capital K to denote the total number of clusters.",
    "output": "もう一度言っておくと、K-meansでは大文字のKをクラスタの総数を表すのに使う。"
  },
  {
    "index": "F16973",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And here lower case k is going to be an index into the cluster centroids and so, lower case k is going to be a number between one and capital K.",
    "output": "そしてこの小文字のkでクラスタ重心のインデックスを表す。つまり小文字のkは1からKまでの間の数字となる。"
  },
  {
    "index": "F16974",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now here's one more bit of notation, which is gonna use mu subscript ci to denote the cluster centroid of the cluster to which example xi has been assigned, right?",
    "output": "さらにもう一つ追加の記法として、ミューの下付き添字c(i)という物でこれはクラスターの重心のうち、サンプルx(i)に割り振られている物を表す。"
  },
  {
    "index": "F16975",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And to explain that notation a little bit more, lets say that xi has been assigned to cluster number five.",
    "output": "この記法についてもうちょっと説明しよう。x(i)がクラスタ重心5に割り振られているとしよう。"
  },
  {
    "index": "F16976",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What that means is that ci, that is the index of xi, that that is equal to five.",
    "output": "それの意味する所はc(i)、このiはx(i)のインデックスだが、c(i)はイコール5となる。"
  },
  {
    "index": "F16977",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right?",
    "output": "でしょ?"
  },
  {
    "index": "F16978",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because having ci equals five, if that's what it means for the example xi to be assigned to cluster number five.",
    "output": "だってc(i)=5となるのがサンプルx(i)がクラスタナンバー5に割り振らたという事だから。"
  },
  {
    "index": "F16979",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so mu subscript ci is going to be equal to mu subscript 5.",
    "output": "だからミュー下付き添字のc(i)はイコール、ミュー下付き添字の5となる。"
  },
  {
    "index": "F16980",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this mu subscript ci is the cluster centroid of cluster number five, which is the cluster to which my example xi has been assigned.",
    "output": "このミュー下付き添字c(i)はクラスターナンバー5のクラスタ重心で、それがサンプルx(i)が割り振られている物だ。"
  },
  {
    "index": "F16981",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Out with this notation, we're now ready to write out what is the optimization objective of the k-means clustering algorithm and here it is.",
    "output": "この記法で、我らはK-meansのクラスタリングアルゴリズムの最適化の目的関数を書き下す、準備が出来た事になる。それはこうだ。"
  },
  {
    "index": "F16982",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The cost function that k-means is minimizing is a function J of all of these parameters, c1 through cm and mu 1 through mu K.",
    "output": "K-meansが最小化するコスト関数はこれらのパラメータ全ての関数Jだ、c1からcmまでと、ミュー1からミューKまでの。"
  },
  {
    "index": "F16983",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That k-means is varying as the algorithm runs.",
    "output": "K-meansは実行していく過程でこれらを変更していく。"
  },
  {
    "index": "F16984",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the optimization objective is shown to the right, is the average of 1 over m of sum from i equals 1 through m of this term here.",
    "output": "そして最適化の目的関数は、右に示した物で、平均としての1/mの、和を取る事のi=1からmまでの、この項で、今赤の箱でくくったこれ。"
  },
  {
    "index": "F16985",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The square distance between each example xi and the location of the cluster centroid to which xi has been assigned.",
    "output": "サンプルx(i)とx(i)に割り振られたクラスタ重心の位置との間の二乗距離。"
  },
  {
    "index": "F16986",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's draw this and just let me explain this.",
    "output": "ちょっと描いて、これを説明しよう。"
  },
  {
    "index": "F16987",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, so here's the location of training example xi and here's the location of the cluster centroid to which example xi has been assigned.",
    "output": "これはトレーニングサンプルのx(i)の位置で、これがサンプルx(i)が割り振られたクラスタ重心の位置とする。"
  },
  {
    "index": "F16988",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So to explain this in pictures, if here's x1, x2, and if a point here is my example xi, so if that is equal to my example xi, and if xi has been assigned to some cluster centroid, I'm gonna denote my cluster centroid with a cross, so if that's the location of mu 5, let's say.",
    "output": "これを図で説明する為に、x1とx2があって、この点、ここがサンブルx(i)とすると、つまりこれがサンプルx(i)とイコールだとする。"
  },
  {
    "index": "F16989",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If x i has been assigned cluster centroid five as in my example up there, then this square distance, that's the square of the distance between the point xi and this cluster centroid to which xi has been assigned.",
    "output": "そしてx(i)があるクラスタ重心に割り振られているとすると、ところでクラスタの重心は十字で表す事にする。"
  },
  {
    "index": "F16990",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what k-means can be shown to be doing is that it is trying to define parameters ci and mu i.",
    "output": "つまり、それがミュー5の場所で、この例ではx(i)がクラスター重心5に割り振られてるとすると。"
  },
  {
    "index": "F16991",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Trying to find c and mu to try to minimize this cost function J.",
    "output": "そしてK-Meansがやる事は、つまり、パラメータであるc(i)とミューiを探そうとする、cとミューで、コスト関数Jを最小化する物を探そうとする。"
  },
  {
    "index": "F16992",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This cost function is sometimes also called the distortion cost function, or the distortion of the k-means algorithm.",
    "output": "このコスト関数はまた、ディストーション(歪み)コスト関数、またはK-meansアルゴリズムのディストーションと呼ばれる。"
  },
  {
    "index": "F16993",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just to provide a little bit more detail, here's the k-means algorithm.",
    "output": "もうちょっと詳細を見ると、これがK-meansのアルゴリズムだ。"
  },
  {
    "index": "F16994",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's exactly the algorithm as we have written it out on the earlier slide.",
    "output": "これは前のスライドにあったのと全く同じ物を実際の形にした物だ。"
  },
  {
    "index": "F16995",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what this first step of this algorithm is, this was the cluster assignment step where we assigned each point to the closest centroid.",
    "output": "そしてこのアルゴリズムの最初のステップはクラスター割り振りのステップで、そこで各点をクラスター重心に割り振る。"
  },
  {
    "index": "F16996",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it's possible to show mathematically that what the cluster assignment step is doing is exactly Minimizing J, with respect to the variables c1, c2 and so on, up to cm, while holding the cluster centroids mu 1 up to mu K, fixed.",
    "output": "クラスター割り振りのステップは実際に変数c1,c2、、、とc(m)までの観点からJを最小化している、という事を数学的に証明する事が出来る。この間、もっとも近い重心である、ミュー1からミューkまでは固定しておく。"
  },
  {
    "index": "F16997",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what the cluster assignment step does is it doesn't change the cluster centroids, but what it's doing is this is exactly picking the values of c1, c2, up to cm. That minimizes the cost function, or the distortion function J.",
    "output": "で、最初の割り振りのステップで何をやるかというと、そのステップではクラスタ重心は変更しない、その代わりにコスト関数、またはディストーション関数であるJを最小化するc1,c2,...cmの値を選ぶ、という事をする。"
  },
  {
    "index": "F16998",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it's possible to prove that mathematically, but I won't do so here.",
    "output": "そして数学的にもやろうと思えば証明出来るが、ここではやらんけど、直感的にも自然だと思うけどこれらの点に対しもっとも近いクラスタ重心を割り振っていく。"
  },
  {
    "index": "F16999",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But it has a pretty intuitive meaning of just well, let's assign each point to a cluster centroid that is closest to it, because that's what minimizes the square of distance between the points in the cluster centroid.",
    "output": "というのはそれが点と対応するクラスタ重心の間の二乗距離の和を最小化する割り振り方だから。"
  }
]