[
  {
    "index": "F17000",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And once again I won't prove it, but it can be shown mathematically that what the move centroid step does is it chooses the values of mu that minimizes J, so it minimizes the cost function J with respect to, wrt is my abbreviation for, with respect to, when it minimizes J with respect to the locations of the cluster centroids mu 1 through mu K.",
    "output": "そして残りのK-meansの部分、2番目のステップは、この二番目のステップとなるが、この二番目のステップは重心を移動するステップで、ここでも、証明はしないが、数学的にも証明出来る事として、重心移動のステップでは、Jを最小化するミューを選ぶ。つまりコスト関数Jを以下の観点から最小化する、ここでwrtはwithrespectto(以下の観点から)の省略だ。"
  },
  {
    "index": "F17001",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if is really is doing is this taking the two sets of variables and partitioning them into two halves right here.",
    "output": "つまり、K-meansが実際にやっているのは、二つの種類の変数群をとり、そしてそれら二つを二つに分けて、まずcの変数群、次にミューの変数群として、そしてやるのは、まず、Jを変数cに関して最小化する、変数ミューに関して最小化する、そしてそれを繰り返し続ける。"
  },
  {
    "index": "F17002",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what it does is it first minimizes J with respect to the variable c and then it minimizes J with respect to the variables mu and then it keeps on.",
    "output": "以上がK-meansのやる事の全てだ。そして今やK-meansを理解したので、コスト関数Jを最小化しよう。"
  },
  {
    "index": "F17003",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, so all that's all that k-means does.",
    "output": "また我らのK-meansの実装が正しく走っているかを確認するのにも使える。"
  },
  {
    "index": "F17004",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And now that we understand k-means as trying to minimize this cost function J, we can also use this to try to debug other any algorithm and just kind of make sure that our implementation of k-means is running correctly.",
    "output": "つまり、いまや我らはK-meansアルゴリズムを、コスト関数Jを最小化する物として理解した。"
  },
  {
    "index": "F17005",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we now understand the k-means algorithm as trying to optimize this cost function J, which is also called the distortion function.",
    "output": "コスト関数はディストーション関数とも呼ばれるんだった。"
  },
  {
    "index": "F17006",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can use that to debug k-means and help make sure that k-means is converging and is running properly.",
    "output": "その事実を用いて、K-meansをデバッグしたり、K-meansが収束しているのを見たり出来る。そしてそれが正しく実行されているかも。"
  },
  {
    "index": "F17007",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in the next video we'll also see how we can use this to help k-means find better clusters and to help k-means to avoid",
    "output": "次のビデオでは、どうこれを用いてK-meansがより良いクラスタを見つける助けと出来るか、そしてどうK-meansが局所最適を避ける助けと出来るかを話していく。"
  },
  {
    "index": "F17008",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Hello and welcome to advanced competitive strategy.",
    "output": "こんにちは、競争戦略上級クラスへようこそ。"
  },
  {
    "index": "F17009",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "My name is Tobias Kretschmer and I'm a professor of , strategy technology and organization at Ludwig- Maximilians-University or LMU in Munich.",
    "output": "私は、ミュンヘンのルドウィグ-マキシミリアン大学の戦略、テクノロジー、組織の教授をしているトビアス・クレチメルです。"
  },
  {
    "index": "F17010",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We are about to start a new course and a new adventure.",
    "output": "私たちはあたらしいコースという冒険を始めようとしています。"
  },
  {
    "index": "F17011",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Advanced competitive strategy consists of seven different modules in which you will learn how you can apply business strategy in competitive situations.",
    "output": "競争戦略上級クラスは7つの異なるモジュールをもっており、ビジネス戦略を競争的状況でどのように応用するかを学びます。"
  },
  {
    "index": "F17012",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Each one of our seven modules consists of a variety of around 10 different videos all of which include one or two individual quizzes, that are going to let you test your newly acquired knowledge immediately.",
    "output": "それぞれの異なる7つのモジュールは約10のビデオからなっており、それぞれに、一つか二つのクイズがついています。そこで皆さんが新しく学んだ知識をすぐにテストすることができます。"
  },
  {
    "index": "F17013",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And of course, they should also be fun.",
    "output": "もちろん、これらは楽しくなくてはいけません。"
  },
  {
    "index": "F17014",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There will also be a longer quiz at the end of each module.",
    "output": "それぞれのモジュールの最後に長い小テストがあります。"
  },
  {
    "index": "F17015",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, once you've completed all seven modules, you can also eventually take the final exam.",
    "output": "つまり、皆さんが7つのすべてのモジュールを終えると自動的に最終試験を受けているのです。"
  },
  {
    "index": "F17016",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And please, do check out our forum.",
    "output": "そして、私たちのフォーラムを確認してください。"
  },
  {
    "index": "F17017",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because it's a brilliant opportunity to connect to fellow students, to build study groups, to openly discuss questions, and to communicate with our wonderful and knowledgeable taching assistants.",
    "output": "なぜなら、クラスメイトとつながって、スタディグループなどを作る素晴らしい機会だからです。そこで、疑問について議論したり、素晴らしくて知識豊富なティーチングアシスタントとコミュニケーションをとる機会でもあります。"
  },
  {
    "index": "F17018",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to talk about how to initialize K-means and more importantly, this will lead into a discussion of how to make K-means avoid local optima as well.",
    "output": "このビデオでは、K-meansをどう初期化するのかと、その延長としてより重要な、K-meansでどう局所最適の問題を回避するのかについての2つを話す。"
  },
  {
    "index": "F17019",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's the K-means clustering algorithm that we talked about earlier.",
    "output": "ここに、以前に話したK-meansのクラスタリングアルゴリズムがある。"
  },
  {
    "index": "F17020",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One step that we never really talked much about was this step of how you randomly initialize the cluster centroids.",
    "output": "ここまでちゃんと話してなかったステップの一つにクラスタ重心を、どうランダムに初期化するか、というのがある。"
  },
  {
    "index": "F17021",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, it turns out that there is one method that is much more recommended than most of the other options one might think about.",
    "output": "ランダムにクラスタ重心を初期化する、と言われた時には、いくつかの方法が考えられるが、その中の一つが、それ以外の思いつく選択肢の多くよりも優れている、というものがある。"
  },
  {
    "index": "F17022",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let me tell you about that option since it's what often seems to work best.",
    "output": "だからその方法について議論しようと思う、何故ならそれがしばしば一番良く機能する選択肢だからだ。"
  },
  {
    "index": "F17023",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's how I usually initialize my cluster centroids.",
    "output": "これが私が普段クラスタ重心を初期化するやり方だ。"
  },
  {
    "index": "F17024",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When running K-means, you should have the number of cluster centroids, K, set to be less than the number of training examples M.",
    "output": "K-meansを実行する時にはクラスタ重心の数、Kをトレーニング手本の数mよりも小さい数に設定しなくてはならない。"
  },
  {
    "index": "F17025",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It would be really weird to run K-means with a number of cluster centroids that's, you know, equal or greater than the number of examples you have, right?",
    "output": "K-meansを、手本の数と同じかそれより多い数のクラスタ重心に対して実行する、というのはヘンテコなことだってのは分かるでしょ?"
  },
  {
    "index": "F17026",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the way I usually initialize K-means is, I would randomly pick k training examples.",
    "output": "で、私が普段K-meansを初期化するのに使う方法はK個のトレーニング手本をランダムに取り出す。"
  },
  {
    "index": "F17027",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, and, what I do is then set Mu1 of MuK equal to these k examples.",
    "output": "そして次にやるのは、ミュー1からミューKに、これらをセットする、という事。具体例を見てみよう。"
  },
  {
    "index": "F17028",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Lets say that k is equal to 2 and so on this example on the right let's say I want to find two clusters.",
    "output": "Kをイコール2だとしてみよう。つまりこの右側の手本に対し、2つのクラスタを見つけたい、とする。"
  },
  {
    "index": "F17029",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what I'm going to do in order to initialize my cluster centroids is, I'm going to randomly pick a couple examples.",
    "output": "その場合、クラスタ重心を初期化する為に私がやるのは2つの手本をランダムに選ぶ。"
  },
  {
    "index": "F17030",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say, I pick this one and I pick that one.",
    "output": "そして、例えばこれとそれを選んだとしよう。"
  },
  {
    "index": "F17031",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the way I'm going to initialize my cluster centroids is, I'm just going to initialize my cluster centroids to be right on top of those examples.",
    "output": "その場合私がクラスタ重心を初期化するやり方はそれらの手本の真上にクラスタ重心を置く、という方法。"
  },
  {
    "index": "F17032",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's my first cluster centroid and that's my second cluster centroid, and that's one random initialization of K-means.",
    "output": "これが最初のクラスタ重心となり、これが二番目のクラスタ重心となる。これがK-meansをランダムに初期化する方法だ。"
  },
  {
    "index": "F17033",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And sometimes I might get less lucky and maybe I'll end up picking that as my first random initial example, and that as my second one.",
    "output": "ここに描いたのは極めて良い物っぽいけど、たまにはもっとついてなくて、結局最初の奴としてこんなのをランダムな最初の手本として、そして二番目としてこんなのを選ぶはめになるかもしれない。"
  },
  {
    "index": "F17034",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And here I'm picking two examples because k equals 2.",
    "output": "ここで2つの手本を選んだのはK=2としたから。"
  },
  {
    "index": "F17035",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Some we have randomly picked two training examples and if I chose those two then I'll end up with, may be this as my first cluster centroid and that as my second initial location of the cluster centroid.",
    "output": "適当に2つのトレーニング手本をランダムに2つ選んだ、それをこれら2つとすると、その時はこれを最初のクラスタ重心にこれを2つ目のクラスタ重心の初期位置とする。"
  },
  {
    "index": "F17036",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's how you can randomly initialize the cluster centroids.",
    "output": "以上がランダムにクラスタ重心を初期化するやり方だ。"
  },
  {
    "index": "F17037",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so at initialization, your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on, if you have more clusters and more cluster centroid.",
    "output": "つまり初期化の時にはクラスタ重心ミュー1は適当に選んだ値iの元にx(i)と等しくなる。ミュー2は、iとは別の異なるランダムに選ばれた数jを使ってx(j)と表せる物と等しくなる、などなどと、もっと多くのクラスタとクラスタ重心があるなら続けていく。"
  },
  {
    "index": "F17038",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I should say that in the earlier video where I first illustrated K-means with the animation.",
    "output": "ここでちょっと補足しておく。以前の動画で最初にK-meansをアニメーションで例示した時の話だ。"
  },
  {
    "index": "F17039",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that set of slides. Only for the purpose of illustration.",
    "output": "そこでのスライドは、例示の為だけの目的の物だった。"
  },
  {
    "index": "F17040",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I actually used a different method of initialization for my cluster centroids.",
    "output": "私は実際には、そこで書いていたのとは違うクラスタ重心の初期化の仕方をする。"
  },
  {
    "index": "F17041",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the method described on this slide, this is really the recommended way.",
    "output": "このスライドで説明している方法こそが本当に推奨する方法だ。"
  },
  {
    "index": "F17042",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the way that you should probably use, when you implement K-means.",
    "output": "そして恐らく、あなたがK-meansを実装する時にも使うべき方法と言える。"
  },
  {
    "index": "F17043",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized, and so, depending on the random initialization.",
    "output": "さて、この右側の二つの図で例示出来ているかもしれない事として、K-meansは実際にどう初期化されるかに応じてつまりどうランダム初期化されるかに応じて異なる解に収束しうるという事が想像出来るかもしれない。"
  },
  {
    "index": "F17044",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "K-means can end up at different solutions.",
    "output": "K-meansは異なる解に収束しうる。"
  },
  {
    "index": "F17045",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, in particular, K-means can actually end up at local optima.",
    "output": "そして実際、K-meansは異なる局所最適解に落ち着きうる。"
  },
  {
    "index": "F17046",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you're given the data sale like this.",
    "output": "もしこんなデータセットを与えられたとするとうーん、見た所、ここには三つのクラスタがあるように見える。"
  },
  {
    "index": "F17047",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, it looks like, you know, there are three clusters, and so, if you run K-means and if it ends up at a good local optima this might be really the global optima, you might end up with that cluster ring.",
    "output": "だからもしK-meansを実行したらもし良い局所最適解に落ち着けば、これが極めて良い局所最適解に見える、その時には、このクラスタの輪となるだろう。"
  },
  {
    "index": "F17048",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you had a particularly unlucky, random initialization, K-means can also get stuck at different local optima.",
    "output": "だがあなたが特にアンラッキーでランダム初期化によっては、K-meansは異なる局所最適にスタックしてしまうかもしれない。"
  },
  {
    "index": "F17049",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points.",
    "output": "だから、この左側の例では、青いクラスタがたくさんの点を捕捉してしまい、緑と赤のクラスタはそれぞれ相対的にはちょっとの点しか捕捉出来ていない。"
  },
  {
    "index": "F17050",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore, has split the second cluster into two separate sub-clusters like so, and it has also taken the second cluster and split it into two separate sub-clusters like so, and so, both of these examples on the lower right correspond to different local optima of K-means and in fact, in this example here, the cluster, the red cluster has captured only a single optima example.",
    "output": "つまりこれは、悪い局所最適解に対応している。何故ならそれは、これら二つのクラスタを一個に取り出してしまっていて、さらに、二番目のクラスタを二つのサブクラスタに分割してしまっている。"
  },
  {
    "index": "F17051",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the term local optima, by the way, refers to local optima of this distortion function J, and what these solutions on the lower left, what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it's not doing a very good job minimizing this distortion function J.",
    "output": "ところで局所最適という用語は、このディストーション関数Jの局所最適を示している、そしてこれらの左下の解は、これらの局所最適な解はK-meansが局所最適にスタックしてしまい、このディストーション関数Jを最小化するのにあんまり良い仕事はしていない事に対応する。"
  },
  {
    "index": "F17052",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you're worried about K-means getting stuck in local optima, if you want to increase the odds of K-means finding the best possible clustering, like that shown on top here, what we can do, is try multiple, random initializations.",
    "output": "だから、もしK-meansが局所最適にスタックしてしまいそうと心配だったら、もしK-meansが可能な中でベストなクラスタリングを見つけるオッズを高めたければ、この上に見せたように、我らに取れる手段としては複数のランダム初期化を試みる事だ。"
  },
  {
    "index": "F17053",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, instead of just initializing K-means once and hopping that that works, what we can do is, initialize K-means lots of times and run K-means lots of times, and use that to try to make sure we get as good a solution, as good a local or global optima as possible.",
    "output": "つまりK-meansを一回だけ初期化してうまく行く事を祈る代わりにやれる事として、K-meansをたくさん初期化してそしてK-meansをたくさん実行する事だ。そしてそれを用いて、可能な限り一番良い局所最適、つまりはグローバル最適を得ている事を確認するのだ。"
  },
  {
    "index": "F17054",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Concretely, here's how you could go about doing that.",
    "output": "具体的には、これが、そのやり方だ。"
  },
  {
    "index": "F17055",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say, I decide to run K-meanss a hundred times so I'll execute this loop a hundred times and it's fairly typical a number of times when came to will be something from 50 up to may be 1000.",
    "output": "つまりこのループを100回実行する。そしてそれはかなりありがちな数で、だいたい50から1000の間のあたりの回数だと思う。"
  },
  {
    "index": "F17056",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's say you decide to say K-means one hundred times.",
    "output": "さて、K-meansを100回実行しよう、と決めたとしよう。"
  },
  {
    "index": "F17057",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what that means is that we would randomnly initialize K-means.",
    "output": "それはどういう事かというと、K-meansをランダム初期化するという事でそしてこれらの100回のランダム初期化の時に、毎回K-meansを走らせる。"
  },
  {
    "index": "F17058",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings, and a set of cluster centroids, and then we would then compute the distortion J, that is compute this cause function on the set of cluster assignments and cluster centroids that we got.",
    "output": "するとクラスタとクラスタ重心の集合が得られて、そしてそれを用いてディストーション関数Jを計算する。つまりこの得られたクラスタ割り振りとクラスタ重心の集合に対して、コスト関数をそれぞれ計算していく。"
  },
  {
    "index": "F17059",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data, just pick one, that gives us the lowest cost.",
    "output": "最後に、これらのプロセス全てを100回行ったら、100個の異なる、データをクラスタリングする方法が得られる事になる。"
  },
  {
    "index": "F17060",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it turns out that if you are running K-means with a fairly small number of clusters , so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often, can sometimes make sure that you find a better local optima.",
    "output": "そこから最後に行うのは、これら全ての100通りの見つけたクラスタリングのデータから、単純に一番コストが低い物を選ぶだけ。"
  },
  {
    "index": "F17061",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Make sure you find the better clustering data.",
    "output": "もしかなり少ない数のクラスター数に対してK-meansを走らせれば、つまり、そうだなぁ、クラスタの数がだいたい2から10とかその位の範囲なら、複数回のランダム初期化を行う事はかなりしばしばより良い局所最適を見つけた事を確認出来る、より良いクラスタリングのデータを見つけた事を。"
  },
  {
    "index": "F17062",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if K is very large, so, if K is much greater than 10, certainly if K were, you know, if you were trying to find hundreds of clusters, then, having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing, doing multiple random initializations will probably give you a slightly better solution but, but maybe not that much.",
    "output": "でもKがとても大きい時は、Kが10よりもずっと大きく、そうだなぁ、だいたいもしあなたが何百ものクラスタを見つけようとしている時には、その時には複数回のランダム初期化を行っても、そんなに大きな違いは無いだろう。そして最初のランダム初期化ですでにかなり良い解が得られている公算が高い。"
  },
  {
    "index": "F17063",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But it's really in the regime of where you have a relatively small number of clusters, especially if you have, maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering.",
    "output": "そして複数回、ランダム初期化を行ったら、たぶん、ちょっとはマシな解は得られるだろうが、でもそんなには変わらない。だが、相対的に小さな数のクラスタ数の範囲に居る時は、とくに2とか3とか4個のクラスタの時には、複数回ランダム初期化は、ディストーション関数をちゃんと最小化し、ひいては良いクラスタリングを与えてくれている、と確認するのに、やるとやらないとでは大違いとなる。"
  },
  {
    "index": "F17064",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's K-means with random initialization.",
    "output": "さて、以上がK-meansのランダム初期化だ。"
  },
  {
    "index": "F17065",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you're trying to learn a clustering with a relatively small number of clusters, 2, 3, 4, 5, maybe, 6, 7, using multiple random initializations can sometimes, help you find much better clustering of the data.",
    "output": "もし相対的に小さな数のクラスタ、たとえば2とか3とか4とか5とか、うーん、6とか7でも、それらの数のクラスタで学習させたい時には複数回ランダム初期化を用いる事で、より良いデータのクラスタリングが得られる助けとなる事がある。"
  },
  {
    "index": "F17066",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, even if you are learning a large number of clusters, the initialization, the random initialization method that I describe here. That should give K-means a reasonable starting point to start from for finding a good set of clusters.",
    "output": "だが、大きな数のクラスタの時だって、ここで説明したランダム初期化の方法はK-meansに良いクラスタを探させるリーズナブルなスタート地点となるだろう。"
  },
  {
    "index": "F17067",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I'd like to talk about one last detail of K-means clustering which is how to choose the number of clusters, or how to choose the value of the parameter capsule K.",
    "output": "このビデオでは、最後に残った一つのK-meansについての詳細であるどうやってクラスタの数を選ぶのか、またはパラメータの大文字Kをどうやって選ぶのか、という事を議論する。"
  },
  {
    "index": "F17068",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To be honest, there actually isn't a great way of answering this or doing this automatically and by far the most common way of choosing the number of clusters, is still choosing it manually by looking at visualizations or by looking at the output of the clustering algorithm or something else.",
    "output": "これを自動で解決するような方法も。現在の所、もっとも一般的なクラスタ数の選び方は可視化した結果を見てか、クラスタアルゴリズムの結果を見てか、その他何かを見て、手動で選ぶという方法だ。"
  },
  {
    "index": "F17069",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But I do get asked this question quite a lot of how do you choose the number of clusters, and so I just want to tell you know what are peoples' current thinking on it although, the most common thing is actually to choose the number of clusters by hand.",
    "output": "だがどうやってクラスタ数を選ぶかという質問をほんとに良く受けるので、現在のところの人々のこの件に関する考えをお話したい。"
  },
  {
    "index": "F17070",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "A large part of why it might not always be easy to choose the number of clusters is that it is often generally ambiguous how many clusters there are in the data.",
    "output": "何故クラスタの総数を選ぶのがいつも簡単にはいかないかというと、その理由の大部分はそもそもにデータに幾つのクラスタがあるのか、というのは曖昧な事があるからだ。"
  },
  {
    "index": "F17071",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Looking at this data set some of you may see four clusters and that would suggest using K equals 4.",
    "output": "このデータセットを見てくれ。何人かは4つのクラスタに見えるかもしれない、その場合はK=4を提案する事になる。"
  },
  {
    "index": "F17072",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or some of you may see two clusters and that will suggest K equals 2 and now this may see three clusters.",
    "output": "またある人々には二つのクラスタに見えるかもしれない。その場合はK=2を提案する事になる。"
  },
  {
    "index": "F17073",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, looking at the data set like this, the true number of clusters, it actually seems genuinely ambiguous to me, and I don't think there is one right answer.",
    "output": "つまり、こんなデータセットを見てみる場合、クラスタの総数は、これは本質的に曖昧に見える。そして一つの正しい正解があるようには思えない。"
  },
  {
    "index": "F17074",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is part of our supervised learning.",
    "output": "これは教師無し学習という物の一部だ。"
  },
  {
    "index": "F17075",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We are aren't given labels, and so there isn't always a clear cut answer.",
    "output": "ラベルが与えられていないので、明確な答えがいつもあるとは限らない。"
  },
  {
    "index": "F17076",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is one of the things that makes it more difficult to say, have an automatic algorithm for choosing how many clusters to have.",
    "output": "そしてこれが、幾つのクラスタが望ましいかを自動で決めるアルゴリズムを作るのがより難しい理由の一つである。"
  },
  {
    "index": "F17077",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When people talk about ways of choosing the number of clusters, one method that people sometimes talk about is something called the Elbow Method.",
    "output": "人々がクラスタの総数を選ぶ方法を議論している時に、ちょくちょく出てくる方法にエルボー(肘)法というのがある。"
  },
  {
    "index": "F17078",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me just tell you a little bit about that, and then mention some of its advantages but also shortcomings.",
    "output": "それについてちょっと説明しよう。そしてその後にその利点と欠点についても言及する。"
  },
  {
    "index": "F17079",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the Elbow Method, what we're going to do is vary K, which is the total number of clusters.",
    "output": "さて、エルボー法。我らがやる事は、Kを変化させていって、、、ここでKはクラスタの総数だ。"
  },
  {
    "index": "F17080",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we're going to run K-means with one cluster, that means really, everything gets grouped into a single cluster and compute the cost function or compute the distortion J and plot that here.",
    "output": "そしてまず、K-meansを、1個のクラスタで実行する、それはつまり、全てを一つのクラスタにグルーピングして、そしてコスト関数またはディストーション関数であるJを計算してここにプロットする、という事。"
  },
  {
    "index": "F17081",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we're going to run K means with two clusters, maybe with multiple random initial agents, maybe not.",
    "output": "次にK-meansを二つのクラスタに対して実行する、ランダムの初期化に対してかもしれないし、そうで無いかもしれない。"
  },
  {
    "index": "F17082",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But then, you know, with two clusters we should get, hopefully, a smaller distortion, and so plot that there.",
    "output": "なんにせよ、二つのクラスタに対しての方がディストーションは小さくなる事が期待される、だからこんな感じにプロットしておく。"
  },
  {
    "index": "F17083",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then run K-means with three clusters, hopefully, you get even smaller distortion and plot that there.",
    "output": "そして次にK-meansを3つのクラスタに対して実行すると、期待される事としてはより小さなディストーションとなる。"
  },
  {
    "index": "F17084",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm gonna run K-means with four, five and so on.",
    "output": "さらにK-meansを4、5などに対して走らせていく。"
  },
  {
    "index": "F17085",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so we end up with a curve showing how the distortion, you know, goes down as we increase the number of clusters.",
    "output": "最終的には、クラスタ数を増加させていくと、ディストーションがどう現象していくかを示す曲線となる。"
  },
  {
    "index": "F17086",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so we get a curve that maybe looks like this.",
    "output": "だから、こんな曲線が得られる。"
  },
  {
    "index": "F17087",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you look at this curve, what the Elbow Method does it says \"Well, let's look at this plot.",
    "output": "そしてこのカーブを見てみると、エルボー法がやる事は、「あー、このプロットを見て見てくれ、ここにエルボー(肘)っぽい形がはっきり見えるだろ?"
  },
  {
    "index": "F17088",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, this is, would be by analogy to the human arm where, you know, if you imagine that you reach out your arm, then, this is your shoulder joint, this is your elbow joint and I guess, your hand is at the end over here.",
    "output": "つまり、これは人間の手みたいだ、という事だ。手を伸ばした所を想像してみると、これが肩の関節、これがエルボーの関節、そして手はここで終わる。"
  },
  {
    "index": "F17089",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is the Elbow Method.",
    "output": "だからこれはエルボー法と呼ばれる。"
  },
  {
    "index": "F17090",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then you find this sort of pattern where the distortion goes down rapidly from 1 to 2, and 2 to 3, and then you reach an elbow at 3, and then the distortion goes down very slowly after that.",
    "output": "そしてこの手のパターンが見つかったら、つまりディストーションが1から2、2から3へと急激に変化したら、そして3で肘に到達したら、そしてそこからディストーションがとてもゆっくりに低下したら、その場合はどうやら、三つのクラスタを使うのがクラスタの正しい総数かもしれない。"
  },
  {
    "index": "F17091",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then it looks like, you know what, maybe using three clusters is the right number of clusters, because that's the elbow of this curve, right?",
    "output": "何故ならそれがこの曲線の肘(エルボー)だからだ。でしょ?"
  },
  {
    "index": "F17092",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That it goes down, distortion goes down rapidly until K equals 3, really goes down very slowly after that.",
    "output": "そこではディストーションはKが3になるまで急激に減少していて、そこからはとても緩慢にしか減少しなくなる。"
  },
  {
    "index": "F17093",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's pick K equals 3.",
    "output": "だからK=3を選ぶ事としよう。"
  },
  {
    "index": "F17094",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you apply the Elbow Method, and if you get a plot that actually looks like this, then, that's pretty good, and this would be a reasonable way of choosing the number of clusters.",
    "output": "もしエルボー法を適用して実際にこんな見た目のプロットが得られたなら、それはとてもグッド!これは合理的なクラスタの総数の決め方となる。"
  },
  {
    "index": "F17095",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out the Elbow Method isn't used that often, and one reason is that, if you actually use this on a clustering problem, it turns out that fairly often, you know, you end up with a curve that looks much more ambiguous, maybe something like this.",
    "output": "実のところ、エルボー法は、そんなに良くは使われていない。その理由の一つには、実際にこれをクラスタの問題に使ってみると、かなりしばしば、もっと曖昧な、こんな感じの曲線が得られる事がある。"
  },
  {
    "index": "F17096",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you look at this, I don't know, maybe there's no clear elbow, but it looks like distortion continuously goes down, maybe 3 is a good number, maybe 4 is a good number, maybe 5 is also not bad.",
    "output": "どうも、明確なエルボーは無く見える。"
  },
  {
    "index": "F17097",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, if you actually do this in a practice, you know, if your plot looks like the one on the left and that's great.",
    "output": "だからこれを実際にやってみると、もしプロットした結果が左側みたいだったら、それは素晴らしい。"
  },
  {
    "index": "F17098",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It gives you a clear answer, but just as often, you end up with a plot that looks like the one on the right and is not clear where the ready location of the elbow is.",
    "output": "割と明確な答えを提供してくれている。でも同じくらいしばしば、右側みたいなプロットを得るはめにもなり、そこではどこがエルボーの場所なのか、あまりはっきりしない。"
  },
  {
    "index": "F17099",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It makes it harder to choose a number of clusters using this method.",
    "output": "その場合はこの手法を使ってクラスタの数を決めるのはより難しい。"
  },
  {
    "index": "F17100",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So maybe the quick summary of the Elbow Method is that is worth the shot but I wouldn't necessarily, you know, have a very high expectation of it working for any particular problem.",
    "output": "つまり簡単にエルボー法について要約すると、一発試してみる価値はある、だがどんな問題にもとてもうまくいく、などと期待はしない方がいい。"
  },
  {
    "index": "F17101",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, here's one other way of how, thinking about how you choose the value of K, very often people are running K-means in order you get clusters for some later purpose, or for some sort of downstream purpose.",
    "output": "だいたいは、人々はK-meansを、のちの問題で使う為のクラスタを得る為に用いる。言い換えるとある種の下流の目的の為に。"
  },
  {
    "index": "F17102",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe you want to use K-means in order to do market segmentation, like in the T-shirt sizing example that we talked about.",
    "output": "K-meansを、マーケットのセグメンテーションの為に使いたいのかもしれない。ここまで話してきたTシャツの例のように。"
  },
  {
    "index": "F17103",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe you want K-means to organize a computer cluster better, or maybe a learning cluster for some different purpose, and so, if that later, downstream purpose, such as market segmentation.",
    "output": "K-meansをコンピュータのクラスタをより良く構成する為に使いたいのかもしれない。または別の目的の為にクラスタを学習させたいのかもしれない。"
  },
  {
    "index": "F17104",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If that gives you an evaluation metric, then often, a better way to determine the number of clusters, is to see how well different numbers of clusters serve that later downstream purpose.",
    "output": "もしもあとに続く、下流の目的、マーケットのセグメンテーションのような物、もしそれが評価指標を与えてくれるなら、その場合はより良いクラスタ総数の決定方法はそれぞれのクラスタ数がどれだけ下流の目的にうまく貢献出来るかを見てみる事だ。"
  },
  {
    "index": "F17105",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me step through a specific example.",
    "output": "具体例を見ていこう。"
  },
  {
    "index": "F17106",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me go through the T-shirt size example again, and I'm trying to decide, do I want three T-shirt sizes?",
    "output": "ここでもTシャツのサイズの例を見ていく事にする。三つのTシャツのサイズにしたいのか?"
  },
  {
    "index": "F17107",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I choose K equals 3, then I might have small, medium and large T-shirts.",
    "output": "そうなら、K=3を選ぶ、たぶんsmall、medium、largeのTシャツとなるだろう。"
  },
  {
    "index": "F17108",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or maybe, I want to choose K equals 5, and then I might have, you know, extra small, small, medium, large and extra large T-shirt sizes.",
    "output": "またはK=5を選ぶかもしれない、その時は、extrasmall、small、medium、largeとextralargeのTシャツサイズとなるだろう。"
  },
  {
    "index": "F17109",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you can have like 3 T-shirt sizes or four or five T-shirt sizes.",
    "output": "つまり、3つとか4つとか5つのTシャツサイズがあり得る。"
  },
  {
    "index": "F17110",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We could also have four T-shirt sizes, but I'm just showing three and five here, just to simplify this slide for now.",
    "output": "4つのTシャツのサイズはあり得るのだが、しかし便宜上、だが簡単の為、ここでは3と5をスライドをシンプルにする為見せておく。"
  },
  {
    "index": "F17111",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if I run K-means with K equals 3, maybe I end up with, that's my small and that's my medium and that's my large.",
    "output": "すると、K-MeansをK=3で走らせると、結果としては例えばここがsmallで、ここがmedium、ここがlargeという感じになる。"
  },
  {
    "index": "F17112",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas, if I run K-means with 5 clusters, maybe I end up with, those are my extra small T-shirts, these are my small, these are my medium, these are my large and these are my extra large.",
    "output": "一方、5クラスタでK-meansを実行すると、結局得られるのはたとえば、これらがextrasmallのTシャツ、これらがsmall、これらがmedium、これらがlarge、これらがextralargeだ。"
  },
  {
    "index": "F17113",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the nice thing about this example is that, this then maybe gives us another way to choose whether we want 3 or 4 or 5 clusters, and in particular, what you can do is, you know, think about this from the perspective of the T-shirt business and ask: \"Well if I have five segments, then how well will my T-shirts fit my customers and so, how many T-shirts can I sell?",
    "output": "そしてこの例で良い事として、これはまた、3か4か5個のクラスターを選ぶ、もう一つの方法を与える。"
  },
  {
    "index": "F17114",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How happy will my customers be?\" What really makes sense, from the perspective of the T-shirt business, in terms of whether, I want to have Goer T-shirt sizes so that my T-shirts fit my customers better.",
    "output": "具体的には、行える事は、Tシャツビジネスの観点からこの事を考えてみて、こう問うてみる事だ:「うーん、もし私が5つの顧客セグメントを持ったとしたら、どれだけの顧客に私のTシャツはフィットして、どれだけの数私のTシャツは売れるのか?私の顧客は、どれだけ幸せだろうか?"
  },
  {
    "index": "F17115",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or do I want to have fewer T-shirt sizes so that I make fewer sizes of T-shirts.",
    "output": "Tシャツビジネスの観点から何が最も説得力があるか、私は自分たちのTシャツが顧客にフィットするようにより多くのサイズがある事を望んでいるのか、それともTシャツのサイズを少なくして作らなくてはいけないTシャツのサイズを減らしたいのか。"
  },
  {
    "index": "F17116",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, the t-shirt selling business, that might give you a way to decide, between three clusters versus five clusters.",
    "output": "つまり、Tシャツを売るビジネスという観点が3つのクラスタと5つのクラスタのどちらを選ぶのかを決める方法を、提供してくれるかもしれない。"
  },
  {
    "index": "F17117",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that gives you an example of how a later downstream purpose like the problem of deciding what T-shirts to manufacture, how that can give you an evaluation metric for choosing the number of clusters.",
    "output": "以上が、下流でどう使うかの目的が例えばどんなTシャツを作るか、とかが、クラスタ数を選ぶ為にクラスタの数を評価する指標を与えてくれる事がどんな風にありうるかの例だ。"
  },
  {
    "index": "F17118",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For those of you that are doing the program exercises, if you look at this week's program exercise associative K-means, that's an example there of using K-means for image compression.",
    "output": "プログラムの課題をやってる人なら、もし今週のK-meansに関するプログラムの課題を見てみたら、そこにはK-meansを画像圧縮に使う例がある。"
  },
  {
    "index": "F17119",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if you were trying to choose how many clusters to use for that problem, you could also, again use the evaluation metric of image compression to choose the number of clusters, K?",
    "output": "そこでは、どれだけの数のクラスタをその問題に使うべきかを選ぼうとするなら、そこでもまた、画像圧縮の評価指標を使って、クラスターの総数Kを選ぶ事になる。"
  },
  {
    "index": "F17120",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how good do you want the image to look versus, how much do you want to compress the file size of the image, and, you know, if you do the programming exercise, what I've just said will make more sense at that time.",
    "output": "つまり、どれだけ画像を良く保ちたいかと、どれだけ画像のファイルサイズを圧縮したいのか。実際にプログラムの課題をやったら、今私の言った事がもっと良く分かるだろう。"
  },
  {
    "index": "F17121",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just summarize, for the most part, the number of customers K is still chosen by hand by human input or human insight.",
    "output": "ではまとめだ。大多数のケースで、クラスター数Kを選ぶのは未だに手動で、人間による入力または洞察によって為されている。"
  },
  {
    "index": "F17122",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One way to try to do so is to use the Elbow Method, but I wouldn't always expect that to work well, but I think the better way to think about how to choose the number of clusters is to ask, for what purpose are you running K-means?",
    "output": "だがそれはいつもうまくいく、と期待出来るほどの物じゃない。それよりも、クラスターの数を選ぶより良い方法と考えられるのは、どういう目的であなたがK-meansを実行しているのか、という事を問うてみる事だ。"
  },
  {
    "index": "F17123",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then to think, what is the number of clusters K that serves that, you know, whatever later purpose that you actually run the K-means for.",
    "output": "そしてどんなクラスターの数Kが、K-meansを走らせる目的となっているあとに続く問題にーーそれがなんであれーーうまく寄与するかを。"
  },
  {
    "index": "F17124",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to start talking about a second type of unsupervised learning problem called dimensionality reduction.",
    "output": "このビデオでは、二番目の種類の教師なし学習の問題である、次元削減について始めたいと思う。"
  },
  {
    "index": "F17125",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There are a couple of different reasons why one might want to do dimensionality reduction.",
    "output": "次元を削減したい、と思う幾つかの異なる理由が存在する。"
  },
  {
    "index": "F17126",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is data compression, and as we'll see later, a few videos later, data compression not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms.",
    "output": "一つはデータを圧縮したい時、そして後で見るように、2,3後のビデオで見るように、データ圧縮はデータを圧縮してより少ないメモリやディスク容量を占有するようになるだけでなく、また学習アルゴリズムのスピードアップにもなる。"
  },
  {
    "index": "F17127",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But first, let's start by talking about what is dimensionality reduction.",
    "output": "だがまずは、次元削減とは何かを話す事から始めよう。"
  },
  {
    "index": "F17128",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As a motivating example, let's say that we've collected a data set with many, many, many features, and I've plotted just two of them here.",
    "output": "動機を理解する例として、たくさん、たくさん、たーくさんのフィーチャーのデータセットを集めたとしよう。そしてそれらのうちの二つだけをここにプロットした。"
  },
  {
    "index": "F17129",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say that unknown to us two of the features were actually the length of something in centimeters, and a different feature, x2, is the length of the same thing in inches.",
    "output": "そして例えばそれら二つのフィーチャーは実際は何かのセンチメートルによる長さと、それとは別のフィーチャーx2はインチによる長さだとしよう。"
  },
  {
    "index": "F17130",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this gives us a highly redundant representation and maybe instead of having two separate features x1 then x2, both of which basically measure the length, maybe what we want to do is reduce the data to one-dimensional and just have one number measuring this length.",
    "output": "つまりこれは、極めて冗長な表現となっていて、x1とx2の別々のフィーチャーを保持し続けるよりは、それら両方が基本的には同じ長さを測っているのだから、我らはデータを削減して一次元にしたい、と思うだろう。そしてこの長さを測る数字一つだけを保持すれば良い。"
  },
  {
    "index": "F17131",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case this example seems a bit contrived, this centimeter and inches example is actually not that unrealistic, and not that different from things that I see happening in industry.",
    "output": "この例はちょっとうさんくさいように思うかもしれない。でも実の所、このセンチメーターとインチの例は、そんなに非現実的でも無い。"
  },
  {
    "index": "F17132",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have hundreds or thousands of features, it is often this easy to lose track of exactly what features you have.",
    "output": "もし何百、何千ものフィーチャーがあるのなら、すぐに、簡単に何のフィーチャーを持っていたのか、トラック出来なくなるものだ。"
  },
  {
    "index": "F17133",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And sometimes may have a few different engineering teams, maybe one engineering team gives you two hundred features, a second engineering team gives you another three hundred features, and a third engineering team gives you five hundred features so you have a thousand features all together, and it actually becomes hard to keep track of you know, exactly which features you got from which team, and it's actually not that want to have highly redundant features like these.",
    "output": "そして時には、別々のエンジニアのチームが、例えばあるエンジニアのチームが200のフィーチャーをあなたに与え、二番目のエンジニアチームがまた別の300フィーチャーを与え、そして三番目のエンジニアチームが500フィーチャーをあなたに提供したとする。すると全部で1000フィーチャーとなる訳だが、それは実際にどのフィーチャーがどこのチームから来た物かなどを正確にトラックするのは難しくなる。"
  },
  {
    "index": "F17134",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if the length in centimeters were rounded off to the nearest centimeter and lengthened inches was rounded off to the nearest inch. Then, that's why these examples don't lie perfectly on a straight line, because of, you know, round-off error to the nearest centimeter or the nearest inch.",
    "output": "だからもしセンチメーターの長さが近傍のセンチメーターに丸めてしまって、インチの長さは近傍のインチに丸めてしまえば、それが理由でこの手本は直線に完全には乗らない事になる、何故なら、近傍のセンチメーターなり近傍のインチなりへの丸め誤差の為に。"
  },
  {
    "index": "F17135",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if we can reduce the data to one dimension instead of two dimensions, that reduces the redundancy.",
    "output": "もしデータを2次元の代わりに1次元に削減出来たら、冗長性も削減出来る。"
  },
  {
    "index": "F17136",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For may years I've been working with autonomous helicopter pilots.",
    "output": "別の例としては、これもまた嘘っぽいかもしれないが、自律的なヘリコプターパイロットと何年も仕事をしてきた。"
  },
  {
    "index": "F17137",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or I've been working with pilots that fly helicopters.",
    "output": "言い換えると私はヘリコプターを飛ばすパイロットと仕事をしてきた。"
  },
  {
    "index": "F17138",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so.",
    "output": "そして、だ。"
  },
  {
    "index": "F17139",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you were to measure--if you were to, you know, do a survey or do a test of these different pilots--you might have one feature, x1, which is maybe the skill of these helicopter pilots, and maybe \"x2\" could be the pilot enjoyment.",
    "output": "もし仮に、、、もし仮に調査なり試験なりをこれらの別々のパイロットに実施したとして、そこでは一つのフィーチャーx1として例えばこれらのヘリコプターパイロットのスキルだとして、そしてx2は例えばパイロットの楽しみ度合いだとする。"
  },
  {
    "index": "F17140",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, you know, how much they enjoy flying, and maybe these two features will be highly correlated.",
    "output": "つまり、彼らがどれだけ飛行を楽しんだのか、だ。これら二つのフィーチャーは高く相関しているだろう。"
  },
  {
    "index": "F17141",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you really care about might be this sort of this sort of, this direction, a different feature that really measures pilot aptitude.",
    "output": "そして本当に気になっているのは、この種の、この種の、この方向、本当にパイロットの適性を測っている、別のフィーチャーかもしれない。"
  },
  {
    "index": "F17142",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm making up the name aptitude of course, but again, if you highly correlated features, maybe you really want to reduce the dimension.",
    "output": "私は適性、という名前を創り上げた。だがふたたび、もし高く相関したフィーチャーがあれば、実際に次元を削減したくなるだろう。"
  },
  {
    "index": "F17143",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let me say a little bit more about what it really means to reduce the dimension of the data from 2 dimensions down from 2D to 1 dimensional or to 1D.",
    "output": "だから、もうちょっと詳しく、データの次元を2次元から、2Dから1次元、1Dへと削減する、という事が本当は何を意味しているかを説明しよう。"
  },
  {
    "index": "F17144",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me color in these examples by using different colors.",
    "output": "これらの手本を別々の色で、色付けしよう。"
  },
  {
    "index": "F17145",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this case by reducing the dimension what I mean is that I would like to find maybe this line, this, you know, direction on which most of the data seems to lie and project all the data onto that line which is true, and by doing so, what I can do is just measure the position of each of the examples on that line.",
    "output": "そしてこのケースでは、次元を削減する、という時に意味している事は、例えばこの直線を探して、もっとも多くのデータが載るような直線の方向を探して、そこに全てのデータを射影する、そうする事で、この直線上の各手本の位置を測る事が出来る。"
  },
  {
    "index": "F17146",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what I can do is come up with a new feature, z1, and to specify the position on the line I need only one number, so it says z1 is a new feature that specifies the location of each of those points on this green line.",
    "output": "そして新しい一つのフィーチャーz1という考えに行き着く。直線上の位置を示すのに一つの数だけしか必要としなくなったので、つまりz1はこれらの点のこの緑の直線上の位置を示す、新しいフィーチャーなのだ。"
  },
  {
    "index": "F17147",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what this means, is that where as previously if i had an example x1, maybe this was my first example, x1.",
    "output": "これの意味する所は、前と同じに手本x1があったとすると、例えばこれは最初の手本x1だとする。"
  },
  {
    "index": "F17148",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in order to represent x1 originally x1. I needed a two dimensional number, or a two dimensional feature vector.",
    "output": "そして、x1を表す為にはもともとのx1には二次元の数が必要だった、または二次元のフィーチャーベクトルが必要だった。"
  },
  {
    "index": "F17149",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Instead now I can represent z1.",
    "output": "その代わりにここでは、z1で表す事が出来る。"
  },
  {
    "index": "F17150",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I could use just z1 to represent my first example, and that's going to be a real number.",
    "output": "私は最初の手本を表すのに、z1だけで表す事が出来る、ここでz1は実数。"
  },
  {
    "index": "F17151",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly x2 you know, if x2 is my second example there, then previously, whereas this required two numbers to represent if I instead compute the projection of that black cross onto the line.",
    "output": "同様にx2は、x2をここの二番目の手本とすると、以前はこれを表すのに二つの数が必要だったが、もし代わりに直線上の黒い線に射影した物を計算すれば、そうすれば今や、私はこのz2の線上の位置を示すには、たった一つの実数しか必要としない。"
  },
  {
    "index": "F17152",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And now I only need one real number which is z2 to represent the location of this point z2 on the line. And so on through my M examples.",
    "output": "もし元のデータセットを、全ての手本を緑の直線に射影する、という近似を許容するなら、その時は一つの数しか必要としない、直線の上の点の場所を示すのに、たった一つの実数しか必要としない。"
  },
  {
    "index": "F17153",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just to summarize, if we allow ourselves to approximate the original data set by projecting all of my original examples onto this green line over here, then I need only one number, I need only real number to specify the position of a point on the line, and so what I can do is therefore use just one number to represent the location of each of my training examples after they've been projected onto that green line.",
    "output": "つまり、ただ一つの数を使って、各手本の位置を表す事が出来る、各手本を、緑の直線に射影した後では。"
  },
  {
    "index": "F17154",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is an approximation to the original training self because I have projected all of my training examples onto a line.",
    "output": "つまりこれは、元のトレーニングセットを近似した物となっている、何故ならトレーニング手本を直線に射影しているから。"
  },
  {
    "index": "F17155",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But now, I need to keep around only one number for each of my examples.",
    "output": "だが、いまや私は各手本に対してたった一つの数を保持するだけで良くなっている。"
  },
  {
    "index": "F17156",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this halves the memory requirement, or a space requirement, or what have you, for how to store my data.",
    "output": "だからこれは必要なメモリ量、または必要なスペース、またはデータを保存する方法がなんであれ、それを半減させる。"
  },
  {
    "index": "F17157",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And perhaps more interestingly, more importantly, what we'll see later, in the later video as well is that this will allow us to make our learning algorithms run more quickly as well.",
    "output": "そしてもっと興味深い事には、もっと重要な事としては、後で観る事になるが、あとのビデオで、それは、これが我らの学習アルゴリズムをもっと早く走らせてくれる、という事だ。"
  },
  {
    "index": "F17158",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that is actually, perhaps, even the more interesting application of this data compression rather than reducing the memory or disk space requirement for storing the data.",
    "output": "そしてそれは実際、たぶん、データを保持するディスクスペース要件やメモリ要件を減らす、という事よりも、より興味深いデータ圧縮の適用例と言えるだろう。"
  },
  {
    "index": "F17159",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "On the previous slide we showed an example of reducing data from 2D to 1D.",
    "output": "前のスライドでは、データを2Dから1Dへと削減する例を見た。"
  },
  {
    "index": "F17160",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "On this slide, I'm going to show another example of reducing data from three dimensional 3D to two dimensional 2D.",
    "output": "このスライドでは、別のデータ削減の例である、3次元の3Dから二次元の2Dへの削減をお見せする。"
  },
  {
    "index": "F17161",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By the way, in the more typical example of dimensionality reduction we might have a thousand dimensional data or 1000D data that we might want to reduce to let's say a hundred dimensional or 100D, but because of the limitations of what I can plot on the slide.",
    "output": "ところで、より典型的な次元削減の例としては、1000次元、つまり1000Dとかのデータとかもあり、それを例えば100次元または100Dに削減したい、とかいう事がある。"
  },
  {
    "index": "F17162",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to use examples of 3D to 2D, or 2D to 1D.",
    "output": "だがスライドにプロット可能な限界という制限の為に、3Dから2Dと、2Dから1Dの例を使っていく。"
  },
  {
    "index": "F17163",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's have a data set like that shown here.",
    "output": "さて、ここに見せたようなデータセットがあるとする。"
  },
  {
    "index": "F17164",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, I would have a set of examples x(i) which are points in r3.",
    "output": "つまり、手本の集合x(i)があり、それはR3の点の集まりだ。"
  },
  {
    "index": "F17165",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I have three dimension examples.",
    "output": "つまり、三次元の手本がある。"
  },
  {
    "index": "F17166",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I know it might be a little bit hard to see this on the slide, but I'll show a 3D point cloud in a little bit.",
    "output": "これはスライド上では見づらいとは思うが、3次元プロットの雲を一応見せておく。"
  },
  {
    "index": "F17167",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it might be hard to see here, but all of this data maybe lies roughly on the plane, like so.",
    "output": "ちょっと見づらいけど、このデータはだいたい全部平面に乗っている。こんな感じ。"
  },
  {
    "index": "F17168",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what we can do with dimensionality reduction, is take all of this data and project the data down onto a two dimensional plane.",
    "output": "そこで次元削減でやる事としては、このデータを全部持ってきて二次元平面に射影する事。"
  },
  {
    "index": "F17169",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here what I've done is, I've taken all the data and I've projected all of the data, so that it all lies on the plane.",
    "output": "ここで私がやった事は、データを全部持ってきて、全てのデータを平面上に乗るように射影した、という事。"
  },
  {
    "index": "F17170",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, finally, in order to specify the location of a point within a plane, we need two numbers, right?",
    "output": "さて、最終的に、平面上の位置を示す為に、二つの数字が必要だ。でしょ?"
  },
  {
    "index": "F17171",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We need to, maybe, specify the location of a point along this axis, and then also specify it's location along that axis.",
    "output": "この軸に沿った点の位置を、示す必要があり、そしてまたこの軸に沿った場所も示す必要がある。つまり我らは二つの数字が要る。"
  },
  {
    "index": "F17172",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we need two numbers, maybe called z1 and z2 to specify the location of a point within a plane.",
    "output": "z1とz2と呼ぼうか。平面上の点の場所を示す為に。"
  },
  {
    "index": "F17173",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, what that means, is that we can now represent each example, each training example, using two numbers that I've drawn here, z1, and z2.",
    "output": "つまりそれの意味する所は、今や我らは各手本を各トレーニング手本を、ここに書いた二つの数z1とz2で、表す事が出来る。"
  },
  {
    "index": "F17174",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, our data can be represented using vector z which are in r2.",
    "output": "つまり我らはデータを、R2のベクトルzを用いて表す事が出来る、という事だ。"
  },
  {
    "index": "F17175",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And these subscript, z subscript 1, z subscript 2, what I just mean by that is that my vectors here, z, you know, are two dimensional vectors, z1, z2.",
    "output": "そしてこれらの下付き添字、z下付き添字1、z下付き添字2は、それが意味するのは、このベクトルzは、二次元のベクトルz1とz2だ、という事だ。"
  },
  {
    "index": "F17176",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if I have some particular examples, z(i), or that's the two dimensional vector, z(i)1, z(i)2.",
    "output": "そしてもし私はある特定の手本、z(i)があるとする、またはそれは二次元ベクトルz(i)1とz(i)2だ。"
  },
  {
    "index": "F17177",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And on the previous slide when I was reducing data to one dimensional data then I had only z1, right?",
    "output": "そして前のスライドで、データを一次元に削減した時、z1しか無かった。"
  },
  {
    "index": "F17178",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that is what a z1 subscript 1 on the previous slide was, but here I have two dimensional data, so I have z1 and z2 as the two components of the data.",
    "output": "そしてそれこそが前のスライドでのz(1)下付き添字1だった。だがここでは二次元のデータなので、z1とz2を持ってる。"
  },
  {
    "index": "F17179",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, let me just make sure that these figures make sense.",
    "output": "さてこれらの図の意味を確認しよう。"
  },
  {
    "index": "F17180",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let me just reshow these exact three figures again but with 3D plots.",
    "output": "その為に、これらの三つの図を全く同じ物を、けど3Dプロットで再掲する。"
  },
  {
    "index": "F17181",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the process we went through was that shown in the lab is the optimal data set, in the middle the data set projects on the 2D, and on the right the 2D data sets with z1 and z2 as the axis.",
    "output": "我らがやってきたプロセスは、まず左にあるのがもともとのデータセットで、真ん中が2Dに投影したデータセット。そして右側が、z1とz2としての、2Dのデータセット。"
  },
  {
    "index": "F17182",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at them a little bit further.",
    "output": "それらをもうちょっと詳しく見てみよう。"
  },
  {
    "index": "F17183",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's my original data set, shown on the left, and so I had started off with a 3D point cloud like so, where the axis are labeled x1, x2, x3, and so there's a 3D point but most of the data, maybe roughly lies on some, you know, not too far from some 2D plain.",
    "output": "つまり私は、3Dの点の雲から始めた。そこでは、軸はx1,x2,x3などとラベルづけされていた。"
  },
  {
    "index": "F17184",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what we can do is take this data and here's my middle figure.",
    "output": "そこでこんな事が出来る、このデータを持ってきて、これが真ん中の図だ。"
  },
  {
    "index": "F17185",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to project it onto 2D.",
    "output": "それを2Dに射影する。"
  },
  {
    "index": "F17186",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I've projected this data so that all of it now lies on this 2D surface.",
    "output": "つまり、これら全てのデータが2Dの表面に乗るように射影した。見ての通り、全てのデータは平面上にある。"
  },
  {
    "index": "F17187",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As you can see all the data lies on a plane, 'cause we've projected everything onto a plane, and so what this means is that now I need only two numbers, z1 and z2, to represent the location of point on the plane.",
    "output": "何故なら、全てを平面に射影したから。つまりこれが意味する事は、今や平面上の点の位置を表すには、たった二つの数、z1とz2しか、必要としない。"
  },
  {
    "index": "F17188",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's the process that we can go through to reduce our data from three dimensional to two dimensional.",
    "output": "以上が、データを3次元から2次元に削減する時に行う手続きとなる。"
  },
  {
    "index": "F17189",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's dimensionality reduction and how we can use it to compress our data.",
    "output": "以上が次元削減で、そしてそれを用いてデータを圧縮する方法だ。"
  },
  {
    "index": "F17190",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as we'll see later this will allow us to make some of our learning algorithms run much later as well, but we'll get to that only in a later video.",
    "output": "そして後で見るように、これは我らの学習アルゴリズムをもっと早く走らせる時にもまた用いる事が出来るが、それはあとのビデオでやろう。"
  },
  {
    "index": "F17191",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last video, we talked about dimensionality reduction for the purpose of compressing the data.",
    "output": "前回のビデオで、次元圧縮の話をしてきた。"
  },
  {
    "index": "F17192",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to tell you about a second application of dimensionality reduction and that is to visualize the data.",
    "output": "このビデオでは二番目の適用例である、データの可視化についてお話する。"
  },
  {
    "index": "F17193",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For a lot of machine learning applications, it really helps us to develop effective learning algorithms, if we can understand our data better.",
    "output": "たくさんの機械学習の応用に対してそれは本当に効率的な学習アルゴリズムを構築するのを助けてくれる。"
  },
  {
    "index": "F17194",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If there is some way of visualizing the data better, and so, dimensionality reduction offers us, often, another useful tool to do so.",
    "output": "我らがデータをより良く理解出来るか、データを可視化する、より良い方法があるかは。次元削減は我らにそれを行う新たな手段を与えてくれる。"
  },
  {
    "index": "F17195",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's start with an example.",
    "output": "例を見る事から始めよう。"
  },
  {
    "index": "F17196",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we've collected a large data set of many statistics and facts about different countries around the world.",
    "output": "世界中の様々な国に対する、たくさんの統計量や事実に関する大量のデータを収集したとしよう。"
  },
  {
    "index": "F17197",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, maybe the first feature, X1 is the country's GDP, or the Gross Domestic Product, and X2 is a per capita, meaning the per person GDP, X3 human development index, life expectancy, X5, X6 and so on.",
    "output": "例えば最初のフィーチャーx1は国のGDP、つまりGrossDomesticProduct(国民総生産)かもしれない。x2はpercapita、つまり一人あたりGDPかもしれないし、x3は人間開発指数(HumanDevelopmentIndex)とか平均余命とか、x5、x6などなど。"
  },
  {
    "index": "F17198",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we may have a huge data set like this, where, you know, maybe 50 features for every country, and we have a huge set of countries.",
    "output": "そしてこんな風に膨大なデータセットを持ってるとする。例えば国ごとに50フィーチャーを、そして国の数も膨大とする。"
  },
  {
    "index": "F17199",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So is there something we can do to try to understand our data better?",
    "output": "すると、我らがデータをより良く理解する為に何かやれる事は無いか?"
  },
  {
    "index": "F17200",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I've given this huge table of numbers.",
    "output": "この膨大な数のテーブルを与えられたとする。"
  },
  {
    "index": "F17201",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How do you visualize this data?",
    "output": "このデータをどう可視化する?"
  },
  {
    "index": "F17202",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have 50 features, it's very difficult to plot 50-dimensional data.",
    "output": "もし50のフィーチャーがあると、50次元のデータをプロットするのは計算する。"
  },
  {
    "index": "F17203",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What is a good way to examine this data?",
    "output": "このデータを調べる、良い方法は無いものか?"
  },
  {
    "index": "F17204",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Using dimensionality reduction, what we can do is, instead of having each country represented by this featured vector, xi, which is 50-dimensional, so instead of, say, having a country like Canada, instead of having 50 numbers to represent the features of Canada, let's say we can come up with a different feature representation that is these z vectors, that is in R2.",
    "output": "次元削減を用いると出来る事としては、各国をこのフィーチャーベクトルxiで表す代わりに、それは50次元な訳だが、つまり例えばカナダのような国をカナダを表す50個のフィーチャーを持つ代わりに、別のフィーチャーベクトルで表現して、それはこれらのR2のベクトルzのような物を得られるとしよう。"
  },
  {
    "index": "F17205",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If that's the case, if we can have just a pair of numbers, z1 and z2 that somehow, summarizes my 50 numbers, maybe what we can do to.",
    "output": "もしそれが可能なら、もし我らは50個の数字を要約するような、2つの数字のペアz1とz2が得られるなら、我らに取れる手段としては、これらの国々をR2にプロットする、そしてそれを用いて、別々の国のフィーチャー空間をより良く理解する事を試みる、という手が考えられる。"
  },
  {
    "index": "F17206",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's often up to us to figure out you know, roughly what these features means.",
    "output": "そうする事で、これを2次元プロットしてプロット出来るようになる。"
  },
  {
    "index": "F17207",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, And if you plot those features, here is what you might find.",
    "output": "だが、もしこれらのフィーチャーをプロットすると、例えばこんな結果が得られる。"
  },
  {
    "index": "F17208",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here, every country is represented by a point ZI, which is an R2 and so each of those.",
    "output": "ここで、各国は、点z(i)で表される。"
  },
  {
    "index": "F17209",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Dots, and this figure represents a country, and so, here's Z1 and here's Z2, and of these.",
    "output": "だからこの図で、各点が、一つの国を表している。つまりこれがz1でこれがz2。"
  },
  {
    "index": "F17210",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you might find, for example, That the horizontial axis the Z1 axis corresponds roughly to the overall country size, or the overall economic activity of a country.",
    "output": "だから、例えば横軸のz1軸はだいたいの国のサイズ、または国の全体的な経済活動のサイズに、対応している、と気づくかもしれない。"
  },
  {
    "index": "F17211",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the overall GDP, overall economic size of a country.",
    "output": "つまり、全体のGDP、国全体の経済活動のサイズ。"
  },
  {
    "index": "F17212",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas the vertical axis in our data might correspond to the per person GDP.",
    "output": "一方このデータの縦軸は、一人当たりGDPなどに対応しているかもしれない。"
  },
  {
    "index": "F17213",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or the per person well being, or the per person economic activity, and, you might find that, given these 50 features, you know, these are really the 2 main dimensions of the deviation, and so, out here you may have a country like the U.S.A., which is a relatively large GDP, you know, is a very large GDP and a relatively high per-person GDP as well.",
    "output": "または個人ごとの暮らし向き、または一人当たりの経済活動。そしてこれら50個のフィーチャーが与えられた時に、これら二つの次元が、もっとも主要な分散となっている、と気づくかもしれない。"
  },
  {
    "index": "F17214",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas here you might have a country like Singapore, which actually has a very high per person GDP as well, but because Singapore is a much smaller country the overall economy size of Singapore is much smaller than the US.",
    "output": "一方、ここには、シンガポールのような国、一人当たりのGDPがとても高くて、でもシンガポールはもっとずっと小さい国なので、シンガポールの経済全体のサイズはUSよりもずっと小さい。"
  },
  {
    "index": "F17215",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, over here, you would have countries where individuals are unfortunately some are less well off, maybe shorter life expectancy, less health care, less economic maturity that's why smaller countries, whereas a point like this will correspond to a country that has a fair, has a substantial amount of economic activity, but where individuals tend to be somewhat less well off.",
    "output": "そして、こちらは、不幸にもより厚生が低く、平均余命が短かったりだとか、ヘルスケアがいまいちだったりとか、より経済的に成熟していないだとかで、かつ小さな国で、一方でこの点などは、かなりの経済活動を行っていながら、しかし個人はあまり良い暮らし向きじゃない傾向にある国に対応している。"
  },
  {
    "index": "F17216",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you might find that the axes Z1 and Z2 can help you to most succinctly capture really what are the two main dimensions of the variations amongst different countries.",
    "output": "つまり、z1軸とz2軸は様々な国の間の中でもっとも違う二つの次元は何なのか、を完結に捕捉する事を助けてくれる事に、気づいただろうか。"
  },
  {
    "index": "F17217",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Such as the overall economic activity of the country projected by the size of the country's overall economy as well as the per-person individual well-being, measured by per-person GDP, per-person healthcare, and things like that.",
    "output": "国の経済活動全体だとか、国のサイズを反映した経済活動全体だとか、そういった物や、各個人の健康状態や厚生、一人あたりで測ったGDP、一人あたり健康保険、などなど。"
  },
  {
    "index": "F17218",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's how you can use dimensionality reduction, in order to reduce data from 50 dimensions or whatever, down to two dimensions, or maybe down to three dimensions, so that you can plot it and understand your data better.",
    "output": "以上が、どうやって次元削減を用いて、データを50次元からであれ何からであれ、そこから二次元とか、もしくは三次元に削減する方法だ、それをプロットして、よりデータを理解出来るように。"
  },
  {
    "index": "F17219",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video, we'll start to develop a specific algorithm, called PCA, or Principal Component Analysis, which will allow us to do this and also do the earlier application I talked about of compressing the data.",
    "output": "次のビデオでは、PCAとかPrincipalComponentAnalysis(主成分分析)などと呼ばれるアルゴリズムを開発する。それは我らがここで述べた事や以前に述べたデータの圧縮のような応用に用いる事が出来る。"
  },
  {
    "index": "F17220",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For the problem of dimensionality reduction, by far the most popular, by far the most commonly used algorithm is something called principle components analysis, or PCA.",
    "output": "次元削減の問題において、現在の所一番人気で、一番良く使われているアルゴリズムは、主成分分析(PrincipalComponentsAnalysis)、またはPCAと呼ばれる物だ。"
  },
  {
    "index": "F17221",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to start talking about the problem formulation for PCA.",
    "output": "このビデオでは、PCAの問題の定式化について議論を開始する。"
  },
  {
    "index": "F17222",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In other words, let's try to formulate, precisely, exactly what we would like PCA to do.",
    "output": "言い換えると、PCAに何をして欲しいのか、を詳細かつ厳密に定式化しよう。"
  },
  {
    "index": "F17223",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we have a data set like this.",
    "output": "こんなデータセットがあるとしよう。"
  },
  {
    "index": "F17224",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is a data set of examples x and R2 and let's say I want to reduce the dimension of the data from two-dimensional to one-dimensional.",
    "output": "つまり手本xがR2にあるようなデータセット。そしてデータの次元を2次元から1次元に削減したいとしよう。"
  },
  {
    "index": "F17225",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In other words, I would like to find a line onto which to project the data.",
    "output": "言い換えると、私は、データを射影する先の直線を探したい。"
  },
  {
    "index": "F17226",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what seems like a good line onto which to project the data, it's a line like this, might be a pretty good choice.",
    "output": "では、データを射影するのに良さそうな直線とは、どんな物か?"
  },
  {
    "index": "F17227",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason we think this might be a good choice is that if you look at where the projected versions of the point scales, so I take this point and project it down here.",
    "output": "こんな感じの直線はかなり良い選択だと言えそうだ。"
  },
  {
    "index": "F17228",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Get that, this point gets projected here, to here, to here, to here.",
    "output": "そしてこれが良い選択だと思う理由は、もし射影された点がどこに行くのかを見てみると、つまりこの点に対して、ここに射影するとこれを得る。"
  },
  {
    "index": "F17229",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we find is that the distance between each point and the projected version is pretty small.",
    "output": "この点はここに射影される、ここ、ここ、ここ、ここ、各点と射影された点との距離がとても小さくなっている事に気付くだろう。"
  },
  {
    "index": "F17230",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, these blue line segments are pretty short.",
    "output": "つまり、この青い線はきわめて短い。"
  },
  {
    "index": "F17231",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what PCA does formally is it tries to find a lower dimensional surface, really a line in this case, onto which to project the data so that the sum of squares of these little blue line segments is minimized.",
    "output": "つまり、PCAがやる事はより低次元の平面、この場合は実際は直線になるが、そういう平面で射影する先として、これらの青い線分の二乗和を最小化する物を探そうと試みる事だ。"
  },
  {
    "index": "F17232",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The length of those blue line segments, that's sometimes also called the projection error.",
    "output": "これらの青い線分の長さは射影誤差と呼ばれる事もある。"
  },
  {
    "index": "F17233",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what PCA does is it tries to find a surface onto which to project the data so as to minimize that.",
    "output": "するとPCAがやる事はそれを最小化するような射影先の平面を探す事と言える。"
  },
  {
    "index": "F17234",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As an aside, before applying PCA, it's standard practice to first perform mean normalization at feature scaling so that the features x1 and x2 should have zero mean, and should have comparable ranges of values.",
    "output": "ちよっと脇にそれるが、PCAを適用する前にはます平均標準化を、そしてフィーチャースケーリングをかけておく、そうする事でフィーチャーx1とフィーチャーx2が平均0で、比較可能な範囲の値を持つようにしておく。"
  },
  {
    "index": "F17235",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I've already done this for this example, but I'll come back to this later and talk more about feature scaling and the normalization in the context of PCA later.",
    "output": "この例に関しては既に私がやった。だがこの件については、PCAという文脈でのフィーチャースケーリングと平均標準化については、後でもっと詳しく議論する事にする。"
  },
  {
    "index": "F17236",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But coming back to this example, in contrast to the red line that I just drew, here's a different line onto which I could project my data, which is this magenta line.",
    "output": "この例に戻ると、このさっき引いた赤の直線とは別のデータを射影する直線もあり得る。このマゼンタの直線とか。"
  },
  {
    "index": "F17237",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, as we'll see, this magenta line is a much worse direction onto which to project my data, right?",
    "output": "見て分かるように、このマゼンタの直線はデータの射影先としては、よりまずい方向だ。でしょ?"
  },
  {
    "index": "F17238",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if I were to project my data onto the magenta line, we'd get a set of points like that.",
    "output": "つまりもしこのマゼンタの直線にデータを射影すると、他の点もこんな感じで。"
  },
  {
    "index": "F17239",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the projection errors, that is these blue line segments, will be huge.",
    "output": "そして射影誤差、つまり青い線分は巨大になる。"
  },
  {
    "index": "F17240",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So these points have to move a huge distance in order to get projected onto the magenta line.",
    "output": "つまりこれらの点はマゼンタの直線の上に移動するには、つまり射影するには、大きな距離を移動しなくてはならない。"
  },
  {
    "index": "F17241",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's why PCA, principal components analysis, will choose something like the red line rather than the magenta line down here.",
    "output": "つまりPCA、PrincipalComponentAnalysis(主成分分析)は、ここのマゼンタの直線のような物じゃなく赤い線みたいな物を選ぶ物だ。"
  },
  {
    "index": "F17242",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's write out the PCA problem a little more formally.",
    "output": "PCAの問題をよりフォーマルに書こう。"
  },
  {
    "index": "F17243",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The goal of PCA, if we want to reduce data from two-dimensional to one-dimensional is, we're going to try find a vector that is a vector u1, which is going to be an Rn, so that would be an R2 in this case.",
    "output": "PCAのゴールは仮にデータを2次元から1次元に削減したいとすると、以下のようなベクトルを探す、と言える。"
  },
  {
    "index": "F17244",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm gonna find the direction onto which to project the data, so it's to minimize the projection error.",
    "output": "そのベクトルはuiと呼ぶ事にしよう、それはRnのベクトルで、この場合はR2だ。それは射影誤差を最小化するようなデータの射影先の方向を持つようなベクトルだ。"
  },
  {
    "index": "F17245",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in this example I'm hoping that PCA will find this vector, which l wanna call u(1), so that when I project the data onto the line that I define by extending out this vector, I end up with pretty small reconstruction errors.",
    "output": "つまりこの例では、PCAにはこのベクトルを探してくれる事を期待している、それをu1と呼ぼう、それの持つ性質は、そのベクトルを延長して定義した直線にデータを射影すると、とても小さな射影誤差となるようなベクトルだ。"
  },
  {
    "index": "F17246",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that reference of data that looks like this.",
    "output": "データの例はこんな感じだ。"
  },
  {
    "index": "F17247",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way, I should mention that where the PCA gives me u(1) or -u(1), doesn't matter.",
    "output": "ところで、PCAはu1を与えるか-u1を与えるかは、重要では無い、という事は言っておこう。"
  },
  {
    "index": "F17248",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if it gives me a positive vector in this direction, that's fine.",
    "output": "だからPCAが正のベクトルでこの向きの物を与えたら、それはそれで良い。"
  },
  {
    "index": "F17249",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If it gives me the opposite vector facing in the opposite direction, so that would be like minus u(1). Let's draw that in blue instead, right?",
    "output": "またもし、反対方向のベクトル、反対の向きを向いたベクトルを与えたら、つまり-u1だったとして、代わりに青で描くと、正のu1を与えようが-u1を与えようが、それはどっちでも良い。"
  },
  {
    "index": "F17250",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But it gives a positive u(1) or negative u(1), it doesn't matter because each of these vectors defines the same red line onto which I'm projecting my data.",
    "output": "何故ならこれらのベクトルはどちらも、同じ赤い直線を定義する物で、そこに私はデータを射影する訳だから。"
  },
  {
    "index": "F17251",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is a case of reducing data from two-dimensional to one-dimensional.",
    "output": "以上が、データを2次元から1次元に削減するケースだ。"
  },
  {
    "index": "F17252",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the more general case we have n-dimensional data and we'll want to reduce it to k-dimensions.",
    "output": "より一般的には、N次元のデータセットを持っていて、そしてそれをK次元へと削減したい。"
  },
  {
    "index": "F17253",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case we want to find not just a single vector onto which to project the data but we want to find k-dimensions onto which to project the data.",
    "output": "その場合は、データを射影する先の単独のベクトルを探したいのでは無く、データを射影するK次元を探したい。"
  },
  {
    "index": "F17254",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So as to minimize this projection error.",
    "output": "射影誤差を最小化するように。"
  },
  {
    "index": "F17255",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's the example.",
    "output": "例えばこんなだ。"
  },
  {
    "index": "F17256",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I have a 3D point cloud like this, then maybe what I want to do is find vectors. So find a pair of vectors.",
    "output": "仮に、こんな感じの3D点の雲があったとしよう、そこて私が見つけたいのは、ベクトル、、、じゃなかった、ベクトルのペアを探したい。"
  },
  {
    "index": "F17257",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to find a pair of vectors, sustained from the origin.",
    "output": "私はベクトルのペアを探したい、ここを原点として、これがu1、これが二番目のベクトルu2と呼ぼう。"
  },
  {
    "index": "F17258",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And together, these two vectors define a plane, or they define a 2D surface, right?",
    "output": "そしてこれらを合わせて、これら二つのベクトルが、平面を定義する。言い換えるとそれらが2D平面を定義する。"
  },
  {
    "index": "F17259",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Like this with a 2D surface onto which I am going to project my data.",
    "output": "たとえばこんな感じの2D平面、データを射影する先の。"
  },
  {
    "index": "F17260",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we're going to do is project the data onto the linear subspace spanned by this set of k vectors.",
    "output": "線形代数に通じた視聴者の方々には、真の線形代数マスターの方々にとっては、これの正式な定義は、我らはu1,u2,...,ukまでのベクトルを探す、そして我らがやる事は、このk本のベクトルが張る線形部分空間に、データを射影する、という事だ。"
  },
  {
    "index": "F17261",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you're not familiar with linear algebra, just think of it as finding k directions instead of just one direction onto which to project the data.",
    "output": "だが線形代数にあんまり慣れていないなら、データを射影する先として、1方向の代わりにk方向を探す、と考えておけばよろしい。"
  },
  {
    "index": "F17262",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So finding a k-dimensional surface is really finding a 2D plane in this case, shown in this figure, where we can define the position of the points in a plane using k directions.",
    "output": "さて、k次元の平面を探す為に、このケースでは実際は2D平面だが、この図に示したように、ここでは平面上の点の位置を、k本の方向で定義出来る。"
  },
  {
    "index": "F17263",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's why for PCA we want to find k vectors onto which to project the data.",
    "output": "そんな訳で、PCAにおいては、我らはデータを射影する先となるk本のベクトルを見つけたい。"
  },
  {
    "index": "F17264",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so more formally in PCA, what we want to do is find this way to project the data so as to minimize the sort of projection distance, which is the distance between the points and the projections.",
    "output": "つまり、よりフォーマルに言うと、PCAにおいては、我らがやりたい事は、ある種の射影距離、つまり射影先の点と射影元の点との距離を最小化するような、射影方法を見つけたい。"
  },
  {
    "index": "F17265",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so in this 3D example too.",
    "output": "つまりこの3Dの例でも、所与の点に対し、この各点を2Dの平面に射影する。"
  },
  {
    "index": "F17266",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given a point we would take the point and project it onto this 2D surface. We are done with that.",
    "output": "それを行う時には、射影誤差は、元の点と2D平面へ射影した先の点との距離となる。"
  },
  {
    "index": "F17267",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the projection error would be, the distance between the point and where it gets projected down to my 2D surface.",
    "output": "つまりPCAがやる事とは、データを射影する、直線なり平面なりそれ以外なり、とにかく射影先で、射影の二乗を最小化するような物を探すという事だ、射影とは90度、または直行する射影の誤差だ。"
  },
  {
    "index": "F17268",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what PCA does is I try to find the line, or a plane, or whatever, onto which to project the data, to try to minimize that square projection, that 90 degree or that orthogonal projection error.",
    "output": "最後に、たまに質問される事の一つに、PCAと線形回帰の関係について、というのがある。"
  },
  {
    "index": "F17269",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, one question I sometimes get asked is how does PCA relate to linear regression? Because when explaining PCA, I sometimes end up drawing diagrams like these and that looks a little bit like linear regression.",
    "output": "何故なら私がPCAを説明する時に、たまにこんな図を描くので、これが線形回帰みたいだからだろう。"
  },
  {
    "index": "F17270",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out PCA is not linear regression, and despite some cosmetic similarity, these are actually totally different algorithms.",
    "output": "実はPCAは、線形回帰では無い。表面上はある程度似ているにもかかわらず、これらは実は全く異なるアルゴリズムなのだ。"
  },
  {
    "index": "F17271",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we were doing linear regression, what we would do would be, on the left we would be trying to predict the value of some variable y given some info features x.",
    "output": "線形回帰をやる時というのは、我らがやるのは、この左側で、あるフィーチャーxを入力としてある変数yを予想しようと試みる、という事だ。"
  },
  {
    "index": "F17272",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so linear regression, what we're doing is we're fitting a straight line so as to minimize the square error between point and this straight line.",
    "output": "つまり線形回帰においては、我らがやっている事は、点と直線との距離による二乗誤差を最小化するような、直線をフィッティングしているのだ。"
  },
  {
    "index": "F17273",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what we're minimizing would be the squared magnitude of these blue lines.",
    "output": "だから我らが最小化するのは、この青い線の二乗だ。"
  },
  {
    "index": "F17274",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And notice that I'm drawing these blue lines vertically.",
    "output": "そして私はこれらの青い線を垂直に書いた事に気づいただろうか。"
  },
  {
    "index": "F17275",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That these blue lines are the vertical distance between the point and the value predicted by the hypothesis. Whereas in contrast, in PCA, what it does is it tries to minimize the magnitude of these blue lines, which are drawn at an angle.",
    "output": "それらは点と仮説が予測した値との垂直な距離となっている、一方で対称的にPCAにおいては、これらの青い線の大きさを最小化したい、これは角度をつけて描いてあり、実際は直行した最短距離で、点とこの赤い直線との最短距離で、そしてこれは、データセットによっては、大きく異なった結果となる。"
  },
  {
    "index": "F17276",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And more generally, when you're doing linear regression, there is this distinguished variable y they we're trying to predict.",
    "output": "より一般的には、、、より一般的には、線形回帰をする時には、特別な変数yという物が予測しようとする変数として存在する。"
  },
  {
    "index": "F17277",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "All that linear regression as well as taking all the values of x and try to use that to predict y.",
    "output": "線形回帰とは、Xを全て使ってYを予測しようとする物だ。"
  },
  {
    "index": "F17278",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in PCA, there is no distinguish, or there is no special variable y that we're trying to predict.",
    "output": "一方でPCAは、特別な役割の変数Y、予測しようとする変数Yは存在しない。"
  },
  {
    "index": "F17279",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And instead, we have a list of features, x1, x2, and so on, up to xn, and all of these features are treated equally, so no one of them is special.",
    "output": "その代わりに、フィーチャーのリストx1,x2...とxnまでのフィーチャーがあって、これらのフィーチャーは全て等しく扱われる。つまりどれも特別では無い。"
  },
  {
    "index": "F17280",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As one last example, if I have three-dimensional data and I want to reduce data from 3D to 2D, so maybe I wanna find two directions, u(1) and u(2), onto which to project my data.",
    "output": "最後の例として、三次元のデータの場合、そしてこれを3Dから2Dに削減したい場合、つまり例えば二つの方向、u1とu2を探して、そこにデータを射影したい。"
  },
  {
    "index": "F17281",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then what I have is I have three features, x1, x2, x3, and all of these are treated alike.",
    "output": "その場合、最初にあるのは、3つのフィーチャーx1,x2,x3で、これらは全て同じように扱われる。"
  },
  {
    "index": "F17282",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "All of these are treated symmetrically and there's no special variable y that I'm trying to predict.",
    "output": "これら三つは全て対称的に扱われて、予測をしたいと思う特別な変数yは存在していない。"
  },
  {
    "index": "F17283",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so PCA is not a linear regression, and even though at some cosmetic level they might look related, these are actually very different algorithms.",
    "output": "つまり、PCAは線形回帰じゃないって事だ。表面上はある程度関係しているように見えるかもしれないが、これらは実際のところ、とても異なったアルゴリズムだ。"
  },
  {
    "index": "F17284",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully you now understand what PCA is doing.",
    "output": "以上で、PCAが何をしているか、わかったかな。"
  },
  {
    "index": "F17285",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's trying to find a lower dimensional surface onto which to project the data, so as to minimize this squared projection error. To minimize the square distance between each point and the location of where it gets projected.",
    "output": "PCAは、以下の条件を満たすような、より低い次元の平面を探す事を試みる、その条件とはこの二乗射影誤差を最小化するような物、各点と射影先の点の位置との距離の二乗を最小化する、という条件。"
  },
  {
    "index": "F17286",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video, we'll start to talk about how to actually find this lower dimensional surface onto which to project the data.",
    "output": "次のビデオでは、データを射影する先の低次元の平面を実際にどうやって探すのかについて、議論を開始する。"
  },
  {
    "index": "F17287",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I'd like to tell you about the principle components analysis algorithm.",
    "output": "このビデオでは、主成分分析のアルゴリズムについてお話ししたい。"
  },
  {
    "index": "F17288",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the end of this video you know to implement PCA for yourself.",
    "output": "このビデオが終わる頃には、あなたはPCAを自分自身で実装する方法を知る事になる。"
  },
  {
    "index": "F17289",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And use it reduce the dimension of your data.",
    "output": "そしてそれを用いてあなたのデータの次元を削減する。"
  },
  {
    "index": "F17290",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Before applying PCA, there is a data pre-processing step which you should always do.",
    "output": "PCAを適用する前に、必ずやらなくてはいけないデータの前処理のステップがある。"
  },
  {
    "index": "F17291",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given the trading sets of the examples is important to always perform mean normalization, and then depending on your data, maybe perform feature scaling as well.",
    "output": "トレーニングセットの手本が与えられたとして、必ずやらなくてはいけない事は、平均標準化だ。そして次にデータによっては、フィーチャースケーリングも実行する場合がある。"
  },
  {
    "index": "F17292",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "this is very similar to the mean normalization and feature scaling process that we have for supervised learning.",
    "output": "これは教師あり学習での平均標準化とフィーチャースケーリングのプロセスととても似た物だ。"
  },
  {
    "index": "F17293",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In fact it's exactly the same procedure except that we're doing it now to our unlabeled data, X1 through Xm.",
    "output": "実のところ、それはラベル無しデータのx1からxmに対して行う、という事以外は完全に同じ物だ。"
  },
  {
    "index": "F17294",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for mean normalization we first compute the mean of each feature and then we replace each feature, X, with X minus its mean, and so this makes each feature now have exactly zero mean The different features have very different scales.",
    "output": "平均標準化の為に、各フィーチャーの平均をまず計算する必要がある。そして各フィーチャーxをx-平均で置き換える。"
  },
  {
    "index": "F17295",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for example, if x1 is the size of a house, and x2 is the number of bedrooms, to use our earlier example, we then also scale each feature to have a comparable range of values.",
    "output": "もしそれぞれのフィーチャーがとても異なるスケールを持つなら、例えばx1が家の大きさ、x2が寝室の数なら、以前の例を引っ張り出してみたが、そうすると各フィーチャーが比較可能な範囲になるように、スケールする必要もある。"
  },
  {
    "index": "F17296",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, similar to what we had with supervised learning, we would take x, i substitute j, that's the j feature and so we would subtract of the mean, now that's what we have on top, and then divide by sj.",
    "output": "だから教師あり学習でやったのと同様に、x(i)の下付き添字jをこれはjのフィーチャーだが、そこから平均を引いて、これが分子となる、そしてそれをsjで割る。"
  },
  {
    "index": "F17297",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here, sj is some measure of the beta values of feature j.",
    "output": "ここでsjはフィーチャーjの値の範囲に関する、なんらかの指標だ。"
  },
  {
    "index": "F17298",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, it could be the max minus min value, or more commonly, it is the standard deviation of feature j.",
    "output": "つまり、max-minでも良いし、もっと良く使われている物としては、フィーチャーjの標準偏差がある。"
  },
  {
    "index": "F17299",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having done this sort of data pre-processing, here's what the PCA algorithm does.",
    "output": "この種のデータの前処理を行った上で、PCAはこんな事をする。"
  },
  {
    "index": "F17300",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We saw from the previous video that what PCA does is, it tries to find a lower dimensional sub-space onto which to project the data, so as to minimize the squared projection errors, sum of the squared projection errors, as the square of the length of those blue lines that and so what we wanted to do specifically is find a vector, u1, which specifies that direction or in the 2D case we want to find two vectors, u1 and u2, to define this surface onto which to project the data.",
    "output": "または2Dのケースでは二つのベクトル、u1とu2を見つけたい、データを射影するこの平面を定義するような。"
  },
  {
    "index": "F17301",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just as a quick reminder of what reducing the dimension of the data means, for this example on the left we were given the examples xI, which are in r2.",
    "output": "ではちょっと簡単に、データの次元を削減する、というのが何を意味するのかを振り返ってみよう。この例では、左側には手本xiが与えられていて、それはR2に存在する。"
  },
  {
    "index": "F17302",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we like to do is find a set of numbers zI in r push to represent our data.",
    "output": "そして我らがやりたい事は、我らのデータを表すRの数字ziを、探す事だ。"
  },
  {
    "index": "F17303",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's what from reduction from 2D to 1D means.",
    "output": "以上が2Dから1Dへの削減、の意味する所だ。"
  },
  {
    "index": "F17304",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So specifically by projecting data onto this red line there. We need only one number to specify the position of the points on the line.",
    "output": "具体的には、この赤い直線へデータを射影する事で、この直線の上の点の位置を示すのは、数字一つで十分だ。"
  },
  {
    "index": "F17305",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So i'm going to call that number z or z1.",
    "output": "その数字を、zとかz1と呼ぶ事にしよう。"
  },
  {
    "index": "F17306",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Z here real number, so that's like a one dimensional vector.",
    "output": "ここでzは実数だ。つまりこれは一次元ベクトルみたいな物だ。"
  },
  {
    "index": "F17307",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So z1 just refers to the first component of this, you know, one by one matrix, or this one dimensional vector.",
    "output": "z1は単にこの1x1行列、または1次元ベクトルの最初の要素を表す。"
  },
  {
    "index": "F17308",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so we need only one number to specify the position of a point.",
    "output": "つまり点の位置を示すのに一つの数しか必要としない。"
  },
  {
    "index": "F17309",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if this example here was my example X1, then maybe that gets mapped here.",
    "output": "そしてこの手本がx2なら、それはここにマップされるかな。"
  },
  {
    "index": "F17310",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if this example was X2 maybe that example gets mapped And so this point here will be Z1 and this point here will be Z2, and similarly we would have those other points for These, maybe X3, X4, X5 get mapped to Z1, Z2, Z3.",
    "output": "つまりこのここの点はz1となり、ここのこの点はz2となる。同様に、これら他の点も、x3,x4,x5とすると、これらはz3,z4,z5にマップされるなど。"
  },
  {
    "index": "F17311",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So What PCA has to do is we need to come up with a way to compute two things.",
    "output": "つまりPCAの任務は、二つの事を計算する方法を考え出す必要がある、という事だ。"
  },
  {
    "index": "F17312",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is to compute these vectors, u1, and in this case u1 and u2.",
    "output": "一つ目はこれらのベクトル、u1と、こちらのケースではu1とu2。"
  },
  {
    "index": "F17313",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the other is how do we compute these numbers, Z.",
    "output": "そしてもう一つは、これらの数、Zを計算する方法。"
  },
  {
    "index": "F17314",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So on the example on the left we're reducing the data from 2D to 1D.",
    "output": "つまり左側の例では、データを2Dから1Dへと削減した。"
  },
  {
    "index": "F17315",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the example on the right, we would be reducing data from 3 dimensional as in r3, to zi, which is now two dimensional.",
    "output": "右側の例では、データを3次元、R3から、zi、これはここでは2次元へと削減している。"
  },
  {
    "index": "F17316",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So these z vectors would now be two dimensional.",
    "output": "つまりこれらのzベクトルは、いまは2次元だ。"
  },
  {
    "index": "F17317",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it would be z1 z2 like so, and so we need to give away to compute these new representations, the z1 and z2 of the data as well.",
    "output": "つまりそれはz1とz2、みたいな形。だから我らはこれらの新しい表現z1とz2を計算する方法を同様に知らなくてはならない。"
  },
  {
    "index": "F17318",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So how do you compute all of these quantities?",
    "output": "ではこれら全ての量を、どうやって計算したら良いだろうか?"
  },
  {
    "index": "F17319",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that a mathematical derivation, also the mathematical proof, for what is the right value U1, U2, Z1, Z2, and so on.",
    "output": "数学的な導出は、数学的な証明も、u1,u2,z1,z2などの正しい値は何か、を証明する事は、その数学的証明はとても複雑で、このコースの範囲を越える。"
  },
  {
    "index": "F17320",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But once you've done it turns out that the procedure to actually find the value of u1 that you want is not that hard, even though so that the mathematical proof that this value is the correct value is someone more involved and more than i want to get into.",
    "output": "だが一度その手順をマスターしてしばえば、実際に望みのu1の値を探す手続きというのは、そんなに大変でも無い事が分かる。この値が正しい値だ、と数学的に証明するのは、よりしっかりとやらないと分からない事で、それは私がこのコースでやりたいレベルを越えてしまう。"
  },
  {
    "index": "F17321",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But let me just describe the specific procedure that you have to implement in order to compute all of these things, the vectors, u1, u2, the vector z.",
    "output": "だが、具体的な手順は、記述してみよう、これら全てのベクトル、u1,u2,ベクトルzを計算する為にやる必要のある事を。"
  },
  {
    "index": "F17322",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's the procedure.",
    "output": "これがその手順だ。"
  },
  {
    "index": "F17323",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we want to reduce the data to n dimensions to k dimension What we're going to do is first compute something called the covariance matrix, and the covariance matrix is commonly denoted by this Greek alphabet which is the capital Greek alphabet sigma.",
    "output": "そこで我らがやるのは、まず、共分散行列と呼ばれる物を計算する。共分散行列は普通ギリシャ文字の大文字のギリシャ文字のシグマで表す慣例となっている。"
  },
  {
    "index": "F17324",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's a bit unfortunate that the Greek alphabet sigma looks exactly like the summation symbols.",
    "output": "和のシグマとギリシャ文字のシグマがまったく同じなのは、ちょっとした不幸だ。"
  },
  {
    "index": "F17325",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is the Greek alphabet Sigma is used to denote a matrix and this here is a summation symbol.",
    "output": "つまりこのギリシャ文字シグマは行列を示すのに使われ、このこれは和の記号だ。"
  },
  {
    "index": "F17326",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully in these slides there won't be ambiguity about which is Sigma Matrix, the matrix, which is a summation symbol, and hopefully it will be clear from context when I'm using each one.",
    "output": "これらのスライドにおいては、どれがSigma行列、行列か、それが和の記号か、どれがどちらか、その区別は、文脈から私がどっちを使ってるかははっきり分かると思う。"
  },
  {
    "index": "F17327",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How do you compute this matrix let's say we want to store it in an octave variable called sigma.",
    "output": "この行列をどう計算するか?Octaveの変数Sigmaにそれが入っているとしよう。"
  },
  {
    "index": "F17328",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we need to do is compute something called the eigenvectors of the matrix sigma.",
    "output": "我らがやるべき事は、行列Sigmaの固有ベクトルと呼ばれる物を計算する事だ。"
  },
  {
    "index": "F17329",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And an octave, the way you do that is you use this command, u s v equals s v d of sigma.",
    "output": "Octaveでは、それをやる方法は、以下のコマンドを使う事だ:USVイコールのsvdのSigma。"
  },
  {
    "index": "F17330",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "SVD, by the way, stands for singular value decomposition.",
    "output": "ところで、svdは、SingularValueDecomposition(特異値分解)の略。"
  },
  {
    "index": "F17331",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It is much more advanced linear algebra than you actually need to know but now It turns out that when sigma is equal to matrix there is a few ways to compute these are high in vectors and If you are an expert in linear algebra and if you've heard of high in vectors before you may know that there is another octet function called I, which can also be used to compute the same thing.",
    "output": "そしてもしあなたが線形代数のエキスパートで、前から固有ベクトルを知ってたなら、ひょっとしたらoctaveのeigと呼ばれる関数を知ってるかもしれない。これもまた同じ物を計算するのに使う事が出来る。"
  },
  {
    "index": "F17332",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "and It turns out that the SVD function and the I function it will give you the same vectors, although SVD is a little more numerically stable.",
    "output": "結局は、svd関数とeig関数は、同じベクトルを返す。でもsvdの方がちょっとだけ数値計算的に安定だが。"
  },
  {
    "index": "F17333",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I tend to use SVD, although I have a few friends that use the I function to do this as wellbut when you apply this to a covariance matrix sigma it gives you the same thing.",
    "output": "だから私はsvdを使う事にしている、でも私には、同じ事をするのにeig関数を使う友人が何人かいるが。だが、これを共分散行列Sigmaに適用する場合、どちらも同じ結果を返す。"
  },
  {
    "index": "F17334",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is because the covariance matrix always satisfies a mathematical Property called symmetric positive definite You really don't need to know what that means, but the SVD and I-functions are different functions but when they are applied to a covariance matrix which can be proved to always satisfy this mathematical property; they'll always give you the same thing.",
    "output": "それの意味する所は知る必要は無いが、svdとeig関数は、異なる関数だが、共分散行列に適用する時には、いつもこの数学的性質を満たす事が証明されていて、いつも同じ結果を返す、という事だ。"
  },
  {
    "index": "F17335",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Okay, that was probably much more linear algebra than you needed to know.",
    "output": "まあいいや。以上はたぶん知る必要があるよりも大分たくさんの線形代数の詳細だろう。"
  },
  {
    "index": "F17336",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case none of that made sense, don't worry about it.",
    "output": "これら全部が何言ってるかさっぱり分からなくても、気にしなくてよろしい。"
  },
  {
    "index": "F17337",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "All you need to know is that this system command you should implement in Octave.",
    "output": "知らなきゃいけない事の全ては、Octaveで、このシステムコマンドを実装しなくてはいけない、って事だけ。"
  },
  {
    "index": "F17338",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you're implementing this in a different language than Octave or MATLAB, what you should do is find the numerical linear algebra library that can compute the SVD or singular value decomposition, and there are many such libraries for probably all of the major programming languages.",
    "output": "そしてもしOctaveやMATLAB以外の別の言語でこれを実装しなくてはいけない、としたら、その時はあなたは、数値計算の線形代数ライブラリで、svd、つまり特異値分解が計算出来る物を探すべきだ。そんなライブラリは、主要なプログラミング言語ならどれにもたくさんあるはず。"
  },
  {
    "index": "F17339",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "People can use that to compute the matrices u, s, and d of the covariance matrix sigma.",
    "output": "人々はそれを用いて共分散行列シグマの行列U,S,Dを計算するのに使う事が出来る。"
  },
  {
    "index": "F17340",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just to fill in some more details, this covariance matrix sigma will be an n by n matrix.",
    "output": "ではより詳細を埋めていこう。この共分散行列シグマは、n掛けるnの行列だ。"
  },
  {
    "index": "F17341",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And one way to see that is if you look at the definition this is an n by 1 vector and this here I transpose is 1 by N so the product of these two things is going to be an N by N matrix.",
    "output": "それを確認する一つの方法としては、定義を見ると、これはn掛ける1ベクトルで、そしてこれはxiの転置は1掛けるn。だからこれら二つの積は、n掛けるnの行列となる。"
  },
  {
    "index": "F17342",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "1xN transfers, 1xN, so there's an NxN matrix and when we add up all of these you still have an NxN matrix.",
    "output": "1xNの転置と1xNだから、NxN行列となり、これらを全て足しあわせても、NxN行列のまま。"
  },
  {
    "index": "F17343",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what the SVD outputs three matrices, u, s, and v.",
    "output": "そしてsvdの出力は、3つの行列U,S,Vだ。"
  },
  {
    "index": "F17344",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The thing you really need out of the SVD is the u matrix.",
    "output": "svdの結果でこの場合本当に必要とするのは、U行列のみだ。"
  },
  {
    "index": "F17345",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The u matrix will also be a NxN matrix.",
    "output": "U行列もまたNxN行列だ。"
  },
  {
    "index": "F17346",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if we look at the columns of the U matrix it turns out that the columns of the U matrix will be exactly those vectors, u1, u2 and so on.",
    "output": "そしてもしU行列の列を見てみると、U行列の列は、ベクトルu1,u2,などなどとまったく同じ事が分かる。"
  },
  {
    "index": "F17347",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So u, will be matrix.",
    "output": "分かる。つまりUは行列。"
  },
  {
    "index": "F17348",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if we want to reduce the data from n dimensions down to k dimensions, then what we need to do is take the first k vectors.",
    "output": "そしてN次元からk次元へとデータを削減したい場合、やるべき事はまず最初のk本のベクトルを取りそれによって我らがデータを射影したいと思う、u1からukまでの、k本の方向が得られる。"
  },
  {
    "index": "F17349",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "the rest of the procedure from this SVD numerical linear algebra routine we get this matrix u.",
    "output": "そこからの手順としては、SVD数値計算線形代数ライブラリのルーチンからこの行列Uが得られる。"
  },
  {
    "index": "F17350",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just to wrap up the description of the rest of the procedure, from the SVD numerical linear algebra routine we get these matrices u, s, and d.",
    "output": "さて、残りの手順をまとめて記述してみよう。SVD数値計算線形代数ルーチンから、これらの行列、U,S,Dを得る。"
  },
  {
    "index": "F17351",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "we're going to use the first K columns of this matrix to get u1-uK. Now the other thing we need to is take my original data set, X which is an RN And find a lower dimensional representation Z, which is a R K for this data.",
    "output": "さて、次にやるべき事は、元になるデータセットのXを持ってきて、このXはRnだが、そしてより低じ次元の表現であるzを見つけてくる、それはRkだ。"
  },
  {
    "index": "F17352",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the way we're going to do that is take the first K Columns of the U matrix. Construct this matrix.",
    "output": "それを行う方法としては、U行列の最初のk列を取り出して、この行列を構築する。"
  },
  {
    "index": "F17353",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Stack up U1, U2 and so on up to U K in columns.",
    "output": "u1,u2,...とukまで列に積み上げる。"
  },
  {
    "index": "F17354",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's really basically taking, you know, this part of the matrix, the first K columns of this matrix.",
    "output": "それは基本的には、行列のこの部分を取り出す事だ、この行列の最初のk列を。"
  },
  {
    "index": "F17355",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is going to be an N by K matrix.",
    "output": "だからこれは、n掛けるk行列となる。"
  },
  {
    "index": "F17356",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to give this matrix a name.",
    "output": "この行列に名前を与えておこう。"
  },
  {
    "index": "F17357",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to call this matrix U, subscript \"reduce,\" sort of a reduced version of the U matrix maybe.",
    "output": "この行列を、Uの下付き添字reduce、と呼ぶ事にする、reduce(削減)されたバージョンのU、みたいな意味だ。"
  },
  {
    "index": "F17358",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to use it to reduce the dimension of my data.",
    "output": "これを私のデータの次元を削減するのに使っていく。"
  },
  {
    "index": "F17359",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the way I'm going to compute Z is going to let Z be equal to this U reduce matrix transpose times X.",
    "output": "そしてzを計算する方法は、zイコールの、このUreduce行列の転置に、掛ける事のx。"
  },
  {
    "index": "F17360",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When I take this transpose of this U matrix, what I'm going to end up with is these vectors now in rows.",
    "output": "または別の書き方をすると、この転置が意味する所を書きくだして、このU行列の転置を取ると、最終的に得られるのは、今や列に並んでいるこれらのベクトルだ。"
  },
  {
    "index": "F17361",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I have U1 transpose down to UK transpose.",
    "output": "u1転置からuk転置まで。"
  },
  {
    "index": "F17362",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then take that times X, and that's how I get my vector Z.",
    "output": "そしてそれに、掛けることx。これがベクトルzを得る方法だ。"
  },
  {
    "index": "F17363",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just to make sure that these dimensions make sense, this matrix here is going to be k by n and x here is going to be n by 1 and so the product here will be k by 1.",
    "output": "これらの次元を納得する為に確認してみよう。このここの行列は、k掛けるnとなり、ここのxは、n掛ける1となる。"
  },
  {
    "index": "F17364",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so z is k dimensional, is a k dimensional vector, which is exactly what we wanted.",
    "output": "つまりzはk次元の、、、k次元ベクトルで、それはまさに我らの求める物だ。"
  },
  {
    "index": "F17365",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And of course these x's here right, can be Examples in our training set can be examples in our cross validation set, can be examples in our test set, and for example if you know, I wanted to take training example i, I can write this as xi XI and that's what will give me ZI over there.",
    "output": "そしてもちろんこれらのxは、トレーニングセットの手本でも良いし、クロスバリデーションセットの手本でも良いし、テストセットの手本でも良い。例えば、トレーニング手本のxiをとりたいとする。"
  },
  {
    "index": "F17366",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, to summarize, here's the PCA algorithm on one slide.",
    "output": "ではまとめだ。これはPCAアルゴリズムを一つのスライドに納めた。"
  },
  {
    "index": "F17367",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "After mean normalization, to ensure that every feature is zero mean and optional feature scaling whichYou really should do feature scaling if your features take on very different ranges of values.",
    "output": "平均標準化の後で、各フィーチャーの平均は0なのを保証した後で、さらにオプションでフィーチャースケーリングも、もし本当にフィーチャースケーリングが必要なら、もしあなたのフィーチャーがそれぞれとても異なるレンジの値を取るなら、行う。"
  },
  {
    "index": "F17368",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "After this pre-processing we compute the carrier matrix Sigma like so by the way if your data is given as a matrix like hits if you have your data Given in rows like this. If you have a matrix X which is your time trading sets written in rows where x1 transpose down to x1 transpose, this covariance matrix sigma actually has a nice vectorizing implementation.",
    "output": "ところで、こんな形で行列としてデータがある時に、こんな風にデータが行で与えられている時には、もし行列Xとしてトレーニングセットが行の形で書かれていて、x1の転置から、xnの転置までが並んでいるとしたら、これの共分散行列Sigmaは実は、ナイスなベクトル化された実装が存在する。"
  },
  {
    "index": "F17369",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You can implement in octave, you can even run sigma equals 1 over m, times x, which is this matrix up here, transpose times x and this simple expression, that's the vectorize implementation of how to compute the matrix sigma.",
    "output": "Sigmaイコール、1/m掛ける事の、ここにあるXに転置をとって、さらにXを掛ける。このシンプルな式が、行列Sigmaを計算する為のベクトル化した実装だ。"
  },
  {
    "index": "F17370",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm not going to prove that today.",
    "output": "今日のところはこれを証明したりはしない。"
  },
  {
    "index": "F17371",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is the correct vectorization whether you want, you can either numerically test this on yourself by trying out an octave and making sure that both this and this implementations give the same answers or you Can try to prove it yourself mathematically.",
    "output": "もしやりたければ、簡単に自分でOctaveで試す事で、数値的に確認出来るし、つまりこれと、この実装が同じ結果を返す、と。または自分で数学的に証明する事も、やれば出来る。"
  },
  {
    "index": "F17372",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Either way but this is the correct vectorizing implementation, without compusingnext we can apply the SVD routine to get u, s, and d.",
    "output": "どちらにしろ、これは正しいベクトル化した実装だ。"
  },
  {
    "index": "F17373",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we grab the first k columns of the u matrix you reduce and finally this defines how we go from a feature vector x to this reduce dimension representation z.",
    "output": "計算量がより少ない。次にsvdルーチンを適用し、U、S、Dを取得する。"
  },
  {
    "index": "F17374",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similar to k Means if you're apply PCA, they way you'd apply this is with vectors X and RN.",
    "output": "そして次に、U行列の最初のk列をつかみとって、Ureduceとし、そして最後に、これがフィーチャーベクトルxから、この削減された次元の表現zへと変換する方法だ。"
  },
  {
    "index": "F17375",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is not done with X-0 1.",
    "output": "つまりx0=1となるx0無しで行う。"
  },
  {
    "index": "F17376",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that was the PCA algorithm.",
    "output": "以上がPCAアルゴリズムだ。"
  },
  {
    "index": "F17377",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One thing I didn't do is give a mathematical proof that this There it actually give the projection of the data onto the K dimensional subspace onto the K dimensional surface that actually minimizes the square projection error Proof of that is beyond the scope of this course.",
    "output": "それはk次元部分空間への、k次元平面へのデータの射影が、実際に二乗射影誤差を最小化する、という事。それの証明はこのコースのスコープを越える。"
  },
  {
    "index": "F17378",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Fortunately the PCA algorithm can be implemented in not too many lines of code.",
    "output": "幸運な事に、PCAのアルゴリズムはそんなたくさんのコードにならずに実装する事が可能だ。"
  },
  {
    "index": "F17379",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "and if you implement this in octave or algorithm, you actually get a very effective dimensionality reduction algorithm.",
    "output": "そしてこのアルゴリズムをoctaveで実装してしまえば、あなたはとても効率的な次元削減のアルゴリズムを得る事が出来る。"
  },
  {
    "index": "F17380",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that was the PCA algorithm.",
    "output": "以上がPCAアルゴリズムだ。"
  },
  {
    "index": "F17381",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One thing I didn't do was give a mathematical proof that the U1 and U2 and so on and the Z and so on you get out of this procedure is really the choices that would minimize these squared projection error.",
    "output": "一つまだやってない事としては、u1とかu2などや、zなどが、ここまでの手続きで得られるこれらのベクトルがこれらの二乗射影誤差を本当に最小化している、という数学的な証明を与える事だ。"
  },
  {
    "index": "F17382",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, remember we said What PCA tries to do is try to find a surface or line onto which to project the data so as to minimize to square projection error.",
    "output": "思い出してみよう。PCAがやろうとしているのは、データを射影する平面とか直線で、二乗射影誤差を最小化する物を探す、という事だった。"
  },
  {
    "index": "F17383",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I didn't prove that this that, and the mathematical proof of that is beyond the scope of this course.",
    "output": "そしてこれが、その条件をみたしている、とは証明していない。そしてその数学的証明はこのコースの範囲を超えちゃう。"
  },
  {
    "index": "F17384",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But fortunately the PCA algorithm can be implemented in not too many lines of octave code.",
    "output": "だが幸運な事に、PCAのアルゴリズムはOctaveのコードではそんなにたくさんの行をかけずに実装出来る。"
  },
  {
    "index": "F17385",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you implement this, this is actually what will work, or this will work well, and if you implement this algorithm, you get a very effective dimensionality reduction algorithm.",
    "output": "そしてもしこれを実装すれば、これは実際にうまく機能する。そしてこのアルゴリズムを実装する事で、とても有効な次元削減のアルゴリズムを得る事が出来るのだ。"
  },
  {
    "index": "F17386",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That does do the right thing of minimizing this square projection error.",
    "output": "それは射影二乗誤差を最小化するという仕事を正しくこなす。"
  },
  {
    "index": "F17387",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In an earlier video, I had said that PCA can be sometimes used to speed up the running time of a learning algorithm.",
    "output": "以前の動画で、私はPCAは時には、学習アルゴリズムの実行時間をスピードアップする為に使う事が出来る、という事に言及した。"
  },
  {
    "index": "F17388",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to explain how to actually do that, and also say some, just try to give some advice about how to apply PCA.",
    "output": "このビデオで私はそれを実際にどうやるのかを説明する。そしてまた、PCAをどう適用するのかについて、幾つか助言も与えておきたい。"
  },
  {
    "index": "F17389",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's how you can use PCA to speed up a learning algorithm, and this supervised learning algorithm speed up is actually the most common use that I personally make of PCA.",
    "output": "これがPCAを学習アルゴリズムのスピードアップに使う方法だ。そしてこの教師あり学習のスピードアップが私が個人的にPCAを使うもっとも一般的な用途だ。"
  },
  {
    "index": "F17390",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say you have a supervised learning problem, note this is a supervised learning problem with inputs X and labels Y, and let's say that your examples xi are very high dimensional.",
    "output": "入力xとラベルyの教師あり学習。そしてあなたの手本xiが凄い高次元だとしよう。"
  },
  {
    "index": "F17391",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, lets say that your examples, xi are 10,000 dimensional feature vectors.",
    "output": "例えば、あなたの手本xiは、1万次元のフィーチャーベクトルだとする。"
  },
  {
    "index": "F17392",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One example of that, would be, if you were doing some computer vision problem, where you have a 100x100 images, and so if you have 100x100, that's 10000 pixels, and so if xi are, you know, feature vectors that contain your 10000 pixel intensity values, then you have 10000 dimensional feature vectors.",
    "output": "そこでは100x100の画像だとすると、つまり100x100は、1万ピクセルだ。だから例えば、xiが1万ピクセルの明度の値のフィーチャーベクトルだとすれば、1万次元のフィーチャーベクトルを持つ事になる。"
  },
  {
    "index": "F17393",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So with very high-dimensional feature vectors like this, running a learning algorithm can be slow, right?",
    "output": "そのようなとても高い次元のフィーチャーベクトルの場合、学習アルゴリズムを走らせると、遅くなりがちだ。"
  },
  {
    "index": "F17394",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just, if you feed 10,000 dimensional feature vectors into logistic regression, or a new network, or support vector machine or what have you, just because that's a lot of data, that's 10,000 numbers, it can make your learning algorithm run more slowly.",
    "output": "1万次元のフィーチャーベクトルを食わせようとすれば、それがロジスティック回帰でもニューラルネットワークでもサポートベクタマシンでも、その他何でも、単にデータが多量だ、という理由なだけで、それは1万個の数字なので、学習アルゴリズムの実行を遅くしうる。"
  },
  {
    "index": "F17395",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Fortunately with PCA we'll be able to reduce the dimension of this data and so make our algorithms run more efficiently.",
    "output": "幸運にも、PCAでもって、我らはこのデータの次元を減らす事が出来る。だからアルゴリズムをもっと効率的に走らせる事が出来る。"
  },
  {
    "index": "F17396",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's how you do that.",
    "output": "そのやり方はこうだ。"
  },
  {
    "index": "F17397",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We are going first check our labeled training set and extract just the inputs, we're just going to extract the X's and temporarily put aside the Y's.",
    "output": "まず最初にラベル付けされたトレーニングセットをチェックして、入力のみを引っ張り出す。x達を引きぬいて、一時的にyの事は脇にどけておく。"
  },
  {
    "index": "F17398",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this will now give us an unlabelled training set x1 through xm which are maybe there's a ten thousand dimensional data, ten thousand dimensional examples we have.",
    "output": "そうすると、ここには、ラベル無しのトレーニングセットx1からxmが、得られる。それらは、例えば1万次元とかのデータだ。"
  },
  {
    "index": "F17399",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just extract the input vectors x1 through xm.",
    "output": "つまり単に入力ベクトルである所のx1からxmまでを取り出しただけ。"
  },
  {
    "index": "F17400",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then we're going to apply PCA and this will give me a reduced dimension representation of the data, so instead of 10,000 dimensional feature vectors I now have maybe one thousand dimensional feature vectors.",
    "output": "そして次にPCAを適用して、その結果我らは、データの削減された次元の表現を得る、つまり1万次元のフィーチャーベクトルの代わりに、いまや例えば1000次元のフィーチャーベクトルを得る事になる。つまりこれは、10倍の節約になる。"
  },
  {
    "index": "F17401",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this gives me, if you will, a new training set.",
    "output": "つまりこれは我らに、新しいトレーニングセットを与えるのだ。"
  },
  {
    "index": "F17402",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So whereas previously I might have had an example x1, y1, my first training input, is now represented by z1. And so we'll have a new sort of training example, which is Z1 paired with y1.",
    "output": "つまり以前には、私は手本としてx1,y1を持っていた訳だが、それが今や最初のトレーニングの入力はz1となり、つまり我らは、ある種の新しいトレーニング手本を得る訳だ、それはz1とy1がペアになって、同様にz2,y2,点点点と続きzm,ymまで。"
  },
  {
    "index": "F17403",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because my training examples are now represented with this much lower dimensional representation Z1, Z2, up to ZM.",
    "output": "何故なら、トレーニング手本はいまやこのより低い次元の表現、z1,z2,...,zmで表されるから。"
  },
  {
    "index": "F17404",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, I can take this reduced dimension training set and feed it to a learning algorithm maybe a neural network, maybe logistic regression, and I can learn the hypothesis H, that takes this input, these low-dimensional representations Z and tries to make predictions.",
    "output": "最後に、この削減された次元のトレーニングセットに対して、これらを学習アルゴリズム、例えばニューラルネットワークとか、ロジスティック回帰とかに食わせて、そして仮説hを学習する事が出来る、この仮説はこれらの低次元の表現zを入力として受け取り、予測を試みる物だ。"
  },
  {
    "index": "F17405",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if I were using logistic regression for example, I would train a hypothesis that outputs, you know, one over one plus E to the negative-theta transpose Z, that takes this input to one these z vectors, and tries to make a prediction.",
    "output": "例えばロジスティック回帰を使ってるとするなら、1割ることの1足すeのマイナスシータ転置zを出力する仮説を訓練する、これは入力としてこれらのzベクトルの一つを受け取り、予測を試みる。"
  },
  {
    "index": "F17406",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, if you have a new example, maybe a new test example X.",
    "output": "そして最後に、新しい手本を得たら、それは新しいテスト手本のxかもしれないが、そこであなたがすべきは、テストの手本xをPCAで見つけられた同じマッピングを用いて対応するzにマッピングする。"
  },
  {
    "index": "F17407",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What you do is you would take your test example x, map it through the same mapping that was found by PCA to get you your corresponding z.",
    "output": "そして次にそのzをこの仮説に食わせる、そしてこの仮説が入力xに対応する予測を行う。最後に一つ注意を。"
  },
  {
    "index": "F17408",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that z then gets fed to this hypothesis, and this hypothesis then makes a prediction on your input x.",
    "output": "PCAがやっている事はxからzへのマッピングを定義する、という事だ。"
  },
  {
    "index": "F17409",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One final note, what PCA does is it defines a mapping from x to z and this mapping from x to z should be defined by running PCA only on the training sets.",
    "output": "そしてこのxからzへのマッピングはPCAをトレーニングセットだけに対して走らせる事で定義するべきだ。"
  },
  {
    "index": "F17410",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, this mapping that PCA is learning, right, this mapping, what that does is it computes the set of parameters.",
    "output": "具体的に言うと、このマッピング、PCAが学習するこのマッピングは、パラメータの集合を計算する訳だ。"
  },
  {
    "index": "F17411",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That's the feature scaling and mean normalization. And there's also computing this matrix U reduced.",
    "output": "フィーチャースケーリングして平均標準化して、そしてこのUreduce行列を計算する。"
  },
  {
    "index": "F17412",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But all of these things that U reduce, that's like a parameter that is learned by PCA and we should be fitting our parameters only to our training sets and not to our cross validation or test sets and so these things the U reduced so on, that should be obtained by running PCA only on your training set.",
    "output": "これもまたPCAで学習されたパラメータのような物で、我らはパラメータのフィッティングはトレーニングセットだけに対して行うべきだ。そしてクロスバリデーションセットやテストセットに対してフィッティングしてはいけない。"
  },
  {
    "index": "F17413",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then having found U reduced, or having found the parameters for feature scaling where the mean normalization and scaling the scale that you divide the features by to get them on to comparable scales.",
    "output": "だからこれらの事、Ureduceとかは、トレーニングセットだけにPCAを適用して取得するべきである。そしてUreduceを見出したら、またフィーチャースケーリングのパラメータを見出したら、平均標準化して、スケールでフィーチャーを割って比較可能なスケールにするのだった。"
  },
  {
    "index": "F17414",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having found all those parameters on the training set, you can then apply the same mapping to other examples that may be In your cross-validation sets or in your test sets, OK?",
    "output": "これら全てのパラメータをトレーニングセットに対して見出したら、その次には、同じマッピングをその他の手本、クロスバリデーションセットとかテストセットにある手本に対して適用出来るのだ。まとめておこう。"
  },
  {
    "index": "F17415",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just to summarize, when you're running PCA, run your PCA only on the training set portion of the data not the cross-validation set or the test set portion of your data.",
    "output": "PCAを走らせる時は、手持ちのデータのうちトレーニングセットにだけ走らせないといけない。クロスバリデーションセットとテストセットの部分には実行してはいけない。"
  },
  {
    "index": "F17416",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that defines the mapping from x to z and you can then apply that mapping to your cross-validation set and your test set and by the way in this example I talked about reducing the data from ten thousand dimensional to one thousand dimensional, this is actually not that unrealistic.",
    "output": "そしてそうする事で、xからzへのマッピングの定義が得られる。そこで次にそのマッピングをクロスバリデーションセットやテストセットに適用する。"
  },
  {
    "index": "F17417",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For many problems we actually reduce the dimensional data.",
    "output": "ところで、私はデータの削減として、1万次元から1000次元にする、という話をしているが、これはそんなに非現実的な数字では無い。"
  },
  {
    "index": "F17418",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know by 5x maybe by 10x and still retain most of the variance and we can do this barely effecting the performance, in terms of classification accuracy, let's say, barely affecting the classification accuracy of the learning algorithm.",
    "output": "多くの問題で、我らは実際には高次元データを、5倍とか10倍とか削減して、しかもほとんどの分散を保持したままに出来て、だからパフォーマンスにほとんど影響を与えずに行える、パフォーマンスというのは分類の正確さとかの観点という事だが、学習アルゴリズムの正確さにほとんど影響を与えずに行う事が出来る。"
  },
  {
    "index": "F17419",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by working with lower dimensional data our learning algorithm can often run much much faster.",
    "output": "そしてより低い次元のデータで作業を行う事で、学習アルゴリズムは、しばしばずっと早く走る。"
  },
  {
    "index": "F17420",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To summarize, we've so far talked about the following applications of PCA.",
    "output": "まとめると、ここまでに我らは以下のようなPCAの応用例を話してきた。"
  },
  {
    "index": "F17421",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First is the compression application where we might do so to reduce the memory or the disk space needed to store data and we just talked about how to use this to speed up a learning algorithm.",
    "output": "圧縮したいのは、データを保存するのに必要なメモリやディスク容量を減らす為かもしれないし、今話したように、学習アルゴリズムをスピードアップする為かもしれない。"
  },
  {
    "index": "F17422",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In these applications, in order to choose K, often we'll do so according to, figuring out what is the percentage of variance retained, and so for this learning algorithm, speed up application often will retain 99% of the variance.",
    "output": "これらの応用では、kを選ぶ為にはしばしば、保持される分散のパーセンテージを調べる。つまり、この学習アルゴリズムのスピードアップという応用例の場合、よく使われるのは99%の分散を保持する、というライン。"
  },
  {
    "index": "F17423",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That would be a very typical choice for how to choose k.",
    "output": "それはとても典型的なkを選ぶ為の選択と言える。"
  },
  {
    "index": "F17424",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's how you choose k for these compression applications.",
    "output": "以上がこれらの圧縮の応用に際し、kを選ぶ方法だ。"
  },
  {
    "index": "F17425",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas for visualization applications while usually we know how to plot only two dimensional data or three dimensional data, and so for visualization applications, we'll usually choose k equals 2 or k equals 3, because we can plot only 2D and 3D data sets.",
    "output": "つまり可視化の応用では、普通はkを2か3と選ぶ。何故なら我らは2Dか3Dのデータセットしかプロット出来ないから。"
  },
  {
    "index": "F17426",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that summarizes the main applications of PCA, as well as how to choose the value of k for these different applications.",
    "output": "以上がPCAの主な応用の要約だ。それとそれぞれの応用に際してのkの選び方だ。"
  },
  {
    "index": "F17427",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I should mention that there is often one frequent misuse of PCA and you sometimes hear about others doing this hopefully not too often.",
    "output": "あなたも時には、他の人がこれをやってしまっている、という事を耳にする事があるだろう。そんなに多くは無いとは思いたいが。"
  },
  {
    "index": "F17428",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I just want to mention this so that you know not to do it.",
    "output": "私がこれを言及したいのは、あなたにやって欲しくないからだ。"
  },
  {
    "index": "F17429",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And there is one bad use of PCA, which iss to try to use it to prevent over-fitting.",
    "output": "そんな良く無いPCAの誤用としては、オーバーフィッティングを防ぐ為に使ってしまう、という物。"
  },
  {
    "index": "F17430",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's the reasoning.",
    "output": "それはこういう理由だ。"
  },
  {
    "index": "F17431",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is not a great way to use PCA, but here's the reasoning behind this method, which is,you know if we have Xi, then maybe we'll have n features, but if we compress the data, and use Zi instead and that reduces the number of features to k, which could be much lower dimensional.",
    "output": "これはPCAの良い使い方では無い、だが、これがこの手法を用いる背後にある理由だ、それは、えーと、xiがあるとして、それがnフィーチャーだったとする。そしてそのデータを圧縮する、ziを代わりに使う、するとフィーチャーの数をk個に減らせられる、それはnよりもっと低い次元のはずだ。"
  },
  {
    "index": "F17432",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if we have a much smaller number of features, if k is 1,000 and n is 10,000, then if we have only 1,000 dimensional data, maybe we're less likely to over-fit than if we were using 10,000-dimensional data with like a thousand features.",
    "output": "kが1000でnが1万なら、我らは1000次元のデータしか持たなくなるのだから、オーバーフィットもしにくくなるだろう、1万次元のデータを使うよりは、1000個のフィーチャーを使う方が。"
  },
  {
    "index": "F17433",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So some people think of PCA as a way to prevent over-fitting.",
    "output": "つまり、PCAをオーバーフィットを防止する方法と考える人が居る。"
  },
  {
    "index": "F17434",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But just to emphasize this is a bad application of PCA and I do not recommend doing this.",
    "output": "だが、強調しておきたいが、これはPCAの悪い使い方で、これをやるのを、私は推奨しない。"
  },
  {
    "index": "F17435",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it's not that this method works badly.",
    "output": "それはこの手法が悪い振る舞いをするって訳じゃない。"
  },
  {
    "index": "F17436",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you want to use this method to reduce the dimensional data, to try to prevent over-fitting, it might actually work OK.",
    "output": "もしあなたがオーバーフィットを防止する為にデータの次元を減らす為にこの手法を使ったとする、たぶんそれはうまく行くと思う。"
  },
  {
    "index": "F17437",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But this just is not a good way to address over-fitting and instead, if you're worried about over-fitting, there is a much better way to address it, to use regularization instead of using PCA to reduce the dimension of the data.",
    "output": "だが、これは単にオーバーフィッティングに対応するのに良い方法じゃない、とういだけ。その代わりに、もしオーバーフィットに悩んでいるなら、それに対処するもっと良い方法は、PCAを使ってデータの次元を削減する代わりに正規化を使うという事だ。"
  },
  {
    "index": "F17438",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason is, if you think about how PCA works, it does not use the labels y.",
    "output": "その理由は、PCAがどう機能するかを考えてみると、それはラベルyを使わない。"
  },
  {
    "index": "F17439",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You are just looking at your inputs xi, and you're using that to find a lower-dimensional approximation to your data.",
    "output": "単に入力のxiだけを見ていって、そしてそれを用いてデータの低次元の近似を探していく。"
  },
  {
    "index": "F17440",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what PCA does, is it throws away some information.",
    "output": "つまりPCAが行うのは、なんらかの情報を捨て去るって事だが、PCAはyの値が何かを知らずにデータの次元を捨てる、あるいは削減する。"
  },
  {
    "index": "F17441",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It throws away or reduces the dimension of your data without knowing what the values of y is, so this is probably okay using PCA this way is probably okay if, say 99 percent of the variance is retained, if you're keeping most of the variance, but it might also throw away some valuable information.",
    "output": "だから、これはたぶんOKなんだが、こういう風にPCAを使うのはたぶんOKなんだが、、、99%の分散とかを保持している限りは。分散のほとんどを維持しているのだから。"
  },
  {
    "index": "F17442",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it turns out that if you're retaining 99% of the variance or 95% of the variance or whatever, it turns out that just using regularization will often give you at least as good a method for preventing over-fitting and regularization will often just work better, because when you are applying linear regression or logistic regression or some other method with regularization, well, this minimization problem actually knows what the values of y are, and so is less likely to throw away some valuable information, whereas PCA doesn't make use of the labels and is more likely to throw away valuable information.",
    "output": "分散の99%を保持していようが分散の95%を保持していようが、はたまたどれだけの分散を保持していようが、単に正規化を使うだけの方がどんなに悪くても同程度に良いオーバーフィッティングを防ぐ手法だという事が分かっている。そして多くの場合には、正規化の方が単によりうまく機能する。"
  },
  {
    "index": "F17443",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, to summarize, it is a good use of PCA, if your main motivation to speed up your learning algorithm, but using PCA to prevent over-fitting, that is not a good use of PCA, and using regularization instead is really what many people would recommend doing instead.",
    "output": "もしあなたの主なモチベーションが学習アルゴリズムのスピードアップなら、それはPCAの良い使い方だ。だがオーバーフィッティングを防止する為にPCAを使うなら、それは良く無いPCAの使い方だ、その場合は正規化を代わりに使うべきだ、それこそが多くの人々が代わりに推奨している事でもある。"
  },
  {
    "index": "F17444",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, one last misuse of PCA.",
    "output": "最後に、PCAの誤った使い方を一つ。"
  },
  {
    "index": "F17445",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so I should say PCA is a very useful algorithm, I often use it for the compression on the visualization purposes.",
    "output": "PCAはとても便利なアルゴリズムだと言うべきだろう、私は圧縮とか可視化の目的で、良くPCAを使う。"
  },
  {
    "index": "F17446",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, what I sometimes see, is also people sometimes use PCA where it shouldn't be.",
    "output": "だが、私が時々目にするのは、人々がPCAを、使うべきでは無い所でもまた使ってしまっている事がある。"
  },
  {
    "index": "F17447",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here's a pretty common thing that I see, which is if someone is designing a machine-learning system, they may write down the plan like this: let's design a learning system.",
    "output": "こんな事を私は良く見かける:誰かが機械学習のシステムを設計している時に、こんなプランを書きだすとする。"
  },
  {
    "index": "F17448",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Get a training set and then, you know, what I'm going to do is run PCA, then train logistic regression and then test on my test data.",
    "output": "トレーニングセットを集めて、そして次に、PCAを走らせる、そしてロジスティック回帰を訓練し、そしてテストデータでテストする、っと。"
  },
  {
    "index": "F17449",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So often at the very start of a project, someone will just write out a project plan than says lets do these four steps with PCA inside.",
    "output": "つまり、しばしばプロジェクトの本当にしょっぱなの所で、PCAが組み込まれた、これら四つのステップをやろう、という、プロジェクトのプランを書く人が居る。"
  },
  {
    "index": "F17450",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Before writing down a project plan the incorporates PCA like this, one very good question to ask is, well, what if we were to just do the whole without using PCA.",
    "output": "PCAを用いたこういうプロジェクトのプランを書く前に、自身に問うてみるのがとても有益な問いは、PCA無しでこれら全部を行ったらどうだろうか?"
  },
  {
    "index": "F17451",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And often people do not consider this step before coming up with a complicated project plan and implementing PCA and so on.",
    "output": "そしてしばしば人々は、このような複雑なプロジェクトプランを作り出してPCAを実装したりする前に、このステップを検討していない。"
  },
  {
    "index": "F17452",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And sometime, and so specifically, what I often advise people is, before you implement PCA, I would first suggest that, you know, do whatever it is, take whatever it is you want to do and first consider doing it with your original raw data xi, and only if that doesn't do what you want, then implement PCA before using Zi.",
    "output": "だから具体的に言うと、私が良く人々にアドバイスする事としては、PCAを実装する前に、私が最初に提案するのは、あなたがやっている事がなんであれ、何をやりたいのであれ、まずはオリジナルの生のデータxiでやってみる事を検討せよ、という事だ。それが望む結果を生まなかったら、その時になって初めて、PCAを実装し、ziを使う事を検討すべきだ。"
  },
  {
    "index": "F17453",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, before using PCA you know, instead of reducing the dimension of the data, I would consider well, let's ditch this PCA step, and I would consider, let's just train my learning algorithm on my original data.",
    "output": "つまりPCAを使う前に、データの次元を減らす代わりに、私が代わりに検討するのは、このPCAのステップをさぼってみよう。そして検討する事は、オリジナルのデータに対して単純に学習アルゴリズムを訓練してみよう、って事だ。"
  },
  {
    "index": "F17454",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's just use my original raw inputs xi, and I would recommend, instead of putting PCA into the algorithm, just try doing whatever it is you're doing with the xi first.",
    "output": "オリジナルの生の入力xiを使ってみよう、そして私が推奨するのは、PCAをアルゴリズムに組み込む代わりに、あなたがやってる事がなんであれ、最初のxiでやってみる事を推奨する。"
  },
  {
    "index": "F17455",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And only if you have a reason to believe that doesn't work, so that only if your learning algorithm ends up running too slowly, or only if the memory requirement or the disk space requirement is too large, so you want to compress your representation, but if only using the xi doesn't work, only if you have evidence or strong reason to believe that using the xi won't work, then implement PCA and consider using the compressed representation.",
    "output": "そしてそれがうまくいかない、と信じる理由を得て初めて、つまりあなたの学習アルゴリズムがあまりにも実行速度が遅いという結果になって初めて、または必要メモリの要求量や必要ディスクの要求量があまりにも大きくなり過ぎて、だから表現を圧縮したい、と思った時に、、、それはxiを使ってみてうまく行かなかった時になって、、、xiではうまく行かない、という証拠がある時か、またはそう信じる強い理由があるようになって、、、その時初めて、PCAを実装して表現を圧縮する事に使えば良い。"
  },
  {
    "index": "F17456",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because what I do see, is sometimes people start off with a project plan that incorporates PCA inside, and sometimes they, whatever they're doing will work just fine, even with out using PCA instead.",
    "output": "こんな事を言うのは、人々が最初からプロジェクトのプランにPCAを用いる事を計画していて、そして時々、彼らが何をするにせよ、PCA無しでもうまく行く、という事態を私はちょくちょく目撃して来たからだ。"
  },
  {
    "index": "F17457",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just consider that as an alternative as well, before you go to spend a lot of time to get PCA in, figure out what k is and so on.",
    "output": "だからPCAを使うのは、代替案も検討した後にすべきだ、PCAを得たり、どのkを使うべきかとかを見出したりにたくさん時間を費やしたりする前に。"
  },
  {
    "index": "F17458",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's it for PCA.",
    "output": "さて、以上がPCAだ。"
  },
  {
    "index": "F17459",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Despite these last sets of comments, PCA is an incredibly useful algorithm, when you use it for the appropriate applications and I've actually used PCA pretty often and for me, I use it mostly to speed up the running time of my learning algorithms.",
    "output": "そして現実に、私はPCAをしょっちゅう使う。私の場合は、PCAを使うのは、学習アルゴリズムの実行時間をスピードアップするのに使う事が一番多い。"
  },
  {
    "index": "F17460",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But I think, just as common an application of PCA, is to use it to compress data, to reduce the memory or disk space requirements, or to use it to visualize data.",
    "output": "だが私が思うに、PCAはその他の用途も同じくらい一般的だと思う。データを圧縮するのに使う、これはメモリやディスク容量の要件を減らす事が出来るし、またデータの可視化にも良く使われる。"
  },
  {
    "index": "F17461",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And PCA is one of the most commonly used and one of the most powerful unsupervised learning algorithms.",
    "output": "そしてPCAは教師なし学習のアルゴリズムのうち、もっとも良く使われていて、もっともパワフルな物の一つと言えるだろう。"
  },
  {
    "index": "F17462",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And with what you've learned in these videos, I think hopefully you'll be able to implement PCA and use them through all of these purposes as well.",
    "output": "そしてこれらのビデオで学んだ事を用いて、あなたは、きっとPCAを実装する事が出来て、これらの目的全てに対しても使っていく事が出来るだろう。"
  },
  {
    "index": "F17463",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this next set of videos, I'd like to tell you about a problem called Anomaly Detection.",
    "output": "今後の一連のビデオで、アノマリー検出、と言われる問題を扱いたい。"
  },
  {
    "index": "F17464",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is a reasonably commonly use you type machine learning. And one of the interesting aspects is that it's mainly for unsupervised problem, that there's some aspects of it that are also very similar to sort of the supervised learning problem.",
    "output": "これは割と良く使われる種類の機械学習で、そして興味深い側面の一つに、これはだいたい教師なし学習の問題でありながら、またある側面では教師有り学習の問題にとても似ている部分もある。"
  },
  {
    "index": "F17465",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what is anomaly detection?",
    "output": "で、アノマリー検出とは何か?"
  },
  {
    "index": "F17466",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me use the motivating example of: Imagine that you're a manufacturer of aircraft engines, and let's say that as your aircraft engines roll off the assembly line, you're doing, you know, QA or quality assurance testing, and as part of that testing you measure features of your aircraft engine, like maybe, you measure the heat generated, things like the vibrations and so on.",
    "output": "そしてあなたの航空機のエンジンが組み立てラインからロールオフして、そしてQAをしている、つまり品質保証テストをしているとしよう。そしてそのテストの一貫として、航空機のエンジンのある機能ーーそうだなぁ、生成される熱とか振動とかを測っているとしよう。"
  },
  {
    "index": "F17467",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I share some friends that worked on this problem a long time ago, and these were actually the sorts of features that they were collecting off actual aircraft engines so you now have a data set of X1 through Xm, if you have manufactured m aircraft engines, and if you plot your data, maybe it looks like this.",
    "output": "ずっと昔に私の友人達がこの問題に挑んでいてこれらのフィーチャーは本当に彼らが実際の航空機エンジンから集めた物だ。あなたは今、X1からXmのデータセットを持っている。"
  },
  {
    "index": "F17468",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, each point here, each cross here as one of your unlabeled examples.",
    "output": "この各点、各バッテンはあなたのラベル無し手本だ。"
  },
  {
    "index": "F17469",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, the anomaly detection problem is the following.",
    "output": "そしてアノマリー検出の問題は以下のような感じ。"
  },
  {
    "index": "F17470",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that on, you know, the next day, you have a new aircraft engine that rolls off the assembly line and your new aircraft engine has some set of features x-test.",
    "output": "翌日、新しい航空機のエンジンが組立ラインからロールオフしたとしてみよう。あなたの新しい航空エンジンは幾つかのフィーチャーの集合、x-testを持つ。"
  },
  {
    "index": "F17471",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What the anomaly detection problem is, we want to know if this aircraft engine is anomalous in any way, in other words, we want to know if, maybe, this engine should undergo further testing because, or if it looks like an okay engine, and so it's okay to just ship it to a customer without further testing.",
    "output": "アノマリー検出問題とはこの航空機エンジンがとにかく何かしら普通でないかを知りたい。"
  },
  {
    "index": "F17472",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if your new aircraft engine looks like a point over there, well, you know, that looks a lot like the aircraft engines we've seen before, and so maybe we'll say that it looks okay.",
    "output": "言い換えると、たとえばこのエンジンをさらなるテストに回さなきゃいけないかを知りたい、またはこのエンジンは問題なさそうなのかを。問題なさそうで、追加のテスト無しで客に出荷して良さそうかを。"
  },
  {
    "index": "F17473",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas, if your new aircraft engine, if x-test, you know, were a point that were out here, so that if X1 and X2 are the features of this new example.",
    "output": "つまり、その新しい航空機エンジンがそこの点ならうーむ、それ以前のたくさんの航空機エンジンと似てるので、たぶんOKそうでしょう。"
  },
  {
    "index": "F17474",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If x-tests were all the way out there, then we would call that an anomaly.",
    "output": "一方、新しい航空機エンジンがx-testが、ここの点なら、つまりx1とx2がこの新しい手本のフィーチャーなら、x-testがはるかこの外にあるなら、それはアノマリーと呼んで良かろう。"
  },
  {
    "index": "F17475",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "and maybe send that aircraft engine for further testing before we ship it to a customer, since it looks very different than the rest of the aircraft engines we've seen before.",
    "output": "だからたぶん、顧客に出荷する前に追加のテストに送り出しても良いといえよう。だってこのエンジンは、それ以外に見た物と大きく異なっているから。"
  },
  {
    "index": "F17476",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "More formally in the anomaly detection problem, we're give some data sets, x1 through Xm of examples, and we usually assume that these end examples are normal or non-anomalous examples, and we want an algorithm to tell us if some new example x-test is anomalous.",
    "output": "より正式には、アノマリー検出の問題ではなんらかのデータセットが与えられて、それはx1からxmまでの手本としておく、そして、普通はこれらのm個の手本をノーマル、またはアノマリーでは無い、と想定する。"
  },
  {
    "index": "F17477",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The approach that we're going to take is that given this training set, given the unlabeled training set, we're going to build a model for p of x.",
    "output": "そしてある新しいサンプル、x-testが来た時にに、それがアノマリーっぽいかをアルゴリズムに教えて欲しい。その為に我らがとるアプローチは与えられたデータセットに対しラベル無しのトレーニングセットが与えられた時にモデルp(x)を構築する。"
  },
  {
    "index": "F17478",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In other words, we're going to build a model for the probability of x, where x are these features of, say, aircraft engines.",
    "output": "言い換えると、xの時の確率のモデルを構築するということ、ここでxはこれらのフィーチャー、例えば航空機のエンジンとかの。"
  },
  {
    "index": "F17479",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, having built a model of the probability of x we're then going to say that for the new aircraft engine, if p of x-test is less than some epsilon then we flag this as an anomaly.",
    "output": "そしてxの時の確率のモデルを構築して、新しい航空機エンジンに対し、p(x-test)が、あるエプシロンより小さいかを見る。そしてこれがアノマリーかどうかのフラグをつける。"
  },
  {
    "index": "F17480",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we see a new engine that, you know, has very low probability under a model p of x that we estimate from the data, then we flag this anomaly, whereas if p of x-test is, say, greater than or equal to some small threshold.",
    "output": "つまり新しいエンジンでデータから推計したモデル、p(x)による確率がとても小さいのを見かけたら、これをアノマリーとフラグをつける。もしp(x-test)が、例えばある小さな閾値より大きければそれはオーケーっぽいと言うわけ。"
  },
  {
    "index": "F17481",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, given the training set, like that plotted here, if you build a model, hopefully you will find that aircraft engines, or hopefully the model p of x will say that points that lie, you know, somewhere in the middle, that's pretty high probability, whereas points a little bit further out have lower probability.",
    "output": "そして与えられたトレーニングセットがここにプロットしたような物だとして、以下のようなモデルを構築して、航空機エンジンの、、、いや、モデルp(x)に、どこかこの中のあたりにある点に対しては、とても高い確率だと言って欲しく、他方、ちょっと離れた所にある点には、低い確率だと言って欲しい。"
  },
  {
    "index": "F17482",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Points that are even further out have somewhat lower probability, and the point that's way out here, the point that's way out there, would be an anomaly.",
    "output": "さらに遠く離れた点に対してはなんらかの、より低い確率になって欲しい。そしてこの離れた点やこの離れた点はアノマリーだろう。"
  },
  {
    "index": "F17483",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas the point that's way in there, right in the middle, this would be okay because p of x right in the middle of that would be very high cause we've seen a lot of points in that region.",
    "output": "他方ここにある点、ちょうどなかほどにある点、これはOKだろう、だってp(x)はなかほどの点に対してはとても高くなるだろうから、だってその辺にはたくさんの点が見られているから。"
  },
  {
    "index": "F17484",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Perhaps the most common application of anomaly detection is actually for detection if you have many users, and if each of your users take different activities, you know maybe on your website or in the physical plant or something, you can compute features of the different users activities.",
    "output": "たぷん一番一般的なアノマリー検出の応用例は、たくさんのユーザーが居て、各ユーザーが異なるアクティビティを行なっている時に、たとえばwebサイト上とか物理的な工場とかそういうので、各ユーザーごとのアクティビティのフィーチャーを計算する事が出来る時に、モデルを構築して、いわば、異なるユーザーが別々の行動をとる確率を言わせる事が出来る。"
  },
  {
    "index": "F17485",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you can do is build a model to say, you know, what is the probability of different users behaving different ways.",
    "output": "ユーザー行動を表すフィーチャーのあるベクトルがどの位の確率となるのか、たとえばユーザーアクティビティのフィーチャーの例としては、webサイトの場合なら、x1がユーザーのログインの頻度で、x2は、うーん、訪問したページの総数とか、取引の総数で、x3は、うーん、そのユーザーがフォーラムにポストした投稿の総数で、フィーチャーx4はユーザーのタイピング速度とかの可能性だってありえる。"
  },
  {
    "index": "F17486",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What is the probability of a particular vector of features of a users behavior so you know examples of features of a users activity may be on the website it'd be things like, maybe x1 is how often does this user log in, x2, you know, maybe the number of what pages visited, or the number of transactions, maybe x3 is, you know, the number of posts of the users on the forum, feature x4 could be what is the typing speed of the user and some websites can actually track that was the typing speed of this user in characters per second.",
    "output": "実際一秒あたりのユーザーのタイプした文字速度をトラックしているwebサイトもある。"
  },
  {
    "index": "F17487",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so you can model p of x based on this sort of data.",
    "output": "そしてこんな類のデータに対して、p(x)をモデリング出来るわけだ。"
  },
  {
    "index": "F17488",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally having your model p of x, you can try to identify users that are behaving very strangely on your website by checking which ones have probably effects less than epsilon and maybe send the profiles of those users for further review.",
    "output": "そして最後に、その得られたモデルp(x)を使って、あなたのwebサイトで凄く奇妙な行動をとっているユーザーを特定する事が出来る、どのユーザーが確率的にエプシロン以下なのかをチェックする事によって。"
  },
  {
    "index": "F17489",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or demand additional identification from those users, or some such to guard against you know, strange behavior or fraudulent behavior on your website.",
    "output": "そしてそれらのユーザーのプロファイルをさらなるレビューに送り出すとか、または、それらのユーザーからは追加の身分証明を提出させるとか、そういう、あなたのwebサイトを奇妙な行動や詐欺っぽい行動からガードする何らかの措置を講ずるのだ。"
  },
  {
    "index": "F17490",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This sort of technique will tend of flag the users that are behaving unusually, not just users that maybe behaving fraudulently.",
    "output": "この種の技術は、普通でない行動をしているユーザーをフラグ付けしてしまい、それは必ずしも不正をしているユーザーだけとは限らない。"
  },
  {
    "index": "F17491",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So not just constantly having stolen or users that are trying to do funny things, or just find unusual users.",
    "output": "つまりいっつも盗みを働いているユーザーだけじゃなくたんにふざけてるだけのユーザーも。"
  },
  {
    "index": "F17492",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But this is actually the technique that is used by many online websites that sell things to try identify users behaving strangely that might be indicative of either fraudulent behavior or of computer accounts that have been stolen.",
    "output": "だがこれは実際にたくさんの商品を販売しているオンラインwebサイトにおいて、詐欺行為をしているか不正にのっとったアカウントを使っている事を示す事を期待すべく、奇妙な行動をとっているユーザーを見つける為に使われているテクニックだ。"
  },
  {
    "index": "F17493",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Another example of anomaly detection is manufacturing.",
    "output": "アノマリー検出のもう一つの応用例は製造業だ。"
  },
  {
    "index": "F17494",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, already talked about the aircraft engine thing where you can find unusual, say, aircraft engines and send those for further review.",
    "output": "航空機エンジンのケースを既に話したが、そこでは普通とは異なる航空機エンジンを見つけ出して、それらをさらなるレビューへと送り出すのだった。"
  },
  {
    "index": "F17495",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "A third application would be monitoring computers in a data center.",
    "output": "三番目の応用例はデータセンターのコンピュータをモニタリングするという事。"
  },
  {
    "index": "F17496",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I actually have some friends who work on this too.",
    "output": "これに実際に従事してる友達が何人かいるよ。"
  },
  {
    "index": "F17497",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you have a lot of machines in a computer cluster or in a data center, we can do things like compute features at each machine.",
    "output": "コンピュータのクラスタなりデータセンターなりにたくさんのコンピュータがあったとして、各マシンのフィーチャーを計算出来る。"
  },
  {
    "index": "F17498",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As well as more complex features like what is the CPU load on this machine divided by the amount of network traffic on this machine?",
    "output": "例えばどれだけのメモリを使ってるかとかディスクアクセスの総数だとかCPU負荷だとかもっと複雑なフィーチャーでも良い、このマシンのCPUロードをこのマシンのネットワークトラフィックの量で割ったりだとか、そういう物を捉えたようなフィーチャー。"
  },
  {
    "index": "F17499",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then given the dataset of how your computers in your data center usually behave, you can model the probability of x, so you can model the probability of these machines having different amounts of memory use or probability of these machines having different numbers of disc accesses or different CPU loads and so on.",
    "output": "そしてデータセンターの通常時の振る舞いのデータを与えられた時に、xとなる確率をモデリング出来る。つまりこれらのマシンが様々なメモリ使用量となる確率、またはこれらのマシンが様々なディスクアクセスの回数となる確率、様々なCPU負荷となる確率などをモデリング出来る。"
  },
  {
    "index": "F17500",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you ever have a machine whose probability of x, p of x, is very small then you know that machine is behaving unusually and maybe that machine is about to go down, and you can flag that for review by a system administrator.",
    "output": "そしてもし確率xが、p(x)がとても小さいマシンがあったら、そのマシンは普通でなく振舞ってるという事が分かり、そのマシンは落ちる所かもしれないのでそれをフラグ付けしてシステム管理者にレビューさせたり出来る。"
  },
  {
    "index": "F17501",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is actually being used today by various data centers to watch out for unusual things happening on their machines.",
    "output": "そしてこれは実際に、こんにち様々なデータセンターで自分たちのマシンに普通でない事が起きていないか監視するのに使われている。"
  },
  {
    "index": "F17502",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's anomaly detection.",
    "output": "以上がアノマリー検出。"
  },
  {
    "index": "F17503",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video, I'll talk a bit about the Gaussian distribution and review properties of the Gaussian probability distribution, and in videos after that, we will apply it to develop an anomaly detection algorithm.",
    "output": "次のビデオではガウス分布とガウス分布の性質をちょこっと議論し、そしてその後のビデオでアノマリー検出のアルゴリズムを開発するのにそれを使っていく。"
  },
  {
    "index": "F17504",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'd like to talk about the Gaussian distribution which is also called the normal distribution.",
    "output": "このビデオでは、ガウス分布について議論したい、それは正規分布とも呼ばれる。"
  },
  {
    "index": "F17505",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case you're already intimately familiar with the Gaussian distribution, it's probably okay to skip this video, but if you're not sure or if it has been a while since you've worked with the Gaussian distribution or normal distribution then please do watch this video all the way to the end.",
    "output": "もしガウス分布に既に十分に慣れ親しんでいるのなら、たぶんこのビデオはスキップしてOKだ。だがもしあんま自信無かったりガウス分布または正規分布を使っていた頃から随分と時間が経っているのなら、このビデオを終わりまで見てみてください。"
  },
  {
    "index": "F17506",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in the video after this we'll start applying the Gaussian distribution to developing an anomaly detection algorithm.",
    "output": "そしてこのビデオの後には、アノマリー検出のアルゴリズムを開発する為にガウス分布を用いていきます。"
  },
  {
    "index": "F17507",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say x is a row value's random variable, so x is a row number.",
    "output": "xは実数のランダムな数とします。つまりxは実数です。"
  },
  {
    "index": "F17508",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If the probability distribution of x is Gaussian with mean mu and variance sigma squared.",
    "output": "もしxの確率分布がガウス分布で、平均ミューと分散シグマ二乗である事をこう書ける:ランダム変数xチルダここでこの小さなチルダ記号の意味する所は分布が等しいという事で、その後にガウス分布を記述する訳だが、それには、記号のNにかっこでミュー、シグマ二乗と書く事になっている。"
  },
  {
    "index": "F17509",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this script N stands for normal since Gaussian and normal they mean the thing are synonyms.",
    "output": "つまりこの記号NはノーマルのN、だってガウス分布は正規(ノーマル)分布だから。"
  },
  {
    "index": "F17510",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the Gaussian distribution is parametarized by two parameters, by a mean parameter which we denote mu and a variance parameter which we denote via sigma squared.",
    "output": "それは同じ意味で、Normalの省略系でガウス分布は2つのパラメータでパラメトライズされてる、という事を表す。2つのパラメータとは平均を表すミューとシグマ二乗で示される分散のパラメータ。"
  },
  {
    "index": "F17511",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we plot the Gaussian distribution or Gaussian probability density. It'll look like the bell shaped curve which you may have seen before.",
    "output": "ガウス分布またはガウス確率密度をプロットすると、ベル型のカーブとなる、見たことあるかもしれないね。"
  },
  {
    "index": "F17512",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this bell shaped curve is paramafied by those two parameters, mu and sequel.",
    "output": "そしてつまり、このベル型のカーブは2つのパラメータ、ミューとシグマでパラメトライズされてる。"
  },
  {
    "index": "F17513",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the location of the center of this bell shaped curve is the mean mu.",
    "output": "このベル型のカーブの中心は平均のミューで、そしてこのベル型のカーブの幅がだいたい、このパラメータシグマであり、1標準偏差とも呼ばれている。"
  },
  {
    "index": "F17514",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the width of this bell shaped curve, roughly that, is this parameter, sigma, is also called one standard deviation, and so this specifies the probability of x taking on different values.",
    "output": "これはxが様々な値を取る確率を示している、つまりxがこの真ん中の値を取ると、極めて高くなる。"
  },
  {
    "index": "F17515",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, x taking on values here in the middle here it's pretty high, since the Gaussian density here is pretty high, whereas x taking on values further, and further away will be diminishing in probability.",
    "output": "なぜならガウス分布のここはとても高い一方でxがこの遥か離れた所の値を取る場合は確率は減衰するだろう。"
  },
  {
    "index": "F17516",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally just for completeness let me write out the formula for the Gaussian distribution.",
    "output": "最後に、完璧を期するという目的の為だけに、ガウス分布の式を書き下しておこう。"
  },
  {
    "index": "F17517",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the probability of x, and I'll sometimes write this as the p (x) when we write this as P ( x ; mu, sigma squared), and so this denotes that the probability of X is parameterized by the two parameters mu and sigma squared.",
    "output": "xの確率は、、、ところでたまに、p(x)と書く代わりに、これをpのxにセミコロンでミュー、シグマ二乗とつなげて書くことがある。これはxの確率は2つのパラメータミューとシグマ二乗でパラメトライズされている事を示す。"
  },
  {
    "index": "F17518",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the formula for the Gaussian density is this 1/ root 2 pi, sigma e (-(x-mu/g) squared/2 sigma squared.",
    "output": "そしてガウス密度の式はこれだ。2パイのシグマ分の一のeのマイナスx引くミューの二乗割ることの2シグマ二乗。"
  },
  {
    "index": "F17519",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there's no need to memorize this formula.",
    "output": "この式を暗記する必要は無い。"
  },
  {
    "index": "F17520",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is just the formula for the bell-shaped curve over here on the left.",
    "output": "これは単にここの左にあるベル型のカーブの式ってだけに過ぎない。"
  },
  {
    "index": "F17521",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's no need to memorize it, and if you ever need to use this, you can always look this up.",
    "output": "それを暗記する必要は無いし、もしこれを使う必要が出てきても、その時調べれば良い。"
  },
  {
    "index": "F17522",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that figure on the left, that is what you get if you take a fixed value of mu and take a fixed value of sigma, and you plot P(x) so this curve here.",
    "output": "以上がこの左にある図でこれがミューとシグマを固定してp(x)をプロットしたら得る物だ。"
  },
  {
    "index": "F17523",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is really p(x) plotted as a function of X for a fixed value of Mu and of sigma squared.",
    "output": "このここのカーブ、これがミューとシグマ二乗、つまり分散を固定してp(x)をxの関数としてプロットした物だ。"
  },
  {
    "index": "F17524",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And sometimes is easier to think in terms of sigma.",
    "output": "時には(シグマ二乗より)シグマで考えた方が楽な事もある。"
  },
  {
    "index": "F17525",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So sigma is called the standard deviation, and so it specifies the width of this Gaussian probability density, where as the square sigma, or sigma squared, is called the variance.",
    "output": "シグマは標準偏差と呼ばれる物でそれはガウスの確率分布の幅を規定するもので、一方シグマ二乗、シグマの二乗は分散と呼ばれる物。"
  },
  {
    "index": "F17526",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at some examples of what the Gaussian distribution looks like.",
    "output": "ガウス分布が実際にどんな感じか、例を見てみよう。"
  },
  {
    "index": "F17527",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then we have a Gaussian distribution that's centered around zero, because that's mu and the width of this Gaussian, so that's one standard deviation is sigma over there.",
    "output": "ミューが0でシグマが1だとすると、ゼロを中心に持つガウス分布となる、何故ならそれはミューの事で、ガウス分布の幅はつまり1標準偏差はここのシグマとなる。"
  },
  {
    "index": "F17528",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at some examples of Gaussians.",
    "output": "ガウス分布の例を見てみよう。"
  },
  {
    "index": "F17529",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If mu is equal to zero and sigma equals one, then that corresponds to a Gaussian distribution that is centered at zero, since mu is zero, and the width of this Gaussian is is controlled by sigma by that variance parameter sigma.",
    "output": "ミューが0でシグマが1の時はゼロの所を中心とするガウス分布に対応する、何故ならミューが0だから。そしてガウス分布の幅は、、、ガウス分布では幅はシグマにより、つまり分散のパラメータ、シグマにより制御されている。"
  },
  {
    "index": "F17530",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's another example.",
    "output": "もう一つの例はこんな感じ。"
  },
  {
    "index": "F17531",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That same mu is equal to 0 and sigma is equal to .5 so the standard deviation is .5 and the variance sigma squared would therefore be the square of 0.5 would be 0.25 and in that case the Gaussian distribution, the Gaussian probability density goes like this.",
    "output": "ミューが0でシグマが1/2としよう。つまり標準偏差が1/2で分散、シグマ二乗は0.5の二乗、つまり0.25となる。"
  },
  {
    "index": "F17532",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But now the width of this is much smaller because the smaller the area is, the width of this Gaussian density is roughly half as wide.",
    "output": "その場合、ガウス分布、ガウス確率密度はこんな感じとなる、今回もゼロを中心としているが、だが今回は幅がより狭くなっている、何故なら分散が小さくなったから。ガウス密度はだいたい半分の幅となっている。"
  },
  {
    "index": "F17533",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But because this is a probability distribution, the area under the curve, that's the shaded area there.",
    "output": "だがこれは確率分布なのだから、カーブの下の面積、つまり影をつけた部分は、積分すると必ず1となる。"
  },
  {
    "index": "F17534",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That area must integrate to one this is a property of probability distributing.",
    "output": "これは確率分布の性質だ。"
  },
  {
    "index": "F17535",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is a much taller Gaussian density because this half is Y but half the standard deviation but it twice as tall.",
    "output": "また、そうであるから、これはより背の高いガウス密度となる、だって標準偏差が半分だから幅も半分になるので、高さは二倍となる訳だ。もう一つ例。"
  },
  {
    "index": "F17536",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Another example is sigma is equal to 2 then you get a much fatter a much wider Gaussian density and so here the sigma parameter controls that Gaussian distribution has a wider width.",
    "output": "シグマが2だとより太った、またはより幅の広いガウス密度となる。つまりここでは、パラメータのシグマはガウス密度がどれだけの幅を持つかを制御している。"
  },
  {
    "index": "F17537",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And once again, the area under the curve, that is the shaded area, will always integrate to one, that's the property of probability distributions and because it's wider it's also half as tall in order to still integrate to the same thing.",
    "output": "そして今回も、曲線の下の面積はこの影をつけた領域だが、それはいつでも積分すると1となる。"
  },
  {
    "index": "F17538",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally one last example would be if we now change the mu parameters as well.",
    "output": "それは確率分布の性質で今回はより幅が広くなったのだから、高さは半分になっている、積分した結果が同じになるように。"
  },
  {
    "index": "F17539",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then instead of being centered at 0 we now have a Gaussian distribution that's centered at 3 because this shifts over the entire Gaussian distribution.",
    "output": "そして最後に、最後の例は、ミューも同じように変更していった場合、ゼロを真ん中に分布する代わりに、ここでは3の回りに分布するガウス分布を得る。何故ならこれはガウス分布全体をシフトさせるから。"
  },
  {
    "index": "F17540",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, let's talk about the Parameter estimation problem.",
    "output": "次にパラメータの推計の話題にうつろう。"
  },
  {
    "index": "F17541",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what's the parameter estimation problem?",
    "output": "パラメータの推計問題とはなんだろう?"
  },
  {
    "index": "F17542",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we have a dataset of m examples so exponents x m and lets say each of this example is a row number.",
    "output": "m個の手本データセットがあるとしよう。つまりx1からxmまで。"
  },
  {
    "index": "F17543",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here in the figure I've plotted an example of the dataset so the horizontal axis is the x axis and either will have a range of examples of x, and I've just plotted them on this figure here.",
    "output": "そしてこれらの手本の個々は実数だとしよう。この図で、私は手本のデータセットをプロットした。"
  },
  {
    "index": "F17544",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the parameter estimation problem is, let's say I suspect that these examples came from a Gaussian distribution.",
    "output": "横軸はx軸で手本のデータは、xがある範囲に広がっている。それをここに単純にプロットしてみた。"
  },
  {
    "index": "F17545",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's say I suspect that each of my examples, x i, was distributed. That's what this tilde thing means.",
    "output": "そしてパラメータ推計の問題はこれらの手本がガウス分布だったと思っているとして、つまりこの各手本x(i)が以下のように分布、、、それがこのチルダの意味だったね。"
  },
  {
    "index": "F17546",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's not suspect that each of these examples were distributed according to a normal distribution, or Gaussian distribution, with some parameter mu and some parameter sigma square.",
    "output": "で、これらの各手本が、正規分布またの名をガウス分布に従って分布していると思ってるとする、あるパラメータ、ミューとシグマ二乗の。"
  },
  {
    "index": "F17547",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But I don't know what the values of these parameters are.",
    "output": "だが、これらの値が幾つかは知らない。"
  },
  {
    "index": "F17548",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The problem of parameter estimation is, given my data set, I want to try to figure out, well I want to estimate what are the values of mu and sigma squared.",
    "output": "パラメータ推計の問題とは、与えられたデータセットに対して、ミューやシグマ二乗の値が何になるのかを推計したい、という事。"
  },
  {
    "index": "F17549",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you're given a data set like this, it looks like maybe if I estimate what Gaussian distribution the data came from, maybe that might be roughly the Gaussian distribution it came from.",
    "output": "もしこんなデータセットを与えられたら、どんなガウス分布からこのデータが生成されているかを推計したら、多分だいたいミューを中心としてシグマ、つまり標準偏差がこのガウス分布の幅をコントロールしている。"
  },
  {
    "index": "F17550",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Seems like a reasonable fit to the data.",
    "output": "これはだいたいデータにリーズナブルにフィットしている。"
  },
  {
    "index": "F17551",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because, you know, looks like the data has a very high probability of being in the central region, and a low probability of being further out, even though probability of being further out, and so on.",
    "output": "何故ならデータは見た感じ中心のあたりにとても高い確率を持っていて、外に離れれば離れる程低い確率となっている。"
  },
  {
    "index": "F17552",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So maybe this is a reasonable estimate of mu and sigma squared.",
    "output": "だからこれはたぶんミューとシグマ二乗のリーズナブルな推計となっている。"
  },
  {
    "index": "F17553",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, if it corresponds to a Gaussian distribution function that looks like this.",
    "output": "つまり、そのデータがガウス分布に従っているのなら、それはこんな見た目のはずだ。"
  },
  {
    "index": "F17554",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what I'm going to do is just write out the formula the standard formulas for estimating the parameters Mu and sigma squared.",
    "output": "そこで私は、こうする:式、つまり正規分布の式を書き下して、パラメータのミューとシグマ二乗を推計する。"
  },
  {
    "index": "F17555",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Our estimate or the way we're going to estimate mu is going to be just the average of my example.",
    "output": "ミューを推計する方法は単に平均をとるだけ、手本の全体にわたって。"
  },
  {
    "index": "F17556",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So mu is the mean parameter.",
    "output": "ミューは平均のパラメータだから。"
  },
  {
    "index": "F17557",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just take my training set, take my m examples and average them.",
    "output": "つまりトレーニングセットから、m個の手本から、それらの平均をとる。"
  },
  {
    "index": "F17558",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that just means the center of this distribution.",
    "output": "それは単にこの分布の、中心を与える。"
  },
  {
    "index": "F17559",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How about sigma squared?",
    "output": "ではシグマ二乗はどうか?"
  },
  {
    "index": "F17560",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, the variance, I'll just write out the standard formula again, I'm going to estimate as sum over one through m of x i minus mu squared.",
    "output": "分散に関しては、また通常の式を書き下すと、1からmまでに渡り、x(i)からミューを引いた物の二乗の和を計算する。"
  },
  {
    "index": "F17561",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this mu here is actually the mu that I compute over here using this formula.",
    "output": "ここでこのミューはこの式を使ってここで計算したものだ。"
  },
  {
    "index": "F17562",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what the variance is, or one interpretation of the variance is that if you look at this term, that's the square difference between the value I got in my example minus the mean.",
    "output": "そして分散とは何か、というと一つの解釈としては、この項を見ると分かるように、手本の値と平均との差の二乗となっている。"
  },
  {
    "index": "F17563",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Minus the center, minus the mean of the distribution.",
    "output": "つまり中心との差、分布の平均との差の。"
  },
  {
    "index": "F17564",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so in the variance I'm gonna estimate as just the average of the square differences between my examples, minus the mean.",
    "output": "そして分散を、差分、つまり手本と平均との間の差分の二乗の平均として推計する訳だ。"
  },
  {
    "index": "F17565",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you're an expert in statistics, and if you've heard of maximum likelihood estimation, then these parameters, these estimates, are actually the maximum likelihood estimates of the primes of mu and sigma squared but if you haven't heard of that before don't worry about it, all you need to know is that these are the two standard formulas for how to figure out what are mu and Sigma squared given the data set.",
    "output": "ここでちょっと補足しておくと、あなたが統計のエキスパートだったら統計のエキスパートだったら最尤法という物について聞いたことがあるかもしれない。そうであるなら、これらの推計は実際には最尤法によるパラメータ、ミューとシグマ二乗の推計である。"
  },
  {
    "index": "F17566",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally one last side comment again only for those of you that have maybe taken the statistics class before but if you've taken statistics This class before.",
    "output": "これもまた、以前に統計のクラスを取った事のある人向けの話だ。"
  },
  {
    "index": "F17567",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Some of you may have seen the formula here where this is M-1 instead of M so this first term becomes 1/M-1 instead of 1/M.",
    "output": "以前に統計のクラスを取った事があるなら、ここにある式を以前に見た時はmではなくてm-1だったかもしれない。"
  },
  {
    "index": "F17568",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In machine learning people tend to learn 1/M formula but in practice whether it is 1/M or 1/M-1 it makes essentially no difference assuming M is reasonably large.",
    "output": "つまりの最初の項が、1/mではなくて、1/(m-1)になる。機械学習では1/mの式が使われる事が多い。"
  },
  {
    "index": "F17569",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "a reasonably large training set size.",
    "output": "だが現実問題としては、1/mだろうが1/(m-1)だろうが、本質的には大差無い、mが普通に考えられる程度に大きい、つまりトレーニングセットのサイズが普通に大きければ。"
  },
  {
    "index": "F17570",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just in case you've seen this other version before.",
    "output": "ようするに、以前に別のバージョンの方を見た事があった場合の為に補足しておくと、どっちのバージョンでもちゃんと機能する。"
  },
  {
    "index": "F17571",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In either version it works just about equally well but in machine learning most people tend to use 1/M in this formula.And the two versions have slightly different theoretical properties like these are different math properties.",
    "output": "だが機械学習では多くの人々はこの公式の1/mの方を使う傾向にある。2つのバージョンは理論的にはわずかに異なった特徴があり、わずかに異なった数学的な特徴があるが、現実の場面ではほとんど違いは無い。"
  },
  {
    "index": "F17572",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Bit of practice it really makes makes very little difference, if any.",
    "output": "以上で、ガウス分布がどんな感じか、感覚的に分かるようになってくれただろうか。"
  },
  {
    "index": "F17573",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, hopefully you now have a good sense of what the Gaussian distribution looks like, as well as how to estimate the parameters mu and sigma squared of Gaussian distribution if you're given a training set, that is if you're given a set of data that you suspect comes from a Gaussian distribution with unknown parameters, mu and sigma squared.",
    "output": "トレーニングセットを与えられた時、ガウス分布に従ったデータだと思われるがそのパラメータ、ミューとシグマ二乗が不明な時。"
  },
  {
    "index": "F17574",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video, we'll start to take this and apply it to develop an anomaly detection algorithm.",
    "output": "次のビデオでは、これを用いて、アノマリー検出のアルゴリズムを開発するのに適用していく。"
  },
  {
    "index": "F17575",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last video, we talked about the Gaussian distribution.",
    "output": "前回のビデオではガウス分布について話した。"
  },
  {
    "index": "F17576",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video lets apply that to develop an anomaly detection algorithm.",
    "output": "今回のビデオではそれを用いてアノマリー検出のアルゴリズムを開発する。"
  },
  {
    "index": "F17577",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that we have an unlabeled training set of M examples, and each of these examples is going to be a feature in Rn so your training set could be, feature vectors from the last M aircraft engines being manufactured.",
    "output": "これらの手本の個々はRnに属するフィーチャーとなっている。つまりトレーニングセットは例えば製造した直近m個の航空機エンジンのフィーチャーベクトルでも良し、m人のユーザーのフィーチャーでもそれ以外の何かでも良い。"
  },
  {
    "index": "F17578",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The way we are going to address anomaly detection, is we are going to model p of x from the data sets.",
    "output": "アノマリー検出にどうアプローチしていくか、というと、データセットからp(x)をモデリングしていきます。"
  },
  {
    "index": "F17579",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to try to figure out what are high probability features, what are lower probability types of features.",
    "output": "どんなフィーチャーの組みの確率が高くてどんなフィーチャーの組みが低い確率なのかを見つけ出したい。"
  },
  {
    "index": "F17580",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, x is a vector and what we are going to do is model p of x, as probability of x1, that is of the first component of x, times the probability of x2, that is the probability of the second feature, times the probability of the third feature, and so on up to the probability of the final feature of Xn.",
    "output": "で、xはベクトルなので、p(x)をモデリングするとはx1の確率、、、ここでx1はxの最初の成分ですが、x1の確率に、掛けることのx2となる確率、これは二番目のフィーチャーの確率で、それに掛けることの三番目のフィーチャーの確率、などなどと、最後のフィーチャーxnの確率まで掛け合わせます。"
  },
  {
    "index": "F17581",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now I'm leaving space here cause I'll fill in something in a minute.",
    "output": "ここにスペースを空けたのは、あとでここに書きたい事があるからです。"
  },
  {
    "index": "F17582",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how do we model each of these terms, p of X1, p of X2, and so on.",
    "output": "で、これらの個々の項、p(x1)、p(x2)などをどうモデリングするのでしょうか?"
  },
  {
    "index": "F17583",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we're going to do, is assume that the feature, X1, is distributed according to a Gaussian distribution, with some mean, which you want to write as mu1 and some variance, which I'm going to write as sigma squared 1, and so p of X1 is going to be a Gaussian probability distribution, with mean mu1 and variance sigma squared 1.",
    "output": "どうやるかというと、x1がガウス分布に従って分布していると想定します、なんらかの平均、それをミュー1と書きましょう、そしてなんらかの分散、それをシグマ二乗の1と書くことにしましょう。そしてp(x1)は平均がミュー1で分散シグマ二乗1のガウスの確率分布に従うとする。"
  },
  {
    "index": "F17584",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly I'm going to assume that X2 is distributed, Gaussian, that's what this little tilda stands for, that means distributed Gaussian with mean mu2 and Sigma squared 2, so it's distributed according to a different Gaussian, which has a different set of parameters, mu2 sigma square 2.",
    "output": "同様にx2はガウス分布に従って分布していると、、、ところで、この小さなチルダ記号は(右辺に従い)分布している、という事を意味する。で、平均がミュー2で分散がシグマ二乗の2のガウス分布に従っている、つまり異なるガウス分布に従って分布している、と想定する。"
  },
  {
    "index": "F17585",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly, you know, X3 is yet another Gaussian, so this can have a different mean and a different standard deviation than the other features, and so on, up to XN.",
    "output": "同様に、x3もまた別のガウス分布で、これもまた他のフィーチャーとは異なる平均と標準偏差を持ちえる。などなどと、xnまで続く。"
  },
  {
    "index": "F17586",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's my model.",
    "output": "以上が私のモデルとなる。"
  },
  {
    "index": "F17587",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just as a side comment for those of you that are experts in statistics, it turns out that this equation that I just wrote out actually corresponds to an independence assumption on the values of the features x1 through xn.",
    "output": "あなたたちの中のうち、統計のスペシャリストの人向けの余談ですが、私の書き下した方程式の実際にはフィーチャーの値、x1からxnまでの値が独立である事を仮定している。"
  },
  {
    "index": "F17588",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in practice it turns out that the algorithm of this fragment, it works just fine, whether or not these features are anywhere close to independent and even if independence assumption doesn't hold true this algorithm works just fine.",
    "output": "だが実際には、ここに書いたアルゴリズム片は、これらのフィーチャーが独立に近いかどうかに関わらず機能する、実は独立の仮定が成り立たない時ですらこのアルゴリズムはうまく機能する。"
  },
  {
    "index": "F17589",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in case you don't know those terms I just used independence assumptions and so on, don't worry about it.",
    "output": "だけどもしあなたが、たった今、私が使った独立の仮定だとかの用語の意味が分からなければ気にしないでよろしい。"
  },
  {
    "index": "F17590",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You'll be able to understand it and implement this algorithm just fine and that comment was really meant only for the experts in statistics.",
    "output": "そのうち分かるようになるだろうし、このアルゴリズムは正しく実装出来るだろうから。さきのコメントは単に統計の専門家向けのコメントに過ぎない。"
  },
  {
    "index": "F17591",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, in order to wrap this up, let me take this expression and write it a little bit more compactly.",
    "output": "最後に、これのまとめとして、この式をもうちょっとだけコンパクトに書く。"
  },
  {
    "index": "F17592",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we're going to write this is a product from J equals one through N, of P of XJ parameterized by mu j comma sigma squared j.",
    "output": "これを、以下のようにjが1からnまでの以下のpの積として書くことにする。pのxjで、xjはミューjとシグマ二乗jでパラメトライズされている。"
  },
  {
    "index": "F17593",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this funny symbol here, there is capital Greek alphabet pi, that funny symbol there corresponds to taking the product of a set of values.",
    "output": "つまりこのファニーな記号、大文字のギリシャ文字のアルファベット、パイだが、そのファニーな記号は、値の集合に対して積をとる事に対応する。"
  },
  {
    "index": "F17594",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, you're familiar with the summation notation, so the sum from i equals one through n, of i. This means 1 + 2 + 3 plus dot dot dot, up to n.",
    "output": "つまり、和の記号に慣れているなら、iを1からnまでのiの和を取ると、この意味は1+2+3+...とnまでの和という意味。"
  },
  {
    "index": "F17595",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where as this funny symbol here, this product symbol, right product from i equals 1 through n of i.",
    "output": "一方このファニーな記号は、この掛け算記号は、iが1からnまでのiの掛け算。"
  },
  {
    "index": "F17596",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then this means that, it's just like summation except that we're now multiplying.",
    "output": "これの意味する所は和の記号とほとんど同じだけど、今回は足す代わりに掛け算する、という意味。"
  },
  {
    "index": "F17597",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so using this product notation, this product from j equals 1 through n of this expression.",
    "output": "で、この積の記法を使うと、このjが1からnまでこの式を掛け合わせる、というのを使うともっとコンパクトになる。"
  },
  {
    "index": "F17598",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's just more compact, it's just shorter way for writing out this product of of all of these terms up there.",
    "output": "これらの項全部を書く、より短い方法となるワケ。"
  },
  {
    "index": "F17599",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Since we're are taking these p of x j given mu j comma sigma squared j terms and multiplying them together.",
    "output": "だってこれらのpの、ミューjとシグマjが与えられた時のxjの値をとって、それらを掛け合わせているのだから。"
  },
  {
    "index": "F17600",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, by the way the problem of estimating this distribution p of x, they're sometimes called the problem of density estimation.",
    "output": "ところで、このpのxという分布を推計する問題は密度推計の問題、と呼ばれる事がある。"
  },
  {
    "index": "F17601",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Hence the title of the slide.",
    "output": "それがこのスライドのタイトルでもある。"
  },
  {
    "index": "F17602",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So putting everything together, here is our anomaly detection algorithm.",
    "output": "では、全部合わせると、我らのアノマリー検出のアルゴリズムはこうなる。"
  },
  {
    "index": "F17603",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first step is to choose features, or come up with features xi that we think might be indicative of anomalous examples.",
    "output": "最初のステップはフィーチャーを選ぶ事、または見つけ出すことだ、アノマリーであるサンプルを示してくれると思われるようなフィーチャーxiを。"
  },
  {
    "index": "F17604",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what I mean by that, is, try to come up with features, so that when there's an unusual user in your system that may be doing fraudulent things, or when the aircraft engine examples, you know there's something funny, something strange about one of the aircraft engines.",
    "output": "つまり、どういう意味かというと、以下のようなフィーチャーを探し出すという事です:あなたのシステムに詐欺行為を働いているかもしれない普通でないユーザーがいた時に、または航空機エンジンの例では、何かおかしな事、何か奇妙な事が航空機エンジンに起こっている時には普通でないほど大きな値、または普通でないほど小さな値をとる、とあなたが思うようなフィーチャーxiを選ぶ、アノマリーのサンプルがどんな感じかを知る為に。"
  },
  {
    "index": "F17605",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Choose features X I, that you think might take on unusually large values, or unusually small values, for what an anomalous example might look like.",
    "output": "またはより一般的に、集めているデータの対象の一般的な性質を良くとらえているフィーチャーを集めても良い。"
  },
  {
    "index": "F17606",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But more generally, just try to choose features that describe general properties of the things that you're collecting data on.",
    "output": "これらの式は前回のビデオでやった式と似ている。これらのパラメータの推計にこの式を使っていく。"
  },
  {
    "index": "F17607",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, given a training set, of M, unlabled examples, X1 through X M, we then fit the parameters, mu 1 through mu n, and sigma squared 1 through sigma squared n, and so these were the formulas similar to the formulas we have in the previous video, that we're going to use the estimate each of these parameters, and just to give some interpretation, mu J, that's my average value of the j feature.",
    "output": "いくつか解釈を与えておくと、ミューj、これはフィーチャーjの平均だ。"
  },
  {
    "index": "F17608",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Mu j goes in this term p of xj. which is parametrized by mu J and sigma squared J.",
    "output": "そしてこのミューjはpのxjの式にパラメータとして入る、それはミューjとシグマ二乗jでパラメトライズされていたのだった。"
  },
  {
    "index": "F17609",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this says for the mu J just take the mean over my training set of the values of the j feature.",
    "output": "だからこれの言ってる事はミューjは単にトレーニングセットに渡ってフィーチャーjの平均を取った物、という事。"
  },
  {
    "index": "F17610",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, just to mention, that you do this, you compute these formulas for j equals one through n.",
    "output": "そして補足しておくと、あなたは、これらの式をjが1からnまでに渡って計算することになる。"
  },
  {
    "index": "F17611",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So use these formulas to estimate mu 1, to estimate mu 2, and so on up to mu n, and similarly for sigma squared, and it's also possible to come up with vectorized versions of these.",
    "output": "つまりこれらの式を使ってミュー1、ミュー2、とミューnまで、推計する。シグマ二乗も同様だ。"
  },
  {
    "index": "F17612",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you think of mu as a vector, so mu if is a vector there's mu 1, mu 2, down to mu n, then a vectorized version of that set of parameters can be written like so sum from 1 equals one through n xi.",
    "output": "ミューをベクトルとして考えると、つまりミュー1があって、ミュー2があって、、、とミューnまで。するとベクトル化したバージョンのパラメータのセットは1からnまでのxiの和と書ける。"
  },
  {
    "index": "F17613",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this formula that I just wrote out estimates this xi as the feature vectors that estimates mu for all the values of n simultaneously.",
    "output": "つまり今書いたこの式でこのxiをフィーチャーベクトルとしてn個全てのミューの値を同時に推計する。"
  },
  {
    "index": "F17614",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it's also possible to come up with a vectorized formula for estimating sigma squared j.",
    "output": "そしてまた、シグマ二乗jの推計についてもベクトル化した式を作れる。"
  },
  {
    "index": "F17615",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, when you're given a new example, so when you have a new aircraft engine and you want to know is this aircraft engine anomalous.",
    "output": "最後に、新しいサンプルを与えられたら、つまり、新しい航空エンジンが来て、この航空機エンジンがアノマリーなのかどうかを知りたいとすると、やらなくてはならない事はp(x)を、この新しいエンジンの確率を計算する事だ。"
  },
  {
    "index": "F17616",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we need to do is then compute p of x, what's the probability of this new example?",
    "output": "で、p(x)はこの積と等しくて、実装としては、計算するのは、この式で、ここにある、これはガウス確率の式だ。"
  },
  {
    "index": "F17617",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, p of x is equal to this product, and what you implement, what you compute, is this formula and where over here, this thing here this is just the formula for the Gaussian probability, so you compute this thing, and finally if this probability is very small, then you flag this thing as an anomaly.",
    "output": "だからこれを計算し、最終的に、この確率がとても小さければ、これをアノマリーとフラグづけする。"
  },
  {
    "index": "F17618",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's an example of an application of this method.",
    "output": "これはこの手法の適用例だ。"
  },
  {
    "index": "F17619",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we have this data set plotted on the upper left of this slide.",
    "output": "このスライドの左上にプロットしたようなデータがあるとしよう。"
  },
  {
    "index": "F17620",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "if you look at this, well, lets look the feature of x1. If you look at this data set, it looks like on average, the features x1 has a mean of about 5 and the standard deviation, if you only look at just the x1 values of this data set has the standard deviation of maybe 2.",
    "output": "もしこの、フィーチャーx1を見てみると、もしこのデータセットを見てみると、見た感じだいたいフィーチャーx1の平均は5のあたりで、標準偏差はこのデータセットのx1の値だけを見ると、標準偏差はだいたい2くらい。"
  },
  {
    "index": "F17621",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that sigma 1 and looks like x2 the values of the features as measured on the vertical axis, looks like it has an average value of about 3, and a standard deviation of about 1.",
    "output": "そしてフィーチャーx2の値を見ると、それは縦軸で測れてその平均は見たところだいたい3くらいで、標準偏差はだいたい1。"
  },
  {
    "index": "F17622",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you take this data set and if you estimate mu1, mu2, sigma1, sigma2, this is what you get.",
    "output": "だからこのデータセットに対してミュー1、ミュー2と、シグマ1、シグマ2を推計すると、これが得られる物だ。"
  },
  {
    "index": "F17623",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And again, I'm writing sigma here, I'm think about standard deviations, but the formula on the previous 5 actually gave the estimates of the squares of theses things, so sigma squared 1 and sigma squared 2.",
    "output": "標準偏差について考えているけれど、前のスライドの式はこれらの二乗の推計を与える物だった。つまりシグマ二乗の1、シグマ二乗の2。"
  },
  {
    "index": "F17624",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just be careful whether you are using sigma 1, sigma 2, or sigma squared 1 or sigma squared 2.",
    "output": "だから、シグマ1とシグマ2を使ってるのかそれともシグマ二乗1とシグマ二乗2を使ってるのかに、注意しなさい。"
  },
  {
    "index": "F17625",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, sigma squared 1 of course would be equal to 4, for example, as the square of 2.",
    "output": "そしてシグマ二乗1はもちろん、イコール4となる。"
  },
  {
    "index": "F17626",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in pictures what p of x1 parametrized by mu1 and sigma squared 1 and p of x2, parametrized by mu 2 and sigma squared 2, that would look like these two distributions over here.",
    "output": "図では、ミュー1とシグマ二乗1でパラメトライズされているp(x1)と、ミュー2とシグマ二乗2でパラメトライズされたp(x2)はここにあるこれら2つの分布のような見た目となるだろう。"
  },
  {
    "index": "F17627",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, turns out that if were to plot of p of x, right, which is the product of these two things, you can actually get a surface plot that looks like this.",
    "output": "そして結局、p(x)をプロットするとそれはこれら2つの積だったのだから実際にはこんな感じの平面プロットが得られる。"
  },
  {
    "index": "F17628",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is a plot of p of x, where the height above of this, where the height of this surface at a particular point, so given a particular x1 x2 values of x2 if x1 equals 2, x equal 2, that's this point.",
    "output": "これはp(x)のプロットでその高さ、この上の高さ、平面上のある特定の点の高さは、つまりあるx1、x2が与えられた時にx2の値が2でx1の値が2なら、この点だ。"
  },
  {
    "index": "F17629",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the height of this 3-D surface here, that's p of x.",
    "output": "そしてこの三次元平面の高さがそれこそがp(x)だ。"
  },
  {
    "index": "F17630",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So p of x, that is the height of this plot, is literally just p of x1 parametrized by mu 1 sigma squared 1, times p of x2 parametrized by mu 2 sigma squared 2.",
    "output": "つまりp(x)、それがこのプロットの高さだ。それは文字通りミュー1とシグマ二乗1でパラメトライズされたp(x1)掛けることのミュー2とシグマ二乗2でパラメトライズされたp(x2)である。"
  },
  {
    "index": "F17631",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, so this is how we fit the parameters to this data.",
    "output": "つまり、これがパラメータをこのデータにフィットさせる方法だ。"
  },
  {
    "index": "F17632",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's see if we have a couple of new examples.",
    "output": "新しいサンプルが幾つか来た場合を考えてみよう。"
  },
  {
    "index": "F17633",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe I have a new example there.",
    "output": "新しいサンプルはここかもしれない。"
  },
  {
    "index": "F17634",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Is this an anomaly or not?",
    "output": "これはアノマリーか?"
  },
  {
    "index": "F17635",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, is that an anomaly or not?",
    "output": "これはアノマリーか?"
  },
  {
    "index": "F17636",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "They way we do that is, we would set some value for Epsilon, let's say I've chosen Epsilon equals 0.02.",
    "output": "そうでないか?それを区別する方法はある値エプシロンをセットする、ここではエプシロンを0.02としたとしよう。"
  },
  {
    "index": "F17637",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'll say later how we choose Epsilon.",
    "output": "どうやってエプシロンを選ぶかは後で話す。"
  },
  {
    "index": "F17638",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But let's take this first example, let me call this example X1 test.",
    "output": "まずは最初のサンプルを取ってみよう。このサンプルをx1testと呼ぶ事にする。"
  },
  {
    "index": "F17639",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let me call the second example X2 test.",
    "output": "そして二番目のサンプルをx2testと呼ぶことにする。"
  },
  {
    "index": "F17640",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we do is, we then compute p of X1 test, so we use this formula to compute it and this looks like a pretty large value.",
    "output": "その為に、この式を使ってそれを計算する。これは見たところかなり大きな値に見える。"
  },
  {
    "index": "F17641",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, this is greater than, or greater than or equal to epsilon.",
    "output": "特に、これはエプシロン以上となっている。"
  },
  {
    "index": "F17642",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is a pretty high probability at least bigger than epsilon, so we'll say that X1 test is not an anomaly.",
    "output": "だからこれは、きわめて大きい確率で少なくともエプシロンより大きいとは言える。だからx1testはアノマリーでは無い、と言って良かろう。"
  },
  {
    "index": "F17643",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas, if you compute p of X2 test, well that is just a much smaller value.",
    "output": "一方p(x2test)を計算してみると、これはもっとずっと小さい値となる。"
  },
  {
    "index": "F17644",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is less than epsilon and so we'll say that that is indeed an anomaly, because it is much smaller than that epsilon that we then chose.",
    "output": "だから、これは確かにアノマリーだと言える。何故ならそれは、我らが選んだエプシロンより小さいから。"
  },
  {
    "index": "F17645",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What this is really saying is that, you look through the 3d surface plot.",
    "output": "そして実際、ここで言った事は、これの本当に意味している事は、三次元での表面プロットを見ると、それの言わんとしている事は、その点の上の表面までの高さが高い点となるx1,x2は、全て非アノマリーのサンプルに、つまりOKというかノーマルなサンプルに対応しているという事。"
  },
  {
    "index": "F17646",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's saying that all the values of x1 and x2 that have a high height above the surface, corresponds to an a non-anomalous example of an OK or normal example.",
    "output": "一方、遥か離れたこの辺の点は全て遥か離れたこの辺の点は全て、これらの点は全てとても低い確率となっている。つまり、我らとしては、これらの点をアノマリーとフラグづけする事になる。"
  },
  {
    "index": "F17647",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas all the points far out here, all the points out here, all of those points have very low probability, so we are going to flag those points as anomalous, and so it's gonna define some region, that maybe looks like this, so that everything outside this, it flags as anomalous, whereas the things inside this ellipse I just drew, if it considers okay, or non-anomalous, not anomalous examples.",
    "output": "つまりある領域をそれはこんな感じとなるだろうが、その外の全ての点をアノマリーとフラグづけする事となる。一方で、このエプシロンの内部にある物たちは、ここに書いたように、これはOK、または非アノマリーなサンプルとみなす。"
  },
  {
    "index": "F17648",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this example x2 test lies outside that region, and so it has very small probability, and so we consider it an anomalous example.",
    "output": "つまりこのサンプルx2testはその領域の外に位置しているのでつまりそれはとても低い確率なので、それはアノマリーのサンプルとみなす訳だ。"
  },
  {
    "index": "F17649",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video we talked about how to estimate p of x, the probability of x, for the purpose of developing an anomaly detection algorithm.",
    "output": "このビデオでは、どうやってp(x)を推計するかを議論してきた。xとなる確率を。"
  },
  {
    "index": "F17650",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this video, we also stepped through an entire process of giving data set, we have, fitting the parameters, doing parameter estimations.",
    "output": "そしてこのビデオではまた、所与のデータセットに対してパラメータをフィッティングする、パラメータを推計する全体の手順も見ていった。"
  },
  {
    "index": "F17651",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We get mu and sigma parameters, and then taking new examples and deciding if the new examples are anomalous or not.",
    "output": "パラメータであるミューとシグマを取得して、次に新しいサンプルに対してそれがアノマリーかどうかを判断した。"
  },
  {
    "index": "F17652",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos we will delve deeper into this algorithm, and talk a bit more about how to actually get this to work well.",
    "output": "次のビデオでは、このアルゴリズムに対してより深く見ていく。そしてこれを実際にうまく運用する為にちょっとした補足の話もしていく。"
  },
  {
    "index": "F17653",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last video, we developed an anomaly detection algorithm.",
    "output": "前回のビデオではアノマリー検出のアルゴリズムを開発した。"
  },
  {
    "index": "F17654",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I like to talk about the process of how to go about developing a specific application of anomaly detection to a problem and in particular this will focus on the problem of how to evaluate an anomaly detection algorithm.",
    "output": "このビデオでは、具体的な問題に対してどのようにアノマリー検出を適用するか、そのプロセスを議論していく。特に、今回はアノマリー検出アルゴリズムの評価をどう行うかについてフォーカスしていきたい。"
  },
  {
    "index": "F17655",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In previous videos, we've already talked about the importance of real number evaluation and this captures the idea that when you're trying to develop a learning algorithm for a specific application, you need to often make a lot of choices like, you know, choosing what features to use and then so on.",
    "output": "前回のビデオでは、実数による評価の重要性について話してきた。これは、特定の応用に関する学習アルゴリズムを開発しようとしている時に、通常たくさんの選択を行う必要がある、たとえばどのフィーチャーを使うか、などを選択する必要がある、という発想を捉えている。"
  },
  {
    "index": "F17656",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And making decisions about all of these choices is often much easier, and if you have a way to evaluate your learning algorithm that just gives you back a number.",
    "output": "これらの選択の全てについて、決断を下すには、あなたの学習アルゴリズムを評価する数字が得られる方が、より簡単になる事がしばしばある。"
  },
  {
    "index": "F17657",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you can run the algorithm with the feature, and run the algorithm without the feature, and just get back a number that tells you, you know, did it improve or worsen performance to add this feature?",
    "output": "たとえば、もう一つ追加のフィーチャーを加えるかについて、何か追加のフィーチャーについて心当たりがある時、もしそのフィーチャーを加えてアルゴリズムを走らせて、さらにフィーチャー無しでアルゴリズムを走らせてそこから数字を得たら、それがこのフィーチャーを加えた事でパフォーマンスが改善したか悪化したかを教えてくれる。"
  },
  {
    "index": "F17658",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then it gives you a much better way, a much simpler way, with which to decide whether or not to include that feature.",
    "output": "それはつまり、そのフィーチャーを含めるべきかどうかを決定する為のより良い、よりシンプルな方法を提供してくれる。"
  },
  {
    "index": "F17659",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in order to be able to develop an anomaly detection system quickly, it would be a really helpful to have a way of evaluating an anomaly detection system.",
    "output": "だから、アノマリー検出のシステムを手早く開発する為には、そのアノマリー検出のシステムを評価する方法がある事は、とても有用だ。"
  },
  {
    "index": "F17660",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to do this, in order to evaluate an anomaly detection system, we're actually going to assume have some labeled data.",
    "output": "これを行う為に、アノマリー検出のシステムを評価する為に、実際にはあるラベル付けされたデータを仮定する。"
  },
  {
    "index": "F17661",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, so far, we'll be treating anomaly detection as an unsupervised learning problem, using unlabeled data.",
    "output": "ここまでの所、アノマリー検出を教師なし学習として扱ってきた、ラベル付けされていないデータを扱って。"
  },
  {
    "index": "F17662",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you have some labeled data that specifies what are some anomalous examples, and what are some non-anomalous examples, then this is how we actually think of as the standard way of evaluating an anomaly detection algorithm.",
    "output": "でも、もしどれがアノマリーのサンプルかを示すラベルのついたデータがあれば、そしてどれが非アノマリーのサンプルかを示すデータがあれば、これは普通の、アノマリー検出のアルゴリズムを評価する方法と考える事が出来る。"
  },
  {
    "index": "F17663",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So taking the aircraft engine example again.",
    "output": "航空機野エンジンの例をふたたび考えよう。"
  },
  {
    "index": "F17664",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that, you know, we have some label data of just a few anomalous examples of some aircraft engines that were manufactured in the past that turns out to be anomalous.",
    "output": "ラベル付けされたデータで、そのうちのちょっとだけがアノマリーの航空機エンジンのサンプル、つまり過去に製造されていてアノマリーだと、欠陥品か、とにかく何らかの意味で異常な物だと判明しているとする。"
  },
  {
    "index": "F17665",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we use we also have some non-anomalous examples, so some perfectly okay examples.",
    "output": "また、非アノマリーの完璧にオーケーなサンプルも幾つかあるとしよう。"
  },
  {
    "index": "F17666",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm going to use y equals 0 to denote the normal or the non-anomalous example and y equals 1 to denote the anomalous examples.",
    "output": "y=0をノーマル、または非アノマリーのサンプルを表すのに使い、そしてy=1を、アノマリーのサンプルを表すのに使う。"
  },
  {
    "index": "F17667",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The process of developing and evaluating an anomaly detection algorithm is as follows.",
    "output": "アノマリー検出のアルゴリズムを開発し、評価していくプロセスは以下のようになる。"
  },
  {
    "index": "F17668",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to think of it as a training set and talk about the cross validation in test sets later, but the training set we usually think of this as still the unlabeled training set.",
    "output": "まずはトレーニングセットについて、ところでクロスバリデーションセットとテストセットについては後で話すが、まずはトレーニングセットについて。これは普通、ラベルづけされてないトレーニングセットと考える。"
  },
  {
    "index": "F17669",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is our large collection of normal, non-anomalous or not anomalous examples.",
    "output": "これは普通の、アノマリーで無いデータが大量に集まっている、と考える。"
  },
  {
    "index": "F17670",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And usually we think of this as being as non-anomalous, but it's actually okay even if a few anomalies slip into your unlabeled training set.",
    "output": "普通はこれを非アノマリーと考えるが、だが実際はちょっとアノマリーなのが紛れ込むくらいはオーケーだ。それがラベル無しトレーニングセットに入っちゃってても。"
  },
  {
    "index": "F17671",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And next we are going to define a cross validation set and a test set, with which to evaluate a particular anomaly detection algorithm.",
    "output": "次に、クロスバリデーションセットとテストセットを定義する。それで個々のアノマリー検出アルゴリズムの評価を行う。"
  },
  {
    "index": "F17672",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, specifically, for both the cross validation test sets we're going to assume that, you know, we can include a few examples in the cross validation set and the test set that contain examples that are known to be anomalous.",
    "output": "つまり、具体的には、クロスバリデーションとテストセットではどちらも、アノマリーだと分かっているサンプルが含まれているという前提だ。"
  },
  {
    "index": "F17673",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the test sets say we have a few examples with y equals 1 that correspond to anomalous aircraft engines.",
    "output": "つまりテストセットにはy=1となるサンプルが幾つか含まれている。それはアノマリーな航空機エンジンに対応している。"
  },
  {
    "index": "F17674",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's a specific example.",
    "output": "具体的にはこうだ。"
  },
  {
    "index": "F17675",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that, altogether, this is the data that we have.",
    "output": "これらを合わせて、我らの持ってるデータだとしよう。"
  },
  {
    "index": "F17676",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We have manufactured 10,000 examples of engines that, as far as we know we're perfectly normal, perfectly good aircraft engines.",
    "output": "1万機の、分かってる範囲では完全に正常な航空機エンジンを製造したとしよう。"
  },
  {
    "index": "F17677",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And again, it turns out to be okay even if a few flawed engine slips into the set of 10,000 is actually okay, but we kind of assumed that the vast majority of these 10,000 examples are, you know, good and normal non-anomalous engines.",
    "output": "そしてここでも、ちょっとの欠陥品がこの1万の中に紛れ込んでたとしても、実際はオーケーだ。だがこれらの中の大多数は良い、普通の、アノマリーでないエンジンである、と仮定する。"
  },
  {
    "index": "F17678",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say that, you know, historically, however long we've been running on manufacturing plant, let's say that we end up getting features, getting 24 to 28 anomalous engines as well.",
    "output": "そしてこれまでに長い期間工場を運営してきて、フィーチャーを計測していてだいたい20機の欠陥エンジン、アノマリーのエンジンを得ているとする。"
  },
  {
    "index": "F17679",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for a pretty typical application of anomaly detection, you know, the number non-anomalous examples, that is with y equals 1, we may have anywhere from, you know, 20 to 50.",
    "output": "ところでアノマリー検出のきわめて典型的な適用ケースでは、アノマリーのサンプルの数は、つまりy=1となるサンプルの数はだいたい20から50ってあたりだ。"
  },
  {
    "index": "F17680",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It would be a pretty typical range of examples, number of examples that we have with y equals 1.",
    "output": "この辺が典型的なy=1となる数の範囲。"
  },
  {
    "index": "F17681",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And usually we will have a much larger number of good examples.",
    "output": "そして通常は、もっとずっと多くの正常なサンプルがある。"
  },
  {
    "index": "F17682",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, given this data set, a fairly typical way to split it into the training set, cross validation set and test set would be as follows.",
    "output": "だからデータセットが与えられた時の極めて標準的なトレーニングセット、クロスバリデーションセット、テストセットの分割方法は以下のようになる。"
  },
  {
    "index": "F17683",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's take 10,000 good aircraft engines and put 6,000 of that into the unlabeled training set.",
    "output": "1万の良品の航空機エンジンから6000をラベル無しのトレーニングセットとして取り分ける。"
  },
  {
    "index": "F17684",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I'm calling this an unlabeled training set but all of these examples are really ones that correspond to y equals 0, as far as we know.",
    "output": "これをラベル無しと言ったが、これらは全てy=0に対応したサンプルだ、我らの知る限りは。"
  },
  {
    "index": "F17685",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, we will use this to fit p of x, right.",
    "output": "そしてこれをp(x)をフィットするのに使う。"
  },
  {
    "index": "F17686",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we will use these 6000 engines to fit p of x, which is that p of x one parametrized by Mu 1, sigma squared 1, up to p of Xn parametrized by Mu N sigma squared n.",
    "output": "つまり我らはこれら6000のエンジンをp(x)をフィットするのに使う、ここでp(x1)はミュー1とシグマ二乗1でパラメトライズされていて、これはp(xn)がミューnとシグマ二乗nでパラメトライズされている所まで続く。"
  },
  {
    "index": "F17687",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so it would be these 6,000 examples that we would use to estimate the parameters Mu 1, sigma squared 1, up to Mu N, sigma squared N.",
    "output": "つまりこれら6000の手本を使ってパラメータのミュー1、シグマ二乗1からミューn、シグマ二乗nまでを推計する。"
  },
  {
    "index": "F17688",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's our training set of all, you know, good, or the vast majority of good examples.",
    "output": "以上が我らのトレーニングセットでそれらは全て正常な、または少なくとも大部分が正常なサンプルだ。"
  },
  {
    "index": "F17689",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next we will take our good aircraft engines and put some number of them in a cross validation set plus some number of them in the test sets.",
    "output": "次に、正常な航空機エンジンの中からいくらかをクロスバリデーションセットに、さらにまた幾らかをテストセットに入れます。"
  },
  {
    "index": "F17690",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we also have 20 flawed aircraft engines, and we'll take that and maybe split it up, you know, put ten of them in the cross validation set and put ten of them in the test sets.",
    "output": "次に、20機の不良エンジンがあるとして、それを、例えば10個ずつに分けて、クロスバリデーションセットとテストセットにそれぞれ入れます。"
  },
  {
    "index": "F17691",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in the next slide we will talk about how to actually use this to evaluate the anomaly detection algorithm.",
    "output": "そして次のスライドでこれらを実際に、どのように使ってアノマリー検出のアルゴリズムを評価するかを議論していく。"
  },
  {
    "index": "F17692",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what I have just described here is a you know probably the recommend a good way of splitting the labeled and unlabeled example.",
    "output": "ここまでで私が書いてきた事は、ラベル付けされたデータとラベル付けされていないデータの分割する望ましい方法だ。"
  },
  {
    "index": "F17693",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The good and the flawed aircraft engines.",
    "output": "航空機エンジンのうち、良品と欠陥品の。"
  },
  {
    "index": "F17694",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where we use like a 60, 20, 20% split for the good engines and we take the flawed engines, and we put them just in the cross validation set, and just in the test set, then we'll see in the next slide why that's the case.",
    "output": "我らは良品に関しては60、20,20%にそれぞれ分割し、欠陥品に関してはクロスバリデーションセットとテストセットだけに入れた。次のスライドでどうしてそうしたかが分かる。"
  },
  {
    "index": "F17695",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just as an aside, if you look at how people apply anomaly detection algorithms, sometimes you see other peoples' split the data differently as well.",
    "output": "補足になるが、たまに世の中の人々がアノマリー検出のアルゴリズムを適用するやり方として、たまにデータを異なったやり方で分割してるのを見かける事もあるかもしれない。"
  },
  {
    "index": "F17696",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, another alternative, this is really not a recommended alternative, but some people want to take off your 10,000 good engines, maybe put 6000 of them in your training set and then put the same 4000 in the cross validation set and the test set.",
    "output": "これはあまりオススメしない代替案だが代替案としては、ある人々は1万個の良品を6000個をトレーニングセットに、残りの4000をクロスバリデーションセットとテストセットの両方に入れる、という事をやるかもしれない。"
  },
  {
    "index": "F17697",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, you know, we like to think of the cross validation set and the test set as being completely different data sets to each other.",
    "output": "でもご存知の通り、クロスバリデーションセットとテストセットを完全に異なるデータと我々は思いたいのだった。"
  },
  {
    "index": "F17698",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But you know, in anomaly detection, you know, for sometimes you see people, sort of, use the same set of good engines in the cross validation sets, and the test sets, and sometimes you see people use exactly the same sets of anomalous engines in the cross validation set and the test set.",
    "output": "だがアノマリー検出では、たまにテストセットとクロスバリデーションセットの両方に同じ良品のエンジンを使うのを見かける場合があり、時には欠陥品の方のエンジンまで全く同じセットをクロスバリデーションセットとテストセットに使ってるのを見る事すらある。"
  },
  {
    "index": "F17699",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, all of these are considered, you know, less good practices and definitely less recommended.",
    "output": "これらは皆、より悪いやり方だとみなす事が出来るので、決してオススメしない。"
  },
  {
    "index": "F17700",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Certainly using the same data in the cross validation set and the test set, that is not considered a good machine learning practice.",
    "output": "確実に、クロスバリデーションセットとテストセットで同じデータを使うのは機械学習における良い習慣では無い。"
  },
  {
    "index": "F17701",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, sometimes you see people do this too.",
    "output": "でもたまにこれをやってる人を見かけるだろう。"
  },
  {
    "index": "F17702",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, given the training cross validation and test sets, here's how you evaluate or here is how you develop and evaluate an algorithm.",
    "output": "さて、トレーニング、クロスバリデーション、そしてテストセットが与えられたとして、これが評価方法、、、いや、これがアルゴリズムの開発方法と評価方法だ。"
  },
  {
    "index": "F17703",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First, we take the training sets and we fit the model p of x.",
    "output": "まず、トレーニングセットに対し、モデルp(x)をフィッティングする。"
  },
  {
    "index": "F17704",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, we fit, you know, all these Gaussians to my m unlabeled examples of aircraft engines, and these, I am calling them unlabeled examples, but these are really examples that we're assuming our goods are the normal aircraft engines.",
    "output": "m個のラベル無し航空機エンジンのサンプルを。ところでこれらをラベル無しサンプルと呼んだが、実のところこれらは、良品、正常な航空機エンジンであると想定している。"
  },
  {
    "index": "F17705",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then imagine that your anomaly detection algorithm is actually making prediction.",
    "output": "そして次に、アノマリー検出のアルゴリズムが実際に予測をしているのを想像してみよう。"
  },
  {
    "index": "F17706",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, on the cross validation of the test set, given that, say, test example X, think of the algorithm as predicting that y is equal to 1, p of x is less than epsilon, we must be taking zero, if p of x is greater than or equal to epsilon.",
    "output": "つまりクロスバリデーションとテストのセットに対し、サンプルxを検定する。p(x)がエプシロンより小さければ、アルゴリズムはy=1を予測したと考え、もしp(x)がエプシロン以上ならy=0に違いない、と考える。"
  },
  {
    "index": "F17707",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, given x, it's trying to predict, what is the label, given y equals 1 corresponding to an anomaly or is it y equals 0 corresponding to a normal example?",
    "output": "つまりxが与えられた時に、ラベルがどちらかを予測しようとする。y=1はアノマリーに対応し、y=0は通常のサンプルに対応する。"
  },
  {
    "index": "F17708",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So given the training, cross validation, and test sets.",
    "output": "ではトレーニング、クロスバリデーション、テストセットがそれぞれ与えられたとして、どうアルゴリズムを開発していくか?"
  },
  {
    "index": "F17709",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And more specifically, how do you evaluate an anomaly detection algorithm?",
    "output": "より詳細に言うと、アノマリー検出のアルゴリズムをどうやって評価するか?"
  },
  {
    "index": "F17710",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, to this whole, the first step is to take the unlabeled training set, and to fit the model p of x lead training data.",
    "output": "これらを元に、最初のステップはラベル無しのトレーニングセットに対しモデルp(x)をフィッティングする。"
  },
  {
    "index": "F17711",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you take this, you know on I'm coming, unlabeled training set, but really, these are examples that we are assuming, vast majority of which are normal aircraft engines, not because they're not anomalies and it will fit the model p of x.",
    "output": "つまりこれを取るのだが、これはラベル無しトレーニングセットと言っているが、実際は、それらの大多数は通常の航空機エンジンだと想定している物だった。それらはアノマリーでは無さそうだからそれにモデルp(x)をフィッティングする訳だ。"
  },
  {
    "index": "F17712",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It will fit all those parameters for all the Gaussians on this data.",
    "output": "このパラメータ全部で、、、これらガウス分布のデータ全てでフィッティングする。"
  },
  {
    "index": "F17713",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next on the cross validation of the test set, we're going to think of the anomaly detention algorithm as trying to predict the value of y.",
    "output": "次にクロスバリデーションとテストセットに対して、アノマリー検出のアルゴリズムを、yの値を予測する物とみなす。"
  },
  {
    "index": "F17714",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in each of like say test examples.",
    "output": "つまり各テストのサンプルに対して、これらxitestとyitestがある訳だが、ここでyは、これがアノマリーかどうかに応じて1か0のどちらかの値を取る。"
  },
  {
    "index": "F17715",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So given input x in my test set, my anomaly detection algorithm think of it as predicting the y as 1 if p of x is less than epsilon.",
    "output": "テストセットから入力xが与えられた時、このアノマリー検出のアルゴリズムをp(x)がイプシロンより小さかったらyが1と予測している、とみなす。"
  },
  {
    "index": "F17716",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So predicting that it is an anomaly, it is probably is very low.",
    "output": "つまりそれがアノマリーと予測するという事は、そんなサンプルとなる確率がとても低いという事だ。"
  },
  {
    "index": "F17717",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we think of the algorithm is predicting that y is equal to 0. If p of x is greater then or equals epsilon.",
    "output": "そしてアルゴリズムは、p(x)がイプシロンより大きいか等しい場合にy=0を予測しているとみなす。"
  },
  {
    "index": "F17718",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So predicting those normal example if the p of x is reasonably large.",
    "output": "つまりp(x)が普通の感覚で十分に大きければそれらを正常なサンプルとみなす。"
  },
  {
    "index": "F17719",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so we can now think of the anomaly detection algorithm as making predictions for what are the values of these y labels in the test sets or on the cross validation set.",
    "output": "つまり、これでアノマリー検出のアルゴリズムをこれらのテストセット、クロスバリデーションセットのyラベルの値を予測するものとみなす事が出来た訳だ。"
  },
  {
    "index": "F17720",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this puts us somewhat more similar to the supervised learning setting, right?",
    "output": "これはようするに、教師有り学習に似たようなセッティングになる、でしょ?"
  },
  {
    "index": "F17721",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where we have label test set and our algorithm is making predictions on these labels and so we can evaluate it you know by seeing how often it gets these labels right.",
    "output": "そこではラベル付きのテストセットがあって、我らのアルゴリズムはこれらのラベルに対して予測を行う。つまりこれらのラベルをどれだけ当てられるかで、それを評価する訳だ。"
  },
  {
    "index": "F17722",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of course these labels are will be very skewed because y equals zero, that is normal examples, usually be much more common than y equals 1 than anomalous examples.",
    "output": "もちろんこれらのラベルはとても歪んだ割合となっている、何故ならy=0、それは正常なサンプルだが、このケースの方が通常はy=1、つまりアノマリーのサンプルとなるよりもずっと一般的だからだ。"
  },
  {
    "index": "F17723",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, you know, this is much closer to the source of evaluation metrics we can use in supervised learning.",
    "output": "だがそれも、教師有り学習の評価で使った評価指標と似た事情だ。"
  },
  {
    "index": "F17724",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, because the data is very skewed, because y equals 0 is much more common, classification accuracy would not be a good the evaluation metrics.",
    "output": "データはとても歪んでいるのだからy=0の方がずっと一般的なのだから、分類の精度はきっと良い指標では無いだろう。"
  },
  {
    "index": "F17725",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you have a very skewed data set, then predicting y equals 0 all the time, will have very high classification accuracy.",
    "output": "もしとても歪んだデータセットに対するなら、いつもy=0と予測するだけで、とても高い分類精度が得られてしまう。"
  },
  {
    "index": "F17726",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Instead, we should use evaluation metrics, like computing the fraction of true positives, false positives, false negatives, true negatives or compute the position of the v curve of this algorithm or do things like compute the f1 score, right, which is a single real number way of summarizing the position and the recall numbers.",
    "output": "その代わりに、以下のような評価指標を使うべきだ:真陽性(truepositive)、偽陽性(falsepositive)、偽陰性(falsenegative)、真陰性(truenagative)の比率の計算結果とか、適合率(precision)と再現率(recall)を計算するとか、F1スコアなど、、、これは適合率と再現率を一つの実数で要約するような物だったが、それを計算するなどする。"
  },
  {
    "index": "F17727",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so these would be ways to evaluate an anomaly detection algorithm on your cross validation set or on your test set.",
    "output": "これらがクロスバリデーションセットとテストセットでアノマリー検出のアルゴリズムを評価する方法となる。"
  },
  {
    "index": "F17728",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, earlier in the anomaly detection algorithm, we also had this parameter epsilon, right?",
    "output": "最後に、アノマリー検出のアルゴリズムにはこのイプシロン、というバラメータもあった。"
  },
  {
    "index": "F17729",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, epsilon is this threshold that we would use to decide when to flag something as an anomaly.",
    "output": "イプシロンはある物をアノマリーとフラグ付けするかを決定する閾値となる。"
  },
  {
    "index": "F17730",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, if you have a cross validation set, another way to and to choose this parameter epsilon, would be to try a different, try many different values of epsilon, and then pick the value of epsilon that, let's say, maximizes f1 score, or that otherwise does well on your cross validation set.",
    "output": "だからクロスバリデーションセットがある時に、このパラメータイプシロンを選ぶ、もう一つの方法は、たくさんの異なるイプシロンの値を、ほんとにたくさん試して、そして例えばf1スコアを最大化するイプシロンを選ぶという方法が考えられる。そうすれば、クロスバリデーションセットでは良い結果が得られるから。"
  },
  {
    "index": "F17731",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And more generally, the way to reduce the training, testing, and cross validation sets, is that when we are trying to make decisions, like what features to include, or trying to, you know, tune the parameter epsilon, we would then continually evaluate the algorithm on the cross validation sets and make all those decisions like what features did you use, you know, how to set epsilon, use that, evaluate the algorithm on the cross validation set, and then when we've picked the set of features, when we've found the value of epsilon that we're happy with, we can then take the final model and evaluate it, you know, do the final evaluation of the algorithm on the test sets.",
    "output": "より一般的には、トレーニング、テスト、そしてクロスバリデーションセットを減らす方法としては、意思決定をしようとする時には、例えばどのフィーチャーを含めるべきかとか、またはパラメータのイプシロンをチューンしたい時には、クロスバリデーションセットに対して継続的にアルゴリズムを評価して、それらの決定は全て、、、例えばどのフィーチャーを含めるかとかイプシロンを幾つに設定するかとか、そういう時はクロスバリデーションセットに対してアルゴリズムを評価し、そしてフィーチャーを選んだり、これでいい!"
  },
  {
    "index": "F17732",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in this video, we talked about the process of how to evaluate an anomaly detection algorithm, and again, having being able to evaluate an algorithm, you know, with a single real number evaluation, with a number like an F1 score that often allows you to much more efficient use of your time when you are trying to develop an anomaly detection system.",
    "output": "このビデオでは、アノマリー検出のアルゴリズムの評価をどうやるか、の手順を議論した。ここでも、アルゴリズムを評価する時には、単一の実数による評価、例えばF1スコアみたいな物は、しばしばあなたの時間をより効率的に使わせてくれる、アノマリー検出のシステムを開発しようとしている時には。"
  },
  {
    "index": "F17733",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we try to make these sorts of decisions. I have to chose epsilon, what features to include, and so on.",
    "output": "そしてこの種の意思決定、たとえばイプシロンを選ぶとか、どのフィーチャーを含めるかとか、そういう意思決定をしたい時には。"
  },
  {
    "index": "F17734",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, we started to use a bit of labeled data in order to evaluate the anomaly detection algorithm and this takes us a little bit closer to a supervised learning setting.",
    "output": "このビデオでは、アノマリー検出のアルゴリズムを評価する為に、ラベル付けされたデータを少量使う。これはちょっとだけ教師有り学習に近くなる。"
  },
  {
    "index": "F17735",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video, I'm going to say a bit more about that.",
    "output": "次のビデオでは、その事ついてもうすこし説明します。"
  },
  {
    "index": "F17736",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular we'll talk about when should you be using an anomaly detection algorithm and when should we be thinking about using supervised learning instead, and what are the differences between these two formalisms.",
    "output": "特に、どういう時はアノマリー検出アルゴリズムを使うべきで、どういう時はその代わりに教師有り学習を採用すべきか、そしてこれら2つの形式化の違いについても扱います。"
  },
  {
    "index": "F17737",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last video we talked about the process of evaluating an anomaly detection algorithm.",
    "output": "前回のビデオではアノマリー検出のアルゴリズムを評価する手順について話した。"
  },
  {
    "index": "F17738",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And there we started to use some label data with examples that we knew were either anomalous or not anomalous with Y equals one, or Y equals 0.",
    "output": "そこではラベル付きのデータをアノマリーかそうでないか分かっているサンプルに対して、y=1か0かで対応させる事でラベル付けしたデータを用いた。"
  },
  {
    "index": "F17739",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, the question then arises of, and if we have the label data, that we have some examples and know the anomalies, and some of them will not be anomalies. Why don't we just use a supervisor on half of them?",
    "output": "そこで浮かぶ疑問としては、このラベル付きデータがあるならどれがアノマリーでどれがアノマリーじゃない、という事が分かっているデータがあるのなら、何故単純に教師有り学習アルゴリズムを使わないのか?"
  },
  {
    "index": "F17740",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So why don't we just use logistic regression, or a neuro network to try to learn directly from our labeled data to predict whether Y equals one or Y equals 0.",
    "output": "つまりなんでただロジスティック回帰やニューラルネットワークを使ってラベル付けされたデータから直接学習してしまわないのか?そうしてyが1か0かを予測してしまえばいいのでは?"
  },
  {
    "index": "F17741",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, I'll try to share with you some of the thinking and some guidelines for when you should probably use an anomaly detection algorithm, and whether it might be more fruitful instead of using a supervisor in the algorithm.",
    "output": "このビデオでは、どういう時にアノマリー検出アルゴリズムを使いどういう時には教師有り学習アルゴリズムの使用を検討したほうが実りが多いのかについて、いくつかの考え方とガイドラインをお話したい。"
  },
  {
    "index": "F17742",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This slide shows what are the settings under which you should maybe use anomaly detection versus when supervised learning might be more fruitful.",
    "output": "このスライドでは、どういう状況ではアノマリー検出を使った方が良くてどういう時は教師有り学習の方が実りが多いのかを示している。"
  },
  {
    "index": "F17743",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have a problem with a very small number of positive examples, and remember the examples of y equals one are the anomaly examples. Then you might consider using an anomaly detection algorithm instead.",
    "output": "もし陽性のサンプルの数がとても少い場合は、ここでy=1がアノマリーのサンプルだったかだが、その場合はアノマリー検出のアルゴリズムの使用を検討すべきだ。"
  },
  {
    "index": "F17744",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, having 0 to 20, it may be up to 50 positive examples, might be pretty typical.",
    "output": "つまり0から20個とか、まぁ50個くらいまでの陽性のサンプルはとても典型的な範囲で、通常はそんなに陽性のサンプル数が少ない時は、それらの陽性のサンプルをクロスバリデーションセットとテストセットの為に取っておくのが良かろう。"
  },
  {
    "index": "F17745",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And usually we have such a small positive, set of positive examples, we're going to save the positive examples just for the cross validation set in the test set.",
    "output": "逆に、典型的なアノマリー検出の状況としては、相対的にとても大量の陰性のサンプルがある。"
  },
  {
    "index": "F17746",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in contrast, in a typical normal anomaly detection setting, we will often have a relatively large number of negative examples of the normal examples of normal aircraft engines.",
    "output": "これら正常なサンプル、これら正常は航空機エンジンなど。"
  },
  {
    "index": "F17747",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we can then use this very large number of negative examples With which to fit the model p(x).",
    "output": "そしてこれらのとても大量のサンプルを使ってモデルp(x)のフィッティングを行う事が出来る。"
  },
  {
    "index": "F17748",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so there's this idea that in many anomaly detection applications, you have very few positive examples and lots of negative examples.",
    "output": "つまり、だいたいアノマリー検出の適用のケースでは陽性のサンプルがほんのちょっとしか無くて陰性のサンプルは大量にある場合だ。"
  },
  {
    "index": "F17749",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And when we're doing the process of estimating p(x), affecting all those Gaussian parameters, we need only negative examples to do that.",
    "output": "そしてp(x)を推計している時、ガウス分布のパラメータをフィッティングしている時は陰性のサンプルしか必要としない。"
  },
  {
    "index": "F17750",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you have a lot negative data, we can still fit p(x) pretty well.",
    "output": "だからたくさんの陰性のデータがあれば、p(x)をフィッティングするのは極めてうまくやれる。"
  },
  {
    "index": "F17751",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast, for supervised learning, more typically we would have a reasonably large number of both positive and negative examples.",
    "output": "逆に教師有り学習の時は、十分にたくさんの、陽性と陰性のサンプル両方があるのがより典型的だ。"
  },
  {
    "index": "F17752",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is one way to look at your problem and decide if you should use an anomaly detection algorithm or a supervised.",
    "output": "これがあなたの問題に対してアノマリー検出アルゴリズムを使うべきか教師有り学習のアルゴリズムを使うべきかを決めるのに着目すべきポイントの一つだ。"
  },
  {
    "index": "F17753",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's another way that people often think about anomaly detection.",
    "output": "さて、アノマリー検出のアルゴリズムについて皆が良く思う、もう一つの考え方はこうだ。"
  },
  {
    "index": "F17754",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for anomaly detection applications, often there are very different types of anomalies.",
    "output": "アノマリー検出の適用時は、しばしばたくさんの異なる種類のアノマリーがあるものだ。"
  },
  {
    "index": "F17755",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There are so many things that could go wrong that could the aircraft engine.",
    "output": "様々な物が不良品となり得て、それぞれが航空機エンジンの故障の原因たりえる。"
  },
  {
    "index": "F17756",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if that's the case, and if you have a pretty small set of positive examples, then it can be hard for an algorithm, difficult for an algorithm to learn from your small set of positive examples what the anomalies look like.",
    "output": "そしてさらに、もし陽性のサンプルがとても少ししか得られていなければそのちょっとの陽性のサンプルからアノマリーとはどんな物かをそこから学習するのは、難しいかもしれない。"
  },
  {
    "index": "F17757",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, you know future anomalies may look nothing like the ones you've seen so far.",
    "output": "そしてさらに、将来起こりうるアノマリーがここまで見た物と全く似てないかもしれない。"
  },
  {
    "index": "F17758",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So maybe in your set of positive examples, maybe you've seen 5 or 10 or 20 different ways that an aircraft engine could go wrong.",
    "output": "つまりあなたの集めた陽性のサンプルは、5とか10とか20種類の相異なる航空機のエンジンが、どう壊れるかのパターンを示しているかもしれないが、だが明日にはひょっとすると、全く新しい、新種のアノマリーを、全く新しい航空機エンジンの故障の仕方で、まったく見たこと無いような物を検出する必要がある。"
  },
  {
    "index": "F17759",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if that's the case, it might be more promising to just model the negative examples with this sort of calcium model p of x instead of try to hard to model the positive examples.",
    "output": "その場合は単純に陰性のサンプルをモデリングする方が、ガウス分布のモデルp(x)でモデリングする方が、より筋が良いだろう。陽性のサンプルを必死にモデリングするよりも。"
  },
  {
    "index": "F17760",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because tomorrow's anomaly may be nothing like the ones you've seen so far.",
    "output": "何故なら明日のアノマリーはここまで見た物とはまったく似てないかも知れないのだから。"
  },
  {
    "index": "F17761",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast, in some other problems, you have enough positive examples for an algorithm to get a sense of what the positive examples are like.",
    "output": "逆に、他の問題で、十分な陽性のサンプルを持っていて、アルゴリズムに陽性のサンプルとはどんな感じか、という感じが掴めそうならそしてとりわけ、もし将来現れるであろう陽性のサンプルがトレーニングセットにある物と似たような物だろうと思われるなら、その場合には教師有り学習のアルゴリズムを用いる方がより合理的かもしれない。"
  },
  {
    "index": "F17762",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, if you think that future positive examples are likely to be similar to ones in the training set; then in that setting, it might be more reasonable to have a supervisor in the algorithm that looks at all of the positive examples, looks at all of the negative examples, and uses that to try to distinguish between positives and negatives.",
    "output": "それはたくさんの陽性のサンプルを見てたくさんの陰性のサンプルを見て、そしてそれらを用いて陽性と陰性を見分けようとする物だ。"
  },
  {
    "index": "F17763",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Hopefully, this gives you a sense of if you have a specific problem, should you think about using an anomaly detection algorithm, or a supervised learning algorithm.",
    "output": "以上で、もし特定の問題が目の前にあった時にアノマリー検出のアルゴリズムの使用を検討すべきかまたは教師有り学習のアルゴリズムを検討すべきか、だいたいの感じはつかめたかな。"
  },
  {
    "index": "F17764",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And a key difference really is that in anomaly detection, often we have such a small number of positive examples that it is not possible for a learning algorithm to learn that much from the positive examples.",
    "output": "キーとなる重要な違いはアノマリー検出では、陽性のサンプルがちょっとしか無いからそんなちょっとの物から学習アルゴリズムが多くを学ぶなんてそもそも不可能なのだ。"
  },
  {
    "index": "F17765",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what we do instead is take a large set of negative examples and have it just learn a lot, learn p(x) from just the negative examples.",
    "output": "だから代わりにやる事としては、たくさんの陰性のサンプルをとってきて、それについてたくさん学習し、p(x)を陰性のサンプルだけから、例えば正常な航空機エンジンだけから学習する。"
  },
  {
    "index": "F17766",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of the normal and we've reserved the small number of positive examples for evaluating our algorithms to use in the either the transvalidation set or the test set.",
    "output": "そして少ししかない陽性のサンプルを我らのアルゴリズムを評価する為にクロスバリデーションセットかテストセットにとっておく。"
  },
  {
    "index": "F17767",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just as a side comment about this many different types of easier. In some earlier videos we talked about the email spam examples.",
    "output": "これらたくさんの異なるアノマリーについて補足しておくと、以前のビデオではeメールのスパムについて議論した。"
  },
  {
    "index": "F17768",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In those examples, there are actually many different types of spam email, right?",
    "output": "それらの例でも、たくさんの異なる種類のスパムメールが実際にある。"
  },
  {
    "index": "F17769",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's spam email that's trying to sell you things.",
    "output": "スパムメールは物をあなたに売りつけようとする物もあるし、スパムメールはあなたのパスワードを盗もうとする場合もある。"
  },
  {
    "index": "F17770",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Spam email trying to steal your passwords, this is called phishing emails and many different types of spam emails.",
    "output": "他にも様々なスパムメールがある。だがスパムの問題では、普通は調べる事が出来る十分な数のスパムが手に入る。"
  },
  {
    "index": "F17771",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But for the spam problem we usually have enough examples of spam email to see most of these different types of spam email because we have a large set of examples of spam.",
    "output": "これら異なる種類のほとんどのスパムメールを入手出来る。何故なら我らは大量のスパムメールを持ってるからだ。"
  },
  {
    "index": "F17772",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's why we usually think of spam as a supervised learning setting even though there are many different types of.",
    "output": "そしてだからこそ普通スパムの問題を教師有り学習の問題とみなすのだ。たくさんの種類のスパムがあるにも関わらず。"
  },
  {
    "index": "F17773",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we look at some applications of anomaly detection versus supervised learning we'll find fraud detection.",
    "output": "つまり、実際の応用においてアノマリー検出と教師有り学習を比較してみると、欠陥の検出において、もし様々な種類の欠陥とみなしたい物がありそうなら、そして相対的に少しのトレーニングセットしか、詐欺行為をしているユーザーがあなたのwebサイトにはちょっとしかいなければその時は私はアノマリー検出のアルゴリズムを使うだろう。"
  },
  {
    "index": "F17774",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have many different types of ways for people to try to commit fraud and a relatively small number of fraudulent users on your website, then I use an anomaly detection algorithm.",
    "output": "つまりこんな時、たとえばあなたがとても大きなオンラインの小売商だったとして、実際にたくさんの人が詐欺行為を働こうとしているとする。"
  },
  {
    "index": "F17775",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I should say, if you have, if you're a very major online retailer and if you actually have had a lot of people commit fraud on your website, so you actually have a lot of examples of y=1, then sometimes fraud detection could actually shift over to the supervised learning column.",
    "output": "つまり実際たくさんのy=1となるサンプルがあるとすると、そういう場合など、時には詐欺の検出は現実には教師有り学習へと変化する場合がありうる。"
  },
  {
    "index": "F17776",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, if you haven't seen that many examples of users doing strange things on your website, then more frequently fraud detection is actually treated as an anomaly detection algorithm rather than a supervised learning algorithm.",
    "output": "だがもしあなたがあなたのwebサイトにおいて奇妙な行動に出るユーザーのサンプルがそんなには見られないのなら、こちらの方がありがちだが、この場合は詐欺検出はアノマリー検出として扱う事になる、教師有り学習では無く。"
  },
  {
    "index": "F17777",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Other examples, we've talked about manufacturing already.",
    "output": "他の例でも、製造業については既に話したが、望ましいのはたくさんの正常なサンプルとそんなに多くないアノマリーを見る、という状態だが、ある製造工程ではもし大量の製造を行なっていて、既にたくさんの不良品のサンプルが見られているなら、その製造も教師有り学習アルゴリズムに変化しうる。"
  },
  {
    "index": "F17778",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Hopefully, you see more and more examples are not that many anomalies but if again for some manufacturing processes, if you manufacture in very large volumes and you see a lot of bad examples, maybe manufacturing can shift to the supervised learning column as well.",
    "output": "でも過去の製造結果にそんなにたくさんの不良品のサンプルが見られていないなら、私だったらこれをアノマリー検出で扱うね。"
  },
  {
    "index": "F17779",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you haven't seen that many bad examples of so to do the anomaly detection monitoring machines in a data center similar source of apply.",
    "output": "データセンターでのマシーンのモニタリングでも同種の議論が適用出来る。"
  },
  {
    "index": "F17780",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas, you must have classification, weather prediction, and classifying cancers. If you have equal numbers of positive and negative examples.",
    "output": "一方eメールのスパム分類、天気予報、ガンの分類などでは、だいたい同等の数の陽性と陰性のサンプルがあるなら、陽性と陰性のサンプルをたくさん得ているならこれら全てを教師有り学習の問題とみなす傾向にある。"
  },
  {
    "index": "F17781",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully, that gives you a sense of one of the properties of a learning problem that would cause you to treat it as an anomaly detection problem versus a supervisory problem.",
    "output": "以上で問題のどんな性質がそれをあなたがアノマリー検出の問題と扱うか、または教師有り学習の問題と扱うかを決めているかについて、感じがつかめただろうか。"
  },
  {
    "index": "F17782",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for many other problems that are faced by various technology companies and so on, we actually are in the settings where we have very few or sometimes zero positive training examples. There's just so many different types of anomalies that we've never seen them before.",
    "output": "様々な技術の会社において、直面する問題の多くでは、本当にちょっとの陽性のサンプルしか無かったり、時にはゼロ個の陽性のサンプルしかなかったり、あまりにもたくさんの種類のまだ見ぬアノマリーが存在したりする。"
  },
  {
    "index": "F17783",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for those sorts of problems, very often the algorithm that is used is an anomaly detection algorithm.",
    "output": "そういう種類の問題ではとてもしばしば用いられる学習アルゴリズムはアノマリー検出のアルゴリズムだ。"
  },
  {
    "index": "F17784",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By now you've seen the anomaly detection algorithm and we've also talked about how to evaluate an anomaly detection algorithm.",
    "output": "ここまでで我々はアノマリー検出のアルゴリズムを見てきた。また、アノマリー検出のアルゴリズムをどう評価するのかも見てきた。"
  },
  {
    "index": "F17785",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out, that when you're applying anomaly detection, one of the things that has a huge effect on how well it does, is what features you use, and what features you choose, to give the anomaly detection algorithm.",
    "output": "実は、実際にアノマリー検出を適用してみると、それがどれだけうまく機能するかに巨大な影響を与えているのはなんのフィーチャーを使うのか、何のフィーチャーを選んでアノマリー検出のアルゴリズムに与えるか、という部分だ。"
  },
  {
    "index": "F17786",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in this video, what I'd like to do is say a few words, give some suggestions and guidelines for how to go about designing or selecting features give to an anomaly detection algorithm.",
    "output": "だからこのビデオでは、アノマリー検出のアルゴリズムに食わせるフィーチャーをどうデザインするか、どう選ぶかについて二、三の助言、提案をしたいと思う。"
  },
  {
    "index": "F17787",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In our anomaly detection algorithm, one of the things we did was model the features using this sort of Gaussian distribution.",
    "output": "我らのアノマリー検出のアルゴリズムにおいてはこんな種類のガウス分布を用いてフィーチャー達をモデリングするという過程があります。"
  },
  {
    "index": "F17788",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "With xi to mu i, sigma squared i, lets say.",
    "output": "xiを、ミューiとシグマ二乗iで。"
  },
  {
    "index": "F17789",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so one thing that I often do would be to plot the data or the histogram of the data, to make sure that the data looks vaguely Gaussian before feeding it to my anomaly detection algorithm.",
    "output": "そうであるから、私が良くやる事としては、私のアノマリー検出のアルゴリズムにデータを食わせる前にこのデータのヒストグラムをプロットしてみてなんとなくガウス分布っぽいかを見てみる、という事があります。"
  },
  {
    "index": "F17790",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, it'll usually work okay, even if your data isn't Gaussian, but this is sort of a nice sanitary check to run.",
    "output": "あなたのデータがガウス分布でなくても普通はオーケーです。だけどこれは、実行してみるに値する、良いサニティチェックです。"
  },
  {
    "index": "F17791",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way, in case your data looks non-Gaussian, the algorithms will often work just find.",
    "output": "ところで、あなたのデータが非ガウス分布をとっていても、アルゴリズムは普通に正しく機能する事が多い。"
  },
  {
    "index": "F17792",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, concretely if I plot the data like this, and if it looks like a histogram like this, and the way to plot a histogram is to use the HIST, or the HIST command in Octave, but it looks like this, this looks vaguely Gaussian, so if my features look like this, I would be pretty happy feeding into my algorithm.",
    "output": "具体的に見るとデータをこんな風にプロットしてみて、ヒストグラムがこんな感じに見えたらところでヒストグラムのプロットの仕方はhist関数をOctaveでは使いますがそこでこんな見た目なら、これはだいたいガウス分布っぽい。つまりフィーチャーがこんな感じならアルゴリズムに、とても幸せな気持ちで食わせられる。"
  },
  {
    "index": "F17793",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if i were to plot a histogram of my data, and it were to look like this well, this doesn't look at all like a bell shaped curve, this is a very asymmetric distribution, it has a peak way off to one side.",
    "output": "だが、もしもデータのヒストグラムをプロットしてみたらそれがもしもこんな感じなら、うーん、こいつはベル型のカーブにはまったく見えないなぁ。これはとても非対称な分布だ。"
  },
  {
    "index": "F17794",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If this is what my data looks like, what I'll often do is play with different transformations of the data in order to make it look more Gaussian.",
    "output": "もしこんな風に私のデータが見えたら私が良くやる手段は様々なデータ変換を試してもっとガウス分布っぽく見えるようにする。"
  },
  {
    "index": "F17795",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And again the algorithm will usually work okay, even if you don't.",
    "output": "繰り返しになるが、別にそんな事しなくても、だいたいはアルゴリズムはちゃんと機能する。"
  },
  {
    "index": "F17796",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you use these transformations to make your data more gaussian, it might work a bit better.",
    "output": "でももしこれらの変換でデータをよりガウス分布っぽく見えるように出来たら、そっちの方がアルゴリズムはちょっとだけ改善される。"
  },
  {
    "index": "F17797",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So given the data set that looks like this, what I might do is take a log transformation of the data and if i do that and re-plot the histogram, what I end up with in this particular example, is a histogram that looks like this.",
    "output": "だからこんな見た目のデータセットが与えられた時、私が試すだろう事はlogをとってデータを変換しその後にデータをヒストグラムとして再プロットしてこの例の場合は例えば結果としてこんなヒストグラムを得る。"
  },
  {
    "index": "F17798",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this looks much more Gaussian, right?",
    "output": "こっちの方がガウス分布っぽいでしょう?"
  },
  {
    "index": "F17799",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This looks much more like the classic bell shaped curve, that we can fit with some mean and variance paramater sigma.",
    "output": "こっちの方がより古典的なベル型のカーブに見える。それはある平均と分散のパラメータでフィッティング出来る。"
  },
  {
    "index": "F17800",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what I mean by taking a log transform, is really that if I have some feature x1 and then the histogram of x1 looks like this then I might take my feature x1 and replace it with log of x1 and this is my new x1 that I'll plot to the histogram over on the right, and this looks much more Guassian.",
    "output": "ログ変換を取る、と言っているのは具体的にはあるフィーチャーx1があったとして、そのx1のヒストグラムがこんな見た目だとするとフィーチャーx1をlogx1で置き換える。そしてこれを新しいx1とみなし、それのヒストグラムを右にプロットすると、よりガウス分布っぽくなっている。"
  },
  {
    "index": "F17801",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Rather than just a log transform some other things you can do, might be, let's say I have a different feature x2, maybe I'll replace that will log x plus 1, or more generally with log x with x2 and some constant c and this constant could be something that I play with, to try to make it look as Gaussian as possible.",
    "output": "log変換の他に、選択肢として考えられる物は異なるフィーチャーx2があったとして、それをlog(x+1)で置き換える、またはより一般にlogxのxをx2とある定数cで置き換えた物で置き換える、というのが考えられる。この定数cはいろいろ調整してなるべくガウス分布っぽく見えるようにする。"
  },
  {
    "index": "F17802",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or for a different feature x3, maybe I'll replace it with x3, I might take the square root.",
    "output": "他には別のフィーチャーx3に対しx3をルートを取った物で置き換えても良いかもしれない。"
  },
  {
    "index": "F17803",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The square root is just x3 to the power of one half, right?",
    "output": "ルートってのはx3の、単なる1/2乗に過ぎない、でしょ?"
  },
  {
    "index": "F17804",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this one half is another example of a parameter I can play with.",
    "output": "そしてこの1/2というのもいろいろ調整する事が出来るパラメータとなる。"
  },
  {
    "index": "F17805",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I might have x4 and maybe I might instead replace that with x4 to the power of something else, maybe to the power of 1/3.",
    "output": "x4があったとして、そのx4を代わりにx4の違う指数乗、例えば1/3乗とかで置き換えても良い。"
  },
  {
    "index": "F17806",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And these, all of these, this one, this exponent parameter, or the C parameter, all of these are examples of parameters that you can play with in order to make your data look a little bit more Gaussian.",
    "output": "そしてこれら全て、これ、この指数乗のパラメータ、またはパラメータCも、これら全てがあなたのデータをちょっとでもガウス分布っぽく見せる為にいじれるパラメータの例となる。"
  },
  {
    "index": "F17807",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let me show you a live demo of how I actually go about playing with my data to make it look more Gaussian.",
    "output": "ではここで、私のデータを実際にいろいろいじってよりガウス分布っぽくする生のデモをお見せしよう。"
  },
  {
    "index": "F17808",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I have already loaded in to octave here a set of features x I have a thousand examples loaded over there.",
    "output": "その為、ここに既にOctaveにロードしておいた、幾つかのフィーチャーxを。"
  },
  {
    "index": "F17809",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's pull up the histogram of my data.",
    "output": "では私のデータのヒストグラムを出しておこう、このhistxのコマンドで。"
  },
  {
    "index": "F17810",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there's my histogram.",
    "output": "これが私のヒストグラムだ。"
  },
  {
    "index": "F17811",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By default, I think this uses 10 bins of histograms, but I want to see a more fine grid histogram.",
    "output": "デフォルトだと確かこの10個のビンをヒストグラムは使うと思うが私はもっと粒度の細かいグリッドのヒストグラムを見たい。"
  },
  {
    "index": "F17812",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we do hist to the x, 50, so, this plots it in 50 different bins.",
    "output": "だからhistにx,50と渡すこれは50個のことなるビンでプロットする。"
  },
  {
    "index": "F17813",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Okay, that looks better.",
    "output": "オーケー。良くなった。"
  },
  {
    "index": "F17814",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, this doesn't look very Gaussian, does it?",
    "output": "現在のところ、これはそんなにガウス分布っぽくは無い。でしょ?"
  },
  {
    "index": "F17815",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Lets try a hist of x to the 0.5.",
    "output": "まずhistのxの0.5乗を試してみよう。"
  },
  {
    "index": "F17816",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we take the square root of the data, and plot that histogram.",
    "output": "つまりデータのルートを取ってそのヒストグラムをプロットする。"
  },
  {
    "index": "F17817",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, okay, it looks a little bit more Gaussian, but not quite there, so let's play at the 0.5 parameter.",
    "output": "だがまだまだだね。ではこの0.5というパラメータをいじってみよう。"
  },
  {
    "index": "F17818",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Set this to 0.2.",
    "output": "これを0.2にセットする。"
  },
  {
    "index": "F17819",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Looks a little bit more Gaussian.",
    "output": "もうちょっとガウス分布っぽくなった。"
  },
  {
    "index": "F17820",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's reduce a little bit more 0.1.",
    "output": "もうちょっと減らして0.1にしてみよう。"
  },
  {
    "index": "F17821",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, let's reduce it to 0.05.",
    "output": "さらに減らして0.05にしてみよう。"
  },
  {
    "index": "F17822",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Okay, this looks pretty Gaussian, so I can define a new feature which is x mu equals x to the 0.05, and now my new feature x Mu looks more Gaussian than my previous one and then I might instead use this new feature to feed into my anomaly detection algorithm.",
    "output": "だから新しいフィーチャーとしてxミューイコールxの0.05乗と定義する。するとこの新しいフィーチャーxミューは、以前のよりもよりガウス分布っぽくなって、これを代わりにアノマリー検出のアルゴリズムに食わせても良い。"
  },
  {
    "index": "F17823",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And of course, there is more than one way to do this.",
    "output": "もちろん、これを行う方法は一つだけでは無い。"
  },
  {
    "index": "F17824",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You could also have hist of log of x, that's another example of a transformation you can use.",
    "output": "histのlogxというのも試しても良い。それはもう一つの試してみる価値のある変換の例だ。"
  },
  {
    "index": "F17825",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, you know, that also look pretty Gaussian.",
    "output": "そしてこれも、見ての通りなかなかガウスっぽい。"
  },
  {
    "index": "F17826",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, I can also define x mu equals log of x.",
    "output": "だからxミューをlogxと定義しても良い。"
  },
  {
    "index": "F17827",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "and that would be another pretty good choice of a feature to use.",
    "output": "これもまたなかなか良いフィーチャーのチョイスといえる。"
  },
  {
    "index": "F17828",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So to summarize, if you plot a histogram with the data, and find that it looks pretty non-Gaussian, it's worth playing around a little bit with different transformations like these, to see if you can make your data look a little bit more Gaussian, before you feed it to your learning algorithm, although even if you don't, it might work okay.",
    "output": "まとめると、データのヒストグラムをプロットしてみて、それがだいぶガウス分布っぽくなかったら今回試したような変換でちょっとデータをつついてみるのは試してみる価値がある。もうちょっとガウス分布っぽくならないかなぁ、と学習アルゴリズムに食わしてみる前に。"
  },
  {
    "index": "F17829",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But I usually do take this step.",
    "output": "でも普段、私はこのステップを踏む。"
  },
  {
    "index": "F17830",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, the second thing I want to talk about is, how do you come up with features for an anomaly detection algorithm.",
    "output": "ここで二番目に話したい事として、アノマリー検出に使うフィーチャーをどう見つけるか、というのがある。"
  },
  {
    "index": "F17831",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the way I often do so, is via an error analysis procedure.",
    "output": "私が良くやるのは誤差分析の手順だ。"
  },
  {
    "index": "F17832",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what I mean by that, is that this is really similar to the error analysis procedure that we have for supervised learning, where we would train a complete algorithm, and run the algorithm on a cross validation set, and look at the examples it gets wrong, and see if we can come up with extra features to help the algorithm do better on the examples that it got wrong in the cross-validation set.",
    "output": "それの意味する所は教師有り学習での誤差分析の手順と本当に似た物で、そこでは完全にアルゴリズムを学習させて、それをクロスバリデーションセットにかける、そして間違いのサンプルを見る、そしてフィーチャーを追加してそれがクロスバリデーションセットで誤りだった物を正しく扱う助けとなるかを見てみる。"
  },
  {
    "index": "F17833",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So lets try to reason through an example of this process.",
    "output": "ではこのプロセスを例を通して見てみよう。"
  },
  {
    "index": "F17834",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In anomaly detection, we are hoping that p of x will be large for the normal examples and it will be small for the anomalous examples.",
    "output": "アノマリー検出のアルゴリズムにおいては通常のサンプルについてはpのxが大きく、アノマリーなサンプルでは小さくなる事を期待している。"
  },
  {
    "index": "F17835",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so a pretty common problem would be if p of x is comparable, maybe both are large for both the normal and the anomalous examples.",
    "output": "だから、良くある問題としてはpのxが同じような値という場合、例えば普通の物とアノマリーな物が両方とも大きい場合などだ。"
  },
  {
    "index": "F17836",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Lets look at a specific example of that.",
    "output": "その具体例を見てみよう。"
  },
  {
    "index": "F17837",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that this is my unlabeled data.",
    "output": "これが私のラベル無しデータとする。"
  },
  {
    "index": "F17838",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here I have just one feature, x1 and so I'm gonna fit a Gaussian to this.",
    "output": "ここではたった一つのフィーチャーx1しか無いのでこれをガウス分布でフィッティングする。"
  },
  {
    "index": "F17839",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And maybe my Gaussian that I fit to my data looks like that.",
    "output": "そしてフィッティングしたガウス曲線がこんな感じだったとしよう。"
  },
  {
    "index": "F17840",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And now let's say I have an anomalous example, and let's say that my anomalous example takes on an x value of 2.5.",
    "output": "そしてアノマラスなサンプルがあったとしよう。"
  },
  {
    "index": "F17841",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you know, it's kind of buried in the middle of a bunch of normal examples, and so, just this anomalous example that I've drawn in green, it gets a pretty high probability, where it's the height of the blue curve, and the algorithm fails to flag this as an anomalous example.",
    "output": "つまりこのアノマラスなサンプルは緑で描いた物だが、それはとても高い確率となる。それは青い曲線の高さで表される訳だ。"
  },
  {
    "index": "F17842",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, if this were maybe aircraft engine manufacturing or something, what I would do is, I would actually look at my training examples and look at what went wrong with that particular aircraft engine, and see, if looking at that example can inspire me to come up with a new feature x2, that helps to distinguish between this bad example, compared to the rest of my red examples, compared to all of my normal aircraft engines.",
    "output": "ここで、これは航空機エンジンの製造とかだとしよう。"
  },
  {
    "index": "F17843",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if I managed to do so, the hope would be then, that, if I can create a new feature, X2, so that when I re-plot my data, if I take all my normal examples of my training set, hopefully I find that all my training examples are these red crosses here.",
    "output": "ここで実際に取れる対策としてはトレーニングサンプルを実際に見て、うまく行っていない特定の不良エンジンについてどこが悪くなっているかを調べる。そしてもしそのエンジンを調べていてそれにインスパイアされて新たなフィーチャーx2を思いついたとする。"
  },
  {
    "index": "F17844",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And hopefully, if I find that for my anomalous example, the feature x2 takes on the the unusual value.",
    "output": "それがこの不良エンジンを識別するには有用な訳だ。残りの赤いサンプル、私の全ての正常な航空機エンジンと比較して。"
  },
  {
    "index": "F17845",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for my green example here, this anomaly, right, my X1 value, is still 2.5.",
    "output": "そしてなんとかそこまで行ったら以下のような事を期待する訳だ:新しいフィーチャーx2を作って、私のデータを再プロットしたらトレーニングセットの正常なサンプルはプロットしてみたら正常なサンプルは赤いバツのサンプルはこんな感じで見えたとして、アノマラスなサンプルはフィーチャーx2が異常な値を取るという風に出来る事を期待する訳だ。"
  },
  {
    "index": "F17846",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then maybe my X2 value, hopefully it takes on a very large value like 3.5 over there, or a very small value.",
    "output": "例えば私のこの緑のサンプルこのアノマリーはx1の値は2.5のままだけど、x2の値は例えば、とても大きな値とか、こっちの3.5とかそういう値か、またらとても小さな値を期待する訳だ。"
  },
  {
    "index": "F17847",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But now, if I model my data, I'll find that my anomaly detection algorithm gives high probability to data in the central regions, slightly lower probability to that, sightly lower probability to that.",
    "output": "ここでこのデータをモデリングすると、アノマリー検出のアルゴリズムはまんなかのあたりの領域のデータに高い確率を与え、そのちょっと低い確率がこの辺に、それよりさらにちょっと低い確率がこの辺に、となる。"
  },
  {
    "index": "F17848",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "An example that's all the way out there, my algorithm will now give very low probability to.",
    "output": "そしてこの遥か遠くのサンプルに対しては私のアルゴリズムはここではとても低い確率を与えるだろう。"
  },
  {
    "index": "F17849",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, the process of this is, really look at the mistakes that it is making.",
    "output": "つまりこのプロセスは実際に間違いだった物、アルゴリズムがフラグ付けに失敗したアノマリーを直接調べて、それが新しいフィーチャーを作るようインスパイアしてくれるかどうかを見てみる。"
  },
  {
    "index": "F17850",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Look at the anomaly that the algorithm is failing to flag, and see if that inspires you to create some new feature.",
    "output": "その航空機エンジンの何か普通で無い所を探してそれを用いて新しいフィーチャーを作る。"
  },
  {
    "index": "F17851",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So find something unusual about that aircraft engine and use that to create a new feature, so that with this new feature it becomes easier to distinguish the anomalies from your good examples.",
    "output": "この新しいフィーチャーで正常なサンプルとアノマリー達をより簡単に区別出来るようになるように。"
  },
  {
    "index": "F17852",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's the process of error analysis and using that to create new features for anomaly detection.",
    "output": "以上が誤差解析の手順であり、それをアノマリー検出に使う新しいフィーチャーを作るのに使うやり方だ。"
  },
  {
    "index": "F17853",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, let me share with you my thinking on how I usually go about choosing features for anomaly detection.",
    "output": "最後に、私が普段アノマリー検出のフィーチャーを選ぶのにどうやってるのかについて、私の考えを共有しておきたい。"
  },
  {
    "index": "F17854",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, usually, the way I think about choosing features is I want to choose features that will take on either very, very large values, or very, very small values, for examples that I think might turn out to be anomalies.",
    "output": "普段私がフィーチャーの選択についてどう考えているかといえば、アノマリーのサンプルだと思う時にはなるべく凄く凄く小さくなるか凄く凄く大きくなるようなフィーチャーを探したい、と考えている。"
  },
  {
    "index": "F17855",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's use our example again of monitoring the computers in a data center.",
    "output": "ここでも再びコンピューターのデータセンターのモニタリングの例を考えてみよう。"
  },
  {
    "index": "F17856",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so you have lots of machines, maybe thousands, or tens of thousands of machines in a data center.",
    "output": "たくさんのマシンがあってたとえば何千とか何万とかのマシンがデータセンターにあるかもしれない。"
  },
  {
    "index": "F17857",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we want to know if one of the machines, one of our computers is acting up, so doing something strange.",
    "output": "そしてマシンの一つがコンピュータの一つがいかれているか、つまり何か妙な事をしているかを知りたい。"
  },
  {
    "index": "F17858",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here are examples of features you may choose, maybe memory used, number of disc accesses, CPU load, network traffic.",
    "output": "そこでこんなフィーチャーを選ぶかもしれない。メモリー使用量とかディスクアクセスの回数とかCPUロードとかネットワークトラフィックとか。"
  },
  {
    "index": "F17859",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But now, lets say that I suspect one of the failure cases, let's say that in my data set I think that CPU load the network traffic tend to grow linearly with each other.",
    "output": "だがここで、失敗してるケースの一つに、私のデータセットの失敗してるケースの一つが、CPUロードとネットワークトラフィックが共にリニアに増えていくと疑っているとしよう。"
  },
  {
    "index": "F17860",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe I'm running a bunch of web servers, and so, here if one of my servers is serving a lot of users, I have a very high CPU load, and have a very high network traffic.",
    "output": "例えばたくさんのwebサーバを走らせていてそのサービスの一つがたくさんのユーザに対してサービスを提供していたらとても高いCPUロードと高いネットワークトラフィックを得るだろう。"
  },
  {
    "index": "F17861",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But let's say, I think, let's say I have a suspicion, that one of the failure cases is if one of my computers has a job that gets stuck in some infinite loop.",
    "output": "だが例えば、失敗のケースの一つはもしコンピュータの一つが無限ループで固まってるジョブを持っていると疑ってるとしよう。"
  },
  {
    "index": "F17862",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if I think one of the failure cases, is one of my machines, one of my web servers--server code-- gets stuck in some infinite loop, and so the CPU load grows, but the network traffic doesn't because it's just spinning it's wheels and doing a lot of CPU work, you know, stuck in some infinite loop.",
    "output": "つまり私はある失敗のケースはあるマシンのあるwebserver、もといサーバーのcodeが無限ループで詰まってる、と思ってるとすると、その時はCPUロードは上がるだろうがネットワークトラフィックは上昇しないだろう。何故ならそれは単に糸車がくるくると回っていてたくさんのCPUの仕事をしている、つまり無限ループに詰まってる。"
  },
  {
    "index": "F17863",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case, to detect that type of anomaly, I might create a new feature, X5, which might be CPU load divided by network traffic.",
    "output": "その場合、その種のアノマリーを検出する為には、新たなフィーチャーX5として、CPUロードをネットワークトラフィックで割ったような新たなフィーチャーを作るかもしれない。"
  },
  {
    "index": "F17864",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so here X5 will take on a unusually large value if one of the machines has a very large CPU load but not that much network traffic and so this will be a feature that will help your anomaly detection capture, a certain type of anomaly.",
    "output": "するとx5は、もしあるマシンがとても大きなCPUロードでありながらそんなに大きくないネットワークトラフィックの時には異常に大きな値をとるだろう。だからこのフィーチャーはあなたのアノマリー検出がある種のアノマリーを検出するのを助ける事になる。"
  },
  {
    "index": "F17865",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you can also get creative and come up with other features as well.",
    "output": "こんな風にさらに別のフィーチャーも思いつくかもしれない。"
  },
  {
    "index": "F17866",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Like maybe I have a feature x6 thats CPU load squared divided by network traffic.",
    "output": "例えばx6としてCPUロードを二乗した物をネットワークトラフィックで割った物を使うかもしれない。"
  },
  {
    "index": "F17867",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this would be another variant of a feature like x5 to try to capture anomalies where one of your machines has a very high CPU load, that maybe doesn't have a commensurately large network traffic.",
    "output": "それはマシンの一つがとても高いCPUロードにありながらそれに見合うネットワークトラフィックが無い物を見分けようとしているフィーチャーだ。"
  },
  {
    "index": "F17868",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by creating features like these, you can start to capture anomalies that correspond to unusual combinations of values of the features.",
    "output": "これらのようなフィーチャーを作る事でフィーチャーの異常な組み合わせを捉えていく事が出来る。"
  },
  {
    "index": "F17869",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in this video we talked about how to and take a feature, and maybe transform it a little bit, so that it becomes a bit more Gaussian, before feeding into an anomaly detection algorithm.",
    "output": "このビデオではどうフィーチャーを選び必要ならアルゴリズムに食わせる前にちょっと変換してよりガウス分布っぽくするかを議論した。"
  },
  {
    "index": "F17870",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And also the error analysis in this process of creating features to try to capture different types of anomalies.",
    "output": "また、新しいフィーチャーを作って異なる種類のアノマリーを捕捉する為のエラー分析の手順も議論した。"
  },
  {
    "index": "F17871",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And with these sorts of guidelines hopefully that will help you to choose good features, to give to your anomaly detection algorithm, to help it capture all sorts of anomalies.",
    "output": "これらのガイドラインがあなたが良いフィーチャーを選びすべてのアノマリーをあなたの検出アルゴリズムが捕捉出来る一助にならんことを!"
  },
  {
    "index": "F17872",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this and the next video, I'd like to tell you about one possible extension to the anomaly detection algorithm that we've developed so far.",
    "output": "このビデオとこの次のビデオで、ここまでに開発してきたアノマリー検出のアルゴリズムの考えられる一つの拡張をお話しよう。"
  },
  {
    "index": "F17873",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This extension uses something called the multivariate Gaussian distribution, and it has some advantages, and some disadvantages, and it can sometimes catch some anomalies that the earlier algorithm didn't.",
    "output": "この拡張は、多変量ガウス分布と呼ばれる物を使う。そしてそれはある長所があり、そしてある短所もある。"
  },
  {
    "index": "F17874",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To motivate this, let's start with an example.",
    "output": "この動機付けを理解する為に、具体例から始めよう。"
  },
  {
    "index": "F17875",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that so our unlabeled data looks like what I have plotted here.",
    "output": "ここにプロットしたようなラベル無しデータがあるとする。"
  },
  {
    "index": "F17876",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm going to use the example of monitoring machines in the data center, monitoring computers in the data center.",
    "output": "そしてデータセンターのマシンのモニタリングの例を使っていく事にする。データセンターのコンピュータをモニタリングする。"
  },
  {
    "index": "F17877",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So my two features are x1 which is the CPU load and x2 which is maybe the memory use.",
    "output": "つまり私の二つのフィーチャーはx1がCPUロードで、x2が例えばメモリの使用量。"
  },
  {
    "index": "F17878",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if I take my two features, x1 and x2, and I model them as Gaussians then here's a plot of my X1 features, here's a plot of my X2 features, and so if I fit a Gaussian to that, maybe I'll get a Gaussian like this, so here's P of X 1, which depends on the parameters mu 1, and sigma squared 1, and here's my memory used, and, you know, maybe I'll get a Gaussian that looks like this, and this is my P of X 2, which depends on mu 2 and sigma squared 2.",
    "output": "そこで二つのフィーチャー、x1とx2に対して、ガウス分布でモデリングすると、これがx1フィーチャーによるプロットで、これがx2フィーチャーによるプロットだ。ここにガウス分布をフィッティングすると、こんな感じのガウス分布が得られるだろう。"
  },
  {
    "index": "F17879",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is how the anomaly detection algorithm models X1 and X2.",
    "output": "さて、以上がアノマリー検出のアルゴリズムがx1とx2をモデリングする方法だ。"
  },
  {
    "index": "F17880",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now let's say that in the test sets I have an example that looks like this.",
    "output": "ここで、テストセットにこんな手本があったとする。"
  },
  {
    "index": "F17881",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The location of that green cross, so the value of X 1 is about 0.4, and the value of X 2 is about 1.5.",
    "output": "この緑のバッテンの位置、つまりx1の値がだいたい0.4で、x2の値がだいたい1.5くらいの位置。"
  },
  {
    "index": "F17882",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, if you look at the data, it looks like, yeah, most of the data data lies in this region, and so that green cross is pretty far away from any of the data I've seen.",
    "output": "今、データを見てみると、ぱっと見た感じ多くのデータはこの範囲に存在しているので、この緑のバッテンは観測されているデータのどれとも、かなり離れている。"
  },
  {
    "index": "F17883",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It looks like that should be raised as an anomaly.",
    "output": "これはアノマリーとして提起されるべきに見える。"
  },
  {
    "index": "F17884",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in my data, in my, in the data of my good examples, it looks like, you know, the CPU load, and the memory use, they sort of grow linearly with each other.",
    "output": "つまり、私のデータにおいては、私の正常な手本のデータでは、CPUロードとメモリ使用量は、お互いに線形に上昇する傾向にあるように見える。"
  },
  {
    "index": "F17885",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if I have a machine using lots of CPU, you know memory use will also be high, whereas this example, this green example it looks like here, the CPU load is very low, but the memory use is very high, and I just have not seen that before in my training set.",
    "output": "一方でこの例、この緑の例では、CPUロードはとても低いが、メモリ使用量はとても高い。こんな物はトレーニングセットでは見られなかった物だ。"
  },
  {
    "index": "F17886",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It looks like that should be an anomaly.",
    "output": "これはアノマリーだとみなすべきだろう。"
  },
  {
    "index": "F17887",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But let's see what the anomaly detection algorithm will do.",
    "output": "だが、アノマリー検出のアルゴリズムが何をするか、見てみよう。"
  },
  {
    "index": "F17888",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, for the CPU load, it puts it at around there 0.5 and this reasonably high probability is not that far from other examples we've seen, maybe, whereas, for the memory use, this appointment, 0.5, whereas for the memory use, it's about 1.5, which is there.",
    "output": "これはリーズナブルに高い確率の範囲で、これまで見た手本の集団からそれ程離れた場所では無い。一方、メモリ使用量に関しては、ここの点は0.5のあたりだが、一方でメモリ使用量は、1.5くらいで、それはここ。"
  },
  {
    "index": "F17889",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Again, you know, it's all to us, it's not terribly Gaussian, but the value here and the value here is not that different from many other examples we've seen, and so P of X 1, will be pretty high, reasonably high.",
    "output": "これもまた、ガウス分布の尻尾の方ではあるが、でもこの値とこの値は、観測されているその他のたくさんの手本と比べても、そんなに違う物では無い。つまりp(x1)はかなり大きくなる、リーズナブルな程度には大きくなるだろうし、p(x2)もリーズナブルな程度には大きい。"
  },
  {
    "index": "F17890",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I mean, if you look at this plot right, this point here, it doesn't look that bad, and if you look at this plot, you know across here, doesn't look that bad.",
    "output": "つまり、この右側のプロットを見ると、この、ここの点は、そんなに悪くは見えないし、そしてこのプロットを見るとここのバッテンを見ると、そんなに悪くも見えない。"
  },
  {
    "index": "F17891",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I mean, I have had examples with even greater memory used, or with even less CPU use, and so this example doesn't look that anomalous.",
    "output": "つまり、もっとメモリ使用量が多い手本もある訳だし、また、もっと少ないCPU使用量の手本もある。つまり、この手本はそんなにアノマリーっぽくは見えない。"
  },
  {
    "index": "F17892",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, an anomaly detection algorithm will fail to flag this point as an anomaly.",
    "output": "つまり、アノマリー検出のアルゴリズムはこの点をアノマリーだとフラグ付けするのに失敗するだろう。"
  },
  {
    "index": "F17893",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it turns out what our anomaly detection algorithm is doing is that it is not realizing that this blue ellipse shows the high probability region, is that, one of the thing is that, examples here, a high probability, and the examples, the next circle of from a lower probably, and examples here are even lower probability, and somehow, here are things that are, green cross there, it's pretty high probability, and in particular, it tends to think that, you know, everything in this region, everything on the line that I'm circling over, has, you know, about equal probability, and it doesn't realize that something out here actually has much lower probability than something over there.",
    "output": "そしてこの隣の円は、一段低い確率となり、そしてここの手本は、さらに低い確率となり、そして、ここの緑のバッテンは、かなり高い確率となってしまう。そして具体的には、この領域は全て、私が円でくくってる直線上の点は全て、だいたい同じ確率だ、と考える。"
  },
  {
    "index": "F17894",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in order to fix this, we can, we're going to develop a modified version of the anomaly detection algorithm, using something called the multivariate Gaussian distribution also called the multivariate normal distribution.",
    "output": "そこで、これを修正する為に、修正版のアノマリー検出のアルゴリズムを開発していこう、多変量ガウス分布とか、多変量正規分布と呼ばれる物を用いて。"
  },
  {
    "index": "F17895",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's what we're going to do.",
    "output": "これが、我らがやる事だ。"
  },
  {
    "index": "F17896",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We have features x which are in Rn and instead of P of X 1, P of X 2, separately, we're going to model P of X, all in one go, so model P of X, you know, all at the same time.",
    "output": "我らはフィーチャーとしてRnのxを持つ。p(x1),p(x2)と別々にする代わりに、p(x)、という風に全部をいっぺんにモデリングする。"
  },
  {
    "index": "F17897",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the parameters of the multivariate Gaussian distribution are mu, which is a vector, and sigma, which is an n by n matrix, called a covariance matrix, and this is similar to the covariance matrix that we saw when we were working with the PCA, with the principal components analysis algorithm.",
    "output": "さて、多変量ガウス分布のパラメータとしては、まずミュー、これはベクトル。そしてシグマ、これはn掛けるn行列で、共分散行列とも呼ばれる。"
  },
  {
    "index": "F17898",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For the second complete is, let me just write out the formula for the multivariate Gaussian distribution.",
    "output": "だが完全を期して、多変量ガウス分布の式を書いてみよう。"
  },
  {
    "index": "F17899",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we say that probability of X, and this is parameterized by my parameters mu and sigma that the probability of x is equal to once again there's absolutely no need to memorize this formula.",
    "output": "そしてここのこれ、シグマの絶対値、ここのこれ、この記号を書くと、これはシグマの行列式(determinant)と呼ばれる物で、これは行列に対する、数学的な関数だ。"
  },
  {
    "index": "F17900",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, you can look it up whenever you need to use it, but this is what the probability of X looks like.",
    "output": "そしてあなたは、行列の行列式が何かを実際にしっている必要は全くない。"
  },
  {
    "index": "F17901",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this thing here, the absolute value of sigma, this thing here when you write this symbol, this is called the determent of sigma and this is a mathematical function of a matrix and you really don't need to know what the determinant of a matrix is, but really all you need to know is that you can compute it in octave by using the octave command DET of sigma.",
    "output": "あなたが実際に知る必要があるのは、それをOctaveで計算するにはコマンドdet(Sigma)を使う、という事だけだ。"
  },
  {
    "index": "F17902",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Okay, and again, just be clear, alright?",
    "output": "よし。"
  },
  {
    "index": "F17903",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this expression, these sigmas here, these are just n by n matrix.",
    "output": "この式において、これらのシグマ、これらは単なるn掛けるn行列だ。"
  },
  {
    "index": "F17904",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is not a summation and you know, the sigma there is an n by n matrix.",
    "output": "これは和の記号じゃない。そしてこのシグマは、n掛けるn行列。"
  },
  {
    "index": "F17905",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the formula for P of X, but it's more interestingly, or more importantly, what does P of X actually looks like?",
    "output": "さて、以上がp(x)の式だ。だが、もっと興味深い事としては、あるいはもっと重要な事としては、p(x)は実際に、どんな感じになるのだろうか?"
  },
  {
    "index": "F17906",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Lets look at some examples of multivariate Gaussian distributions.",
    "output": "多変量のガウス分布の例を幾つか見ていこう。"
  },
  {
    "index": "F17907",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's take a two dimensional example, say if I have N equals 2, I have two features, X 1 and X 2.",
    "output": "さて、二次元の例を考えよう。n=2なら、二つのフィーチャー、x1とx2を持つ。"
  },
  {
    "index": "F17908",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Lets say I set MU to be equal to 0 and sigma to be equal to this matrix here.",
    "output": "ミューをイコール0にセットして、シグマをこの行列としよう。"
  },
  {
    "index": "F17909",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "With 1s on the diagonals and 0s on the off-diagonals, this matrix is sometimes also called the identity matrix.",
    "output": "対角成分が1で、非対角成分が0。この行列はまた、単位行列と呼ばれる事もある。"
  },
  {
    "index": "F17910",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case, p of x will look like this, and what I'm showing in this figure is, you know, for a specific value of X1 and for a specific value of X2, the height of this surface the value of p of x.",
    "output": "この場合、pのxはこんな見た目となる。そしてこの図で私が示しているのは、ある特定のx1の値とある特定のx2の値の時に、この表面の高さ、これがpのxの値だ。"
  },
  {
    "index": "F17911",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so with this setting the parameters p of x is highest when X1 and X2 equal zero 0, so that's the peak of this Gaussian distribution, and the probability falls off with this sort of two dimensional Gaussian or this bell shaped two dimensional bell-shaped surface.",
    "output": "そしてこのパラメータの設定では、x1とx2がイコール0の時もっとも高くなる、つまりそこがガウス分布のピークとなり、そして確率分布は、こんな感じの二次元ガウス分布として、あるいはこの二次元のベル型の平面として減衰していく。"
  },
  {
    "index": "F17912",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Down below is the same thing but plotted using a contour plot instead, or using different colors, and so this heavy intense red in the middle, corresponds to the highest values, and then the values decrease with the yellow being slightly lower values the cyan being lower values and this deep blue being the lowest values so this is really the same figure but plotted viewed from the top instead, using colors instead.",
    "output": "この下には、同じ事を代わりに等高線プロットを用いてプロットしてみた、等高線は様々な色を使う事で表現される。つまりこの真中の、赤が激しく集中している所は、もっとも高い値に対応している。"
  },
  {
    "index": "F17913",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, with this distribution, you see that it faces most of the probability near 0,0 and then as you go out from 0,0 the probability of X1 and X2 goes down.",
    "output": "さて、この分布では、確率の大半は0,0のそばにあり、そして0,0から外に出て行くと、x1とx2の確率は下がっていく。"
  },
  {
    "index": "F17914",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now lets try varying some of the parameters and see what happens.",
    "output": "ここでパラメータの幾つかを変更していき、何が起こるか見ていこう。"
  },
  {
    "index": "F17915",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's take sigma and change it so let's say sigma shrinks a little bit.",
    "output": "シグマを変えてみよう。シグマをちょっと縮めてみよう。"
  },
  {
    "index": "F17916",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Sigma is a covariance matrix and so it measures the variance or the variability of the features X1 X2.",
    "output": "シグマは共分散行列なので、フィーチャーx1,x2の分散、または変わりやすさを測った物だ。"
  },
  {
    "index": "F17917",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if the shrink sigma then what you get is what you get is that the width of this bump diminishes and the height also increases a bit, because the area under the surface is equal to 1.",
    "output": "だからシグマを縮めると、得られるのは、、、得られるのは、このコブの幅が減少し、高さもちょっとだけ増加する。何故なら、平面の下の体積は1だから。"
  },
  {
    "index": "F17918",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the integral of the volume under the surface is equal to 1, because probability distribution must integrate to one.",
    "output": "つまり平面の下の体積の積分結果は、イコール1だ。何故なら確率分布の積分は1にならなくてはいけないから。"
  },
  {
    "index": "F17919",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, if you shrink the variance, it's kinda like shrinking sigma squared, you end up with a narrower distribution, and one that's a little bit taller.",
    "output": "だがもし分散を縮めると、それはシグマ二乗を縮める事に相当し、より狭い分布が得られる、そしてその背の高さはちょっとだけ高くなる。"
  },
  {
    "index": "F17920",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so you see here also the concentric ellipsis has shrunk a little bit.",
    "output": "そして見て分かるように、同心の楕円もちょっとだけ縮む。"
  },
  {
    "index": "F17921",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in contrast if you were to increase sigma to 2 2 on the diagonals, so it is now two times the identity then you end up with a much wider and much flatter Gaussian.",
    "output": "一方で、対照的に、もしシグマの対角成分を2,2に増加させると、するとこれは単位行列の2倍となり、もっと幅広くて、もっと平坦なガウス分布が得られる。"
  },
  {
    "index": "F17922",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the width of this is much wider.",
    "output": "つまり、この幅はもっと広くなる。"
  },
  {
    "index": "F17923",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is hard to see but this is still a bell shaped bump, it's just flattened down a lot, it has become much wider and so the variance or the variability of X1 and X2 just becomes wider.",
    "output": "これは見づらいが、これもまだベル型のコブになっていて、ただ凄い平坦になっているだけだ、それはより幅広くなった、つまり、x1とx2の分散、あるいは変わりやすさは単により広くなった。"
  },
  {
    "index": "F17924",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now lets try varying one of the elements of sigma at the time.",
    "output": "今度は、一度にシグマの要素の一方だけを変更していこう。"
  },
  {
    "index": "F17925",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What this does, is this reduces the variance of the first feature, X 1, while keeping the variance of the second feature X 2, the same.",
    "output": "シグマのこっちを0.6にこっちを1にしてみよう。こうすると、最初のフィーチャーx1の分散が減少し、同時にフィーチャーx2の分散は同一に保たれている。"
  },
  {
    "index": "F17926",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so with this setting of parameters, you can model things like that.",
    "output": "すると、このパラメータの設定では、こんな物をモデリング出来る。"
  },
  {
    "index": "F17927",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas if I do this, if I set this matrix to 2, 1 then you can also model examples where you know here we'll say X1 can have take on a large range of values whereas X2 takes on a relatively narrower range of values.",
    "output": "他方、もし私がこの行列、2,1をセットすると、今度は以下のような手本をモデリングする事が出来る。それはx1が広い範囲の値をとり、一方x2は相対的に狭い範囲の値を、取る、というような。"
  },
  {
    "index": "F17928",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's reflected in this figure as well, you know where, the distribution falls off more slowly as X 1 moves away from 0, and falls off very rapidly as X 2 moves away from 0.",
    "output": "そしてその事は、この図にも反映されている。ここでは、分布はx1から離れるに連れてよりゆっくりと減衰していて、そしてx2が0から離れると急激に減衰している。"
  },
  {
    "index": "F17929",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly if we were to modify this element of the matrix instead, then similar to the previous slide, except that here where you know playing around here saying that X2 can take on a very small range of values and so here if this is 0.6, we notice now X2 tends to take on a much smaller range of values than the original example, whereas if we were to set sigma to be equal to 2 then that's like saying X2 you know, has a much larger range of values.",
    "output": "そして同様に、代わりに行列のこの要素を変更すると、前のスライドと似ているが、違いとしては、ここでいじっている所は、x2がとても小さい範囲の値をとるようになるという事で、つまりここでは、これが0.6だと、x2がオリジナルの例よりもより小さな範囲を取るようになるという事に気づく。一方で、もしシグマをイコール2にセットしたら、それはx2がもっと幅広い範囲の値を取る、という事だ。"
  },
  {
    "index": "F17930",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, one of the cool things about the multivariate Gaussian distribution is that you can also use it to model correlations between the data.",
    "output": "ここで、多変量ガウス分布のクールな事の一つには、データ同士の相関をモデリングするのに使う事が出来る、という事がある。"
  },
  {
    "index": "F17931",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is we can use it to model the fact that X1 and X2 tend to be highly correlated with each other for example.",
    "output": "つまり、多変量ガウス分布を用いて、例えばx1とx2が互いに高く相関する傾向にある、という事実をモデリング出来る。"
  },
  {
    "index": "F17932",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So specifically if you start to change the off diagonal entries of this covariance matrix you can get a different type of Gaussian distribution.",
    "output": "具体的には、この共分散行列の非対角成分を変更する事で、異なる種類のガウス分布を得る事が出来る。"
  },
  {
    "index": "F17933",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so as I increase the off-diagonal entries from .5 to .8, what I get is this distribution that is more and more thinly peaked along this sort of x equals y line.",
    "output": "つまり、非対角成分の要素を0.5から0.8に増加させると、この分布が得られる。これはこの、x=yの直線に沿ったより狭い範囲に山があるような分布。"
  },
  {
    "index": "F17934",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so here the contour says that x and y tend to grow together and the things that are with large probability are if either X1 is large and Y2 is large or X1 is small and Y2 is small.",
    "output": "つまり、ここの等高線は、xとyがともに増加する傾向にある、と主張していて、x1が大きくてx2も大きい、という時とx1が小さくてx2も小さい、という時は大きな確率となっている。あるいはその両者の間も高い確率となっている。"
  },
  {
    "index": "F17935",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as this entry, 0.8 gets large, you get a Gaussian distribution, that's sort of where all the probability lies on this sort of narrow region, where x is approximately equal to y.",
    "output": "そしてこの要素、0.8が大きくなると、この種の狭い領域に全ての確率が存在するようなガウス分布が得られ、xはだいたいイコールyとなる。"
  },
  {
    "index": "F17936",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is a very tall, thin distribution you know line mostly along this line central region where x is close to y.",
    "output": "これはとても背が高く、薄い分布で、だいたいこの直線に沿った、xがyと近い領域を中心とした直線に沿った。"
  },
  {
    "index": "F17937",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast if we set these to negative values, as I decreases it to -.5 down to -.8, then what we get is a model where we put most of the probability in this sort of negative X one in the next 2 correlation region, and so, most of the probability now lies in this region, where X 1 is about equal to -X 2, rather than X 1 equals X 2.",
    "output": "逆に、もしこれらの値を負の値にセットすると、これを-0.5から-0.8まで減少させていくと、以下のようなモデルが得られる:確率の多くをいわゆるx1とx2が負の相関となっている範囲に置くようなモデル。つまり、確率のほとんどが、今度はこの範囲に存在し、そこはx1がだいたいイコール-x2となっている、x1=x2の代わりに。"
  },
  {
    "index": "F17938",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this captures a sort of negative correlation between x1 and x2.",
    "output": "つまり、これは、いわゆるx1とx2の間の負の相関を捉えている。"
  },
  {
    "index": "F17939",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is a hopefully this gives you a sense of the different distributions that the multivariate Gaussian distribution can capture.",
    "output": "以上で、あなたも多変量ガウス分布が捉える事が出来る、様々な分布が、感覚的につかめただろうか。"
  },
  {
    "index": "F17940",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So follow up in varying, the covariance matrix sigma, the other thing you can do is also, vary the mean parameter mu, and so operationally, we have mu equal 0 0, and so the distribution was centered around X 1 equals 0, X2 equals 0, so the peak of the distribution is here, whereas, if we vary the values of mu, then that varies the peak of the distribution and so, if mu equals 0, 0.5, the peak is at, you know, X1 equals zero, and X2 equals 0.5, and so the peak or the center of this distribution has shifted, and if mu was 1.5 minus 0.5 then OK, and similarly the peak of the distribution has now shifted to a different location, corresponding to where, you know, X1 is 1.5 and X2 is -0.5, and so varying the mu parameter, just shifts around the center of this whole distribution.",
    "output": "一方、ミューの値を変えていくと、分布のピークが変わっていって、もしミューがイコール、0,0.5なら、ピークはちょうどx1=0でx2=0.5の所にあり、つまり分布のピーク、あるいは中心は、シフトする。そしてもしミューが1.5,-0.5なら、ふたたび同様に、分布のピークは今度は別の場所にシフトする。"
  },
  {
    "index": "F17941",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, hopefully, looking at all these different pictures gives you a sense of the sort of probability distributions that the Multivariate Gaussian Distribution allows you to capture.",
    "output": "これら全ての別々の図を見る事で、多変量のガウス分布で捉える事の出来る、確率分布の感じがつかめたかな。"
  },
  {
    "index": "F17942",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the key advantage of it is it allows you to capture, when you'd expect two different features to be positively correlated, or maybe negatively correlated.",
    "output": "そしてそのキーとなる利点としては、それを用いると二つの別々のフィーチャーが正の相関を持っている、とか負の相関を持っている、という事が期待される時にそれらを捕捉出来るようになる、という事だ。"
  },
  {
    "index": "F17943",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video, we'll take this multivariate Gaussian distribution and apply it to anomaly detection.",
    "output": "次のビデオでは、多変量ガウス分布を、アノマリー検出に適用する。"
  },
  {
    "index": "F17944",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last video we talked about the Multivariate Gaussian Distribution and saw some examples of the sorts of distributions you can model, as you vary the parameters, mu and sigma.",
    "output": "前回のビデオで多変量ガウス分布を議論した。そしてパラメータのミューとシグマを変えていく事でモデリング出来る分布の例を幾つか見た。"
  },
  {
    "index": "F17945",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, let's take those ideas, and apply them to develop a different anomaly detection algorithm.",
    "output": "このビデオでは、それらのアイデアを用いて、別のアノマリー検出のアルゴリズムを開発していこう。"
  },
  {
    "index": "F17946",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To recap the multivariate Gaussian distribution and the multivariate normal distribution has two parameters, mu and sigma.",
    "output": "復習しておくと、多変量ガウス分布、またの名を多変量正規分は、二つのパラメータ、ミューとシグマを持っている。"
  },
  {
    "index": "F17947",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where mu this an n dimensional vector and sigma, the covariance matrix, is an n by n matrix.",
    "output": "ここでミューはn次元ベクトルで、シグマ、共分散行列は、n掛けるn行列。"
  },
  {
    "index": "F17948",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And here's the formula for the probability of X, as parameterized by mu and sigma, and as you vary mu and sigma, you can get a range of different distributions, like, you know, these are three examples of the ones that we saw in the previous video.",
    "output": "そしてこれがxの確率の式である、ミューとシグマでパラメトライズされた。そしてミューとシグマを変更していく事で、あなたは様々な範囲の分布を得る事が出来る、例えばこれら三つの例は我らが前回のビデオで見た、そんな物の中の一例だ。"
  },
  {
    "index": "F17949",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's talk about the parameter fitting or the parameter estimation problem.",
    "output": "ではパラメータのフィッティングについて議論しよう、言い換えるとパラメータ推計の問題について。"
  },
  {
    "index": "F17950",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The question, as usual, is if I have a set of examples X1 through XM and here each of these examples is an n dimensional vector and I think my examples come from a multivariate Gaussian distribution.",
    "output": "問いは、いつも通り、x1からxmまでの手本の集合があったとして、そしてここではこれらの手本はおのおのn次元ベクトルで、そしてこの手本は多変量ガウス分布の分布に従っている、と思ったとする。"
  },
  {
    "index": "F17951",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "How do I try to estimate my parameters mu and sigma?",
    "output": "どうやってパラメータ、ミューとシグマを推計したらいいだろうか?"
  },
  {
    "index": "F17952",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well the standard formulas for estimating them is you set mu to be just the average of your training examples.",
    "output": "それらを推計する標準的な公式は、ミューにトレーニング手本の平均をセットすれば良い。"
  },
  {
    "index": "F17953",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is actually just like the sigma that we had written out, when we were using the PCA or the Principal Components Analysis algorithm.",
    "output": "これは実際に、PCA、あるいは主成分分析を使った時に我らが書き下した物と同様の物だ。"
  },
  {
    "index": "F17954",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you just plug in these two formulas and this would give you your estimated parameter mu and your estimated parameter sigma.",
    "output": "さて、これら二つの式に代入するだけで、推計されたパラメータ、ミューと、推計されたパラメータのシグマが得られる。"
  },
  {
    "index": "F17955",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So given the data set here is how you estimate mu and sigma.",
    "output": "つまりデータセットが与えられた時に、ミューとシグマを推計する方法だ。"
  },
  {
    "index": "F17956",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's take this method and just plug it into an anomaly detection algorithm.",
    "output": "ではこの手法をアノマリー検出のアルゴリズムに組み込んでみよう。"
  },
  {
    "index": "F17957",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So how do we put all of this together to develop an anomaly detection algorithm?",
    "output": "では、どうやったら我らはこれらを全て組み合わせてアノマリー検出のアルゴリズムを開発出来るだろうか?"
  },
  {
    "index": "F17958",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here 's what we do.",
    "output": "我らがやるべきは、こうだ。"
  },
  {
    "index": "F17959",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First we take our training set, and we fit the model, we fit P of X, by, you know, setting mu and sigma as described on the previous slide.",
    "output": "まず、トレーニングセットに対し、モデルをフィッティングする、p(x)をフィッティングする、前のスライドで記述したミューとシグマを設定する事で。"
  },
  {
    "index": "F17960",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next when you are given a new example X. So if you are given a test example, lets take an earlier example to have a new example out here.",
    "output": "次に、新しい手本xが与えられた時に、テストの手本が与えられたとすると、、、前の例から、新しい手本をここに得たとしよう。"
  },
  {
    "index": "F17961",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that is my test example.",
    "output": "これが私のテスト手本。"
  },
  {
    "index": "F17962",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given the new example X, what we are going to do is compute P of X, using this formula for the multivariate Gaussian distribution.",
    "output": "新しい手本xが与えられた時に、我らがやる事は、p(x)を計算する事、この多変量ガウス分布の式を用いる事で。"
  },
  {
    "index": "F17963",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then, if P of X is very small, then we flagged it as an anomaly, whereas, if P of X is greater than that parameter epsilon, then we don't flag it as an anomaly.",
    "output": "そして次に、p(x)がとても小さかったら、それをアノマリーとフラグ立てする。他方、p(x)がパラメータのイプシロンより大きければ、それにはアノマリーのフラグは立てない。"
  },
  {
    "index": "F17964",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it turns out, if we were to fit a multivariate Gaussian distribution to this data set, so just the red crosses, not the green example, you end up with a Gaussian distribution that places lots of probability in the central region, slightly less probability here, slightly less probability here, slightly less probability here, and very low probability at the point that is way out here.",
    "output": "つまり結局、もしこのデータセットに多変量ガウス分布をフィッティングするとすると、このデータセットというのは赤のクロスで、緑の手本は含まないが、そうすると結局、以下のようなガウス分布が得られる:中心の領域にたくさんの確率を、ここにはちょっとだけ低い確率を、ここにはさらにちょっとだけ低い確率を、ここにはさらにちょっとだけ低い確率を、そしてここより外はとても低い確率となるような物だ。"
  },
  {
    "index": "F17965",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, if you apply the multivariate Gaussian distribution to this example, it will actually correctly flag that example. as an anomaly.",
    "output": "つまり、この例で多変量ガウス分布を適用すると、これは実際にただしくこの手本をフラグ立てする、アノマリーだと。"
  },
  {
    "index": "F17966",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally it's worth saying a few words about what is the relationship between the multivariate Gaussian distribution model, and the original model, where we were modeling P of X as a product of this P of X1, P of X2, up to P of Xn.",
    "output": "最後に、多変量ガウス分布モデルと元のモデルとの間の関係について2,3言っておく価値があるだろう、ここで元のモデルとは、p(x)をp(x1),p(x2),...,p(xn)までの積でモデリングしたもの。"
  },
  {
    "index": "F17967",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that you can prove mathematically, I'm not going to do the proof here, but you can prove mathematically that this relationship, between the multivariate Gaussian model and this original one.",
    "output": "ここで証明はしないが。だが、多変量ガウス分布のモデルとオリジナルのモデルの間のこの関係を数学的に証明する事が出来る。"
  },
  {
    "index": "F17968",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, it turns out that the original model corresponds to multivariate Gaussians, where the contours of the Gaussian are always axis aligned.",
    "output": "より詳細には、オリジナルのモデルは、多変量ガウス分布の、ガウス分布の等高線がいつも軸に沿っている場合に対応している事が知られている。"
  },
  {
    "index": "F17969",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So all three of these are examples of Gaussian distributions that you can fit using the original model.",
    "output": "つまり、これら三つは全てオリジナルのモデルを使ってフィッティングする事が可能なガウス分布の例だ。"
  },
  {
    "index": "F17970",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that that corresponds to multivariate Gaussian, where, you know, the ellipsis here, the contours of this distribution--it turns out that this model actually corresponds to a special case of a multivariate Gaussian distribution.",
    "output": "これは結局、多変量ガウス分布のうち、この楕円、分布の等高線が、、、このモデルは実際に多変量ガウス分布の特別なケースに対応している。"
  },
  {
    "index": "F17971",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, this special case is defined by constraining the distribution of p of x, the multivariate a Gaussian distribution of p of x, so that the contours of the probability density function, of the probability distribution function, are axis aligned.",
    "output": "具体的には、この特別なケースとは、p(x)の分布、多変量ガウス分布としてのp(x)の分布を、確率密度関数の、、、確率分布関数の等高線を軸に沿った物に制限した物、として定義する事が出来る。"
  },
  {
    "index": "F17972",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so you can get a p of x with a multivariate Gaussian that looks like this, or like this, or like this.",
    "output": "つまり多変量ガウス分布のp(x)として、このような物や、このような物、またはこのような物を、得る事が出来る。"
  },
  {
    "index": "F17973",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you notice, that in all 3 of these examples, these ellipses, or these ovals that I'm drawing, have their axes aligned with the X1 X2 axes.",
    "output": "そして見て分かるように、これら三つは全て、これらの楕円は、または私が今描いた楕円は、その軸がx1x2軸に沿っている。"
  },
  {
    "index": "F17974",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we do not have, is a set of contours that are at an angle, right?",
    "output": "そして我らが持っていない物として、角度のある等高線の集合だ。でしょ?"
  },
  {
    "index": "F17975",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this corresponded to examples where sigma is equal to 1 1, 0.8, 0.8.",
    "output": "そしてこの事は、シグマがイコール1,1とか0.8,0.8などの例に対応した物で、非対角成分に非0成分が存在していないという事。"
  },
  {
    "index": "F17976",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, it turns out that it's possible to show mathematically that this model actually is the same as a multivariate Gaussian distribution but with a constraint.",
    "output": "つまり、このモデルは実際に多変量ガウス分布に制約を加えた物である、という事を数学的に証明出来る事が知られている。"
  },
  {
    "index": "F17977",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the constraint is that the covariance matrix sigma must have 0's on the off diagonal elements.",
    "output": "そしてその制約とは、共分散行列のシグマは非対角成分が0でなくてはならない、とい事だ。"
  },
  {
    "index": "F17978",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, the covariance matrix sigma, this thing here, it would be sigma squared 1, sigma squared 2, down to sigma squared n, and then everything on the off diagonal entries, all of these elements above and below the diagonal of the matrix, all of those are going to be zero.",
    "output": "具体的に言うと、共分散行列シグマはここのこれは、シグマ二乗1、シグマ二乗2,、、、とシグマ二乗nまでと、そしてそれ以外の全ての非対角成分の要素は、行列の、対角線より上側のこれら全てと、下側のこれら全て、それら全てが、ゼロとなる。"
  },
  {
    "index": "F17979",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact if you take these values of sigma, sigma squared 1, sigma squared 2, down to sigma squared n, and plug them into here, and you know, plug them into this covariance matrix, then the two models are actually identical.",
    "output": "そして実際、もしこれらのシグマの値、シグマ二乗の1,シグマ二乗の2,...とシグマ二乗のnまでの値を、ここに代入して、この共分散行列に代入すると、二つのモデルは、実際に同一となる。"
  },
  {
    "index": "F17980",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, this new model, using a multivariate Gaussian distribution, corresponds exactly to the old model, if the covariance matrix sigma, has only 0 elements off the diagonals, and in pictures that corresponds to having Gaussian distributions, where the contours of this distribution function are axis aligned.",
    "output": "つまり、この新しいモデル、多変量ガウス分布を用いたこのモデルは、古いモデルと厳密に一致する、もし非対角成分が全てゼロとなるような共分散行列のシグマの場合には。それは具体的には、ガウス分布のうち、この分布関数の等高線が軸に沿っている物に対応している。"
  },
  {
    "index": "F17981",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you aren't allowed to model the correlations between the diffrent features.",
    "output": "だから異なるフィーチャー間の相関をモデリングする事は許されていない。"
  },
  {
    "index": "F17982",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in that sense the original model is actually a special case of this multivariate Gaussian model.",
    "output": "つまりその意味で、オリジナルのモデルは、実際に多変量ガウス分布のモデルの特別な場合となっている。"
  },
  {
    "index": "F17983",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when would you use each of these two models?",
    "output": "では、どういう場合に、これら二つのモデルのどちらを使ったらいいか?"
  },
  {
    "index": "F17984",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when would you the original model and when would you use the multivariate Gaussian model?",
    "output": "つまりあなたはいつ、オリジナルのモデルを使うべきで、そしてあなたはいつ、多変量ガウス分布のモデルを使うべきなのだろうか?"
  },
  {
    "index": "F17985",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The original model is probably used somewhat more often, and whereas the multivariate Gaussian distribution is used somewhat less but it has the advantage of being able to capture correlations between features.",
    "output": "オリジナルのモデルは、おそらくより頻繁に用いられるだろう、一方で多変量ガウス分布はおそらくより少ししか用いられないだろう、だがそちらの方には、フィーチャー間の相関を捉える事が出来る、という利点がある。"
  },
  {
    "index": "F17986",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So suppose you want to capture anomalies where you have different features say where features x1, x2 take on unusual combinations of values so in the earlier example, we had that example where the anomaly was with the CPU load and the memory use taking on unusual combinations of values, if you want to use the original model to capture that, then what you need to do is create an extra feature, such as X3 equals X1/X2, you know equals maybe the CPU load divided by the memory used, or something, and you need to create extra features if there's unusual combinations of values where X1 and X2 take on an unusual combination of values even though X1 by itself and X2 by itself looks like it's taking a perfectly normal value.",
    "output": "つまり、例えば異なるフィーチャー同士、ここではフィーチャーx1,x2としよう、これらのフィーチャー同士の異常な組み合わせの値となるアノマリーを検出したいとしよう、つまり前の例で言うと、CPUロードとメモリ使用量が異常な値の組み合わせとなっているアノマリーの手本があった。もしオリジナルのモデルを使ってこれを捉えたいと思えば、あなたはこんな追加のフィーチャーを作る必要がある、それは例えばx3=x1/x2みたいな。"
  },
  {
    "index": "F17987",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you're willing to spend the time to manually create an extra feature like this, then the original model will work fine.",
    "output": "だが、もしこのような追加のフィーチャーを作る事に喜んで時間を差し出せるなら、オリジナルなモデルはちゃんと機能するだろう。"
  },
  {
    "index": "F17988",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in contrast, the multivariate Gaussian model can automatically capture correlations between different features.",
    "output": "一方、対照的に、多変量ガウス分布のモデルは、自動的にフィーチャー同士の相関を捉える事が出来る。"
  },
  {
    "index": "F17989",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the original model has some other more significant advantages, too, and one huge advantage of the original model is that it is computationally cheaper, and another view on this is that is scales better to very large values of n and very large numbers of features, and so even if n were ten thousand, or even if n were equal to a hundred thousand, the original model will usually work just fine.",
    "output": "そんな利点のうち大きな物としては、オリジナルのモデルの方が計算量的に安上がりだ、という事。これを別の見方をすると、これは非常に大きなnの値に、つまり非常に大きなフィーチャーの数により良くスケールする。"
  },
  {
    "index": "F17990",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in contrast for the multivariate Gaussian model notice here, for example, that we need to compute the inverse of the matrix sigma where sigma is an n by n matrix and so computing sigma if sigma is a hundred thousand by a hundred thousand matrix that is going to be very computationally expensive.",
    "output": "他方、対照的に多変量ガウス分布のモデルでは、例えばここを見ると、行列シグマの逆行列を計算する必要があり、このシグマはn掛けるn行列だ。つまり、シグマで計算する時には、シグマが10万掛ける10万の行列の時には、計算量的にとても高価になってしまう。"
  },
  {
    "index": "F17991",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the multivariate Gaussian model scales less well to large values of N.",
    "output": "つまり、多変量ガウス分布のモデルは、大きなnの値に対しては、相対的には、よりいまいちスケールしない。"
  },
  {
    "index": "F17992",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally for the original model, it turns out to work out ok even if you have a relatively small training set this is the small unlabeled examples that we use to model p of x of course, and this works fine, even if M is, you know, maybe 50, 100, works fine.",
    "output": "そして最後に、オリジナルの方のモデルでは、たとえトレーニングセットが相対的に小さくても、うまく機能する事が分かっている。これはp(x)をモデリングする時に用いるラベル無しのトレーニング手本が小さくても、うまく機能する、たとえmが、50とか100でも、たぶんうまく機能する。"
  },
  {
    "index": "F17993",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas for the multivariate Gaussian, it is sort of a mathematical property of the algorithm that you must have m greater than n, so that the number of examples is greater than the number of features you have.",
    "output": "他方、多変量ガウシアンでは、アルゴリズムにある種の数学的な性質があって、mはnより大きくないといけない。つまり、手本の数は、フィーチャーの数よりは大きくないといけない。"
  },
  {
    "index": "F17994",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And there's a mathematical property of the way we estimate the parameters that if this is not true, so if m is less than or equal to n, then this matrix isn't even invertible, that is this matrix is singular, and so you can't even use the multivariate Gaussian model unless you make some changes to it.",
    "output": "もしこれが真で無いと、つまりmがn以下だと、この行列が可逆じゃなくなる、つまりこの行列が特異行列となり、何かしら変更しないと多変量ガウス分布のモデルは、使う事すら出来ない。だけど典型的なルールとしての経験則として私が採用しているのは、私はmがnよりもずっと大きい時しか多変量ガウス分布は使わない、という物。"
  },
  {
    "index": "F17995",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But a typical rule of thumb that I use is, I will use the multivariate Gaussian model only if m is much greater than n, so this is sort of the narrow mathematical requirement, but in practice, I would use the multivariate Gaussian model, only if m were quite a bit bigger than n.",
    "output": "つまりこれは、実際的な物よりちょっと狭い数学的な要求と言える。実際には、私は多変量ガウス分布のモデルは、mがかなりnより大きい時しか使わない。"
  },
  {
    "index": "F17996",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if m were greater than or equal to 10 times n, let's say, might be a reasonable rule of thumb, and if it doesn't satisfy this, then the multivariate Gaussian model has a lot of parameters, right, so this covariance matrix sigma is an n by n matrix, so it has, you know, roughly n squared parameters, because it's a symmetric matrix, it's actually closer to n squared over 2 parameters, but this is a lot of parameters, so you need make sure you have a fairly large value for m, make sure you have enough data to fit all these parameters.",
    "output": "例えばmが10掛けるn以上なら、とかは、リーズナブルな経験則と言えそう。そしてこれを満たしていなければ、、、多変量ガウス分布のモデルがたくさんのパラメータを持っていると、この共分散行列シグマはn掛けるn行列なので、これはだいたいn二乗のパラメータを持つ、これは対称行列だから、実際はn二乗割る2に近いくらいのパラメータだが。"
  },
  {
    "index": "F17997",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And m greater than or equal to 10 n would be a reasonable rule of thumb to make sure that you can estimate this covariance matrix sigma reasonably well.",
    "output": "そこでmが10n以上、というのは、この共分散行列を推計出来る為の、リーズナブルな経験則と言えるだろう。"
  },
  {
    "index": "F17998",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in practice the original model shown on the left that is used more often.",
    "output": "さて、実践の場では、左に示したオリジナルのモデルの方が、より頻繁に使われる。"
  },
  {
    "index": "F17999",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you suspect that you need to capture correlations between features what people will often do is just manually design extra features like these to capture specific unusual combinations of values.",
    "output": "そしてもしあなたが、フィーチャー同士の相関を捉える必要があるんじゃないか、と思っているなら、そういう時は人々はよく、単に人力でそれらの異常な値の組み合わせを捉える追加のフィーチャーをデザインする、という事をする。"
  }
]