[
  {
    "index": "F18000",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in problems where you have a very large training set or m is very large and n is not too large, then the multivariate Gaussian model is well worth considering and may work better as well, and can save you from having to spend your time to manually create extra features in case the anomalies turn out to be captured by unusual combinations of values of the features.",
    "output": "だが、とても大量のトレーニングセットを持っていて、つまりmがとても大きくて、そしてnがそんなに大きく無いような問題に直面している時は、多変量ガウス分布のモデルは、検討してみる価値があり、よりうまく機能する事もあり、異常なフィーチャーの値の組を捉える結果となるような追加のフィーチャーを人力で作るのに時間を費やさずに済ます事が出来るかもしれない。"
  },
  {
    "index": "F18001",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally I just want to briefly mention one somewhat technical property, but if you're fitting multivariate Gaussian model, and if you find that the covariance matrix sigma is singular, or you find it's non-invertible, they're usually 2 cases for this.",
    "output": "最後に、いくらかテクニカルな性質だが、ちょっと簡単に触れておきたい事がある。多変量ガウス分布のモデルをフィッティングする時に、もし共分散行列のシグマが特異行列だと見出したら、あるいはそれが非可逆だと見出したら、それには普通、二つの場合がある。"
  },
  {
    "index": "F18002",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is if it's failing to satisfy this m greater than n condition, and the second case is if you have redundant features.",
    "output": "一つ目は、このmがnより大きい、という条件を満たしていない場合。"
  },
  {
    "index": "F18003",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So by redundant features, I mean, if you have 2 features that are the same.",
    "output": "二番目は、冗長なフィーチャーがある場合。"
  },
  {
    "index": "F18004",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Somehow you accidentally made two copies of the feature, so your x1 is just equal to x2.",
    "output": "冗長なフィーチャーという言葉で私が意味している事は、同一のフィーチャーが二つある、という事。"
  },
  {
    "index": "F18005",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or if you have redundant features like maybe your features X3 is equal to feature X4, plus feature X5.",
    "output": "どうにかして、偶然二つのコピーのフィーチャーを作ってしまった、つまりx1は単に、イコールx2であるようなフィーチャーを作ってしまった、だとか、あるいは例えばフィーチャーx3=フィーチャーx4+フィーチャーx5のような冗長なフィーチャーがあるという場合。"
  },
  {
    "index": "F18006",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Okay, so if you have highly redundant features like these, you know, where if X3 is equal to X4 plus X5, well X3 doesn't contain any extra information, right?",
    "output": "オーケー。で、このようなとても冗長なフィーチャーがある場合、もしx3=x4+x5なら、x3は何も追加の情報を含んでいない。"
  },
  {
    "index": "F18007",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You just take these 2 other features, and add them together.",
    "output": "単にこれら二つの別のフィーチャーを、足し合わせるだけだ。"
  },
  {
    "index": "F18008",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you have this sort of redundant features, duplicated features, or this sort of features, than sigma may be non-invertible.",
    "output": "そしてもしあなたがこの手の冗長な、重複したフィーチャーとかこの種のフィーチャーを保持している時には、シグマは非可逆になってしまう。"
  },
  {
    "index": "F18009",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so there's a debugging set-- this should very rarely happen, so you probably won't run into this, it is very unlikely that you have to worry about this-- but in case you implement a multivariate Gaussian model you find that sigma is non-invertible.",
    "output": "以上はデバッギングの時の豆知識だ。これは本当に稀にしか起こらないはずで、だからあなたが鉢合わせする事も多分無いだろう。"
  },
  {
    "index": "F18010",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I would do is first make sure that M is quite a bit bigger than N, and if it is then, the second thing I do, is just check for redundant features.",
    "output": "だがあなたが多変量ガウス分布のモデルを実装してみて、シグマが非可逆だという事を発見したら、私だったらまず、mがnよりもずっと大きい事を確認し、もしそうであるなら、次に二番目に私がやる事は、冗長なフィーチャーをチェックする事だ。"
  },
  {
    "index": "F18011",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if there are 2 features that are equal, just get rid of one of them, or if you have redundant if these , X3 equals X4 plus X5, just get rid of the redundant feature, and then it should work fine again.",
    "output": "そして、もし単純に等しい二つのフィーチャーがある時には、一方を取り除けば良い。そしてもしx3=x4+x5のような冗長なフィーチャーがある時には、その冗長なフィーチャーを取り除けば良い。"
  },
  {
    "index": "F18012",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As an aside for those of you who are experts in linear algebra, by redundant features, what I mean is the formal term is features that are linearly dependent.",
    "output": "線形代数のエキスパートの聴衆の為にちょっと脇道に逸れると、冗長なフィーチャー、という言葉で私が意味している事の正式な用語は、フィーチャーが線形に従属している、という事。"
  },
  {
    "index": "F18013",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in practice what that really means is one of these problems tripping up the algorithm if you just make you features non-redundant., that should solve the problem of sigma being non-invertable.",
    "output": "だが実践に際しては、それが実際に意味している事は、これらの問題の一つがアルゴリズムをつまづかせていたら、あなたはフィーチャーを冗長では無くすだけで、シグマが非可逆、という問題を解決出来るはずだ。"
  },
  {
    "index": "F18014",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But once again the odds of your running into this at all are pretty low so chances are, you can just apply the multivariate Gaussian model, without having to worry about sigma being non-invertible, so long as m is greater than or equal to n.",
    "output": "だがもう一度繰り返しておくとあなたが生涯でこの問題に遭遇するという事象に対するオッズは極めて低い。だから、おそらくあなたは、mがnより大きい、という事さえ注意しておけば、多変量ガウス分布のモデルをシグマが非可逆である、という事を心配する事無く適用する事が出来るだろう。"
  },
  {
    "index": "F18015",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's it for anomaly detection, with the multivariate Gaussian distribution.",
    "output": "さて、以上が多変量ガウス分布によるアノマリー検出だ。"
  },
  {
    "index": "F18016",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you apply this method you would be able to have an anomaly detection algorithm that automatically captures positive and negative correlations between your different features and flags an anomaly if it sees is unusual combination of the values of the features.",
    "output": "そしてこの手法を適用すれば、あなたのフィーチャー間での正や負の相関を自動で捉えるようなアノマリー検出のアルゴリズムを得る事が出来、そしてフィーチャーの値の組み合わせが異常なのを目撃したら、アノマリーだとフラグを立てる事が出来る。"
  },
  {
    "index": "F18017",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last video, we talked about the recommender systems problem where for example you might have a set of movies and you may have a set of users, each who have rated some subset of the movies.",
    "output": "前回のビデオでは、リコメンダーシステムの問題について議論した。そこでは例えば、映画の集合があって、ユーザーの集合があって、それぞれのユーザーが映画の部分集合をレーティングする。"
  },
  {
    "index": "F18018",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "They've rated the movies one to five stars or zero to five stars.",
    "output": "映画を星一つから星5までとか、または星0から星5までのように。"
  },
  {
    "index": "F18019",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we would like to do is look at these users and predict how they would have rated other movies that they have not yet rated.",
    "output": "そしてやりたい事はこれらのユーザーを見る事で、彼らがまだレーティングしていない映画についてどうレーティングするかを予測したい。"
  },
  {
    "index": "F18020",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I'd like to talk about our first approach to building a recommender system.",
    "output": "このビデオでは、リコメンダーシステムを作り上げる最初のアプローチについて議論する。"
  },
  {
    "index": "F18021",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This approach is called content based recommendations.",
    "output": "このアプローチはコンテントベースのリコメンデーションと言われる物だ。"
  },
  {
    "index": "F18022",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's our data set from before and just to remind you of a bit of notation, I was using nu to denote the number of users and so that's equal to 4, and nm to denote the number of movies, I have 5 movies.",
    "output": "前回から引き続き、これがデータセットで、ちょっと記法を再度説明しておくと、私はnuで、ユーザーの数を表す事にしていた。だからそれはイコール4だ。"
  },
  {
    "index": "F18023",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how do I predict what these missing values would be?",
    "output": "さて、どうやってこの欠けた値を予測出来るか?"
  },
  {
    "index": "F18024",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's suppose that for each of these movies I have a set of features for them.",
    "output": "これらの映画に、それぞれフィーチャーの集合があると仮定しよう。"
  },
  {
    "index": "F18025",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, let's say that for each of the movies have two features which I'm going to denote x1 and x2.",
    "output": "具体的に、今回は各映画に二つのフィーチャーがあるとしよう。それぞれx1とx2で表す事にする。"
  },
  {
    "index": "F18026",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where x1 measures the degree to which a movie is a romantic movie and x2 measures the degree to which a movie is an action movie.",
    "output": "x1はその映画がどの位ロマンティックな映画かの指標として。"
  },
  {
    "index": "F18027",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you take a movie, Love at last, you know it's 0.9 rating on the romance scale. This is a highly romantic movie, but zero on the action scale.",
    "output": "だからもしLoveatlastの映画を選ぶとすると、ロマンスのスケールは0.9レーティングとなっているので、とてもロマンティックな映画だが、アクションのスケールは0なので、映画の中にはほとんどアクションシーンが無い。"
  },
  {
    "index": "F18028",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Romance forever is a 1.0, lot of romance and 0.01 action.",
    "output": "Romanceforeverは1.0で、たくさんのロマンスがあって、アクションは0.01。"
  },
  {
    "index": "F18029",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I don't know, maybe there's a minor car crash in that movie or something.",
    "output": "良く知らんが、映画の中にちょっとだけ車の衝突とかそういうのがあるのかもね。"
  },
  {
    "index": "F18030",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there's a little bit of action.",
    "output": "だからわずかにアクションがあるのみ。"
  },
  {
    "index": "F18031",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Skipping one, let's do Swords vs karate, maybe that has a 0 romance rating and no romance at all in that but plenty of action.",
    "output": "飛ばしてswordsvskarate(ソードvs空手)を見てみると、それはロマンスが0のレーティングとなっているので、ロマンスはまったく無いが、たくさんのアクションがある。"
  },
  {
    "index": "F18032",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And Nonstop car chases, maybe again there's a tiny bit of romance in that movie but mainly action.",
    "output": "そしてnon-stopcarcrashesもほんのちょっとだけロマンスが映画の中にあるみたいだが、ただだいたいはアクションだ。"
  },
  {
    "index": "F18033",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And Cute puppies of love mainly a romance movie with no action at all.",
    "output": "そしてCutepuppiesofloveもだいたいロマンスの映画でアクションは全く無し。"
  },
  {
    "index": "F18034",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if we have features like these, then each movie can be represented with a feature vector.",
    "output": "つまりこれらのようなフィーチャーがあれば、各映画をフィーチャーのベクトルで表現出来る。"
  },
  {
    "index": "F18035",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's take movie one.",
    "output": "映画1を見てみよう。"
  },
  {
    "index": "F18036",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's call these movies 1, 2, 3, 4, and 5.",
    "output": "これらの映画を単に映画1,2,3,4そして5とだけ呼ぶ事にする。"
  },
  {
    "index": "F18037",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But my first movie, Love at last, I have my two features, 0.9 and 0.",
    "output": "最初の映画は、Loveatlastだが、そこには二つのフィーチャー、0.9と0がある。"
  },
  {
    "index": "F18038",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's add an extra feature as usual, which is my interceptor feature x0 = 1.",
    "output": "これらがフィーチャーx1とx2で、そしてさらにいつも通り追加のフィーチャーを足そう、切片項であるフィーチャーx0だ。"
  },
  {
    "index": "F18039",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so putting these together I would then have a feature x1.",
    "output": "以上をあわせると、フィーチャーx1が得られる、ここでこの上付き添字の1は、それが最初の映画のフィーチャーである事を表していて、このフィーチャーベクトルはイコール、1と、、、ここでこの最初の1は切片項だが、そして二つのフィーチャー、0.9と0、となる。"
  },
  {
    "index": "F18040",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The superscript 1 denotes it's the feature vector for my first movie, and this feature vector is equal to 1.",
    "output": "つまり、Loveatlastに関しては、フィーチャーベクトルx1があり、映画、RomanceForeverに関しては別個のフィーチャーベクトルであるx2がある、などなど。"
  },
  {
    "index": "F18041",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for Love at last I would have a feature vector x1, for the movie Romance forever I may have a software feature of vector x2, and so on, and for Swords vs karate I would have a different feature vector x superscript 5.",
    "output": "そしてSwordsvs.karateではまた、別のフィーチャーベクトルであるxに上付き添字5という物が対応する。"
  },
  {
    "index": "F18042",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Also, consistence with our earlier node notation that we were using, we're going to set n to be the number of features not counting this x0 interceptor.",
    "output": "また、以前の記法と一貫させる為にnをフィーチャーの数とする、そしてこれはx0、つまり切片項はカウントしない。"
  },
  {
    "index": "F18043",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So n is equal to 2 because it's we have two features x1 and x2 capturing the degree of romance and the degree of action in each movie.",
    "output": "つまり、n=2だ。何故なら、フィーチャーは二つ、x1とx2で、それが各映画のロマンス度合いとアクション度合いを捕捉しているのだから。"
  },
  {
    "index": "F18044",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now in order to make predictions here's one thing that we do which is that we could treat predicting the ratings of each user as a separate linear regression problem.",
    "output": "ここで、予測を行う為に出来る事として、こんな事が考えられる。それは各ユーザーのレーティングを予測する事を、独立した線形回帰の問題と扱う事だ。"
  },
  {
    "index": "F18045",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So specifically, let's say that for each user j, we're going to learn the parameter vector theta j, which would be an R3 in this case.",
    "output": "具体的には、各ユーザーjに対しパラメータベクトルであるシータjを学習する、これはこの場合、Rの3だ。"
  },
  {
    "index": "F18046",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "More generally, theta (j) would be an R (n+1), where n is the number of features not counting the set term.",
    "output": "より一般的には、シータのjはRのn+1で、ここでnはフィーチャーの数のうち、切片項を考慮に入れない物だ。"
  },
  {
    "index": "F18047",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we're going to predict user j as rating movie i with just the inner product between parameters vectors theta and the features xi.",
    "output": "そしてユーザーjがムービーiをレーティングする事を、単にパラメータベクトルであるシータとフィーチャーx(i)の内積で予測する事とする。"
  },
  {
    "index": "F18048",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's take a specific example.",
    "output": "具体的に例を見てみよう。"
  },
  {
    "index": "F18049",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And associated with Alice would be some parameter vector theta 1.",
    "output": "そしてAliceはなんらかのパラメーターのベクトル、シータ1に関連づけられている。"
  },
  {
    "index": "F18050",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And our second user, Bob, will be associated a different parameter vector theta 2.",
    "output": "そして二番目のユーザー、Bobは別のパラメータベクトル、シータ2に関連づけられている。"
  },
  {
    "index": "F18051",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Carol will be associated with a different parameter vector theta 3 and Dave a different parameter vector theta 4.",
    "output": "Carolはさらに別のパラメータベクトル、シータ3に、そしてDaveも別のパラメータベクトルシータ4に関連づけられている。"
  },
  {
    "index": "F18052",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's say you want to make a prediction for what Alice will think of the movie Cute puppies of love.",
    "output": "つまり、我らは例えばAliceが映画、CutePuppiesofloveをどう思うかなどを予測したい。"
  },
  {
    "index": "F18053",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well that movie is going to have some parameter vector x3 where we have that x3 is going to be equal to 1, which is my intercept term and then 0.99 and then 0.",
    "output": "その映画は何らかのパラメータベクトルx3を持つ事になる。ここでこのx3はイコール、切片項の1と0.99と0と等しい。"
  },
  {
    "index": "F18054",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say, for this example, let's say that we've somehow already gotten a parameter vector theta 1 for Alice.",
    "output": "さらにこの例で、どうにかして既に、Aliceに関するパラメータシータ1を得ているとしよう。"
  },
  {
    "index": "F18055",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But let's just say for now that some unspecified learning algorithm has learned the parameter vector theta 1 and is equal to this 0,5,0.",
    "output": "ここでは今は単に、何らかの学習アルゴリズムでパラメータベクトルのシータ1を学習したとしよう。そしてその結果は0,5,0だったとする。"
  },
  {
    "index": "F18056",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So our prediction for this entry is going to be equal to theta 1, that is Alice's parameter vector, transpose x3, that is the feature vector for the Cute puppies of love movie, number 3.",
    "output": "するとこのエントリの予測はシータ1--これはAliceのパラメータベクトル--これに、x3の転置--これはCutePuppiesofLove、つまり映画3のフィーチャーベクトル--と等しくなる。"
  },
  {
    "index": "F18057",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the inner product between these two vectors is gonna be 5 times 0.99, which is equal to 4.95.",
    "output": "これら二つのベクトルの内積は、5掛ける0.99となる。それはイコール、4.95。"
  },
  {
    "index": "F18058",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so my prediction for this value over here is going to be 4.95.",
    "output": "だからここの値の予測値は4.95となる。それはなかなか良さそうな値に見える。"
  },
  {
    "index": "F18059",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And maybe that seems like a reasonable value if indeed this is my parameter vector theta 1.",
    "output": "これが本当にパラメータベクトルのシータ1ならね。"
  },
  {
    "index": "F18060",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, all we're doing here is we're applying a different copy of this linear regression for each user, and we're saying that what Alice does is Alice has some parameter vector theta 1 that she uses, that we use to predict her ratings as a function of how romantic and how action packed a movie is.",
    "output": "ここまでやってきた事は、ようするに別々の、本質的には線形回帰を、各ユーザーに適用してきた、という事だ。そしてAliceはあるパラメータベクトルシータ1という物を持っている事にして、それを使って、彼女のレーティングをどれだけロマンティックかどれだけアクションか--その映画が--の関数として、予測した。"
  },
  {
    "index": "F18061",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And Bob and Carol and Dave, each of them have a different linear function of the romanticness and actionness, or degree of romance and degree of action in a movie and that that's how we're gonna predict that their star ratings.",
    "output": "そしてBob、Carol、そしてDaveはそれぞれ別の映画に関しての、ロマンティック度合いとアクション度合いに関する別々の線形関数を持つ事になる。そしてそれをもって、彼らの星のレーティングを予測する。"
  },
  {
    "index": "F18062",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "More formally, here's how we can write down the problem.",
    "output": "より正式には、これが我らの問題を書き下した物だ。"
  },
  {
    "index": "F18063",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Our notation is that r(i,j) is equal to 1 if user j has rated movie i and y(i,j) is the rating of that movie, if that rating exists.",
    "output": "我らの記法では、rijはもしユーザーjが映画iをレーティングしていたら1とする。そしてyijはその映画のレーティングがもし存在すればその値とする。"
  },
  {
    "index": "F18064",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, if that user has actually rated that movie.",
    "output": "つまり、そのユーザーが、もし実際にその映画をレーティングしていたら。"
  },
  {
    "index": "F18065",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, on the previous slide we also defined these, theta j, which is a parameter for the user xi, which is a feature vector for a specific movie.",
    "output": "そして前のスライドで、また我らはシータjを定義した。それは各ユーザーのパラメータだった。"
  },
  {
    "index": "F18066",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for each user and each movie, we predict that rating as follows.",
    "output": "そしてxiは個々の映画のフィーチャーベクトルで、そして各ユーザーの各映画ごとにレーティングを、以下のように予測する事が出来る。"
  },
  {
    "index": "F18067",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let me introduce just temporarily introduce one extra bit of notation mj.",
    "output": "一時的にもう一つ、追加の記法mjを導入しよう。"
  },
  {
    "index": "F18068",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're gonna use mj to denote the number of users rated by movie j. We don't need this notation only for this line.",
    "output": "mjを、映画jをレートしたユーザーの数を示すのに用いる事にする。"
  },
  {
    "index": "F18069",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now in order to learn the parameter vector for theta j, well how do we do so.",
    "output": "さて、パラメータベクトルのシータjを学習する為にうーん、どうやったらいいかね?"
  },
  {
    "index": "F18070",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is basically a linear regression problem.",
    "output": "これは基本的には線形回帰の問題だ。"
  },
  {
    "index": "F18071",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what we can do is just choose a parameter vector theta j so that the predicted values here are as close as possible to the values that we observed in our training sets and the values we observed in our data.",
    "output": "だから可能な手段としては、パラメータベクトルのシータjを、予測された値がトレーニングセットで観測されている値にデータとして観測されている値になるべく近くなるように選ぶ、という事だ。"
  },
  {
    "index": "F18072",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's write that down.",
    "output": "それを書き下してみよう。"
  },
  {
    "index": "F18073",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to learn the parameter vector theta j, let's minimize over the parameter vector theta j of sum, and I want to sum over all movies that user j has rated.",
    "output": "パラメータベクトルのシータjを学習する為には、パラメータベクトルのシータjに関して、最小化する事の、和で--ところで和は、ユーザーjがレーティングした映画全てに渡って取る--つまり、和は、以下を満たす全てのiで、そのiとはコロン、rij=1となるi全てだ。"
  },
  {
    "index": "F18074",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the way to read this summation syntax is this is summation over all the values of i, so the r(i.j) is equal to 1.",
    "output": "つまりこの和のインデックスの読み方は、これはrij=1となるような全てのiに渡って和を取る、という事。"
  },
  {
    "index": "F18075",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you'll be summing over all the movies that user j has rated.",
    "output": "つまりこれは、ユーザーjがレーティングした全ての映画に渡って和を取る事になる。"
  },
  {
    "index": "F18076",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then I'm going to compute theta j, transpose x i. So that's the prediction of using j's rating on movie i,- y (i,j).",
    "output": "そして次に、シータj転置xiを計算する、これは、ユーザーjが映画iをどうレーティングするかの予測だ。"
  },
  {
    "index": "F18077",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the actual observed rating squared.",
    "output": "これは実際に観測されたレーティング。そして二乗。"
  },
  {
    "index": "F18078",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then, let me just divide by the number of movies that user j has actually rated.",
    "output": "そして次に、ユーザーjが実際にレーティングした映画の数で割っておく。"
  },
  {
    "index": "F18079",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this is just like the least squares regressions.",
    "output": "以上で、これは単なる最小二乗法の回帰となった。"
  },
  {
    "index": "F18080",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's just like linear regression, where we want to choose the parameter vector theta j to minimize this type of squared error term.",
    "output": "単なる線形回帰だよね。パラメータベクトルのシータjを、この種の二乗誤差項を最小化するように選ぶ、という。"
  },
  {
    "index": "F18081",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you want, you can also add in irregularization terms so plus lambda over 2m and this is really 2mj because we have mj examples.",
    "output": "そしてもしお望みなら、正規化項を足しても良い。つまり足す事の、ラムダ/2m。"
  },
  {
    "index": "F18082",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "User j has rated that many movies, it's not like we have that many data points with which to fit the parameters of theta j.",
    "output": "だってm(j)個の手本があるみたいな物だから。何故ならユーザーjが複数の映画をレーティングしたら、それはパラメータシータjをフィットするデータポイントがそれだけあるみたいな物だから。"
  },
  {
    "index": "F18083",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then let me add in my usual regularization term here of theta j k squared.",
    "output": "そしてここに、通常の正規化項である、シータjkの二乗を足そう。"
  },
  {
    "index": "F18084",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As usual, this sum is from k equals 1 through n, so here, theta j is going to be an n plus 1 dimensional vector, where in our early example n was equal to 2.",
    "output": "いつも通り、この和はk=1からnまでに渡って取る。"
  },
  {
    "index": "F18085",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But more broadly, more generally n is the number of features we have per movie.",
    "output": "だからこのシータjはn+1次元のベクトルとなり、前の例だとnはイコール2だったが、より一般的にはnは各映画の持つフィーチャーの数だ。"
  },
  {
    "index": "F18086",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so as usual we don't regularize over theta 0.",
    "output": "そしていつも通り、シータ0は正規化しない。"
  },
  {
    "index": "F18087",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We don't regularize over the bias terms.",
    "output": "バイアス項は正規化しない。"
  },
  {
    "index": "F18088",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The sum is from k equals 1 through n.",
    "output": "何故なら和はk=1からnに渡ってとっているから。"
  },
  {
    "index": "F18089",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you minimize this as a function of theta j you get a good solution, you get a pretty good estimate of a parameter vector theta j with which to make predictions for user j's movie ratings.",
    "output": "もしこれをシータjの関数として最小化すると、良い解が得られる、パラメータベクトルのシータjについてとても良い推計が得られて、それを用いてユーザーjの映画のレーティングを予測出来る。"
  },
  {
    "index": "F18090",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For recommender systems, I'm gonna change this notation a little bit.",
    "output": "リコメンダーシステムの場合、この記法をちょっと変更する。"
  },
  {
    "index": "F18091",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So to simplify the subsequent math, I with to get rid of this term mj.",
    "output": "以降の計算を簡略化する為にこのmj項を取り除く。"
  },
  {
    "index": "F18092",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's just a constant, right?",
    "output": "これは結局の所、単なる定数だ。でしょ?"
  },
  {
    "index": "F18093",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I can delete it without changing the value of theta j that I get out of this optimization.",
    "output": "だからシータjの値に影響を与えずにこれを削除出来る。"
  },
  {
    "index": "F18094",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you imagine taking this whole equation, taking this whole expression and multiplying it by mj, get rid of that constant.",
    "output": "想像してみよう、この式全体を持ってきて、この式全体にmjを掛けてみる、するとその定数は取り除かれる、そしてそれを最小化した時、得られるシータjは掛ける前と一緒のはずだ。"
  },
  {
    "index": "F18095",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So just to repeat what we wrote on the previous slide, here's our optimization objective. In order to learn theta j which is the parameter for user j, we're going to minimize over theta j of this optimization objectives.",
    "output": "だから前のスライドに書いた事を繰り返すと、これが最適化の目的関数だ:シータjを学習する為に、シータjはユーザーjのパラメータだが、そのシータjを学習する為に、この最適化も目的関数をシータjに関して最小化する。"
  },
  {
    "index": "F18096",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is our usual squared error term and then this is our regularizations term.",
    "output": "つまりこれは、通常の二乗誤差項で、これが正規化項だ。"
  },
  {
    "index": "F18097",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now of course in building a recommender system, we don't just want to learn parameters for a single user.",
    "output": "さて、もちろんリコメンダーシステムを作るときには、一人のユーザーだけのパラメータを学習させたい、という事は無い。"
  },
  {
    "index": "F18098",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We want to learn parameters for all of our users.",
    "output": "我らは全てのユーザーのパラメータを学習させたい。"
  },
  {
    "index": "F18099",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I have n subscript u users, so I want to learn all of these parameters.",
    "output": "n下付き添字uのユーザーが居るのだった。だからこれらのパラメータ全てを学習させたい。"
  },
  {
    "index": "F18100",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, what I'm going to do is take this optimization objective and just add the mixture summation there.",
    "output": "だから我らがやるのは、この最適化の目的関数に対して、単純に追加のシグマを足すだけ。"
  },
  {
    "index": "F18101",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Except that now instead of just doing this for a specific user theta j, I'm going to sum my objective over all of my users and then minimize this overall optimization objective, minimize this overall cost on.",
    "output": "つまりこの、ここの式は先頭にはまた1/2があるが、これは上にあるのと、完全に一致する、ただし特定の一ユーザーのシータjについてだけ、これを行う代わりに、ユーザー全員に渡って目的関数を足して、そしてこの最適化の目的関数全体を最小化する。このコスト関数全体を最小化する。"
  },
  {
    "index": "F18102",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And when I minimize this as a function of theta 1, theta 2, up to theta nu, I will get a separate parameter vector for each user.",
    "output": "そしてこれをシータ1,シータ2、、、、とシータnuまでの関数として最小化すると、各ユーザーごとに別々のパラメータベクトルが得られ、そしてそれを用いてユーザー全員の、全てのn下付き添字uのユーザーの予測を行う事が出来る。"
  },
  {
    "index": "F18103",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And to give this thing a name, I'll just call this J(theta1, ..., theta nu).",
    "output": "では全てをつなげると、この上にあるのが、最適化の目的関数だった、これに名前をつけよう、Jのシータ1,点点点シータnu。"
  },
  {
    "index": "F18104",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So j as usual is my optimization objective, which I'm trying to minimize.",
    "output": "つまりJはいつもどおりの最適化の目的関数で、最小化しようとしている対象。"
  },
  {
    "index": "F18105",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, in order to actually do the minimization, if you were to derive the gradient descent update, these are the equations that you would get.",
    "output": "次に、実際に最小化を実行する為に、最急降下法のアップデートルールを導出すると、これらの等式が得られる。"
  },
  {
    "index": "F18106",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you take theta j, k, and subtract from an alpha, which is the learning rate, times these terms over here on the right.",
    "output": "つまりシータjのkをとってそこから引く事のアルファ、これはラーニングレートで、掛ける事のこれらの右にある項だ。"
  },
  {
    "index": "F18107",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there's slightly different cases when k equals 0 and when k does not equal 0.",
    "output": "k=0の場合とkが0で無い場合でちょっとだけ異なる場合分けが要る。"
  },
  {
    "index": "F18108",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because our regularization term here regularizes only the values of theta jk for k not equal to 0, so we don't regularize theta 0, so with slightly different updates when k equals 0 and k is not equal to 0.",
    "output": "何故なら我らの正規化項はkが0で無い時のシータjのkにだけ存在しているからだ。つまりシータ0を正規化しないので、k=0とkが0以外とでちょっとだけ異なった更新の仕方をする。"
  },
  {
    "index": "F18109",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this term over here, for example, is just the partial derivative with respect to your parameter, that of your optimization objective.",
    "output": "そして、例えばこの項は最適化の目的関数のパラメータに関する単なる偏微分に過ぎない。でしょ?"
  },
  {
    "index": "F18110",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right and so this is just gradient descent and I've already computed the derivatives and plugged them into here.",
    "output": "つまり、これは単なる最急降下法で、すでに微分は計算済みで、それをここに代入しただけ。"
  },
  {
    "index": "F18111",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if this gradient descent update look a lot like what we have here for linear regression. That's because these are essentially the same as linear regression.",
    "output": "もしこれらの最急降下法のアップデートが、線形回帰のそれととても似てると思ったなら、それはこれらが本質的には線形回帰と同じだからだろう。"
  },
  {
    "index": "F18112",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The only minor difference is that for linear regression we have these 1 over m terms, this really would've been 1 over mj.",
    "output": "唯一の小さな違いとしては、線形回帰なら、これらの1/mの項があった。"
  },
  {
    "index": "F18113",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But because earlier when we are deriving the optimization objective, we got rid of this, that's why we don't have this 1 over m term.",
    "output": "それは実際は1/m(j)だが、でも、前に最適化の目的関数を導出した所で、これを取り除いていた。だからこの1/mの項が無くなっている。"
  },
  {
    "index": "F18114",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But otherwise, it's really some of my training examples of the ever times xk plus that regularization term, plus that term of regularization contributes to the derivative.",
    "output": "だがそれ以外は、ほんとうにトレーニング手本に渡って和をとる事の誤差掛けるxkに、足す事の正規化項の微分への寄与だ。"
  },
  {
    "index": "F18115",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if you're using gradient descent here's how you can minimize the cost function j to learn all the parameters.",
    "output": "だから最急降下法を使うとするなら、これがコスト関数Jを最小化するやり方、つまりパラメータを全て学習する方法だ。"
  },
  {
    "index": "F18116",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And using these formulas for the derivative if you want, you can also plug them into a more advanced optimization algorithm, like conjugate gradient or LBFGS or what have you.",
    "output": "そしてこれらの微分の式を使ってもしお望みなら、よりアドバンスドな最適化アルゴリズムであるクラスターグラディエントとかLBFGSとかその他なんでもお持ちのアルゴリズムに代入して、同じようにコスト関数Jを最小化する事も出来る。"
  },
  {
    "index": "F18117",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And use that to try to minimize the cost function j as well. So hopefully you now know how you can apply essentially a deviation on linear regression in order to predict different movie ratings by different users.",
    "output": "以上で、本質的には線形回帰の一種の何かを使って別々のユーザーの別々の映画へのレーティングを予測するやり方が分かったかな。"
  },
  {
    "index": "F18118",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This particular algorithm is called a content based recommendations, or a content based approach, because we assume that we have available to us features for the different movies.",
    "output": "このアルゴリズムはコンテントベースのリコメンデーション、またはコンテントベースのアプローチと呼ばれている。何故なら個々の映画のフィーチャーが使用可能だと仮定しているからだ。"
  },
  {
    "index": "F18119",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so where features that capture what is the content of these movies, of how romantic is this movie, how much action is in this movie.",
    "output": "つまり我らは、これらの映画のコンテンツ(内容)がなんなのか、という事を捕捉するフィーチャーがある、という事だ。この映画はどれくらいロマンティックか?"
  },
  {
    "index": "F18120",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we're really using features of a content of the movies to make our predictions.",
    "output": "そして映画のコンテンツに関するフィーチャーを実際に用いて予測を行っている。"
  },
  {
    "index": "F18121",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But for many movies, we don't actually have such features.",
    "output": "でも実際には、多くの映画について、そんなフィーチャーなんて持ってない。"
  },
  {
    "index": "F18122",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or maybe very difficult to get such features for all of our movies, for all of whatever items we're trying to sell.",
    "output": "またはそんなフィーチャーを全部の映画について取得するのはとても難しい。売ろうとしている商品ならなんでも全てに渡って取るなんて。"
  },
  {
    "index": "F18123",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, in the next video, we'll start to talk about an approach to recommender systems that isn't content based and does not assume that we have someone else giving us all of these features for all of the movies in our data set.",
    "output": "だから次のビデオでは、コンテントベースじゃないリコメンダーシステムのアプローチを議論する。それはつまり、他の誰かがこれらのフィーチャー全てを、我らのデータセットの映画全てに与えてくれている、と仮定しない物だ。"
  },
  {
    "index": "F18124",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video we'll talk about an approach to building a recommender system that's called collaborative filtering.",
    "output": "このビデオでは、協調フィルタリングと呼ばれるリコメンダーシステム構築のアプローチを議論する。"
  },
  {
    "index": "F18125",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The algorithm that we're talking about has a very interesting property that it does what is called feature learning and by that I mean that this will be an algorithm that can start to learn for itself what features to use.",
    "output": "これから議論するアルゴリズムはとても興味深い性質を持っていて、それはフィーチャーラーニングと呼ばれている。それの意味する所は、これはアルゴリズムが、なんのフィーチャーを使うかを自分自身で学習しはじめる事を意味する。"
  },
  {
    "index": "F18126",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here was the data set that we had and we had assumed that for each movie, someone had come and told us how romantic that movie was and how much action there was in that movie.",
    "output": "こんなデータセットがあるとしよう。そして各映画ごとに、誰かがやってきてその映画がどのくらいロマンティックか、どれだけのアクションが入っているかを教えてくれるとしよう。"
  },
  {
    "index": "F18127",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But as you can imagine it can be very difficult and time consuming and expensive to actually try to get someone to, you know, watch each movie and tell you how romantic each movie and how action packed is each movie, and often you'll want even more features than just these two.",
    "output": "でも、想像出来るように、実際に誰かが個々の映画を見ていって、それらがどれくらいロマンティックか、どれだけのアクションが詰め込まれているかをいちいち教えてもらうのはとても難しく、とても時間がかかり、とても高く付く場合がありうる。しかもしばしば、フィーチャーはたった2つよりもずっと多く必要となる。"
  },
  {
    "index": "F18128",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So where do you get these features from?",
    "output": "ではこれらのフィーチャーはどこから得たらいいか?"
  },
  {
    "index": "F18129",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's change the problem a bit and suppose that we have a data set where we do not know the values of these features.",
    "output": "そこでちょっと問題を変更してこれらのフィーチャーを知らないデータセットを持っていたとしよう。"
  },
  {
    "index": "F18130",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we're given the data set of movies and of how the users rated them, but we have no idea how romantic each movie is and we have no idea how action packed each movie is so I've replaced all of these things with question marks.",
    "output": "つまり我らは、映画の集合とそれらをユーザーがどうレートしたのかのデータを与えられたとする。だが個々の映画がどのくらいロマンティックか個々の映画にどのくらいアクションが詰まってるのかはまったく分からないとする、つまりこれらをはてなマークで置き換えた。"
  },
  {
    "index": "F18131",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But now let's make a slightly different assumption.",
    "output": "だがここで、ちょっとだけ別の仮定をおこう。"
  },
  {
    "index": "F18132",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we've gone to each of our users, and each of our users has told has told us how much they like the romantic movies and how much they like action packed movies.",
    "output": "ユーザー一人一人の所におもむく事が出来て、各ユーザーが我らに、彼らがどれくらいロマンティックな映画が好きか、どれくらいアクションの映画が好きか、教えてくれるとしよう。"
  },
  {
    "index": "F18133",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So Alice has associated a current of theta 1.",
    "output": "Aliceはシータ1に関連付けられているとしよう。"
  },
  {
    "index": "F18134",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Bob theta 2.",
    "output": "Bobはシータ2に。"
  },
  {
    "index": "F18135",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Carol theta 3.",
    "output": "Carolはシータ3に。"
  },
  {
    "index": "F18136",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Dave theta 4.",
    "output": "Daveはシータ4に。"
  },
  {
    "index": "F18137",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say we also use this and that Alice tells us that she really likes romantic movies and so there's a five there which is the multiplier associated with X1 and lets say that Alice tells us she really doesn't like action movies and so there's a 0 there.",
    "output": "そしてこれを使う事としよう、Aliceは我らに彼女はロマンティック映画をとても好きだ、と教えてくれて、だからそこには5を、そこはx1の係数に対応する、そしてまたAliceはアクション映画をまったく好きでは無い、とも教えてくれた。"
  },
  {
    "index": "F18138",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And Bob tells us something similar so we have theta 2 over here.",
    "output": "だからそこは0とする。そしてBobも同じような事を言ったとする、だからシータ2はこんな風になる。"
  },
  {
    "index": "F18139",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas Carol tells us that she really likes action movies which is why there's a 5 there, that's the multiplier associated with X2, and remember there's also X0 equals 1 and let's say that Carol tells us she doesn't like romantic movies and so on, similarly for Dave.",
    "output": "そしてまたx0も存在しててそれはイコール1なのも思い出そう。そしてCarolは我らにロマンティックな映画は嫌いだ、とも、教えてくれたとしよう。"
  },
  {
    "index": "F18140",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's assume that somehow we can go to users and each user J just tells us what is the value of theta J for them.",
    "output": "つまり、どうにかして我らが各ユーザーの元におもむき、各ユーザーjは我らにシータjの値を教えてくれたと想定してみよう。"
  },
  {
    "index": "F18141",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so basically specifies to us of how much they like different types of movies.",
    "output": "つまり、基本的には彼らが別々の種類の映画をどのくらい好きかを表明してくれるという事だ。"
  },
  {
    "index": "F18142",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we can get these parameters theta from our users then it turns out that it becomes possible to try to infer what are the values of x1 and x2 for each movie.",
    "output": "もし我らはこれらのパラメータ、シータをユーザーから得る事が出来れば、各映画のx1とx2の値が幾つかを推測出来る、という事が判明する。"
  },
  {
    "index": "F18143",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at an example.",
    "output": "例を見てみよう。"
  },
  {
    "index": "F18144",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's look at movie 1.",
    "output": "映画1を見てみよう。"
  },
  {
    "index": "F18145",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that movie 1 has associated with it a feature vector x1.",
    "output": "映画1はフィーチャーベクトルx1に関連づけられているのだった。"
  },
  {
    "index": "F18146",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you know this movie is called Love at last but let's ignore that.",
    "output": "そしてこの映画がLoveatlastと呼ばれていたのも知ってるだろうが、ここではそれは無視しよう。"
  },
  {
    "index": "F18147",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's pretend we don't know what this movie is about, so let's ignore the title of this movie.",
    "output": "この映画がなんなのか、知らないフリをしよう、だからこの映画のタイトルを無視しよう。"
  },
  {
    "index": "F18148",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "All we know is that Alice loved this move.",
    "output": "Aliceがこの映画を好きだ、というのが我らの知っている全てだ。"
  },
  {
    "index": "F18149",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Bob loved this movie.",
    "output": "Bobはこの映画が好きだ。"
  },
  {
    "index": "F18150",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Carol and Dave hated this movie.",
    "output": "CarolとDaveはこの映画を嫌ってる。"
  },
  {
    "index": "F18151",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what can we infer?",
    "output": "ではそこから、何が推測出来るだろうか?"
  },
  {
    "index": "F18152",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, we know from the feature vectors that Alice and Bob love romantic movies because they told us that there's a 5 here.",
    "output": "フィーチャーベクトルから我らはAliceとBobはロマンティックな映画が好きだという事を知っている。だって彼らはここは5だと告げてたのだから。"
  },
  {
    "index": "F18153",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas Carol and Dave, we know that they hate romantic movies and that they love action movies.",
    "output": "他方、CarolとDaveは彼らはロマンティックな映画を嫌ってる事を我らは知っている。そして彼らはアクション映画を好んでいる。"
  },
  {
    "index": "F18154",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so based on the fact that movie 1 is loved by Alice and Bob and hated by Carol and Dave, we might reasonably conclude that this is probably a romantic movie, it is probably not much of an action movie.",
    "output": "つまりそれらがユーザー3と4、つまりCarolとDaveが我らにくれたパラメーターベクトルなので、そして映画1はAliceとBobに好まれていて、CarolとDaveに嫌われているという事実に基づくと、我らはこれがロマンティックな映画だと合理的に結論出来る。それはたぶん、そんなにアクション映画では無かろう。"
  },
  {
    "index": "F18155",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "this example is a little bit mathematically simplified but what we're really asking is what feature vector should X1 be so that theta 1 transpose x1 is approximately equal to 5, that's Alice's rating, and theta 2 transpose x1 is also approximately equal to 5, and theta 3 transpose x1 is approximately equal to 0, so this would be Carol's rating, and theta 4 transpose X1 is approximately equal to 0.",
    "output": "この例はちょっと計算的に単純化してあるが、我らが真に問うている事はシータ1転置x1がだいたい5となるようなx1とはなんだろう?"
  },
  {
    "index": "F18156",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And from this it looks like, you know, X1 equals one that's the intercept term, and then 1.0, 0.0, that makes sense given what we know of Alice, Bob, Carol, and Dave's preferences for movies and the way they rated this movie.",
    "output": "そしてこの事からx1はイコール、まず切片項の1に続いて1.00.0みたいになる。それは既知のAlice、Bob、Carol、Daveの映画に関する嗜好から、そして彼らがこの映画をどう評価するかの知識から、筋が通ってるように思う。"
  },
  {
    "index": "F18157",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so more generally, we can go down this list and try to figure out what might be reasonable features for these other movies as well.",
    "output": "より一般的にはこのリストを降りていって、これらの他の映画のフィーチャーもどんなだったらリーズナブルか見つけようと試みる事が出来る。"
  },
  {
    "index": "F18158",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's formalize this problem of learning the features XI.",
    "output": "このxiを学習するという問題を定式化しよう。"
  },
  {
    "index": "F18159",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that our users have given us their preferences.",
    "output": "ユーザーが我らに自分の嗜好を教えてくれるとする。"
  },
  {
    "index": "F18160",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's say that our users have come and, you know, told us these values for theta 1 through theta of NU and we want to learn the feature vector XI for movie number I.",
    "output": "つまり我らのユーザーが来てくれて、シータ1からシータnuまでの値を教えてくれるとしよう。そして映画iに関するフィーチャーベクトルxiを学習したい。"
  },
  {
    "index": "F18161",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we can do is therefore pose the following optimization problem.",
    "output": "だから我らがやる事は以下のような最適化問題を解く事だ。"
  },
  {
    "index": "F18162",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we want to sum over all the indices J for which we have a rating for movie I because we're trying to learn the features for movie I that is this feature vector XI.",
    "output": "その為、映画iをレーティングしたユーザー全員にわたってインデックスjに関する和を取りたい。何故なら我らが学習したいのは映画iのフィーチャーだからで、それはフィーチャーベクトルxiだ。"
  },
  {
    "index": "F18163",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So and then what we want to do is minimize this squared error, so we want to choose features XI, so that, you know, the predictive value of how user J rates movie I will be similar, will be not too far in the squared error sense of the actual value YIJ that we actually observe in the rating of user j on movie I.",
    "output": "だから、そこでやりたい事はこの二乗誤差を最小化したい、つまりフィーチャーxiをユーザーjが映画iをどうレーティングするかの予測値が二乗誤差の意味で実際の値yij、つまりユーザーjの映画iに対する実際のレーティングの観測値に近い、そんなに離れていないように、フィーチャーxiを選びたい。"
  },
  {
    "index": "F18164",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just to summarize what this term does is it tries to choose features XI so that for all the users J that have rated that movie, the algorithm also predicts a value for how that user would have rated that movie that is not too far, in the squared error sense, from the actual value that the user had rated that movie.",
    "output": "ではまとめの為に、この項がしている事は、フィーチャーxiを以下の条件を満たすように選ぶ。その条件とは、その映画をレーティングしている全てのユーザーjに対して、アルゴリズムが予測するそのユーザーのその映画のレーティングが、実際の値とそんなに離れていないようにという条件だ。"
  },
  {
    "index": "F18165",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the squared error term.",
    "output": "これが二乗誤差の項だ。"
  },
  {
    "index": "F18166",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As usual, we can also add this sort of regularization term to prevent the features from becoming too big.",
    "output": "いつも通り、この種の正規化項を足して、フィーチャーが大きくなりすぎるのを防止する事も出来る。"
  },
  {
    "index": "F18167",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is how we would learn the features for one specific movie but what we want to do is learn all the features for all the movies and so what I'm going to do is add this extra summation here so I'm going to sum over all Nm movies, N subscript m movies, and minimize this objective on top that sums of all movies.",
    "output": "さて、以上が一つの特定の映画のフィーチャーを学習する方法だが、我らがやりたいのは、全ての映画の全てのフィーチャーを学習する事だ。"
  },
  {
    "index": "F18168",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you do that, you end up with the following optimization problem. And if you minimize this, you have hopefully a reasonable set of features for all of your movies.",
    "output": "だからやるべき事は、この追加のシグマをここに追加して、nm個の映画全てに渡って和を取る、n下付き添字m個の映画について、そしてこの目的関数を最小化する、全ての映画についての和の。"
  },
  {
    "index": "F18169",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So putting everything together, what we, the algorithm we talked about in the previous video and the algorithm that we just talked about in this video.",
    "output": "それを行うと、結局以下のような最適化の問題となる。"
  },
  {
    "index": "F18170",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, what we showed was that you know, if you have a set of movie ratings, so if you have the data the rij's and then you have the yij's that will be the movie ratings.",
    "output": "これを最小化すると、全ての映画についての、リーズナブルなフィーチャーの集合が得られる事が期待出来る。"
  },
  {
    "index": "F18171",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then given features for your different movies we can learn these parameters theta.",
    "output": "全てをあわせると、我らが前のビデオで議論してきたアルゴリズムと、このビデオで今議論してきたアルゴリズムは前のビデオで見てきたのはえーと、映画のレーティングの集合があったとすると、つまりデータrijとyij、それは映画のレーティングだが、それがあったとすると、別々の映画に対してのフィーチャーが所与であれば、これらのパラメータ、シータが学習出来る、という事だった。"
  },
  {
    "index": "F18172",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you knew the features, you can learn the parameters theta for your different users.",
    "output": "つまり、もしフィーチャーを知ってたら、別々のユーザーのパラメータ、シータを学習する事が出来る。"
  },
  {
    "index": "F18173",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we showed earlier in this video is that if your users are willing to give you parameters, then you can estimate features for the different movies.",
    "output": "そしてこのビデオの前半で見てきたように、もしユーザーがあなたに喜んでパラメータを提供してくれれば、別々の映画のフィーチャーを推計する事が出来る。"
  },
  {
    "index": "F18174",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is kind of a chicken and egg problem.",
    "output": "つまりこれは、鶏と卵の問題だ。"
  },
  {
    "index": "F18175",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Which comes first?",
    "output": "どっちが先に来る?"
  },
  {
    "index": "F18176",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, do we want if we can get the thetas, we can know the Xs.",
    "output": "シータが得られれば、xを知る事が出来る。"
  },
  {
    "index": "F18177",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we have the Xs, we can learn the thetas.",
    "output": "xが分かっていれば、シータを学習出来る。"
  },
  {
    "index": "F18178",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you can do is, and then this actually works, what you can do is in fact randomly guess some value of the thetas.",
    "output": "そこでとりうる手段としては、そしてこれは実際にうまく行くのだが、それは、出来る事は、実際にランダムにシータの何らかの値を推測してしまう、という事だ。"
  },
  {
    "index": "F18179",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now based on your initial random guess for the thetas, you can then go ahead and use the procedure that we just talked about in order to learn features for your different movies.",
    "output": "この最初のランダムなシータの推測に基づいて、前進する事が出来て、別々の映画のフィーチャーを学習する為にここまで話してきた手順が使える事になる。"
  },
  {
    "index": "F18180",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now given some initial set of features for your movies you can then use this first method that we talked about in the previous video to try to get an even better estimate for your parameters theta.",
    "output": "今、何らかのフィーチャーの初期の値が映画に対して与えられたとき、この前回のビデオで話した最初の手法を用いてパラメータシータの推計を改善する事が出来る。"
  },
  {
    "index": "F18181",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now that you have a better setting of the parameters theta for your users, we can use that to maybe even get a better set of features and so on.",
    "output": "いまやユーザーのシータの改善された値を得たので、それを用いてさらにフィーチャーを改善した物が得られるかもしれない、などなど。"
  },
  {
    "index": "F18182",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can sort of keep iterating, going back and forth and optimizing theta, x theta, x theta, nd this actually works and if you do this, this will actually cause your album to converge to a reasonable set of features for you movies and a reasonable set of parameters for your different users.",
    "output": "これは実際に機能する。これを行うと、これは実際に映画のフィーチャーとユーザーごとに異なったパラメータのリーズナブルな組に収束する。"
  },
  {
    "index": "F18183",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is a basic collaborative filtering algorithm.",
    "output": "以上が基本的な協調的フィルタリングのアルゴリズムだ。"
  },
  {
    "index": "F18184",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This isn't actually the final algorithm that we're going to use.",
    "output": "これは実際に使う、最終的なアルゴリズムでは無い。"
  },
  {
    "index": "F18185",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next video we are going to be able to improve on this algorithm and make it quite a bit more computationally efficient.",
    "output": "次のビデオで、このアルゴリズムを改善する事が出来る。それでもっとずっと計算量的に効率的になる。"
  },
  {
    "index": "F18186",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, hopefully this gives you a sense of how you can formulate a problem where you can simultaneously learn the parameters and simultaneously learn the features from the different movies.",
    "output": "でもこれで、別々の映画から、パラメータとフィーチャーを同時に学習する、という問題をどう定式化するかがなんとなく分かったんじゃないかな。"
  },
  {
    "index": "F18187",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for this problem, for the recommender system problem, this is possible only because each user rates multiple movies and hopefully each movie is rated by multiple users.",
    "output": "そしてこの問題に関して言うと、リコメンダーシステムの問題に関して言うと、これが可能なのは、各ユーザーが複数の映画をレーティングしていて、さらに出来たら全ての映画が複数のユーザーにレーティングされてて初めて可能となる。"
  },
  {
    "index": "F18188",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so you can do this back and forth process to estimate theta and x.",
    "output": "つまり、この行ったり来たりしてシータとxを推計する手順が使用出来る。"
  },
  {
    "index": "F18189",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So to summarize, in this video we've seen an initial collaborative filtering algorithm.",
    "output": "ではまとめよう。このビデオでは、最初の協調的フィルタリングのアルゴリズムを見てきた。"
  },
  {
    "index": "F18190",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The term collaborative filtering refers to the observation that when you run this algorithm with a large set of users, what all of these users are effectively doing are sort of collaboratively--or collaborating to get better movie ratings for everyone because with every user rating some subset with the movies, every user is helping the algorithm a little bit to learn better features, and then by helping-- by rating a few movies myself, I will be helping the system learn better features and then these features can be used by the system to make better movie predictions for everyone else.",
    "output": "何故なら各ユーザーが映画のサブセットをレーティングする、という行動でもってアルゴリズムがちょっとだけ改善されたフィーチャーを得るのを各ユーザーが助けているからだ。そして助ける事で--2,3の映画を自分でレーティングする事で、私はシステムがより良いフィーチャーを学習するのを助けている事になり、そしてこれらのフィーチャーをシステムがその他全員に対して映画のよりよい予測を行う為に使う事が出来る。"
  },
  {
    "index": "F18191",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so there is a sense of collaboration where every user is helping the system learn better features for the common good.",
    "output": "つまりある種の協調作業がある訳だ:各ユーザーが、公共の利益の為に、システムがより良いフィーチャーを学習するのを手助けする。"
  },
  {
    "index": "F18192",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is this collaborative filtering.",
    "output": "これが協調的フィルタリングだ。"
  },
  {
    "index": "F18193",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, in the next video what we going to do is take the ideas that have worked out, and try to develop a better an even better algorithm, a slightly better technique for collaborative filtering.",
    "output": "そして次のビデオでやる事は、ここまでやってきたアイデアを用いて、より良いアルゴリズムの構築を試みる。協調フィルタリングにとってもうちょっとだけ良い方法を。"
  },
  {
    "index": "F18194",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last couple videos, we talked about the ideas of how, first, if you're given features for movies, you can use that to learn parameters data for users.",
    "output": "前回までのビデオでまずは映画のフィーチャーが与えられた時それを用いてユーザーのパラメータのデータを学習する事が出来る、という話をした。"
  },
  {
    "index": "F18195",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And second, if you're given parameters for the users, you can use that to learn features for the movies.",
    "output": "次に、ユーザーのパラメータを与えられたら、それを使って映画のフィーチャーを学習する事が出来る、という話をした。"
  },
  {
    "index": "F18196",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video we're going to take those ideas and put them together to come up with a collaborative filtering algorithm.",
    "output": "このビデオでは、これら二つのアイデアを用いて、それらを合わせて協調フィルタのアルゴリズムにたどり着く。"
  },
  {
    "index": "F18197",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So one of the things we worked out earlier is that if you have features for the movies then you can solve this minimization problem to find the parameters theta for your users.",
    "output": "つまり以前やった事の一つには映画のフィーチャーがあれば、最小化問題を解く事でユーザーのパラメータであるシータを見つける事が出来た。"
  },
  {
    "index": "F18198",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we also worked that out, if you are given the parameters theta, you can also use that to estimate the features x, and you can do that by solving this minimization problem.",
    "output": "そしてその次に、パラメータであるシータがあれば、それを用いてフィーチャーxを推計する事が出来るのだった。それは最小化問題を解く事で出来るのだった。"
  },
  {
    "index": "F18199",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So one thing you could do is actually go back and forth.",
    "output": "だから取りうる手段として一つ考えられるのは行ったり来たりして実行する事だ。"
  },
  {
    "index": "F18200",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe randomly initialize the parameters and then solve for theta, solve for x, solve for theta, solve for x.",
    "output": "ランダムに初期化されたパラメータで、シータについて解き、xについて解き、シータについて解き、xについて解く。"
  },
  {
    "index": "F18201",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, it turns out that there is a more efficient algorithm that doesn't need to go back and forth between the x's and the thetas, but that can solve for theta and x simultaneously.",
    "output": "だが、シータとxを行ったり来たりしなくても良いもっと効率的なアルゴリズムがある事が知られている、それは行ったり来たりする代わりにシータとxを同時に解く事が出来る。"
  },
  {
    "index": "F18202",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we are going to do, is basically take both of these optimization objectives, and put them into the same objective.",
    "output": "我らがやる事は、基本的にはこれら二つの最適化の目的関数をとり、それを一つの目的関数に突っ込む、という事だ。"
  },
  {
    "index": "F18203",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I'm going to define the new optimization objective j, which is a cost function, that is a function of my features x and a function of my parameters theta.",
    "output": "つまり私は、新しい最適化の目的関数Jを定義する、それはフィーチャーxとパラメータのシータに関する関数としての、コスト関数だ。"
  },
  {
    "index": "F18204",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, it's basically the two optimization objectives I had on top, but I put together.",
    "output": "そして基本的には上の所に二つの最適化の目的関数があったが、それを一つにくっつけた。"
  },
  {
    "index": "F18205",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in order to explain this, first, I want to point out that this term over here, this squared error term, is the same as this squared error term and the summations look a little bit different, but let's see what the summations are really doing.",
    "output": "これを説明する為に、まず以下の二つの項が同じという事を指摘したい:この項、この二乗誤差の項と、この二乗誤差の項、これ。和の取り方がちょっと違って見えるが、この和が実際に何をやっているかを見てみよう。"
  },
  {
    "index": "F18206",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first summation is sum over all users J and then sum over all movies rated by that user.",
    "output": "最初の和は、全てのユーザーjに渡ってとっている、そして次にそのユーザーによってレーティングされた全ての映画について和を取る。"
  },
  {
    "index": "F18207",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is really summing over all pairs IJ, that correspond to a movie that was rated by a user.",
    "output": "つまり、これは実際は、ユーザーにレーティングされた全ての映画に対応したiとjのペアに渡って和を取っている。"
  },
  {
    "index": "F18208",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Sum over J says, for every user, the sum of all the movies rated by that user.",
    "output": "jに渡って取る和は、各ユーザに対して、と言っていてそしてそのユーザーがレーティングした全ての映画に渡って和を取る。"
  },
  {
    "index": "F18209",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This summation down here, just does things in the opposite order.",
    "output": "この下の和は、それを反対の順番でやるだけだ。"
  },
  {
    "index": "F18210",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This says for every movie I, sum over all the users J that have rated that movie and so, you know these summations, both of these are just summations over all pairs ij for which r of i J is equal to 1.",
    "output": "これが言っているのは、各映画iに対してその映画をレーティングした全てのユーザーjに渡って和をとる、つまり、これらの和は、これらは両方とも、rijがイコール1な全てのijのペアに渡って和を取るだけ。"
  },
  {
    "index": "F18211",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's just something over all the user movie pairs for which you have a rating.",
    "output": "それはようするに、レーティングを持っている全てのユーザーと映画の組に渡って和を取る。"
  },
  {
    "index": "F18212",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "and so those two terms up there is just exactly this first term, and I've just written the summation here explicitly, where I'm just saying the sum of all pairs IJ, such that RIJ is equal to 1.",
    "output": "つまりこれら二つの項はこの最初の項と完全に一致している。そして私は和を明示的に書いた、それはようするに、rijがイコール1となる全てのijのペアに渡って和を取る、と言っている。"
  },
  {
    "index": "F18213",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what we're going to do is define a combined optimization objective that we want to minimize in order to solve simultaneously for x and theta.",
    "output": "我らがやりたい事はxとシータを同時に解く為に最小化をする対象となる、最適化の複合目的関数を定義するという事だ。"
  },
  {
    "index": "F18214",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then the other terms in the optimization objective are this, which is a regularization in terms of theta.",
    "output": "そして最適化の目的関数に残ってる他の項はこれだ。これはシータの正規化項だ。"
  },
  {
    "index": "F18215",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that came down here and the final piece is this term which is my optimization objective for the x's and that became this.",
    "output": "それはここに来る。そして最後のピースとなるのはこの項で、これはxの為の最適化の目的関数の中に残ってる物でそれはここに来る。"
  },
  {
    "index": "F18216",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this optimization objective j actually has an interesting property that if you were to hold the x's constant and just minimize with respect to the thetas then you'd be solving exactly this problem, whereas if you were to do the opposite, if you were to hold the thetas constant, and minimize j only with respect to the x's, then it becomes equivalent to this.",
    "output": "そしてこの最適化の目的関数Jは興味深い性質を持っている、それはxを定数に固定してみて、シータに関して最小化してみると、するとまさにこの問題を解いている事になる。"
  },
  {
    "index": "F18217",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because either this term or this term is constant if you're minimizing only the respective x's or only respective thetas.",
    "output": "一方、その反対をやると、シータを仮に定数で固定してみて、Jをxに関してだけ最小化してみると、この項か、またはこの項がxに関してかシータに関してのどちらかに関してだけ最小化するなら、定数になるから。"
  },
  {
    "index": "F18218",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's an optimization objective that puts together my cost functions in terms of x and in terms of theta.",
    "output": "だから、これが、xに関するコスト関数とシータに関するコスト関数をくっつけた最適化の目的関数だ。"
  },
  {
    "index": "F18219",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in order to come up with just one optimization problem, what we're going to do, is treat this cost function, as a function of my features x and of my user pro user parameters data and just minimize this whole thing, as a function of both the Xs and a function of the thetas.",
    "output": "そして一つの最適化問題に帰着される為には、我らがやるべき事は、このコスト関数をフィーチャーxとユーザーのパラメータシータの両方に関する関数として扱う事だ。"
  },
  {
    "index": "F18220",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And really the only difference between this and the older algorithm is that, instead of going back and forth, previously we talked about minimizing with respect to theta then minimizing with respect to x, whereas minimizing with respect to theta, minimizing with respect to x and so on.",
    "output": "そして単純に全体をxとシータの関数として、最小化すれば良い。"
  },
  {
    "index": "F18221",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this new version instead of sequentially going between the 2 sets of parameters x and theta, what we are going to do is just minimize with respect to both sets of parameters simultaneously.",
    "output": "そして実際、これと以前のアルゴリズムの唯一の違いは、行ったり来たりする代わりに、以前のはシータに関して最小化した後次にxについて最小化して、その後シータについて最小化して、xについて最小化して、、、などとやったのだったが、この新しいバージョンでは、xとシータという二つのパラメータのセットの間を順番に行ったり来たりする代わりにたんに両方のパラメータに関して同時に最小化する、という事をする。"
  },
  {
    "index": "F18222",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally one last detail is that when we're learning the features this way. Previously we have been using this convention that we have a feature x0 equals one that corresponds to an interceptor.",
    "output": "最後に一つ詳細な話だが、フィーチャーをこういう風に学ぶ場合、以前はx0がイコール1となるコンベンションを使っていて、これは切片項に対応した物だった。"
  },
  {
    "index": "F18223",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When we are using this sort of formalism where we're are actually learning the features, we are actually going to do away with this convention.",
    "output": "この種の定式化を用いる時には、実際にはフィーチャーも学習する事になるので、このコンベンション無しでいける。"
  },
  {
    "index": "F18224",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the features we are going to learn x, will be in Rn.",
    "output": "だから我らの学習する事になるフィーチャーxはRnだ。"
  },
  {
    "index": "F18225",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas previously we had features x and Rn + 1 including the intercept term.",
    "output": "一方、以前はフィーチャーxはRn+1だった、切片項を含んでいたから。"
  },
  {
    "index": "F18226",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By getting rid of x0 we now just have x in Rn.",
    "output": "そこからx0を取り除いたから、xはRnとなる。"
  },
  {
    "index": "F18227",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so similarly, because the parameters theta is in the same dimension, we now also have theta in RN because if there's no x0, then there's no need parameter theta 0 as well.",
    "output": "そして同様に、パラメータのシータは同じ次元なので、シータもRnとなる。何故ならx0が無いなら、シータ0も同様に不要だからだ。"
  },
  {
    "index": "F18228",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason we do away with this convention is because we're now learning all the features, right?",
    "output": "そしてこのコンベンション無しでいける理由としては、いまや我らはフィーチャーを全て学習する事になった、よね?"
  },
  {
    "index": "F18229",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there is no need to hard code the feature that is always equal to one.",
    "output": "だからいつも1と等しくなるフィーチャーをハードコードする必要が無い。"
  },
  {
    "index": "F18230",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because if the algorithm really wants a feature that is always equal to 1, it can choose to learn one for itself.",
    "output": "何故ならもしアルゴリズムがいつも1となるフィーチャーを本当に必要としているなら、それは自身で勝手にそう学習するはずだからだ。"
  },
  {
    "index": "F18231",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if the algorithm chooses, it can set the feature X1 equals 1.",
    "output": "だからもしアルゴリズムがそう選べばx1=1とセットされる。"
  },
  {
    "index": "F18232",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there's no need to hard code the feature of 001, the algorithm now has the flexibility to just learn it by itself.",
    "output": "だからフィーチャーx0を1とハードコードする必要は無い。アルゴリズムはそれを自分自身で学習する事が出来るだけの柔軟性がある。"
  },
  {
    "index": "F18233",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, putting everything together, here is our collaborative filtering algorithm.",
    "output": "では全部を合わせると、これが協調フィルタリングのアルゴリズムだ。"
  },
  {
    "index": "F18234",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "first we are going to initialize x and theta to small random values.",
    "output": "まず、xとシータをある小さなランダムの値で初期化する。"
  },
  {
    "index": "F18235",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is a little bit like neural network training, where there we were also initializing all the parameters of a neural network to small random values.",
    "output": "これはちょっとニューラルネットワークのトレーニングに似てるね。そこでもニューラルネットワークのパラメータを全て小さなランダムの数で初期化したんだった。"
  },
  {
    "index": "F18236",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next we're then going to minimize the cost function using great intercepts or one of the advance optimization algorithms.",
    "output": "次に、コスト関数を、最急降下法なりアドバンスドな最適化のアルゴリズムなりを使って最小化する。だから微分をとれば、こんな感じの最急降下法の更新ルールが得られる。"
  },
  {
    "index": "F18237",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you take derivatives you find that the great intercept like these and so this term here is the partial derivative of the cost function, I'm not going to write that out, with respect to the feature value Xik and similarly this term here is also a partial derivative value of the cost function with respect to the parameter theta that we're minimizing.",
    "output": "全部書いたりはしないが、それをフィーチャーx(i)kで偏微分した物だ。同様に、この項もまた、コスト関数を偏微分した物で今度はパラメータであるシータに関して偏微分した物で、それに関して最小化する。"
  },
  {
    "index": "F18238",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just as a reminder, in this formula that we no longer have this X0 equals 1 and so we have that x is in Rn and theta is a Rn.",
    "output": "ちょっと注意を。この式では、もはやx0イコール1が無い、だからxはRnとなり、シータもRnとなる。"
  },
  {
    "index": "F18239",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this new formalism, we're regularizing every one of our perimeters theta, you know, every one of our parameters Xn.",
    "output": "この新しい定式化では、各パラメータ、シータも、各パラメータxnも、正規化している。"
  },
  {
    "index": "F18240",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's no longer the special case theta zero, which was regularized differently, or which was not regularized compared to the parameters theta 1 down to theta.",
    "output": "もはやシータ0の特別扱いのケースも存在しない。シータ0は異なる風に正規化していたんだった。"
  },
  {
    "index": "F18241",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there is now no longer a theta 0, which is why in these updates, I did not break out a special case for k equals 0.",
    "output": "もはやシータ0は存在しないので、そんな訳だからこのアップデートでもk=0の特別なケースの場合分けをしていない。"
  },
  {
    "index": "F18242",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we then use gradient descent to minimize the cost function j with respect to the features x and with respect to the parameters theta.",
    "output": "そして最急降下法を使ってコスト関数Jをフィーチャーxとパラメータシータに関して最小化していく。"
  },
  {
    "index": "F18243",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, given a user, if a user has some parameters, theta, and if there's a movie with some sort of learned features x, we would then predict that that movie would be given a star rating by that user of theta transpose j.",
    "output": "そして最後に、あるユーザーに対してユーザーがあるパラメータ、シータを持っているなら、そして映画にはある種の学習されたフィーチャーxがあるなら、そのユーザーが映画に星いくつのレーティングをするかをシータjの転置と、、、または、前の表記と合わせると、もしユーザーjがまだ映画iをレーティングしていなければ、ユーザーjは映画iをシータjの転置xiとレーティングすると予測する。以上が協調フィルタリングアルゴリズムだ。"
  },
  {
    "index": "F18244",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's the collaborative filtering algorithm and if you implement this algorithm you actually get a pretty decent algorithm that will simultaneously learn good features for hopefully all the movies as well as learn parameters for all the users and hopefully give pretty good predictions for how different users will rate different movies that they have not yet rated",
    "output": "もしこのアルゴリズムを実装すれば、おそらく全ての映画の良いフィーチャーと全てのユーザーのパラメータを同時に学習する、かなり良いアルゴリズムを得る事が出来る。そして別々のユーザーが別々の映画をどうレーティングするかのかなり良い予測を与える事も期待出来る。"
  },
  {
    "index": "F18245",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last few videos, we talked about a collaborative filtering algorithm.",
    "output": "ここ何回かのビデオで、協調的フィルタリングのアルゴリズムについて議論してきた。"
  },
  {
    "index": "F18246",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I'm going to say a little bit about the vectorization implementation of this algorithm.",
    "output": "このビデオでは、このアルゴリズムのベクトル化した実装についてちょこっと話しておきたい。"
  },
  {
    "index": "F18247",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And also talk a little bit about other things you can do with this algorithm.",
    "output": "またついでにこのアルゴリズムについて出来るその他の事についても話す。"
  },
  {
    "index": "F18248",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For example, one of the things you can do is, given one product can you find other products that are related to this so that for example, a user has recently been looking at one product.",
    "output": "例えば、出来る事の一つにある商品が与えられた時に、これに関連した商品を探す、というのがある。"
  },
  {
    "index": "F18249",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Are there other related products that you could recommend to this user?",
    "output": "これは例えばユーザーが最近ある商品を見たとすると、このユーザーに推薦する、別の関連した商品は無いか?"
  },
  {
    "index": "F18250",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's see what we could do about that.",
    "output": "ではどうやってこれが出来るか見てみよう。"
  },
  {
    "index": "F18251",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I'd like to do is work out an alternative way of writing out the predictions of the collaborative filtering algorithm.",
    "output": "ここでやろうとしているのは、協調的フィルタリングのアルゴリズムの予測を別のやり方で練習する、という物。"
  },
  {
    "index": "F18252",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To start, here is our data set with our five movies and what I'm going to do is take all the ratings by all the users and group them into a matrix.",
    "output": "はじめに、これが我々のデータセットで五つの映画がある。そして私がやる事は、ユーザーのレーティングを全て取ってきて、一つの行列にグループ化する。"
  },
  {
    "index": "F18253",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here we have five movies and four users, and so this matrix y is going to be a 5 by 4 matrix.",
    "output": "するとここでは、5つの映画と4人のユーザーがいるので、この行列yは5x4行列になる。"
  },
  {
    "index": "F18254",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's just you know, taking all of the elements, all of this data.",
    "output": "これは単純に、これらの要素、これらのデータを全部とってきて、これははてなマークも全部含めて、それをこの行列にグループ化した物だ。"
  },
  {
    "index": "F18255",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Including question marks, and grouping them into this matrix.",
    "output": "もちろんこの行列の要素、この行列の要素ijは、実際は、以前にyの上付き添字ijと書いていた物だ。"
  },
  {
    "index": "F18256",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And of course the elements of this matrix of the (i, j) element of this matrix is really what we were previously writing as y superscript i, j.",
    "output": "それはユーザーjによる映画iのレーティング。我らの持つ全てのレーティングを含むこの行列Yが与えられたとすると、レーティングを予測するアルゴリズムを書き下す代替的な方法がある。"
  },
  {
    "index": "F18257",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's the rating given to movie i by user j.",
    "output": "特に、もしあなたがあるユーザーのある映画に対する予測を調べたいなら、ユーザーjの映画iに対する予測の式はこうなる。"
  },
  {
    "index": "F18258",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given this matrix y of all the ratings that we have, there's an alternative way of writing out all the predictive ratings of the algorithm.",
    "output": "だから、予測されたレーティングの行列があれば、それは以下のような行列だろう、ijエントリがこれがユーザーjが映画iに与えるレーティングの予測値で、それは正確にシータjの転置xiにイコールだ。"
  },
  {
    "index": "F18259",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, in particular if you look at what a certain user predicts on a certain movie, what user j predicts on movie i is given by this formula.",
    "output": "つまり、この行列は、最初の要素、この要素11は、ユーザー1の映画1に対するレーティングの予測値だ。"
  },
  {
    "index": "F18260",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, if you have a matrix of the predicted ratings, what you would have is the following matrix where the i, j entry.",
    "output": "などなど。そしてこれは、ユーザー1の最後の映画に対するレーティングの予想値。"
  },
  {
    "index": "F18261",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this corresponds to the rating that we predict using j will give to movie i is exactly equal to that theta j transpose XI, and so, you know, this is a matrix where this first element the one-one element is a predictive rating of user one or movie one and this element, this is the one-two element is the predicted rating of user two on movie one, and so on, and this is the predicted rating of user one on the last movie and if you want, you know, this rating is what we would have predicted for this value and this rating is what we would have predicted for that value, and so on.",
    "output": "そしてこれは、このレーティングはこの値を予想した物で、このレーティングはこの値を予想した物で、、、などなど。"
  },
  {
    "index": "F18262",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, given this matrix of predictive ratings there is then a simpler or vectorized way of writing these out.",
    "output": "いま、このレーティングの予想値の行列が与えられたとすると、これらを書きだすより簡単な、またはベクトル化した方法が存在する。"
  },
  {
    "index": "F18263",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular if I define the matrix x, and this is going to be just like the matrix we had earlier for linear regression to be sort of x1 transpose x2 transpose down to x of nm transpose.",
    "output": "これは、以前、線形回帰であった行列に似ている。x1の転置にx2の転置に、、、とxのnmの転置まで降りていく。"
  },
  {
    "index": "F18264",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I'm take all the features for my movies and stack them in rows.",
    "output": "つまり全ての映画のフィーチャーを持ってきて列として積んでいく。"
  },
  {
    "index": "F18265",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you think of each movie as one example and stack all of the features of the different movies and rows.",
    "output": "つまり各映画を一つの手本とみなして、それら別々の映画のフィーチャーを列として積んでいく訳だ。"
  },
  {
    "index": "F18266",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if we also to find a matrix capital theta, and what I'm going to do is take each of the per user parameter vectors, and stack them in rows, like so.",
    "output": "そしてまた、行列として大文字のシータも作りたい。その為には、ユーザー毎のパラメータベクトルを取ってきて、同様に列として積んでいく。"
  },
  {
    "index": "F18267",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's theta 1, which is the parameter vector for the first user.",
    "output": "つまり、これがシータ1で、これは最初のユーザーのパラメータベクトルだ。"
  },
  {
    "index": "F18268",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, you know, theta 2, and so, you must stack them in rows like this to define a matrix capital theta and so I have nu parameter vectors all stacked in rows like this.",
    "output": "そしてシータ2、つまり、このように列として積んでいって大文字のシータの行列を定義する。するとこのように列として積み上がったnu個のパラメータベクトルを列に持った物を得る。"
  },
  {
    "index": "F18269",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now given this definition for the matrix x and this definition for the matrix theta in order to have a vectorized way of computing the matrix of all the predictions you can just compute x times the matrix theta transpose, and that gives you a vectorized way of computing this matrix over here.",
    "output": "さて、行列Xと行列シータがこう定義されたとして、全ての予測を計算するベクトル化した方法を得るには、単純にX掛けるシータ転置を計算するだけで良い。それがここにあるこの行列を計算するベクトル化した方法となっている。"
  },
  {
    "index": "F18270",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The algorithm that we're using is also called low rank matrix factorization.",
    "output": "我らの使っているこのアルゴリズムはまた、低ランク行列分解とも呼ばれている。"
  },
  {
    "index": "F18271",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if you hear people talk about low rank matrix factorization that's essentially exactly the algorithm that we have been talking about.",
    "output": "つまりもし人々が低ランク行列分解について話しているのを耳にした時は、それは本質的には我らがここまで議論して来た物と同一の物だ。"
  },
  {
    "index": "F18272",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this term comes from the property that this matrix x times theta transpose has a mathematical property in linear algebra called that this is a low rank matrix and so that's what gives rise to this name low rank matrix factorization for these algorithms, because of this low rank property of this matrix x theta transpose.",
    "output": "そしてこの用語はこの行列X掛けるシータ転置が持つ数学的特徴に、線形代数ではこの低ランク行列と呼ばれる物があって、だからこれらのアルゴリズムには低ランク行列分解という名前がついた。この行列Xシータ転置の持つ低ランクという行列の特徴の為に。"
  },
  {
    "index": "F18273",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case you don't know what low rank means or in case you don't know what a low rank matrix is, don't worry about it.",
    "output": "もし低ランクというのがなんなのか知らなくて、低ランク行列というのが何なのか知らなくても、まぁ気にすんな。"
  },
  {
    "index": "F18274",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You really don't need to know that in order to use this algorithm.",
    "output": "このアルゴリズムを使うのに、そんな事を知ってる必要はまったく無い。"
  },
  {
    "index": "F18275",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you're an expert in linear algebra, that's what gives this algorithm, this other name of low rank matrix factorization.",
    "output": "だがもしあなたが線形代数のエキスパートなら、そんな理由でこのアルゴリズムに低ランク行列分解という別名がついた、というワケ。"
  },
  {
    "index": "F18276",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, having run the collaborative filtering algorithm here's something else that you can do which is use the learned features in order to find related movies.",
    "output": "最後に、協調的フィルタリングのアルゴリズムを実行する時に、実行出来る別の事として、こんなのがある。それは関連する映画を探す為に学習したフィーチャーを用いる、という物。"
  },
  {
    "index": "F18277",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Specifically for each product i really for each movie i, we've learned a feature vector xi.",
    "output": "具体的には、各商品iに関して実際には各映画iに関して、フィーチャーベクトルxiを学習した。"
  },
  {
    "index": "F18278",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you know, when you learn a certain features without really know that can the advance what the different features are going to be, but if you run the algorithm and perfectly the features will tend to capture what are the important aspects of these different movies or different products or what have you.",
    "output": "つまり、あるフィーチャー群を学習した時には、どんなフィーチャーがあるのかとかは前もっては知らなかった。"
  },
  {
    "index": "F18279",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What are the important aspects that cause some users to like certain movies and cause some users to like different sets of movies.",
    "output": "だがアルゴリズムを実行すると、フィーチャーはこれらの別々の映画のまたは別々の商品の、またはなんでも、これらの重要な性質は何か、を捕捉する傾向にある。"
  },
  {
    "index": "F18280",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then some feature x4 which is, you know, some other thing.",
    "output": "何がある種のユーザー達がある映画たちを好きになる重要な特徴なのか、そしてあるユーザー達が異なる映画を好む、重要な特徴とは何か。"
  },
  {
    "index": "F18281",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you have N features all together and after you have learned features it's actually often pretty difficult to go in to the learned features and come up with a human understandable interpretation of what these features really are.",
    "output": "だから例えば、最終的にあなたはフィーチャーx1としてロマンスをx2としてアクションを、これまでの例と同様に学習した上で、さらに別のフィーチャーx3としてコメディー度合いを学習する事になるかもしれない。"
  },
  {
    "index": "F18282",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in practice, you know, the features even though these features can be hard to visualize.",
    "output": "そうやってn個のフィーチャーを得る事になったとして、実際はこれらのフィーチャーを見ていって、これらのフィーチャーが本当は何を意味しているのかを人間が理解するのはとても難しい事が多い。"
  },
  {
    "index": "F18283",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It can be hard to figure out just what these features are. Usually, it will learn features that are very meaningful for capturing whatever are the most important or the most salient properties of a movie that causes you to like or dislike it.",
    "output": "だが実践的には、これらのフィーチャーが、たとえ可視化する事すら大変でも、これらのフィーチャーが何なのかを突き止めるのは大変であっても、だいたいは、それが何にせよあなたが映画を好いたり嫌ったりするもっとも特徴的でとても意義深い特徴を捕捉しているフィーチャーを学習する。"
  },
  {
    "index": "F18284",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so now let's say we want to address the following problem.",
    "output": "では、以下のような問題に取り組むとしよう。"
  },
  {
    "index": "F18285",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Say you have some specific movie i and you want to find other movies j that are related to that movie.",
    "output": "あなたがある映画iを持ってたとする、そしてあなたはその映画に関連した別の映画jを探したい。"
  },
  {
    "index": "F18286",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so well, why would you want to do this?",
    "output": "ところで、いったいなんでそんな事したいのか?"
  },
  {
    "index": "F18287",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, maybe you have a user that's browsing movies, and they're currently watching movie j, than what's a reasonable movie to recommend to them to watch after they're done with movie j?",
    "output": "いいでしょう。例えばユーザーが映画をブラウズしているとする、そして彼は今、映画jを観ているとする、その時、映画jを観終わった後に彼に視聴を推薦すべき、リーズナブルな映画はなんだろう?"
  },
  {
    "index": "F18288",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or if someone's recently purchased movie j, well, what's a different movie that would be reasonable to recommend to them for them to consider purchasing.",
    "output": "または誰かが映画jを購入したとすると、その時に彼らに購入を検討してもらう為に推薦する事がリーズナブルな別の映画はなんだろう?"
  },
  {
    "index": "F18289",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, now that you have learned these feature vectors, this gives us a very convenient way to measure how similar two movies are.",
    "output": "いまやあなたはこれらの学習したフィーチャーを持っているのだから、これを用いて二つの映画がどの位近いかを測るとても便利な方法がある。"
  },
  {
    "index": "F18290",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, movie i has a feature vector xi.",
    "output": "具体的には、映画iはフィーチャーベクトルxiを持つ訳だが、別の映画jを、フィーチャーベクトルxiとxjの距離が短い物として探す事が出来る。"
  },
  {
    "index": "F18291",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "and so if you can find a different movie, j, so that the distance between xi and xj is small, then this is a pretty strong indication that, you know, movies j and i are somehow similar.",
    "output": "これは二つの映画、iとjが類似しているという極めて強い示唆を与える。少なくとも、映画iを好きな人は映画jも好むだろう、という意味において。"
  },
  {
    "index": "F18292",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "At least in the sense that some of them likes movie i, maybe more likely to like movie j as well.",
    "output": "ではまとめると、ユーザーが映画iを見ていたとする、そしてあなたはその映画にもっとも似た5つの新しい映画を彼らにリコメンド(推薦)したいとする。"
  },
  {
    "index": "F18293",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, just to recap, if your user is looking at some movie i and if you want to find the 5 most similar movies to that movie in order to recommend 5 new movies to them, what you do is find the five movies j, with the smallest distance between the features between these different movies.",
    "output": "あなたがする事は、これら別々の映画間でフィーチャーの距離が一番小さい5つの映画を探してくる、という事。"
  },
  {
    "index": "F18294",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this could give you a few different movies to recommend to your user.",
    "output": "これであなたは勧めるべき幾つかの映画を得る事が出来る。"
  },
  {
    "index": "F18295",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So with that, hopefully, you now know how to use a vectorized implementation to compute all the predicted ratings of all the users and all the movies, and also how to do things like use learned features to find what might be movies and what might be products that aren't related to each other.",
    "output": "以上で全てのユーザー、全ての映画の予測レーティングをベクトル化して実装し計算するやり方を理解出来ただろう、そして学習したフィーチャーをどのように用いて、どの映画とか商品が関連しているのかを探す方法も。"
  },
  {
    "index": "F18296",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By now you've seen all of the main pieces of the recommender system algorithm or the collaborative filtering algorithm.",
    "output": "ここまでで、あなたはリコメンダーシステムのアルゴリズム、または協調フィルリングのアルゴリズムの全ての主要な要素を見終わった事になる。"
  },
  {
    "index": "F18297",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I want to just share one last implementational detail, namely mean normalization, which can sometimes just make the algorithm work a little bit better.",
    "output": "このビデオでは、ちょっとした最後の実装の詳細を共有しておきたい。それは平均標準化(meannormalize)と呼ばれる物で、それはときどきアルゴリズムをちょっと良く機能させる程度の物。"
  },
  {
    "index": "F18298",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To motivate the idea of mean normalization, let's consider an example of where there's a user that has not rated any movies.",
    "output": "平均標準化のアイデアのモチベーションを理解する為、なんの映画もレーティングしていないユーザーの例を考えてみよう。"
  },
  {
    "index": "F18299",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in addition to our four users, Alice, Bob, Carol, and Dave, I've added a fifth user, Eve, who hasn't rated any movies.",
    "output": "つまり、我らの四人のユーザー、Alice,Bob,Carol,Daveに加えて、さらに5番目のユーザー、Eveを足したとしよう。彼女は一つの映画もレーティングしていないとする。"
  },
  {
    "index": "F18300",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's see what our collaborative filtering algorithm will do on this user.",
    "output": "我らの協調フィルリングのアルゴリズムがこのユーザーに何をするのか見てみよう。"
  },
  {
    "index": "F18301",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that n is equal to 2 and so we're going to learn two features and we are going to have to learn a parameter vector theta 5, which is going to be in R2, remember this is now vectors in Rn not Rn+1, we'll learn the parameter vector theta 5 for our user number 5, Eve.",
    "output": "そしてパラメータベクトルのシータ5を学習したい。"
  },
  {
    "index": "F18302",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if we look in the first term in this optimization objective, well the user Eve hasn't rated any movies, so there are no movies for which Rij is equal to one for the user Eve and so this first term plays no role at all in determining theta 5 because there are no movies that Eve has rated.",
    "output": "最適化の目的関数の最初の項を見てみると、ユーザーEveはなんの映画もレーティングしていないので、rijが1となる映画は、Eveに関しては一つも無い。"
  },
  {
    "index": "F18303",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the only term that effects theta 5 is this term.",
    "output": "だからシータ5に影響を与える唯一の項はこの項だ。"
  },
  {
    "index": "F18304",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so we're saying that we want to choose vector theta 5 so that the last regularization term is as small as possible.",
    "output": "つまり、我らはベクトルシータ5を、最後の正規化項をなるべく小さくするように選ぶ、と主張している。"
  },
  {
    "index": "F18305",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In other words we want to minimize this lambda over 2 theta 5 subscript 1 squared plus theta 5 subscript 2 squared so that's the component of the regularization term that corresponds to user 5, and of course if your goal is to minimize this term, then what you're going to end up with is just theta 5 equals 0 0.",
    "output": "言い換えると、我らが最小化しようとするのはこのラムダ/2のシータ5下付き添字1の二乗足すことのシータ5下付き添字2の二乗だ。以上がユーザー5に関する正規化項の要素だ。"
  },
  {
    "index": "F18306",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because a regularization term is encouraging us to set parameters close to 0 and if there is no data to try to pull the parameters away from 0, because this first term doesn't effect theta 5, we just end up with theta 5 equals the vector of all zeros.",
    "output": "何故なら正規化項はパラメータを0に近くなるように推奨する訳だが、そこでもしパラメータを0から引き離すデータが存在しなければ、何故ならこの最初の項はシータ5には影響しないので、結局シータ5としては全ての要素が0のベクトルを得る事になる。"
  },
  {
    "index": "F18307",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so when we go to predict how user 5 would rate any movie, we have that theta 5 transpose xi, for any i, that's just going to be equal to zero.",
    "output": "すると、ユーザー5が映画をどうレーティングするかを予測しようとすると、どんな映画に対しても、シータ5の転置xiはいかなるiに対しても、イコール0となる。"
  },
  {
    "index": "F18308",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because theta 5 is 0 for any value of x, this inner product is going to be equal to 0.",
    "output": "シータ5はどのxに対しても0なので、内積は0となる。"
  },
  {
    "index": "F18309",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we're going to have therefore, is that we're going to predict that Eve is going to rate every single movie with zero stars.",
    "output": "だから我らは結局、Eveはどの動画も0とレーティングする、と予測する事になる。"
  },
  {
    "index": "F18310",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But this doesn't seem very useful does it?",
    "output": "でもこの予想はあんまり役に立たない、よね?"
  },
  {
    "index": "F18311",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I mean if you look at the different movies, Love at Last, this first movie, a couple people rated it 5 stars.",
    "output": "つまり別々の映画を見ていくと、LoveatLast、この最初の映画は、何人かは星5とレーティングした。"
  },
  {
    "index": "F18312",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So some people do like some movies.",
    "output": "だからある人々はある種の映画を好むのだ。"
  },
  {
    "index": "F18313",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It seems not useful to just predict that Eve is going to rate everything 0 stars.",
    "output": "だからEveは全ての映画を0と付けると予測するのは、あまり便利では無い。"
  },
  {
    "index": "F18314",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact if we're predicting that eve is going to rate everything 0 stars, we also don't have any good way of recommending any movies to her, because you know all of these movies are getting exactly the same predicted rating for Eve so there's no one movie with a higher predicted rating that we could recommend to her, so, that's not very good.",
    "output": "さらに実際、もし我らがEveは全てに星0のレーティングをする、と予測してしまうと、彼女に推薦する映画を決める良い方法も無くなってしまう。何故ならこれら全ての映画はEveの場合、完全に同じ予測値となってしまうから。"
  },
  {
    "index": "F18315",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The idea of mean normalization will let us fix this problem.",
    "output": "それはあまり良くない。平均標準化法のアイデアはこの問題を修正する。"
  },
  {
    "index": "F18316",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's how it works.",
    "output": "それはこんな風に機能する。"
  },
  {
    "index": "F18317",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As before let me group all of my movie ratings into this matrix Y, so just take all of these ratings and group them into matrix Y.",
    "output": "前と同様、全ての映画のレーティングをこの行列Yにグルーピングする。つまりこれらのレーティングを全て持ってきて、この行列Yにグルーピングする。"
  },
  {
    "index": "F18318",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this column over here of all question marks corresponds to Eve's not having rated any movies.",
    "output": "そしてこのここにある行の、全部はてなマークなのは、Eveがなんの映画もレーティングしていない事に対応している。"
  },
  {
    "index": "F18319",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now to perform mean normalization what I'm going to do is compute the average rating that each movie obtained.",
    "output": "ここで平均標準化を実行する為には、各映画の得たレーティングの平均を計算する。"
  },
  {
    "index": "F18320",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And I'm going to store that in a vector that we'll call mu.",
    "output": "そして、それをミューというベクトルに保存する。"
  },
  {
    "index": "F18321",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the first movie got two 5-star and two 0-star ratings, so the average of that is a 2.5-star rating.",
    "output": "つまり、最初の映画は二つの星5と二つの星0のレーティングを得たのだから、その平均は星2.5だ。"
  },
  {
    "index": "F18322",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The second movie had an average of 2.5-stars and so on.",
    "output": "二番目の映画の平均は星2.5だ。"
  },
  {
    "index": "F18323",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the final movie that has 0, 0, 5, 0. And the average of 0, 0, 5, 0, that averages out to an average of 1.25 rating.",
    "output": "そして最後の映画は、0,0,5,0だから0,0,5,0の平均は平均をとると、平均は1.25レーティング。"
  },
  {
    "index": "F18324",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what I'm going to do is look at all the movie ratings and I'm going to subtract off the mean rating.",
    "output": "そして、全映画のレーティングを見ていき、そこから平均のレーティングを引いていく。"
  },
  {
    "index": "F18325",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this first element 5 I'm going to subtract off 2.5 and that gives me 2.5.",
    "output": "つまりこの最初の要素、5は、2.5を引くから、2.5となる。"
  },
  {
    "index": "F18326",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the second element 5 subtract off of 2.5, get a 2.5.",
    "output": "二番目の要素5からは2.5を引くから2.5となる。"
  },
  {
    "index": "F18327",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In other words, what I'm going to do is take my matrix of movie ratings, take this wide matrix, and subtract form each row the average rating for that movie.",
    "output": "言い換えると、私がやってる事は映画のレーティングの行列をもってきて、この幅の広い行列を持ってきて、各列から、その映画の平均のレーティングを引く、という事。"
  },
  {
    "index": "F18328",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what I'm doing is just normalizing each movie to have an average rating of zero.",
    "output": "つまり、私がやってるのは、各映画を、平均が0になるように標準化しているだけ。"
  },
  {
    "index": "F18329",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so just one last example.",
    "output": "そして最後に一つ例を見る。"
  },
  {
    "index": "F18330",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you look at this last row, 0 0 5 0.",
    "output": "この最後の列を見ると、0050だ。"
  },
  {
    "index": "F18331",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to subtract 1.25, and so I end up with these values over here.",
    "output": "1.25を引くから、結局ここの値となる。"
  },
  {
    "index": "F18332",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So now and of course the question marks stay a question mark.",
    "output": "そして現在、もちろんこのはてなマークははてなマークのままだ。"
  },
  {
    "index": "F18333",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So each movie in this new matrix Y has an average rating of 0.",
    "output": "だから、この新しい行列Yにある各映画も平均のレーティングは0となる。"
  },
  {
    "index": "F18334",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I'm going to do then, is take this set of ratings and use it with my collaborative filtering algorithm.",
    "output": "そこで次にやる事としては、このレーティングの集合を使って、協調フィルタリングアルゴリズムを行う。"
  },
  {
    "index": "F18335",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I'm going to pretend that this was the data that I had gotten from my users, or pretend that these are the actual ratings I had gotten from the users, and I'm going to use this as my data set with which to learn my parameters theta J and my features XI - from these mean normalized movie ratings.",
    "output": "つまり、これをユーザーから取ったデータのフリをさせて、言い換えると、これらをユーザーから取った実際のレーティングのフリをさせて、そしてこれを私のデータセットとしてパラメータのシータjとフィーチャーのxiを学習させる。-これらの平均標準化された映画のレーティングから。"
  },
  {
    "index": "F18336",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When I want to make predictions of movie ratings, what I'm going to do is the following: for user J on movie I, I'm gonna predict theta J transpose XI, where X and theta are the parameters that I've learned from this mean normalized data set.",
    "output": "映画のレーティングの予測をさせたくなったら、以下のようにする:ユーザーjの映画iに関しては、シータj転置xiと予測する。ここでxとシータはこの、平均標準化したデータセットから学習したパラメータ。"
  },
  {
    "index": "F18337",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, because on the data set, I had subtracted off the means in order to make a prediction on movie i, I'm going to need to add back in the mean, and so i'm going to add back in mu i.",
    "output": "だが、このデータセットは既に平均が引かれた物だから、映画iに関して予測を行いたいなら、平均を足し戻す必要がある。だからミューiを足し戻す。"
  },
  {
    "index": "F18338",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's going to be my prediction where in my training data subtracted off all the means and so when we make predictions and we need to add back in these means mu i for movie i.",
    "output": "以上が私の予測となる。そこでは、トレーニングデータから平均を全て引いたのだから、予測を行う時には、これらの平均、ミューiを映画iに対して足し戻さなくてはならない。"
  },
  {
    "index": "F18339",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so specifically if you user 5 which is Eve, the same argument as the previous slide still applies in the sense that Eve had not rated any movies and so the learned parameter for user 5 is still going to be equal to 0, 0.",
    "output": "特に、もしユーザー5、つまりEveに対して、前のスライドの議論がいまだに適用出来て、Eveはなんの映画もレーティングしてないので、だからユーザー5について学習したパラメータはまだイコール0,0だ。"
  },
  {
    "index": "F18340",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what we're going to get then is that on a particular movie i we're going to predict for Eve theta 5, transpose xi plus add back in mu i and so this first component is going to be equal to zero, if theta five is equal to zero.",
    "output": "だから得られる物は、つまり、特定の映画iについてEveの結果を予測すると、シータ5転置xi足すことの、ミューiを足し戻す、のだから、この最初の要素はシータ5が0なら0となる。"
  },
  {
    "index": "F18341",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so on movie i, we are going to end a predicting mu i.",
    "output": "だから映画iに関しては、ミューiになる、と予測する事になる。"
  },
  {
    "index": "F18342",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, this actually makes sense.",
    "output": "これは納得出来る。"
  },
  {
    "index": "F18343",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It means that on movie 1 we're going to predict Eve rates it 2.5.",
    "output": "つまり、映画1に関しては、Eveが2.5とレーティングするだろう、と予測する訳だ。"
  },
  {
    "index": "F18344",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This actually makes sense, because it says that if Eve hasn't rated any movies and we just don't know anything about this new user Eve, what we're going to do is just predict for each of the movies, what are the average rating that those movies got.",
    "output": "これは実際に筋が通っている、何故ならEveがまだ何もレーティングしていないとするとこの新しいユーザーEveについては我らは何も知らない事になる。だから我らがする事といえば、各映画に対してそれらの映画のレーティングの平均と予測する訳だ。"
  },
  {
    "index": "F18345",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, as an aside, in this video we talked about mean normalization, where we normalized each row of the matrix y, to have mean 0.",
    "output": "このビデオでは、平均標準化について議論した。そこでは、Y行列の各列を、平均が0になるように標準化した。"
  },
  {
    "index": "F18346",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In case you have some movies with no ratings, so it is analogous to a user who hasn't rated anything, but in case you have some movies with no ratings, you can also play with versions of the algorithm, where you normalize the different columns to have means zero, instead of normalizing the rows to have mean zero, although that's maybe less important, because if you really have a movie with no rating, maybe you just shouldn't recommend that movie to anyone, anyway.",
    "output": "もし全くレーティングされていない映画がある場合は、それは何もレーティングしていないユーザーと似ているが、レーティングの一切無い映画がある場合、各行の平均が0になるようなバージョンのアルゴリズムを使う事も出来る。列を平均が0になるように標準化する代わりにだ。"
  },
  {
    "index": "F18347",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, taking care of the case of a user who hasn't rated anything might be more important than taking care of the case of a movie that hasn't gotten a single rating.",
    "output": "でもこちらは比較的重要では無いパターンかもしれない。何故なら、実際にレーティングの無い映画があったら、なんにせよその映画は誰にも推薦すべきでは無いかもしれないから。"
  },
  {
    "index": "F18348",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So to summarize, that's how you can do mean normalization as a sort of pre-processing step for collaborative filtering.",
    "output": "ではまとめだ。以上が協調フィルタリングの前処理として平均標準化を行う方法だ。"
  },
  {
    "index": "F18349",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Depending on your data set, this might some times make your implementation work just a little bit better.",
    "output": "あなたのデータセットによっては、この手法はあなたの実装をちょっぴり良く振舞わせてくれるかもしれない。"
  },
  {
    "index": "F18350",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos, we'll talk about large scale machine learning.",
    "output": "続く幾つかのビデオで、大規模スケールの機械学習について話す。"
  },
  {
    "index": "F18351",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, algorithms but viewing with big data sets. If you look back at a recent 5 or 10-year history of machine learning.",
    "output": "つまり、アルゴリズムなんだけれど、とてもビッグなデータセットを見る物だ。"
  },
  {
    "index": "F18352",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One of the reasons that learning algorithms work so much better now than even say, 5-years ago, is just the sheer amount of data that we have now and that we can train our algorithms on.",
    "output": "ここ最近5年とか10年の機械学習の歴史を振り返ると、5年前と比べても凄く学習アルゴリズムがうまく動作するようになった理由の一つには、アルゴリズムのトレーニングに使えるデータの量が単純に増えた、というのがある。"
  },
  {
    "index": "F18353",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In these next few videos, we'll talk about algorithms for dealing when we have such massive data sets.",
    "output": "ここからの幾つかのビデオでは、そんな巨大なデータセットがあった時にそれを扱うアルゴリズムについて議論していきたい。"
  },
  {
    "index": "F18354",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So why do we want to use such large data sets?",
    "output": "さて、そもそも何故そんな大きなデータセットを使いたいと思うのか?"
  },
  {
    "index": "F18355",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We've already seen that one of the best ways to get a high performance machine learning system, is if you take a low-bias learning algorithm, and train that on a lot of data.",
    "output": "我らは既に、機械学習のシステムで高いパフォーマンスを得たい時には、ベストな方法の一つに低いバイアスの学習アルゴリズムを使い、大量のデータでトレーニングさせる、というのがあった。"
  },
  {
    "index": "F18356",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, one early example we have already seen was this example of classifying between confusable words.",
    "output": "既に見た前出の例としては、このややこしい単語の分類の例がある。"
  },
  {
    "index": "F18357",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, for breakfast, I ate two (TWO) eggs and we saw in this example, these sorts of results, where, you know, so long as you feed the algorithm a lot of data, it seems to do very well.",
    "output": "Forbreakfastには、Iateといえばtwoのeggsとなる。この例の問題を解くと、こんな結果となる。"
  },
  {
    "index": "F18358",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so it's results like these that has led to the saying in machine learning that often it's not who has the best algorithm that wins.",
    "output": "だから、これらのような結果から、以下のような事が良く言われる。機械学習においては、勝者はもっとも良いアルゴリズムを持つ物では無く、もっともデータを持っている人だ、と。"
  },
  {
    "index": "F18359",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you want to learn from large data sets, at least when we can get such large data sets.",
    "output": "だから大規模なデータセットから学習したい、少なくともそんなデータセットが入手可能ならば。"
  },
  {
    "index": "F18360",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But learning with large data sets comes with its own unique problems, specifically, computational problems.",
    "output": "しかし大規模なデータセットからの学習は、特有の問題もつきまとう。具体的には、計算的な問題だ。"
  },
  {
    "index": "F18361",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say your training set size is M equals 100,000,000.",
    "output": "トレーニングセットのサイズmが100,000,000だとしよう。"
  },
  {
    "index": "F18362",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is actually pretty realistic for many modern data sets.",
    "output": "これはこんにち的なデータセットしては、普通に現実的な範囲だ。"
  },
  {
    "index": "F18363",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you look at the US Census data set, if there are, you know, 300 million people in the US, you can usually get hundreds of millions of records.",
    "output": "USの国勢調査のデータを見ると、そこには、3億の人がUSには居るから、普通に何億ってデータを扱う事になる。"
  },
  {
    "index": "F18364",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you look at the amount of traffic that popular websites get, you easily get training sets that are much larger than hundreds of millions of examples.",
    "output": "人気のあるwebサイトのトラフィックの量を見ると、簡単に億より多くの手本を得る事になる。"
  },
  {
    "index": "F18365",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say you want to train a linear regression model, or maybe a logistic regression model, in which case this is the gradient descent rule.",
    "output": "線形回帰のモデルをトレーニングしたいとしよう。またはロジスティック回帰でも良い。"
  },
  {
    "index": "F18366",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you look at what you need to do to compute the gradient, which is this term over here, then when M is a hundred million, you need to carry out a summation over a hundred million terms, in order to compute these derivatives terms and to perform a single step of decent.",
    "output": "そして勾配を計算する為に必要な事を見ると、それはここの項だ。そしてmが一億の時は、1億に渡る和を取る必要がある、これらの微分項を計算する為には、そして最急降下法の1ステップを実行する為には。"
  },
  {
    "index": "F18367",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because of the computational expense of summing over a hundred million entries in order to compute just one step of gradient descent, in the next few videos we've spoken about techniques for either replacing this with something else or to find more efficient ways to compute this derivative.",
    "output": "1億に渡る和を取るという計算量的なコストの為に、最急降下法のたった1ステップを計算する為だけに。続く一連のビデオで、これを別の何かに置き換えるテクニックや、この微分項を計算するより効率的な方法について議論する。"
  },
  {
    "index": "F18368",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By the end of this sequence of videos on large scale machine learning, you know how to fit models, linear regression, logistic regression, neural networks and so on even today's data sets with, say, a hundred million examples.",
    "output": "この大規模スケールの機械学習の一連のビデオを観終わった頃には、線形回帰とかロジスティック回帰とかニューラルネットワークのモデルのフィッティングをこんにち的なデータセット、つまり一億の手本とかに行う方法を理解する事になるだろう。"
  },
  {
    "index": "F18369",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of course, before we put in the effort into training a model with a hundred million examples, We should also ask ourselves, well, why not use just a thousand examples.",
    "output": "もちろん、一億の手本でモデルをトレーニングするという努力を払う前に、単に1000の手本を使うだけでダメなのか?という事も自らに問うてみなくてはならない。"
  },
  {
    "index": "F18370",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe we can randomly pick the subsets of a thousand examples out of a hundred million examples and train our algorithm on just a thousand examples.",
    "output": "時には1億の手本からランダムに1000の手本をピックアップして、その1000の手本でトレーニングするだけ、でも良いかもしれない。"
  },
  {
    "index": "F18371",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So before investing the effort into actually developing and the software needed to train these massive models is often a good sanity check, if training on just a thousand examples might do just as well.",
    "output": "だからこれらの巨大なモデルをトレーニングするのに必要なソフトウェアの開発などに投資する前に、1000の手本でトレーニングしてみる事は、良いサニティチェックとなる。"
  },
  {
    "index": "F18372",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The way to sanity check of using a much smaller training set might do just as well, that is if using a much smaller n equals 1000 size training set, that might do just as well, it is the usual method of plotting the learning curves, so if you were to plot the learning curves and if your training objective were to look like this, that's J train theta.",
    "output": "より少ないトレーニングセットでサニティチェックを行う方法は、つまりより少ないm=1000のサイズのトレーニングセットを用いる方法は、通常の学習曲線をプロットする、という方法で良いだろう。もし学習曲線をプロットしてみて、トレーニングの目的関数がこんな感じなら、この目的関数はJtrainのシータだが、そしてクロスバリデーションセットの目的関数、Jcvのシータがこんな感じだったとする。"
  },
  {
    "index": "F18373",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if your cross-validation set objective, Jcv of theta would look like this, then this looks like a high-variance learning algorithm, and we will be more confident that adding extra training examples would improve performance.",
    "output": "その場合、高バリアンスな学習アルゴリズムのようなので、さらに追加でトレーニング手本を足す事は、パフォーマンスを改善すると確信が持てる。"
  },
  {
    "index": "F18374",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in contrast if you were to plot the learning curves, if your training objective were to look like this, and if your cross-validation objective were to look like that, then this looks like the classical high-bias learning algorithm.",
    "output": "一方、対照的に、学習曲線をプロットしたら、トレーニングの目的関数がこんな感じで、クロスバリデーションの目的関数がこんな感じだと、これは高バイアスな学習アルゴリズムに見える。"
  },
  {
    "index": "F18375",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in the latter case, you know, if you were to plot this up to, say, m equals 1000 and so that is m equals 500 up to m equals 1000, then it seems unlikely that increasing m to a hundred million will do much better and then you'd be just fine sticking to n equals 1000, rather than investing a lot of effort to figure out how the scale of the algorithm.",
    "output": "この後者の場合には、例えばこれがm=1000までのプロットだとして、この辺がm=500で、1000までとして、するとたぶん、データを1億に増やしてもたぶんあまり良くはならないだろう。それならばアルゴリズムのスケールを増やす為に努力を費やすよりは、m=1000のままにしておく方が良かろう。"
  },
  {
    "index": "F18376",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of course, if you were in the situation shown by the figure on the right, then one natural thing to do would be to add extra features, or add extra hidden units to your neural network and so on, so that you end up with a situation closer to that on the left, where maybe this is up to n equals 1000, and this then gives you more confidence that trying to add infrastructure to change the algorithm to use much more than a thousand examples that might actually be a good use of your time.",
    "output": "もちろん、この右側の図のような状況だったら、次に行うべき自然なステップとしては、追加のフィーチャーを足すとか、隠れユニットをニューラルネットワークに足すとか、そういう事で、そういう事を通してm=1000のままだと左側の状態に近づいていったらその時は初めて1億以上の手本を使うように、インフラを追加したり、アルゴリズムを変更したりする事に、より確信を持てるようになり、それは実際に良い自分の時間の使い方だと思えるだろう。"
  },
  {
    "index": "F18377",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in large-scale machine learning, we like to come up with computationally reasonable ways, or computationally efficient ways, to deal with very big data sets.",
    "output": "さて、大規模スケールの機械学習においては、とてもビッグなデータを扱うのに、計算量的にリーズナブルな、または効率的な方法を知りたい。"
  },
  {
    "index": "F18378",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos, we'll see two main ideas.",
    "output": "続く幾つかのビデオでは、二つの主なアイデアを見ていく。"
  },
  {
    "index": "F18379",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first is called stochastic gradient descent and the second is called Map Reduce, for viewing with very big data sets.",
    "output": "最初の物は確率的な最急降下法、そして二番目はMapReduceと呼ばれる物。とてもビッグなデータを見るのに。"
  },
  {
    "index": "F18380",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And after you've learned about these methods, hopefully that will allow you to scale up your learning algorithms to big data and allow you to get much better performance on many different applications.",
    "output": "そしてこれらの手法を学んだ後には、あなたの学習アルゴリズムをビッグにスケールアップ出来るようになり、様々な応用に対して、もっと良いパフォーマンスが得られるようになる事を祈る。"
  },
  {
    "index": "F18381",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For many learning algorithms, among them linear regression, logistic regression and neural networks, the way we derive the algorithm was by coming up with a cost function or coming up with an optimization objective. And then using an algorithm like gradient descent to minimize that cost function.",
    "output": "多くの学習アルゴリズムにとって、線形回帰やロジスティック回帰やニューラルネットワークなどにとって、アルゴリズムを導出する方法は、まずコスト関数、または目的関数を考えて、そしてそれを最急降下法なりのアルゴリズムを使って最小化する、という物だった。"
  },
  {
    "index": "F18382",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We have a very large training set gradient descent becomes a computationally very expensive procedure.",
    "output": "大量のトレーニングセットがある時は、最急降下法は極めて計算的に高価な手続きとなる。"
  },
  {
    "index": "F18383",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, we'll talk about a modification to the basic gradient descent algorithm called Stochastic gradient descent, which will allow us to scale these algorithms to much bigger training sets.",
    "output": "このビデオでは、基本的な最急降下法のアルゴリズムを改変した、確率的最急降下法という物を議論する、それはこれらのアルゴリズムをもっとビッグなトレーニングセットにスケール出来るようにする。"
  },
  {
    "index": "F18384",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose you are training a linear regression model using gradient descent.",
    "output": "最急降下法で線形回帰のモデルをトレーニングしているとしよう。"
  },
  {
    "index": "F18385",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As a quick recap, the hypothesis will look like this, and the cost function will look like this, which is the sum of one half of the average square error of your hypothesis on your m training examples, and the cost function we've already seen looks like this sort of bow-shaped function.",
    "output": "軽く復習しておくと、仮説はこんな形、コスト関数はこんな感じ。それは1/2の仮説の二乗誤差の平均の、mのトレーニング手本に渡る和を取った物だ。"
  },
  {
    "index": "F18386",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, plotted as function of the parameters theta 0 and theta 1, the cost function J is a sort of a bow-shaped function.",
    "output": "パラメータシータ0とシータ1の関数としてプロットすると、コスト関数Jは弓型の関数となる。"
  },
  {
    "index": "F18387",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And gradient descent looks like this, where in the inner loop of gradient descent you repeatedly update the parameters theta using that expression.",
    "output": "そして最急降下法はこんな感じだ。最急降下法の内側のループでは、パラメータシータをこの式を使って繰り返しアップデートしていく。"
  },
  {
    "index": "F18388",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now in the rest of this video, I'm going to keep using linear regression as the running example.",
    "output": "さて、このビデオの残りの部分で、線形回帰を実行出来る例として使い続ける。"
  },
  {
    "index": "F18389",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the ideas here, the ideas of Stochastic gradient descent is fully general and also applies to other learning algorithms like logistic regression, neural networks and other algorithms that are based on training gradient descent on a specific training set.",
    "output": "だが確率的最急降下法のアイデア自体は完全に一般的な物で、それはその他の学習アルゴリズム、ロジスティック回帰とかニューラルネットワークとか、その他なんでも特定のトレーニングセットに対して最急降下法で学習するアルゴリズムになら適用出来る。"
  },
  {
    "index": "F18390",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's a picture of what gradient descent does, if the parameters are initialized to the point there then as you run gradient descent different iterations of gradient descent will take the parameters to the global minimum.",
    "output": "さて、これは最急降下法が何をするかだ。もしパラメータがここの点で初期化されたら、最急降下法はパラメータをグローバル最小へと持っていく。"
  },
  {
    "index": "F18391",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So take a trajectory that looks like that and heads pretty directly to the global minimum.",
    "output": "こんな感じの軌跡を通って、だいたいまっすぐグローバル最小へと向かう。"
  },
  {
    "index": "F18392",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, the problem with gradient descent is that if m is large.",
    "output": "ここで、最急降下法の問題点としては、もしmが大きい時には、この微分項を計算するのが、とても高価になってしまう、という事。"
  },
  {
    "index": "F18393",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then computing this derivative term can be very expensive, because the surprise, summing over all m examples.",
    "output": "何故ならこれは全てのm手本に渡って和を取るから。"
  },
  {
    "index": "F18394",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if m is 300 million, alright. So in the United States, there are about 300 million people.",
    "output": "だからもしmが3億なら、、、アメリカ合衆国にはだいたい3億人の人がいる。"
  },
  {
    "index": "F18395",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the US or United States census data may have on the order of that many records.",
    "output": "すると、US、またはアメリカ合衆国の国勢調査のデータは、そんなオーダーの数のレコードとなる。"
  },
  {
    "index": "F18396",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you want to fit the linear regression model to that then you need to sum over 300 million records.",
    "output": "すると、そこに線形回帰のモデルをフィッティングしたいとすると、3億のレコードに渡って和をとらなくてはならない。"
  },
  {
    "index": "F18397",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And that's very expensive.",
    "output": "それはとても高価だ。"
  },
  {
    "index": "F18398",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To give the algorithm a name, this particular version of gradient descent is also called Batch gradient descent.",
    "output": "このアルゴリズムに名前をつけておく。この特定の最急降下法のバージョンは、バッチ最急降下法とも呼ばれる。"
  },
  {
    "index": "F18399",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the term Batch refers to the fact that we're looking at all of the training examples at a time.",
    "output": "ここで「バッチ」という単語は一度に全部のトレーニング手本を見るという事実を表している。"
  },
  {
    "index": "F18400",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We call it sort of a batch of all of the training examples.",
    "output": "それをある種の、全てのトレーニング手本のバッチ、と呼ぶ。"
  },
  {
    "index": "F18401",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it really isn't the, maybe the best name but this is what machine learning people call this particular version of gradient descent.",
    "output": "それは実はあんまりいい名前じゃ、ベストな名前って訳じゃない。だが、機械学習屋の人々がこのバージョンの最急降下法をそう呼んでるんだから仕方がない。"
  },
  {
    "index": "F18402",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you imagine really that you have 300 million census records stored away on disc.",
    "output": "そしてこの3億の国勢調査のレコードをディスクに保存して退避させてしまってる事を想像してみよう。"
  },
  {
    "index": "F18403",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The way this algorithm works is you need to read into your computer memory all 300 million records in order to compute this derivative term.",
    "output": "このアルゴリズムのまわり方としては、この微分項を計算するのに3億のレコードを全てコンピュータのメモリに読み出す必要がある。"
  },
  {
    "index": "F18404",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You need to stream all of these records through computer because you can't store all your records in computer memory.",
    "output": "これらのレコード全てをコンピュータにストリーム処理しなくてはいけない。何故ならコンピュータメモリに全て保存する事は出来ないから。"
  },
  {
    "index": "F18405",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you need to read through them and slowly, you know, accumulate the sum in order to compute the derivative.",
    "output": "だからそれらを読んでいき、ゆっくりと和を蓄積していく事で、はじめて微分が計算出来る。"
  },
  {
    "index": "F18406",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then having done all that work, that allows you to take one step of gradient descent. And now you need to do the whole thing again.",
    "output": "そしてそれを全部終えたら、その結果最急降下法を1ステップだけ進める事が出来る、、、そしてまた全体をやりなおさなくてはいけない。"
  },
  {
    "index": "F18407",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, scan through all 300 million records, accumulate these sums.",
    "output": "つまり、3億のレコードを全部スキャンして、それらの和を蓄積していく。"
  },
  {
    "index": "F18408",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And having done all that work, you can take another little step using gradient descent.",
    "output": "その仕事を全て終えても、最急降下法のちょっとのステップがもう一歩進むだけ。"
  },
  {
    "index": "F18409",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then do that again.",
    "output": "そしてまた、同じ事をする。"
  },
  {
    "index": "F18410",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then you take yet a third step.",
    "output": "そしてまた三歩目が進める。などなど。"
  },
  {
    "index": "F18411",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so it's gonna take a long time in order to get the algorithm to converge.",
    "output": "つまり、アルゴリズムが収束するのに、凄い長い時間がかかる。"
  },
  {
    "index": "F18412",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast to Batch gradient descent, what we are going to do is come up with a different algorithm that doesn't need to look at all the training examples in every single iteration, but that needs to look at only a single training example in one iteration.",
    "output": "バッチ最急降下法と比較して、別のアルゴリズムを考え出していく、それは各イテレーションごとに全てのトレーニング手本を見なくて良く、一回のイテレーションでは一つのトレーニング手本単体だけを見れば良い。"
  },
  {
    "index": "F18413",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Before moving on to the new algorithm, here's just a Batch gradient descent algorithm written out again with that being the cost function and that being the update and of course this term here, that's used in the gradient descent rule, that is the partial derivative with respect to the parameters theta J of our optimization objective, J train of theta.",
    "output": "新しいアルゴリズムにうつる前に、ここにバッチ最急降下法のアルゴリズムを再掲しておこう、これがコスト関数で、これがアップデート、そしてここの項はもちろん最急降下法で用いる、偏微分項だ、パラメータシータjによる、我らが最適化の目的関数Jtrainのシータの。"
  },
  {
    "index": "F18414",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, let's look at the more efficient algorithm that scales better to large data sets.",
    "output": "さて、大規模なデータセットにもっと効率的にスケールするアルゴリズムを見てみよう。"
  },
  {
    "index": "F18415",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to work off the algorithms called Stochastic gradient descent, this vectors the cost function in a slightly different way then they define the cost of the parameter theta with respect to a training example x(i), y(i) to be equal to one half times the squared error that my hypothesis incurs on that example, x(i), y(i).",
    "output": "確率的最急降下法と呼ばれるアルゴリズムを用いる為に、このベクトル、コスト関数をちょっと違う形にして、トレーニング手本、x(i),y(i)に関するパラメータをシータとするコストを定義する、それはイコール、1/2掛ける、二乗誤差のx(i),y(i)に対して仮説が引き起こす分。"
  },
  {
    "index": "F18416",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this cost function term really measures how well is my hypothesis doing on a single example x(i), y(i).",
    "output": "つまりこのコスト関数の項は、私の仮説が、単体の手本、x(i)とy(i)に対してどれだけ良いかを実際に測っている。"
  },
  {
    "index": "F18417",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now you notice that the overall cost function j train can now be written in this equivalent form.",
    "output": "今、全体のコスト関数、Jtrainは、等価な形でこう書ける。"
  },
  {
    "index": "F18418",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So j train is just the average over my m training examples of the cost of my hypothesis on that example x(i), y(i).",
    "output": "つまりJtrainは、m個のトレーニング手本の仮説のコストの平均に過ぎない。"
  },
  {
    "index": "F18419",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Armed with this view of the cost function for linear regression, let me now write out what Stochastic gradient descent does.",
    "output": "線形回帰のコスト関数をこうやってみる見方を身につけた上で、確率的最急降下法が何をする物なのか、書き下してみよう。"
  },
  {
    "index": "F18420",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first step of Stochastic gradient descent is to randomly shuffle the data set.",
    "output": "確率的最急降下法の最初のステップは、データセットをランダムにシャッフルする。"
  },
  {
    "index": "F18421",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So by that I just mean randomly shuffle, or randomly reorder your m training examples.",
    "output": "ランダムにシャッフルする、という事の意味は、m個のトレーニング手本をランダムに並べ替える、という事。"
  },
  {
    "index": "F18422",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's sort of a standard pre-processing step, come back to this in a minute.",
    "output": "これは普通の前処理だ。後でこの件については考える。"
  },
  {
    "index": "F18423",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the main work of Stochastic gradient descent is then done in the following.",
    "output": "だが確率的最急降下法の主な部分は、その次に以下のように続く所だ。"
  },
  {
    "index": "F18424",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're going to repeat for i equals 1 through m. So we'll repeatedly scan through my training examples and perform the following update.",
    "output": "i=1からmまで、リピートする事の、、、つまりトレーニング手本を繰り返しスキャンして、以下のアップデートを実施する。"
  },
  {
    "index": "F18425",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Gonna update the parameter theta j as theta j minus alpha times h of x(i) minus y(i) times x(i)j.",
    "output": "パラメータ、シータjをシータj引くことのアルファ掛けるh(x(i))引くことのy(i)に掛けるx(i)j。"
  },
  {
    "index": "F18426",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we're going to do this update as usual for all values of j.",
    "output": "このアップデートをいつも通り、全てのjの値に対して行う。"
  },
  {
    "index": "F18427",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, you notice that this term over here is exactly what we had inside the summation for Batch gradient descent.",
    "output": "ここで、ここの項はバッチ最急降下法の和の中にある物と、完全に一致する事が分かるだろう。"
  },
  {
    "index": "F18428",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In fact, for those of you that are calculus is possible to show that that term here, that's this term here, is equal to the partial derivative with respect to my parameter theta j of the cost of the parameters theta on x(i), y(i).",
    "output": "実のところ、もし解析学が得意なら、このここの項は、costのシータとx(i)を、パラメータシータjで偏微分した物に等しい事が示せる。"
  },
  {
    "index": "F18429",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where cost is of course this thing that was defined previously.",
    "output": "ここでこのcostはもちろん、前に定義した物。"
  },
  {
    "index": "F18430",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just the wrap of the algorithm, let me close my curly braces over there.",
    "output": "このアルゴリズムのまとめとして、、、その前に中括弧を閉じておこう。"
  },
  {
    "index": "F18431",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what Stochastic gradient descent is doing is it is actually scanning through the training examples. And first it's gonna look at my first training example x(1), y(1).",
    "output": "さて、確率的最急降下法がやる事は、実際にトレーニング手本をスキャンして、そして最初にトレーニング手本の最初のx(1),y(1)を見る時、この最初の例だけを見て、最初の手本に関してだけのコストによる、最急降下法の小さな一ステップを実行する。"
  },
  {
    "index": "F18432",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then looking at only this first example, it's gonna take like a basically a little gradient descent step with respect to the cost of just this first training example.",
    "output": "言い換えると、最初の手本を見て、最初の手本のデータだけにもうちょっとだけフィットするように、パラメータを少しだけ変更する。"
  },
  {
    "index": "F18433",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in other words, we're going to look at the first example and modify the parameters a little bit to fit just the first training example a little bit better.",
    "output": "これを終えたら、この内側のforループの中で、次の二番目のトレーニング手本に進む。"
  },
  {
    "index": "F18434",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having done this inside this inner for-loop is then going to go on to the second training example.",
    "output": "そこで行う事は、またもう一歩、パラメーター空間内を進む事、つまりちょっとだけ良く二番目のトレーニング手本にフィットするようにパラメータを変更する。"
  },
  {
    "index": "F18435",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what it's going to do there is take another little step in parameter space, so modify the parameters just a little bit to try to fit just a second training example a little bit better.",
    "output": "それを終えたら、三番目のトレーニング手本に進む。"
  },
  {
    "index": "F18436",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having done that, is then going to go onto my third training example.",
    "output": "そして三番目のトーレニング手本にちょっとだけ良くフィットするように、パラメータを変更する。"
  },
  {
    "index": "F18437",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And modify the parameters to try to fit just the third training example a little bit better, and so on until you know, you get through the entire training set.",
    "output": "これをトレーニングセット全体に渡って行う。"
  },
  {
    "index": "F18438",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then this ultra repeat loop may cause it to take multiple passes over the entire training set.",
    "output": "そしてこの外側のループが、トレーニングセット全体を複数回繰り返させる。"
  },
  {
    "index": "F18439",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This view of Stochastic gradient descent also motivates why we wanted to start by randomly shuffling the data set.",
    "output": "この確率的最急降下法の見方は、データセットをランダムにシャッフルする事から始める理由も分かる。"
  },
  {
    "index": "F18440",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This doesn't show us that when we scan through the training site here, that we end up visiting the training examples in some sort of randomly sorted order.",
    "output": "もしシャッフルせずにトレーニングセットをここからスキャンして行ったら、むちゃくちゃにソートされてる順番にトレーニング手本を見ていく事になる、その順番はデータが最初からランダムに来たか、変な風にソートされているかに寄ってしまう。"
  },
  {
    "index": "F18441",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Depending on whether your data already came randomly sorted or whether it came originally sorted in some strange order, in practice this would just speed up the conversions to Stochastic gradient descent just a little bit.",
    "output": "実際的には、ランダムにソートする事は確率的最急降下法をちょっとだけスピードアップする、ちょっとだけね。"
  },
  {
    "index": "F18442",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in the interest of safety, it's usually better to randomly shuffle the data set if you aren't sure if it came to you in randomly sorted order.",
    "output": "だから念のため、それがランダムな並びと確信が持てる場合を除いて、普通はとりあえずデータセットをランダムにシャッフルしておく方が良い。"
  },
  {
    "index": "F18443",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But more importantly another view of Stochastic gradient descent is that it's a lot like descent but rather than wait to sum up these gradient terms over all m training examples, what we're doing is we're taking this gradient term using just one single training example and we're starting to make progress in improving the parameters already.",
    "output": "だがもっと重要な点として、確率的最急降下法のもう一つの見方として、それは、通常の最急降下法ととても似ているが、だがこれらの微分項を全てのmトレーニング手本に渡って足すのではなく、この微分項を単に一つのトレーニング手本に対してだけ取る、という事をしている、そしてそこで既にパラメータの改善を開始してしまう。"
  },
  {
    "index": "F18444",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So rather than, you know, waiting 'till taking a path through all 300,000 United States Census records, say, rather than needing to scan through all of the training examples before we can modify the parameters a little bit and make progress towards a global minimum. For Stochastic gradient descent instead we just need to look at a single training example and we're already starting to make progress in this case of parameters towards, moving the parameters towards the global minimum.",
    "output": "つまり、全てのアメリカ合衆国の国勢調査のレコード3億件をなめるのを待つのでは無く、パラメータをちょっとだけ改善してグローバル最小へとちょっとだけ歩を進める為に、全てのトレーニング手本をスキャンする事を必要とするのでは無く、確率的最急降下法では、手本は一つしか見る必要が無くて、この場合のパラメータの改善を既に始めてしまって良い、パラメータをグローバル最小へと進めるという。"
  },
  {
    "index": "F18445",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here's the algorithm written out again where the first step is to randomly shuffle the data and the second step is where the real work is done, where that's the update with respect to a single training example x(i), y(i).",
    "output": "これがアルゴリズムをふたたび書き下した物だ。最初のステップはデータをランダムにシャッフルする事で、二番目のステップは実際の仕事をする所だが、そこでは一つのトレーニング手本、x(i)とy(i)に関してのみでアップデートしてしまう。"
  },
  {
    "index": "F18446",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's see what this algorithm does to the parameters.",
    "output": "ではこのアルゴリズムがパラメータに何をしていくか、見てみよう。"
  },
  {
    "index": "F18447",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Previously, we saw that when we are using Batch gradient descent, that is the algorithm that looks at all the training examples in time, Batch gradient descent will tend to, you know, take a reasonably straight line trajectory to get to the global minimum like that.",
    "output": "前に、バッチ最急降下法を使っている時に、それは全てのトレーニング手本を一度に見る物だという事を見た。"
  },
  {
    "index": "F18448",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast with Stochastic gradient descent every iteration is going to be much faster because we don't need to sum up over all the training examples.",
    "output": "バッチ最急降下法はグローバル最小へと向かう、かなりまっすぐな軌跡を描く傾向になる。"
  },
  {
    "index": "F18449",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But every iteration is just trying to fit single training example better.",
    "output": "それに対して確率的最急降下法は、各イテレーションはもっと早い、何故なら全てのトレーニング手本を足し合わせる必要が無いからだが、しかし各イテレーションは一つのトレーニング手本に対してだけより良くフィットするように試みるだけなので、だからもし確率的最急降下法を始めると、あー、確率的最急降下法をこの点とかから始めたとしよう。"
  },
  {
    "index": "F18450",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if we were to start stochastic gradient descent, oh, let's start stochastic gradient descent at a point like that.",
    "output": "最初のイテレーションでは、この方向に進んだとする、二番目のイテレーションでは、うーん、二番目のイテレーションは偶然、ちょっとツイてなかったとしよう。そして実際には悪い方向にこんな感じでパラメータを進めてしまった。"
  },
  {
    "index": "F18451",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first iteration, you know, may take the parameters in that direction and maybe the second iteration looking at just the second example maybe just by chance, we get more unlucky and actually head in a bad direction with the parameters like that.",
    "output": "三度目のイテレーションでは、三番目のトレーニング手本に対してだけもっと良くフィットするようにパラメータを変更する。"
  },
  {
    "index": "F18452",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the third iteration where we tried to modify the parameters to fit just the third training examples better, maybe we'll end up heading in that direction.",
    "output": "そしてこんな方向に向かったとする。"
  },
  {
    "index": "F18453",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we'll look at the fourth training example and we will do that.",
    "output": "そして四番目のトレーニング手本を見て、同じ事をする。"
  },
  {
    "index": "F18454",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The fifth example, sixth example, 7th and so on.",
    "output": "5番目、6番目、7番目、などなど。"
  },
  {
    "index": "F18455",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as you run Stochastic gradient descent, what you find is that it will generally move the parameters in the direction of the global minimum, but not always.",
    "output": "そして確率的最急降下法を実行すると、こんな結果が見られる:だいたいはパラメータはグローバル最小の方向に向かうが、いつもそうだという訳では無い。"
  },
  {
    "index": "F18456",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so take some more random-looking, circuitous path to watch the global minimum.",
    "output": "つまりもっとデタラメに見える、遠回りの軌跡を通ってグローバル最小を探す。"
  },
  {
    "index": "F18457",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in fact as you run Stochastic gradient descent it doesn't actually converge in the same same sense as Batch gradient descent does and what it ends up doing is wandering around continuously in some region that's in some region close to the global minimum, but it doesn't just get to the global minimum and stay there.",
    "output": "そして実のところ、確率的最急降下法は、バッチ最急降下法がするような意味では収束しない。そして最終的には、それはグローバル最小のそばのある一定の範囲をうろちょろし続けるようになる。"
  },
  {
    "index": "F18458",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in practice this isn't a problem because, you know, so long as the parameters end up in some region there maybe it is pretty close to the global minimum.",
    "output": "だが現実的には、それはそんなに問題じゃない、何故なら、パラメータがグローバル最小のきわめてそばの領域に居続けるなら、パラメータは最終的にグローバル最小に極めて近いはずなので、それは仮説としてはとても良い物となるだろう。"
  },
  {
    "index": "F18459",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, as parameters end up pretty close to the global minimum, that will be a pretty good hypothesis and so usually running Stochastic gradient descent we get a parameter near the global minimum and that's good enough for, you know, essentially any, most practical purposes.",
    "output": "だから通常、確率的最急降下法を実行すると、グローバル最小のそばのパラメータを得る事になり、それは実質的にはほとんどの現実的な目的にとって十分に良い物だ。"
  },
  {
    "index": "F18460",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just one final detail.",
    "output": "最後に詳細を一つ。"
  },
  {
    "index": "F18461",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In Stochastic gradient descent, we had this outer loop repeat which says to do this inner loop multiple times.",
    "output": "確率的最急降下法においては、この内側のループを複数回実行するように指示する、外側のループがある。"
  },
  {
    "index": "F18462",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how many times do we repeat this outer loop?",
    "output": "では、何回外側のループは繰り返せば良い?"
  },
  {
    "index": "F18463",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Depending on the size of the training set, doing this loop just a single time may be enough.",
    "output": "トレーニングセットのサイズによっては、このループは一回で十分かもしれない。"
  },
  {
    "index": "F18464",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And up to, you know, maybe 10 times may be typical so we may end up repeating this inner loop anywhere from once to ten times.",
    "output": "典型的には、10回までのどこかって所かな。"
  },
  {
    "index": "F18465",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if we have a you know, truly massive data set like the this US census gave us that example that I've been talking about with 300 million examples, it is possible that by the time you've taken just a single pass through your training set.",
    "output": "つまりこの内側のループを1回から10回の間のどこかの回数実行すれば良い。"
  },
  {
    "index": "F18466",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, this is for i equals 1 through 300 million. It's possible that by the time you've taken a single pass through your data set you might already have a perfectly good hypothesis.",
    "output": "つまり、もし我らが、真に大量のデータセット、例えばこのUS国勢調査のような物で、3億の手本とかあるならば、トレーニングセットを1パスだけなめる頃には、つまりこのforでi=1から3億まで回せば、そのデータセットを1パスなめ終わる頃には、既に十分完璧な良い仮説に到達しているかもしれない。"
  },
  {
    "index": "F18467",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In which case, you know, this inner loop you might need to do only once if m is very, very large.",
    "output": "その場合は、この内側のループは一回だけ実行すれば良い。凄い凄い大きなmなら。"
  },
  {
    "index": "F18468",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But in general taking anywhere from 1 through 10 passes through your data set, you know, maybe fairly common.",
    "output": "だが一般的には、1と10の間の価数のパスだけデータセットをなめる。この辺が普通だ。"
  },
  {
    "index": "F18469",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But really it depends on the size of your training set.",
    "output": "でもそれは本当にトレーニングセットのサイズに依存した話だ。"
  },
  {
    "index": "F18470",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you contrast this to Batch gradient descent.",
    "output": "バッチ最急降下法と比較してみると、バッチ最急降下法だと、一つのパスでトレーニングセット全体をなめて、それだけやってたった一歩の最急降下法のステップしか進まない。"
  },
  {
    "index": "F18471",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "With Batch gradient descent, after taking a pass through your entire training set, you would have taken just one single gradient descent steps.",
    "output": "つまり最急降下法のこれらの小さなステップの、たった一歩のステップだけ。"
  },
  {
    "index": "F18472",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So one of these little baby steps of gradient descent where you just take one small gradient descent step and this is why Stochastic gradient descent can be much faster.",
    "output": "そしてこれが、確率的最急降下法がもっと早くなりうる理由だ。"
  },
  {
    "index": "F18473",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that was the Stochastic gradient descent algorithm.",
    "output": "以上が確率的最急降下法アルゴリズムだ。"
  },
  {
    "index": "F18474",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you implement it, hopefully that will allow you to scale up many of your learning algorithms to much bigger data sets and get much more performance that way.",
    "output": "そしてこれを実装すれば、あなたは多くの学習アルゴリズムをスケールアップして、よりビッグなデータセットに対して、もっと良いパフォーマンスが得られるように出来るだろう。"
  },
  {
    "index": "F18475",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, we talked about Stochastic gradient descent, and how that can be much faster than Batch gradient descent.",
    "output": "前回のビデオでは、確率的最急降下法について議論した、そしてそれがどうバッチ最急降下法に比べて早くなりうるのかも。"
  },
  {
    "index": "F18476",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, let's talk about another variation on these ideas is called Mini-batch gradient descent they can work sometimes even a bit faster than stochastic gradient descent.",
    "output": "このビデオでは、ミニバッチ最急降下法と呼ばれる、もう一つのこの種の変種について見ていこう。それは時には、確率的最急降下法よりもちょっと早くなる事すらある。"
  },
  {
    "index": "F18477",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In Batch gradient descent we will use all m examples in each generation.",
    "output": "まとめてしまうと、ここまで議論してきたアルゴリズムはバッチ最急降下法は、一回のイテレーションに全てのm個の手本をなめる物だった。"
  },
  {
    "index": "F18478",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in Stochastic gradient descent we will use a single example in each generation.",
    "output": "一方確率的最急降下法は一回のイテレーションで手本一つだけを見る。"
  },
  {
    "index": "F18479",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What Mini-batch gradient descent does is somewhere in between.",
    "output": "そしてミニバッチ最急降下法は、この間のどこかしらに位置する。"
  },
  {
    "index": "F18480",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Specifically, with this algorithm we're going to use b examples in each iteration where b is a parameter called the \"mini batch size\" so the idea is that this is somewhat in-between Batch gradient descent and Stochastic gradient descent.",
    "output": "具体的には、このアルゴリズムで各イテレーションでb個の手本をなめる、ここでbはミニバッチサイズと呼ばれるパラメータ。つまりこれは、バッチ最急降下法と確率的最急降下法の間あたりを狙うアイデアだ。"
  },
  {
    "index": "F18481",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is just like batch gradient descent, except that I'm going to use a much smaller batch size.",
    "output": "これはバッチ最急降下法みたいな物だが、もっと小さなバッチサイズを使う、っていう所が違う。"
  },
  {
    "index": "F18482",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "A typical choice for the value of b might be b equals 10, lets say, and a typical range really might be anywhere from b equals 2 up to b equals 100.",
    "output": "典型的なbの値の選択肢としては、10とかその辺で、典型的と言える範囲はだいたい2から100くらいの間かな。"
  },
  {
    "index": "F18483",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that will be a pretty typical range of values for the Mini-batch size.",
    "output": "以上がミニバッチサイズの典型的なサイズだ。"
  },
  {
    "index": "F18484",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the idea is that rather than using one example at a time or m examples at a time we will use b examples at a time.",
    "output": "そしてアイデアとしては、一度に手本を一つだけ使うのでもm個全部使うのでも無く、一度にb個だけの手本を使っていく、という物。"
  },
  {
    "index": "F18485",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For this example, let's say b equals 10. So we're going to get, the next 10 examples from my training set so that may be some set of examples xi, yi.",
    "output": "これをインフォーマルに書き下すと、まずb個の手本を取り出して、、、この例ではbを10としようか、すると次の10個の手本をトレーニングセットから取り出す。"
  },
  {
    "index": "F18486",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If it's 10 examples then the indexing will be up to x (i+9), y (i+9) so that's 10 examples altogether and then we'll perform essentially a gradient descent update using these 10 examples.",
    "output": "つまり、10個の手本の場合、インデックスはxi,yiからx(i+9),y(i+9)までの10個の手本の集合が得られる。つまり全部で10個の手本が得られて、それに対して本質的には最急降下法のアップデートを実行する。"
  },
  {
    "index": "F18487",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's any rate times one tenth times sum over k equals i through i+9 of h subscript theta of x(k) minus y(k) times x(k)j.",
    "output": "学習率掛ける1/10掛けるk=iからi+9までの和を取る事のhの下付き添字シータx(k)-y(i)掛けるx(k)のj。"
  },
  {
    "index": "F18488",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so in this expression, where summing the gradient terms over my ten examples.",
    "output": "この式では、微分項の和は10個の手本に渡って取られる。"
  },
  {
    "index": "F18489",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's number ten, that's, you know, my mini batch size and just i+9 again, the 9 comes from the choice of the parameter b, and then after this we will then increase, you know, i by tenth, we will go on to the next ten examples and then keep moving like this.",
    "output": "だからこれは10個で、これがバッチサイズで、i+9もまた、この9もパラメータbの選択した値から来てる。そしてこれを増やしていって、10番目のiまで来たら、そのあとは次の10個の手本に進む、それを以後続けて進めていく。"
  },
  {
    "index": "F18490",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to simplify the indexing for this one at the right top, I'm going to assume we have a mini-batch size of ten and a training set size of a thousand, what we're going to do is have this sort of form, for i equals 1 and that in 21's the stepping, in steps of 10 because we look at 10 examples at a time.",
    "output": "だからアルゴリズム全体を書きだすと、ここでのインデクシングをシンプルにする為に、ミニバッチサイズとして10を、トレーニングセットのサイズとして1000を想定すると、我らがやるのは、こんなforループ、fori=1,11,21...など、つまり10ずつのステップで進む、何故なら一度に10個の手本を見るから。"
  },
  {
    "index": "F18491",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we perform this sort of gradient descent update using ten examples at a time so this 10 and this i+9 those are consequence of having chosen my mini-batch to be ten.",
    "output": "そしてこの種の最急降下法アップデートを一度に10個の手本を用いて実行する。"
  },
  {
    "index": "F18492",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you know, this ultimate four-loop, this ends at 991 here because if I have 1000 training samples then I need 100 steps of size 10 in order to get through my training set.",
    "output": "そしてこの外側のforループは、991で終わってる、何故なら、もしトレーニング手本が1000個なら、サイズが10の100ステップが、トレーニングセットを全部なめるのに必要だからだ。"
  },
  {
    "index": "F18493",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is mini-batch gradient descent.",
    "output": "以上がミニバッチ最急降下法だ。"
  },
  {
    "index": "F18494",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Compared to batch gradient descent, this also allows us to make progress much faster.",
    "output": "バッチ最急降下法と比較すると、この手法もまた、進捗がより早くなる。"
  },
  {
    "index": "F18495",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we have again our running example of, you know, U.S.",
    "output": "その場合やるべき事は、まず最初の10個の手本を見て、パラメータシータを改善する為に歩を進める。"
  },
  {
    "index": "F18496",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Census data with 300 million training examples, then what we're saying is after looking at just the first 10 examples we can start to make progress in improving the parameters theta so we don't need to scan through the entire training set.",
    "output": "最初の10個の手本しか見る必要がない。"
  },
  {
    "index": "F18497",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We just need to look at the first 10 examples and this will start letting us make progress and then we can look at the second ten examples and modify the parameters a little bit again and so on.",
    "output": "そうすれば歩を進める事が出来て、そして次にまた、二番目の10個の手本を見る、そしてパラメータをちょっと改善する。"
  },
  {
    "index": "F18498",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that is why Mini-batch gradient descent can be faster than batch gradient descent.",
    "output": "つまり、こんな訳でミニバッチ最急降下法はバッチ最急降下法よりも早くなりうる。"
  },
  {
    "index": "F18499",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Namely, you can start making progress in modifying the parameters after looking at just ten examples rather than needing to wait 'till you've scan through every single training example of 300 million of them.",
    "output": "つまり、パラメータの改善を、たった10個の手本を見た後ですぐに始められる、各イテレーションで3億個の手本を一つ一つスキャンする必要がある代わりに。"
  },
  {
    "index": "F18500",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how about Mini-batch gradient descent versus Stochastic gradient descent.",
    "output": "では、確率的最急降下法と比べた場合ミニバッチ最急降下法はどうだろう?"
  },
  {
    "index": "F18501",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, why do we want to look at b examples at a time rather than look at just a single example at a time as the Stochastic gradient descent?",
    "output": "つまり、一度に一個では無くて、なんでb個の手本を見たい、と思うのだろうか?"
  },
  {
    "index": "F18502",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The answer is in vectorization.",
    "output": "その答えはベクトル化だ。"
  },
  {
    "index": "F18503",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, Mini-batch gradient descent is likely to outperform Stochastic gradient descent only if you have a good vectorized implementation.",
    "output": "具体的には、ミニバッチ最急降下法が確率的最急降下法を大きくアウトパフォームすると期待出来るのは良いベクトル化実装がある時だけだ。"
  },
  {
    "index": "F18504",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In that case, the sum over 10 examples can be performed in a more vectorized way which will allow you to partially parallelize your computation over the ten examples.",
    "output": "その場合、10個の手本に渡る和は、よりベクトル化した形で実行出来て、その方が10個の手本に対する計算が部分的に並列化されやすい。"
  },
  {
    "index": "F18505",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in other words, by using appropriate vectorization to compute the rest of the terms, you can sometimes partially use the good numerical algebra libraries and parallelize your gradient computations over the b examples, whereas if you were looking at just a single example of time with Stochastic gradient descent then, you know, just looking at one example at a time their isn't much to parallelize over.",
    "output": "言い換えると、微分項を計算する為に適切なベクトル化実装を使えば、時には部分的に良い数値計算代数ライブラリが使えて、b手本に渡る微分の計算を並列化出来る、一方で確率的最急降下法のように一度に一つの手本しか見ないと、一度に一つの手本しか見ないと、並列化するような物が無いまま終わってしまう。"
  },
  {
    "index": "F18506",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "At least there is less to parallelize over.",
    "output": "少なくとも、相対的によりちょっとしか並列化する物が無い。"
  },
  {
    "index": "F18507",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One disadvantage of Mini-batch gradient descent is that there is now this extra parameter b, the Mini-batch size which you may have to fiddle with, and which may therefore take time.",
    "output": "ミニバッチ最急降下法の欠点の一つには、追加のパラメータbという、ミニバッチサイズという物が増える事で、そこでも時間を浪費するはめになるかもしれない、つまりもっと時間を食うという事だ。"
  },
  {
    "index": "F18508",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you have a good vectorized implementation this can sometimes run even faster that Stochastic gradient descent.",
    "output": "だが良いベクトル化した実装があれば、確率的最急降下法ですらよりも、早く走る事もある。"
  },
  {
    "index": "F18509",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that was Mini-batch gradient descent which is an algorithm that in some sense does something that's somewhat in between what Stochastic gradient descent does and what Batch gradient descent does.",
    "output": "以上がミニバッチ最急降下法だ。これはある意味でバッチ最急降下法と確率的最急降下法の間をやるような物だ。"
  },
  {
    "index": "F18510",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you choose their reasonable value of b. I usually use b equals 10, but, you know, other values, anywhere from say 2 to 100, would be reasonably common.",
    "output": "そしてもしbにまともな値を選べば、、、私は普通b=10を使ってるが、それ以外の値でも、2から100の間あたりならこれもまともで良く使われている範囲と言える。"
  },
  {
    "index": "F18511",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we choose value of b and if you use a good vectorized implementation, sometimes it can be faster than both Stochastic gradient descent and faster than Batch gradient descent.",
    "output": "そしてbの値を選んで、良いベクトル化した実装を使えば、確率的最急降下法よりもバッチ最急降下法よりも早くなる場合がある。"
  },
  {
    "index": "F18512",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You now know about the stochastic gradient descent algorithm.",
    "output": "いまや、あなたは確率的最急降下法のアルゴリズムについて知った。"
  },
  {
    "index": "F18513",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But when you're running the algorithm, how do you make sure that it's completely debugged and is converging okay?",
    "output": "だがアルゴリズムを実行している時、あなたはどうやってバグが無い、とかちゃんと収束している、という事を確認すれば良いだろうか?"
  },
  {
    "index": "F18514",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Equally important, how do you tune the learning rate alpha with Stochastic Gradient Descent.",
    "output": "同じように重要な事として、どうやって確率的最急降下法においてどうやって学習率のアルファをチューンしたら良い?"
  },
  {
    "index": "F18515",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video we'll talk about some techniques for doing these things, for making sure it's converging and for picking the learning rate alpha.",
    "output": "このビデオでは、これらを行う幾つかのテクニックを紹介する、収束を確認する方法と学習率アルファを選ぶ方法。"
  },
  {
    "index": "F18516",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Back when we were using batch gradient descent, our standard way for making sure that gradient descent was converging was we would plot the optimization cost function as a function of the number of iterations.",
    "output": "バッチ最急降下法を使ってた頃を思い返すと、最急降下法が収束していたかを確認する標準的な方法は、最適化の目的関数の値を繰り返しの回数の関数としてプロットする事だった。"
  },
  {
    "index": "F18517",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that was the cost function and we would make sure that this cost function is decreasing on every iteration.",
    "output": "これがコスト関数で、このコスト関数が各イテレーションで減少している事を確認したい。"
  },
  {
    "index": "F18518",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When the training set sizes were small, we could do that because we could compute the sum pretty efficiently.",
    "output": "トレーニングサイズが小さい時はそれが出来た。何故なら和の計算がとても早く行えたからだ。"
  },
  {
    "index": "F18519",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But when you have a massive training set size then you don't want to have to pause your algorithm periodically.",
    "output": "だが大量のトレーニングセットのサイズがあると、定期的にアルゴリズムを止めて、、、確率的最急降下法を定期的に止めてこのコスト関数を計算したくは無い、何故なら、このコスト関数の計算には、トレーニングセットサイズ全体に渡る和を必要とするから。"
  },
  {
    "index": "F18520",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You don't want to have to pause stochastic gradient descent periodically in order to compute this cost function since it requires a sum of your entire training set size.",
    "output": "そもそもに確率的最急降下法のポイントは、全て、アルゴリズムの途中でトレーニングセット全体をスキャンする必要無しに、一つの手本を見ただけで歩を進める事が出来る、という物だった。"
  },
  {
    "index": "F18521",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the whole point of stochastic gradient was that you wanted to start to make progress after looking at just a single example without needing to occasionally scan through your entire training set right in the middle of the algorithm, just to compute things like the cost function of the entire training set.",
    "output": "コスト関数などを計算する為だけにトレーニングセット全体を見る、というような事無しに。"
  },
  {
    "index": "F18522",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for stochastic gradient descent, in order to check the algorithm is converging, here's what we can do instead.",
    "output": "だから確率的最急降下法においてアルゴリズムが収束しているのを確認する為にやる事としては、代わりにこんな事をやる。"
  },
  {
    "index": "F18523",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's take the definition of the cost that we had previously.",
    "output": "前に定義したコスト関数を使おう。"
  },
  {
    "index": "F18524",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the cost of the parameters theta with respect to a single training example is just one half of the square error on that training example.",
    "output": "単体のトレーニング手本に関するパラメータシータでのコストは、単にそのトレーニング手本における二乗誤差の半分だ。"
  },
  {
    "index": "F18525",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then, while stochastic gradient descent is learning, right before we train on a specific example. So, in stochastic gradient descent we're going to look at the examples xi, yi, in order, and then sort of take a little update with respect to this example.",
    "output": "そして、確率的最急降下法を学習させている間、ある特定のサンプルを学習させる直前、つまり確率的最急降下法で順番に見ていって、あるサンプルxi,yiをこれから見よう、という時、この次にはこのサンプルによるちょっとの更新を行う訳だ。"
  },
  {
    "index": "F18526",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, while the algorithm is looking at the example xi, yi, but before it has updated the parameters theta using that an example, let's compute the cost of that example.",
    "output": "以上が確率的最急降下法がやる事だが、つまりアルゴリズムが手本xi,yiを見ているが、まだパラメータシータをその手本を使ってアップデートしていない時の、その手本のコストを計算してみよう。"
  },
  {
    "index": "F18527",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just to say the same thing again, but using slightly different words.",
    "output": "同じ事をちょっとだけ異なる言葉で言い換えてみよう。"
  },
  {
    "index": "F18528",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "A stochastic gradient descent is scanning through our training set right before we have updated theta using a specific training example x(i) comma y(i) let's compute how well our hypothesis is doing on that training example.",
    "output": "確率的最急降下法がトレーニングセットをスキャンしていく訳だが、ある手本x(i),y(i)を使ってシータをアップデートする直前で、そのトレーニング手本に対し仮説がどれだけ良いかを計算してみよう。"
  },
  {
    "index": "F18529",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we want to do this before updating theta because if we've just updated theta using example, you know, that it might be doing better on that example than what would be representative.",
    "output": "これをシータをアップデートする前に行いたいのは、もしシータをその手本でアップデートした後では、その手本については代表的な値よりももっと良くなってしまうから。"
  },
  {
    "index": "F18530",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, in order to check for the convergence of stochastic gradient descent, what we can do is every, say, every thousand iterations, we can plot these costs that we've been computing in the previous step.",
    "output": "最後に、確率的最急降下法が収束しているかをチェックする為に出来る手段としては、各1000繰り返しごとにその前のステップで計算したこれらのコスト関数をプロットする、というのがある。"
  },
  {
    "index": "F18531",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can plot those costs average over, say, the last thousand examples processed by the algorithm.",
    "output": "アルゴリズムに処理された最後の1000手本に渡るコストの平均をプロット出来る。"
  },
  {
    "index": "F18532",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you do this, it kind of gives you a running estimate of how well the algorithm is doing.",
    "output": "これをやると、アルゴリズムがどれくらいうまく行ってるかのランニングでの推計が得られる。"
  },
  {
    "index": "F18533",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, in contrast to computing J With this other procedure, well, as part of stochastic gradient descent, it doesn't cost much to compute these costs as well right before updating to parameter theta.",
    "output": "Jtrainを定期的に計算するのと比べると、そちらはトレーニングセット全体をスキャンする必要があるが、この方法だと、確率的最急降下法の一部として、パラメータシータをアップデートする直前にこれらのコストを計算するのは、そんなに高くはつかない。"
  },
  {
    "index": "F18534",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And all we're doing is every thousand integrations or so, we just average the last 1,000 costs that we computed and plot that.",
    "output": "我らがやる事は、各1000イテレーションごととかに、そこまでに計算した最後の1000コストを平均して、それをプロットする。"
  },
  {
    "index": "F18535",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by looking at those plots, this will allow us to check if stochastic gradient descent is converging.",
    "output": "そのプロットを見る事で、確率的最急降下法が収束しているかをチェックする事が出来る。"
  },
  {
    "index": "F18536",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here are a few examples of what these plots might look like.",
    "output": "ここに、そのプロットがどんな風になりうるかの例がある。"
  },
  {
    "index": "F18537",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Suppose you have plotted the cost average over the last thousand examples, because these are averaged over just a thousand examples, they are going to be a little bit noisy and so, it may not decrease on every single iteration.",
    "output": "最後1000手本に渡るコストの平均をプロットしたとしよう、これは1000個だけの手本に渡る平均なので、ちょっとノイジーになるだろう、そして各イテレーションで必ず減少する、という訳でも無かろう。"
  },
  {
    "index": "F18538",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then if you get a figure that looks like this, So the plot is noisy because it's average over, you know, just a small subset, say a thousand training examples.",
    "output": "そしてこんな感じの図が得られたとすると、プロットはノイジーでしょう、何故なら小さなサブセット、1000個のトレーニング手本に渡ってだけの平均だから。"
  },
  {
    "index": "F18539",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you get a figure that looks like this, you know that would be a pretty decent run with the algorithm, maybe, where it looks like the cost has gone down and then this plateau that looks kind of flattened out, you know, starting from around that point.",
    "output": "で、もしこんな感じの図を得られたなら、これは結構良くアルゴリズムは実行されている、コストが下がっていって、その先である点から台地のように平坦になってる、こんな場合は。"
  },
  {
    "index": "F18540",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "look like, this is what your cost looks like then maybe your learning algorithm has converged.",
    "output": "コストがこんな感じの時は、学習アルゴリズムはきっと収束している。"
  },
  {
    "index": "F18541",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you want to try using a smaller learning rate, something you might see is that the algorithm may initially learn more slowly so the cost goes down more slowly.",
    "output": "もしもっと小さい学習率を用いて試したければ、その結果はこんな見た目で、アルゴリズムは最初はゆっくりと学習していく。だからコストはもっとゆっくりと下がっていく。"
  },
  {
    "index": "F18542",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But then eventually you have a smaller learning rate is actually possible for the algorithm to end up at a, maybe very slightly better solution.",
    "output": "だがやがてより小さい学習率だと、アルゴリズムは、たぶんちょっとだけ良い解となる。"
  },
  {
    "index": "F18543",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the red line may represent the behavior of stochastic gradient descent using a slower, using a smaller leaning rate.",
    "output": "赤い線で、よりゆっくりな、より小さい学習率を用いた時の確率的最急降下法の場合を表すとする。"
  },
  {
    "index": "F18544",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason this is the case is because, you remember, stochastic gradient descent doesn't just converge to the global minimum, is that what it does is the parameters will oscillate a bit around the global minimum.",
    "output": "この場合により良い解となる理由は、確率的最急降下法はグローバル最小に収束するのでは無く、グローバル最小の回りをちょっとだけ振動するのだった。"
  },
  {
    "index": "F18545",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so by using a smaller learning rate, you'll end up with smaller oscillations.",
    "output": "だからより小さい学習率を使う事で、最終的にはより小さな振幅にする事が出来る。"
  },
  {
    "index": "F18546",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And sometimes this little difference will be negligible and sometimes with a smaller than you can get a slightly better value for the parameters.",
    "output": "時にはこの小さな違いは無視出来る物だろう、時にはより小さい方がわずかに良いパラメータの値を得られるだろう。"
  },
  {
    "index": "F18547",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say you run stochastic gradient descent and you average over a thousand examples when plotting these costs.",
    "output": "確率的最急降下法を走らせて、これらの1000個の手本に渡ってコストを平均してプロットしたとして、こんな結果が得られる場合もある。"
  },
  {
    "index": "F18548",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then again, it kind of looks like it's converged.",
    "output": "この場合も、一種の収束しているように見える。"
  },
  {
    "index": "F18549",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you were to take this number, a thousand, and increase to averaging over 5 thousand examples. Then it's possible that you might get a smoother curve that looks more like this.",
    "output": "もしこの数字、1000を増やして5000手本に渡って平均をとれば、もっとスムースなカーブ、もっとこんな感じのが得られたと思われる。"
  },
  {
    "index": "F18550",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by averaging over, say 5,000 examples instead of 1,000, you might be able to get a smoother curve like this.",
    "output": "1000手本の代わりに5000手本に渡って平均を取るとすると、もっとスムースなカーブ、こんな感じの物が得られるだろう。"
  },
  {
    "index": "F18551",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so that's the effect of increasing the number of examples you average over.",
    "output": "それが平均を取る対象の手本の数を増やす効果だ。"
  },
  {
    "index": "F18552",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The disadvantage of making this too big of course is that now you get one date point only every 5,000 examples.",
    "output": "この値を大きくし過ぎた場合の欠点はもちろん、5000手本につき、たった一つの点しか得られないという事。"
  },
  {
    "index": "F18553",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so the feedback you get on how well your learning learning algorithm is doing is, sort of, maybe it's more delayed because you get one data point on your plot only every 5,000 examples rather than every 1,000 examples.",
    "output": "だからアルゴリズムがどの位良く動いているかのフィードバックを得るのが、より遅れる事になる。何故ならプロット上の1点を得る為に1000手本じゃなくて5000手本ごとになるからだ。"
  },
  {
    "index": "F18554",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Along a similar vein some times you may run a gradient descent and end up with a plot that looks like this.",
    "output": "同様に最急降下法を走らせると、こんなプロットが得られる事もある。"
  },
  {
    "index": "F18555",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And with a plot that looks like this, you know, it looks like the cost just is not decreasing at all.",
    "output": "このプロットでは、コストは全く減少してないように見える。"
  },
  {
    "index": "F18556",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It looks like the algorithm is just not learning.",
    "output": "アルゴリズムは全く学習していないように見える。"
  },
  {
    "index": "F18557",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's just, looks like this here a flat curve and the cost is just not decreasing.",
    "output": "ここはフラットなカーブで、コストは低下してないように見える。"
  },
  {
    "index": "F18558",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But again if you were to increase this to averaging over a larger number of examples it is possible that you see something like this red line it looks like the cost actually is decreasing, it's just that the blue line averaging over 2, 3 examples, the blue line was too noisy so you couldn't see the actual trend in the cost actually decreasing and possibly averaging over 5,000 examples instead of 1,000 may help.",
    "output": "青い線はあまりにもノイジーなので、実際のトレンド、コストが実際に低下しているというトレンドが見えない。そして1000の代わりに5000手本に渡って平均を取るという事が、助けになるかもしれない。"
  },
  {
    "index": "F18559",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of course we averaged over a larger number examples that we've averaged here over 5,000 examples, I'm just using a different color, it is also possible that you that see a learning curve ends up looking like this.",
    "output": "もちろん、大きな数の手本に渡って平均をとっても、ここでは5000手本に渡って平均をとってみたが、ここでは別の色を使ったが、その時に、こんな風に学習曲線がなる場合もありうる。"
  },
  {
    "index": "F18560",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That it's still flat even when you average over a larger number of examples.",
    "output": "大きな数の手本に渡って平均しても、フラットなままだ。"
  },
  {
    "index": "F18561",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And as you get that, then that's maybe just a more firm verification that unfortunately the algorithm just isn't learning much for whatever reason.",
    "output": "そしてそれが得られたら、それは不運にも何らかの理由で、アルゴリズムがあまり学習出来ていない、という事に、より固く確信を持てる。"
  },
  {
    "index": "F18562",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you need to either change the learning rate or change the features or change something else about the algorithm.",
    "output": "そして学習率を変えるなりフィーチャーを変えるなり、またはアルゴリズムに関しての何かを変えるなりをしなくてはならない。"
  },
  {
    "index": "F18563",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, one last thing that you might see would be if you were to plot these curves and you see a curve that looks like this, where it actually looks like it's increasing.",
    "output": "最後にもう一つ、これらの曲線をプロットしてみたら、そうしたらこんな曲線を得たとすると、つまり実際に増加しているようにみえたら、その時はそれはアルゴリズムが発散しているサインだ。"
  },
  {
    "index": "F18564",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you really should do is use a smaller value of the learning rate alpha.",
    "output": "その場合にすべき事は、より小さい値の学習率アルファを使う事だ。"
  },
  {
    "index": "F18565",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So hopefully this gives you a sense of the range of phenomena you might see when you plot these cost average over some range of examples as well as suggests the sorts of things you might try to do in response to seeing different plots.",
    "output": "以上で、ある範囲の手本に渡るコスト関数の平均をプロットした時に、どんな事が起こりうるのか、そのそれぞれのプロットごとのオススメの対応について、感じがつかめたかな。"
  },
  {
    "index": "F18566",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if the plots looks too noisy, or if it wiggles up and down too much, then try increasing the number of examples you're averaging over so you can see the overall trend in the plot better.",
    "output": "もしプロットがあまりにもノイジーに見えたら、またはくねくねあがったり下がったりしすぎているようなら、平均を取る手本の範囲を増やしてみてくれ、するとプロットの全体的なトレンドをより良く分かるようになるだろう。"
  },
  {
    "index": "F18567",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And if you see that the errors are actually increasing, the costs are actually increasing, try using a smaller value of alpha.",
    "output": "そして誤差が実際に増加していたら、コストが実際に増加していたら、より小さい値のアルファを試してみてくれ。"
  },
  {
    "index": "F18568",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, it's worth examining the issue of the learning rate just a little bit more.",
    "output": "最後に、学習率の問題についてもうちょっと良く見てみよう。"
  },
  {
    "index": "F18569",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We saw that when we run stochastic gradient descent, the algorithm will start here and sort of meander towards the minimum And then it won't really converge, and instead it'll wander around the minimum forever.",
    "output": "確率的最急降下法を走らせると、アルゴリズムはここから始まって、最小に向かってくねくねと歩くのを見た。そしてそれは実際には収束せずに、そのかわりに最小の付近を永遠にうろちょろし続ける。"
  },
  {
    "index": "F18570",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so you end up with a parameter value that is hopefully close to the global minimum that won't be exact at the global minimum.",
    "output": "つまり、最終的にはグローバル最小に近いパラメータが得られる事が期待出来るが、完全にグローバル最小に一致する訳では無い。"
  },
  {
    "index": "F18571",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In most typical implementations of stochastic gradient descent, the learning rate alpha is typically held constant.",
    "output": "もっとも典型的な確率的最急降下法の実装では、学習率アルファは定数のまま据え置くのが普通だ。"
  },
  {
    "index": "F18572",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so what you we end up is exactly a picture like this.",
    "output": "つまり最終的に得られるのはまさにこんな図となる。"
  },
  {
    "index": "F18573",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you want stochastic gradient descent to actually converge to the global minimum, there's one thing which you can do which is you can slowly decrease the learning rate alpha over time.",
    "output": "もし確率的最急降下法に実際にグローバル最小に収束してほしい、と思うなら、一つ考えられる手としては、学習率アルファを時間がたつにつれて徐々に下げていく、という物がある。"
  },
  {
    "index": "F18574",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, a pretty typical way of doing that would be to set alpha equals some constant 1 divided by iteration number plus constant 2.",
    "output": "割と良くやるのは、アルファをイコール、constant1割る事のイテレーション数+constant2、とかそんな数字にする。"
  },
  {
    "index": "F18575",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, iteration number is the number of iterations you've run of stochastic gradient descent, so it's really the number of training examples you've seen And const 1 and const 2 are additional parameters of the algorithm that you might have to play with a bit in order to get good performance.",
    "output": "イテレーション数というのは確率的最急降下法の何回目のイテレーションか、を表す物で、ようするにそこまで見たトレーニング手本の数だ。そしてconst1とconst2はアルゴリズムの追加のパラメータで、良いパフォーマンスを得る為にいじらなくてはいけないかもしれない物だ。"
  },
  {
    "index": "F18576",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One of the reasons people tend not to do this is because you end up needing to spend time playing with these 2 extra parameters, constant 1 and constant 2, and so this makes the algorithm more finicky.",
    "output": "この方法を人々があんまり取らない理由としては、これら二つの追加のパラメータ、constant1とconstant2を調整するのに時間を食われるからだ。"
  },
  {
    "index": "F18577",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, it's just more parameters able to fiddle with in order to make the algorithm work well.",
    "output": "そのせいでアルゴリズムが気難しくなる。つまりアルゴリズムがうまく行くように時間を浪費するハメになるパラメータが増えるのだ。"
  },
  {
    "index": "F18578",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you manage to tune the parameters well, then the picture you can get is that the algorithm will actually around towards the minimum, but as it gets closer because you're decreasing the learning rate the meanderings will get smaller and smaller until it pretty much just to the global minimum.",
    "output": "だがもしパラメータをいい感じにチューン出来たら、得られる図は、アルゴリズムが最初はふらつきつつ、最小に向かっていくが、だが近づくと、学習率もそれにつれてどんどん下がっていくので、ふらつきは小さくなり、グローバル最小に至るまで小さくなり続ける。"
  },
  {
    "index": "F18579",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I hope this makes sense, right?",
    "output": "これは納得出来るだろう。"
  },
  {
    "index": "F18580",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the reason this formula makes sense is because as the algorithm runs, the iteration number becomes large So alpha will slowly become small, and so you take smaller and smaller steps until it hopefully converges to the global minimum.",
    "output": "そしてこの式が納得出来る理由は、アルゴリズムが走るにつれて、イテレーション回数も大きくなっていくので、アルファはゆっくりと小さくなっていき、すると一歩一歩がどんどん小さくなっていき、それはグローバル最小に収束するまで小さくなり続ける。"
  },
  {
    "index": "F18581",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So If you do slowly decrease alpha to zero you can end up with a slightly better hypothesis.",
    "output": "つまり、アルファをゆっくりと0へと減少させていくと、最終的にはちょっとだけ良い仮説が得られる。"
  },
  {
    "index": "F18582",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But because of the extra work needed to fiddle with the constants and because frankly usually we're pretty happy with any parameter value that is, you know, pretty close to the global minimum.",
    "output": "だが、定数をいじるのにかかる余計な仕事と、さらにざっくばらんに言ってしまえばグローバル最小に近いんなら、どんなパラメータの値でもまったく幸せなので、典型的には、このアルファをゆっくり減少させる、という手続きは、普通はやらん。"
  },
  {
    "index": "F18583",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Typically this process of decreasing alpha slowly is usually not done and keeping the learning rate alpha constant is the more common application of stochastic gradient descent although you will see people use either version.",
    "output": "で、確率的最急降下法を適用する時には、アルファは定数のままにしておく方がもっと普通だ。どちらのバージョンを使う人も見かけはするけど。"
  },
  {
    "index": "F18584",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To summarize in this video we talk about a way for approximately monitoring how the stochastic gradient descent is doing in terms for optimizing the cost function.",
    "output": "まとめると、このビデオでは、確率的最急降下法がどうなってるかをコスト関数の観点から近似的にモニタリングする方法を議論した。"
  },
  {
    "index": "F18585",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is a method that does not require scanning over the entire training set periodically to compute the cost function on the entire training set.",
    "output": "これはコスト関数を計算する為に定期的にトレーニングセット全体をスキャンする必要が無くて、代わりに例えば最後の1000手本とかを見る手法だ。"
  },
  {
    "index": "F18586",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you can use this method both to make sure the stochastic gradient descent is okay and is converging or to use it to tune the learning rate alpha.",
    "output": "そしてこの手法は確率的最急降下法がうまく機能していて、収束している、という事を確認するのにも、学習率アルファをチューンするのにも用いる事が出来る。"
  },
  {
    "index": "F18587",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the last few videos, we talked about stochastic gradient descent, and, you know, other variations of the stochastic gradient descent algorithm, including those adaptations to online learning, but all of those algorithms could be run on one machine, or could be run on one computer.",
    "output": "ここ数回のビデオでは、確率的最急降下法についてや、その他の確率的最急降下法の変種についてーーオンライン学習アルゴリズムの適用などーー議論して来た。だがそれらは全て一つのマシン、または一つのコンピュータで実行出来る物だった。"
  },
  {
    "index": "F18588",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And some machine learning problems are just too big to run on one machine, sometimes maybe you just so much data you just don't ever want to run all that data through a single computer, no matter what algorithm you would use on that computer.",
    "output": "幾つかの機械学習の問題は一つのマシンで実行するにはあまりにも大きくて、時には単純にデータがあまりにも大きい為に一つのコンピュータで走らせてみようとは考えもしないような状況もあるかもしれない、そのコンピュータでどんなアルゴリズムを使うにせよ、だ。"
  },
  {
    "index": "F18589",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in this video I'd like to talk about different approach to large scale machine learning, called the map reduce approach.",
    "output": "そこでこのビデオでは、大規模機械学習における、異なるアプローチである所の、MapReduceアプローチと呼ばれる物を議論したい。"
  },
  {
    "index": "F18590",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And even though we have quite a few videos on stochastic gradient descent and we're going to spend relative less time on map reduce--don't judge the relative importance of map reduce versus the gradient descent based on the amount amount of time I spend on these ideas in particular.",
    "output": "我らは確率的最急降下法について結構たくさんのビデオを費やしMapReduceに関しては相対的にはちょっとしか扱わないが、それを持って確率的最急降下法に比べるとMapReduceはそんなに重要では無い、とは判断しないでもらいたい、このそれぞれのアイデアに関して私が費やす時間を元に判断するのは。"
  },
  {
    "index": "F18591",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Many people will say that map reduce is at least an equally important, and some would say an even more important idea compared to gradient descent, only it's relatively simpler to explain, which is why I'm going to spend less time on it, but using these ideas you might be able to scale learning algorithms to even far larger problems than is possible using stochastic gradient descent.",
    "output": "多くの人々が、MapReduceは少なくとも同程度には、そして人によってはもっと重要なアイデアだと言うだろう、確率的最急降下法と比較した時に。単にMapReduceは相対的には説明するのが簡単なだけ、その為にそれについてあんまり時間を使わないだけだ。"
  },
  {
    "index": "F18592",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Here's the idea.",
    "output": "そのアイデアとはこうだ。"
  },
  {
    "index": "F18593",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say we want to fit a linear regression model or a logistic regression model or some such, and let's start again with batch gradient descent, so that's our batch gradient descent learning rule.",
    "output": "例えば線形回帰とかロジスティック回帰とか、とにかくその辺のモデルにフィッティングしたい、としよう。ここではバッチ最急降下法から始める事としよう。"
  },
  {
    "index": "F18594",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And to keep the writing on this slide tractable, I'm going to assume throughout that we have m equals 400 examples.",
    "output": "これが我らのバッチ最急降下法のルールとなる。そしてこのスライドの記述が追いやすいように、ここではm=400の手本と仮定して進めていく事にする。"
  },
  {
    "index": "F18595",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of course, by our standards, in terms of large scale machine learning, you know m might be pretty small and so, this might be more commonly applied to problems, where you have maybe closer to 400 million examples, or some such, but just to make the writing on the slide simpler, I'm going to pretend we have 400 examples.",
    "output": "どちらかといえばより一般的な状況としては4億個とかそれに近い数字の方がより典型的な数字と言える。だが、スライドの記述をシンプルにする為に、我らの手持ちの手本が400個であるフリをしてみよう。"
  },
  {
    "index": "F18596",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in that case, the batch gradient descent learning rule has this 400 and the sum from i equals 1 through 400 through my 400 examples here, and if m is large, then this is a computationally expensive step.",
    "output": "その場合、バッチ最急降下法の学習ルールはこの400個に対して行われ、和をi=1から400までのこの400個の手本に渡って取る、もしmが大きければこれは、計算量的に高くつくステップとなる。"
  },
  {
    "index": "F18597",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, what the MapReduce idea does is the following, and I should say the map reduce idea is due to two researchers, Jeff Dean and Sanjay Gimawat.",
    "output": "そこでMapReduceのアイデアが行う事は以下のような事だ。ここでMapReduceのアイデアは二人の研究者による物だと言及しておくべきだろう、JeffDeanとSanjayGimawatだ。"
  },
  {
    "index": "F18598",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Jeff Dean, by the way, is one of the most legendary engineers in all of Silicon Valley and he kind of built a large fraction of the architectural infrastructure that all of Google runs on today.",
    "output": "ところでJeffDeanはシリコンバレー中でももっとも伝説的なエンジニアの一人で、今日Googleで動いているアーキテクチャ的なインフラのかなりの部分を創り上げた男だ。"
  },
  {
    "index": "F18599",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But here's the map reduce idea.",
    "output": "話を戻して、これがMapReduceというアイデアだ。"
  },
  {
    "index": "F18600",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's say I have some training set, if we want to denote by this box here of X Y pairs, where it's X1, Y1, down to my 400 examples, Xm, Ym.",
    "output": "あるトレーニングセットがあるとして、この箱でxyのペアを表すとして、これはx1,y1から400個の手本までxm,ymまで降りていく。"
  },
  {
    "index": "F18601",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, that's my training set with 400 training examples.",
    "output": "つまりこれがトレーニングセットで400個の手本がある。"
  },
  {
    "index": "F18602",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the MapReduce idea, one way to do, is split this training set in to different subsets.",
    "output": "MapReduceのアイデアでは、一つのやり方としては、このトレーニングセットを別々のサブセットに分割する。"
  },
  {
    "index": "F18603",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "assume for this example that I have 4 computers, or 4 machines to run in parallel on my training set, which is why I'm splitting this into 4 machines.",
    "output": "言い換えると4台のマシンがトレーニングセットに渡って並列に走る。そんな訳だから4台のマシンに分割した。"
  },
  {
    "index": "F18604",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have 10 machines or 100 machines, then you would split your training set into 10 pieces or 100 pieces or what have you.",
    "output": "もしあなたの手持ちが10台のマシンだったり100台のマシンなら、その場合はそれに応じてトレーニングセットを10個とか100個とか、持ってるマシンの台数に応じて分割する事になる。"
  },
  {
    "index": "F18605",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what the first of my 4 machines is to do, say, is use just the first one quarter of my training set--so use just the first 100 training examples.",
    "output": "そして4台のマシンの最初の一台がやるべき事は、トレーニングセットのうちの最初の1/4を用いてつまり最初の100個のトレーニング手本を用いて、具体的に言うと、それがやる事はこの和を見てくれ。"
  },
  {
    "index": "F18606",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, what it's going to do is look at this summation, and compute that summation for just the first 100 training examples.",
    "output": "そして最初の100個のトレーニング手本に対してこの和を計算する事だ。書きだしてみよう。"
  },
  {
    "index": "F18607",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let me write that up I'm going to compute a variable temp 1 to superscript 1 the first machine J equals sum from equals 1 through 100, and then I'm going to plug in exactly that term there--so I have X-theta, Xi, minus Yi times Xij, right?",
    "output": "これはイコール和を取る事の1から100までのここでここにある項を代入する。つまり、hシータのxi引く事のyi、掛けるxij。"
  },
  {
    "index": "F18608",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's just that gradient descent term up there.",
    "output": "つまりこれは単なるここの最急降下法の項だ。"
  },
  {
    "index": "F18609",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then similarly, I'm going to take the second quarter of my data and send it to my second machine, and my second machine will use training examples 101 through 200 and you will compute similar variables of a temp to j which is the same sum for index from examples 101 through 200.",
    "output": "それは同様の和をインデックスが101から200までに対するトレーニングセットに対し取った物だ。"
  },
  {
    "index": "F18610",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And similarly machines 3 and 4 will use the third quarter and the fourth quarter of my training set.",
    "output": "そして以下同様にマシン3と4はトレーニングセットの三番目の1/4と、四番目の1/4を使う事になる。"
  },
  {
    "index": "F18611",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So now each machine has to sum over 100 instead of over 400 examples and so has to do only a quarter of the work and thus presumably it could do it about four times as fast.",
    "output": "つまりいまや、各マシンは400に渡る和では無くて100手本に対する和だけとなる。つまりやらなくてはならない仕事が1/4となり、つまり4倍早く終えられる事が期待される。"
  },
  {
    "index": "F18612",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, after all these machines have done this work, I am going to take these temp variables and put them back together.",
    "output": "最後に、これら全てのマシンが仕事を終えたら、これらのtemp変数を取り出して一つに戻さないといけない。"
  },
  {
    "index": "F18613",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I take these variables and send them all to a You know centralized master server and what the master will do is combine these results together.",
    "output": "だからこれらの変数を中央のサーバーに送りつける。そしてマスターサーバーがやる事はこれらの結果を一つに結合する事。"
  },
  {
    "index": "F18614",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "and in particular, it will update my parameters theta j according to theta j gets updated as theta j minus Of the learning rate alpha times one over 400 times temp, 1, J, plus temp 2j plus temp 3j plus temp 4j and of course we have to do this separately for J equals 0.",
    "output": "具体的には、パラメータのシータjをシータjを、以下のように更新する:シータj引く事の学習率のアルファ掛ける事の1/400掛ける事のtemp(i)j足す事のtemp(2)j足す事のtemp(3)j足すことのtemp(4)j。"
  },
  {
    "index": "F18615",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, up to and within this number of features.",
    "output": "そしてもちろん、これをj=0からnまで、nはフィーチャーの数だが、それらのjに対してそれぞれ行わなくてはならない。"
  },
  {
    "index": "F18616",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So operating this equation into I hope it's clear.",
    "output": "こんな風に複数の等式に分割出来る。この式がやってる事は、完全にこれと同じだ。"
  },
  {
    "index": "F18617",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what this equation is doing is exactly the same is that when you have a centralized master server that takes the results, the ten one j the ten two j ten three j and ten four j and adds them up and so of course the sum of these four things.",
    "output": "中央のマスターサーバーがこれらの結果を受け取り、つまりtemp1j、temp2j、temp3j、temp4jと、それらを足し合わせるとつまり当然これら4つの和となる訳だ。いいかい?"
  },
  {
    "index": "F18618",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, that's just the sum of this, plus the sum of this, plus the sum of this, plus the sum of that, and those four things just add up to be equal to this sum that we're originally computing a batch stream descent.",
    "output": "これは単にこれ、足す、この和、足す、この和、足す、この和。そしてこれら4つの物を足しあわせると、この和と等しくなる。"
  },
  {
    "index": "F18619",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we have the alpha times 1 of 400, alpha times 1 of 100, and this is exactly equivalent to the batch gradient descent algorithm, only, instead of needing to sum over all four hundred training examples on just one machine, we can instead divide up the work load on four machines.",
    "output": "そしてこれは厳密にバッチ最急降下法と等価である。ただ、400個のトレーニング手本にたいして和を取らなくてはいけなかった代わりに、ワークロードを4つのマシンに分割出来るようになっている。"
  },
  {
    "index": "F18620",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here's what the general picture of the MapReduce technique looks like.",
    "output": "さて、これは、一般的なMapReduceのテクニックがどんなかを表した図だ。"
  },
  {
    "index": "F18621",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We have some training sets, and if we want to paralyze across four machines, we are going to take the training set and split it, you know, equally. Split it as evenly as we can into four subsets.",
    "output": "あるトレーニングセットがあって、それを4つのマシンに渡って並列化したいとすると、トレーニングセットを同じサイズに分割する、4つのサブセットに等しく分割する。"
  },
  {
    "index": "F18622",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then we are going to take the 4 subsets of the training data and send them to 4 different computers.",
    "output": "そして次に、この4つのトレーニングデータのサブセットを4つの別々のコンピュータに送る。"
  },
  {
    "index": "F18623",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And each of the 4 computers can compute a summation over just one quarter of the training set, and then finally take each of the computers takes the results, sends them to a centralized server, which then combines the results together.",
    "output": "そして4つのコンピュータはおのおの、1/4のトレーニングセットについてだけ和を計算する事が出来る。そして次に、最終的に各コンピュータの結果を取り出して、中央のサーバーに送る。"
  },
  {
    "index": "F18624",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, on the previous line in that example, the bulk of the work in gradient descent, was computing the sum from i equals 1 to 400 of something.",
    "output": "前のスライドの例では最急降下法の仕事の大半は、i=1から400までの何かの和を計算する事だった。"
  },
  {
    "index": "F18625",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So more generally, sum from i equals 1 to m of that formula for gradient descent.",
    "output": "より一般的に言うと、最急降下法の式のi=1からmまでの和。"
  },
  {
    "index": "F18626",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And now, because each of the four computers can do just a quarter of the work, potentially you can get up to a 4x speed up.",
    "output": "そしてここで、各4つのコンピュータはその仕事の1/4しか行わないので、潜在的には4倍のスピードアップの可能性がある。"
  },
  {
    "index": "F18627",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, if there were no network latencies and no costs of the network communications to send the data back and forth, you can potentially get up to a 4x speed up.",
    "output": "特に、もし仮にネットワークの遅延も無くデータをあちこちに送るのにネットワークのコミュニケーションのコストも存在しないとすると、4倍のスピードアップの可能性がある。"
  },
  {
    "index": "F18628",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Of course, in practice, because of network latencies, the overhead of combining the results afterwards and other factors, in practice you get slightly less than a 4x speedup.",
    "output": "もちろん、現実には、ネットワークのレイテンシもあるし、結果を結合するオーバーヘッドもあるし、その他のファクターもあるので、実際には4倍のスピードアップよりはわずかに少ないだろう。"
  },
  {
    "index": "F18629",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But, none the less, this sort of macro juice approach does offer us a way to process much larger data sets than is possible using a single computer.",
    "output": "だが、それにも関わらず、この種のMapReduceのアプローチは単体のコンピュータのみを使う事に比べるとより大きなデータセットを処理する方法を提供してくれるアプローチと言える。"
  },
  {
    "index": "F18630",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you are thinking of applying Map Reduce to some learning algorithm, in order to speed this up.",
    "output": "もしあなたがある学習アルゴリズムを複数のコンピュータに渡って計算を並列化する事でスピードアップをする為にMapReduceを適用することを検討している時は、自身に問うてみるべき鍵となる問いは、あなたの使おうとしている学習アルゴリズムはトレーニングセットに渡る和の形で表現出来るのか?"
  },
  {
    "index": "F18631",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By paralleling the computation over different computers, the key question to ask yourself is, can your learning algorithm be expressed as a summation over the training set?",
    "output": "という事だ。そして多くの学習アルゴリズムは実際にトレーニングセットに渡って関数の和を取る形に表現出来る事が分かっている。"
  },
  {
    "index": "F18632",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And it turns out that many learning algorithms can actually be expressed as computing sums of functions over the training set and the computational expense of running them on large data sets is because they need to sum over a very large training set.",
    "output": "そして大きなデータセットに対してそれらを走らせる時の計算量のコストはとても大きなトレーニングセットに渡って和を取る必要がある事に起因している。"
  },
  {
    "index": "F18633",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, whenever your learning algorithm can be expressed as a sum of the training set and whenever the bulk of the work of the learning algorithm can be expressed as the sum of the training set, then map reviews might a good candidate for scaling your learning algorithms through very, very good data sets.",
    "output": "だから、あなたの学習アルゴリズムがなんであれ、トレーニングセットに渡る和として表現する事が出来て、そして学習アルゴリズムの仕事の大部分がトレーニングセットに渡る和として表現出来れば、MapReduceはあなたの学習アルゴリズムをとても大きなデータセットに対してスケールさせてくれる為の、とても有力な候補となる。"
  },
  {
    "index": "F18634",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Lets just look at one more example.",
    "output": "もう一つ例を見てみよう。"
  },
  {
    "index": "F18635",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that we want to use one of the advanced optimization algorithm.",
    "output": "アドバンスドな最適化のアルゴリズムの一つを使いたいとしよう。"
  },
  {
    "index": "F18636",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, things like, you know, l, b, f, g, s constant gradient and so on, and let's say we want to train a logistic regression of the algorithm.",
    "output": "つまり、L-BFGSとかconjugategradientとかそういう奴。そして、ロジスティック回帰をそのアルゴリズムを使って訓練したいとする。"
  },
  {
    "index": "F18637",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For that, we need to compute two main quantities.",
    "output": "その為には、我らは二つの主な値を計算する必要がある。"
  },
  {
    "index": "F18638",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is for the advanced optimization algorithms like, you know, LPF and constant gradient.",
    "output": "一つめは、L-BFGSやconjugategradientのような最適化アルゴリズムに対して我らは最適化の目的関数である、コスト関数を計算するルーチンを渡してやらないといけない。"
  },
  {
    "index": "F18639",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We need to provide it a routine to compute the cost function of the optimization objective.",
    "output": "そしてロジスティック回帰においてはコスト関数はこんな感じの物をトレーニングセットに渡って和を取る物だった。"
  },
  {
    "index": "F18640",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so for logistic regression, you remember that a cost function has this sort of sum over the training set, and so if youre paralizing over ten machines, you would split up the training set onto ten machines and have each of the ten machines compute the sum of this quantity over just one tenth of the training data.",
    "output": "すると10台のマシンに渡って並列化したいなら、トレーニングセットを10分割して、10台のマシンに割り振り、そして10台の各マシンが、トレーニングデータの十分の一に渡ってこの量の和を計算する。"
  },
  {
    "index": "F18641",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then, the other thing that the advanced optimization algorithms need, is a routine to compute these partial derivative terms.",
    "output": "次に、他にアドバンスドな最適化アルゴリズムが必要としている物としては、これらの偏微分項を計算するルーチンだ。"
  },
  {
    "index": "F18642",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Once again, these derivative terms, for which it's a logistic regression, can be expressed as a sum over the training set, and so once again, similar to our earlier example, you would have each machine compute that summation over just some small fraction of your training data.",
    "output": "ふたたび、これらの微分項は、ロジスティック回帰なら、トレーニングセットの和として表現する事が出来て、だからふたたび、前の例と同様に、各マシンに、トレーニングデータの少しの部分ずつだけを計算させる事が出来る。"
  },
  {
    "index": "F18643",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, having computed all of these things, they could then send their results to a centralized server, which can then add up the partial sums.",
    "output": "そして最後に、これらを全て計算し終えたら、各マシンがその結果を中央のサーバーに送りつける。"
  },
  {
    "index": "F18644",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This corresponds to adding up those tenth i or tenth ij variables, which were computed locally on machine number i, and so the centralized server can sum these things up and get the overall cost function and get the overall partial derivative, which you can then pass through the advanced optimization algorithm.",
    "output": "これはこれらのtempi、あるいはtempij変数を足し合わせる事に対応する、ここでiはマシン番号iを表し、そのマシンローカルで計算された物。つまり中央のサーバーはこれらの物を足し合わせる事が出来て、それで全体のコスト関数を得る事が出来る、そこから全体の偏微分項が得られて、それをアドバンスドな最適化アルゴリズムに渡す事が出来る。"
  },
  {
    "index": "F18645",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, more broadly, by taking other learning algorithms and expressing them in sort of summation form or by expressing them in terms of computing sums of functions over the training set, you can use the MapReduce technique to parallelize other learning algorithms as well, and scale them to very large training sets.",
    "output": "より一般的に言うと、その他の学習アルゴリズムでも、それらを和の形に表現すれば、あるいはトレーニングセットに渡る関数の和の形に表現すれば、その他のアルゴリズムでもMapReduceのテクニックを用いて同様に並列化する事が出来る。そしてとても大きなトレーニングセットに対してスケールさせる事が出来る。"
  },
  {
    "index": "F18646",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, as one last comment, so far we have been discussing MapReduce algorithms as allowing you to parallelize over multiple computers, maybe multiple computers in a computer cluster or over multiple computers in the data center.",
    "output": "最後に、一つコメントを。ここまで我らはMapReduceのアルゴリズムを複数のコンピューターに渡って、またはコンピュータクラスタの複数のコンピュータ、またはデータセンターの複数のコンピュータに渡って並列化する為の物として議論してきた。"
  },
  {
    "index": "F18647",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that sometimes even if you have just a single computer, MapReduce can also be applicable.",
    "output": "だが時には、一つしかコンピュータが無くてもMapReduceが適応可能な場合がある事が分かっている。"
  },
  {
    "index": "F18648",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, on many single computers now, you can have multiple processing cores.",
    "output": "具体的には、こんにちの多くのコンピュータは、複数のプロセッサコアを持っている。"
  },
  {
    "index": "F18649",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You can have multiple CPUs, and within each CPU you can have multiple proc cores.",
    "output": "複数のCPUを持っている事があり得るし、各CPU内にも複数のプロセッサコアがある場合もある。"
  },
  {
    "index": "F18650",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have a large training set, what you can do if, say, you have a computer with 4 computing cores, what you can do is, even on a single computer you can split the training sets into pieces and send the training set to different cores within a single box, like within a single desktop computer or a single server and use MapReduce this way to divvy up work load.",
    "output": "もし大きなトレーニングセットがある時に、あなたがとれる手段としては、もしあなたの手元に4つの計算コアを持つコンピュータがあったとすると、それが一つのコンピュータでしか無かったとしても、トレーニングセットを複数のピースに分割して一つのマシンの中の別々のコアにトレーニングセットを送り込む、という事が出来る。一つのマシンとは一つのデスクトップコンピュータかもしれないし、一つのサーバーかもしれない。"
  },
  {
    "index": "F18651",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Each of the cores can then carry out the sum over, say, one quarter of your training set, and then they can take the partial sums and combine them, in order to get the summation over the entire training set.",
    "output": "そして各コアはトレーニングセットの1/4に渡る和を実行出来る。そして次にそれらの部分和を取ってきて結合する事が出来る、トレーニングセット全体に渡る和を得る為に。"
  },
  {
    "index": "F18652",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The advantage of thinking about MapReduce this way, as paralyzing over cause within a single machine, rather than parallelizing over multiple machines is that, this way you don't have to worry about network latency, because all the communication, all the sending of the back and forth, all that happens within a single machine.",
    "output": "MapReduceをこんな風に、一つのマシンの中の複数コアに対する並列化と考えるメリットは、複数のマシンに渡る並列化として考えることに比べて、ネットワークのレイテンシを気にする必要が無くなる、という事がある。何故なら全てのコミュニケーションは、tempj変数を行ったり来たり送る事は、それらは全て一つのマシン内で起こる事だから。"
  },
  {
    "index": "F18653",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so network latency becomes much less of an issue compared to if you were using this to over different computers within the data sensor.",
    "output": "だからデータセンター内の別々のマシンを使う事と比べると、ネットワークのレイテンシはより重要度が低くなる。"
  },
  {
    "index": "F18654",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, one last caveat on parallelizing within a multi-core machine.",
    "output": "最後にマルチコアのマシンでの並列化の落とし穴の最後の一つを挙げておこう。"
  },
  {
    "index": "F18655",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that the sum numerical linear algebra libraries that can automatically parallelize their linear algebra operations across multiple cores within the machine.",
    "output": "実装の詳細によっては、もしマルチコアのマシンがあって、もしある種の数値計算線形代数ライブラリがあるなら、幾つかの数値計算線形代数ライブラリは線形代数計算を自動的にマシン内の複数コアに並列化する物がある。"
  },
  {
    "index": "F18656",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you're fortunate enough to be using one of those numerical linear algebra libraries and certainly this does not apply to every single library.",
    "output": "つまり、それらの線形代数数値計算ライブラリの一つを使えるような程度の幸運に恵まれたなら、そしてこれは確かにどのライブラリでも適用出来るという訳でも無いが、もしあなたがそれらのライブラリの一つを使っていてそしてとても良いベクトル化した実装の学習アルゴリズムを用いているなら、ただ標準的な学習アルゴリズムをベクトル化した形で実装するだけで、そして並列化について思い煩う事無く、数値計算線形代数ライブラリがそのうちのいくらかをあなたの代わりに受け持ってくれる。"
  },
  {
    "index": "F18657",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Sometimes you can just implement you standard learning algorithm in a vectorized fashion and not worry about parallelization and numerical linear algebra libararies could take care of some of it for you.",
    "output": "だがそれ以外の学習問題では、この種のMapReduceの実装を有効利用する事により、このMapReduceの定式化を用いる事で明示的に複数コアに渡る並列化を自分自身で行う事もまた同様に良いアイデアだと思う事もあるだろう、そしてそれを用いてあなたの学習アルゴリズムを高速化する事が可能かもしれない。"
  },
  {
    "index": "F18658",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "for other any problems, taking advantage of this sort of map reducing commentation, finding and using this MapReduce formulation and to paralelize a cross coarse except yourself might be a good idea as well and could let you speed up your learning algorithm.",
    "output": "このビデオでは、機械学習を並列化するためのアプローチとしてMapReduceを議論する。"
  },
  {
    "index": "F18659",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video, we talked about the MapReduce approach to parallelizing machine learning by taking a data and spreading them across many computers in the data center.",
    "output": "データセンターのたくさんのマシンに対してデータをばらまくやり方だ。"
  },
  {
    "index": "F18660",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Although these ideas are critical to paralysing across multiple cores within a single computer as well.",
    "output": "だが、このアイデアは一つのマシン内の複数コアで並列化する為にも極めて重要な物である。"
  },
  {
    "index": "F18661",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Today there are some good open source implementations of MapReduce, so there are many users in open source system called Hadoop and using either your own implementation or using someone else's open source implementation, you can use these ideas to parallelize learning algorithms and get them to run on much larger data sets than is possible using just a single machine.",
    "output": "Hadoopと呼ばれるオープンソースのシステムにはたくさんのユーザーが居る。だから、自分の独自実装を使うにせよ誰かの作ったオープンソース実装を使うにせよ、これらのアイデアを用いて学習アルゴリズムを並列化する事が出来て、それらを一つのマシンだけを使う場合と比べたらより大きなデータセットに対して走らせる事が可能となる。"
  },
  {
    "index": "F18662",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this and the next few videos, I want to tell you about a machine learning application example, or a machine learning application history centered around an application called Photo OCR .",
    "output": "このビデオと続く幾つかのビデオで機械学習の応用例であるところのPhotoOCRの話、およびそのPhotoOCRにまつわる歴史の話をしていきたい。"
  },
  {
    "index": "F18663",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There are three reasons why I want to do this, first I wanted to show you an example of how a complex machine learning system can be put together.",
    "output": "それをやりたいと思う3つの理由がある。1つ目、複雑な機械学習のシステムがどのように組み合わせられるのかをお見せしたい。"
  },
  {
    "index": "F18664",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Second, once told the concepts of a machine learning a type line and how to allocate resources when you're trying to decide what to do next.",
    "output": "二つ目、機械学習パイプラインのコンセプトをお伝えし、次にやるべき事を決める時に、どうリソースを配分するかを話したい。"
  },
  {
    "index": "F18665",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this can either be in the context of you working by yourself on the big application Or it can be the context of a team of developers trying to build a complex application together.",
    "output": "これは一人で大きなアプリケーションに従事している場合でも、または複雑なアプリケーションをデベロッパのチームで一緒に作ろうという文脈もあり得る。"
  },
  {
    "index": "F18666",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then finally, the Photo OCR problem also gives me an excuse to tell you about just a couple more interesting ideas for machine learning.",
    "output": "そして最後に、PhotoOCRの問題は機械学習のより興味深い幾つかのアイデアについてお伝えする口実にもなる。"
  },
  {
    "index": "F18667",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One is some ideas of how to apply machine learning to computer vision problems, and second is the idea of artificial data synthesis, which we'll see in a couple of videos.",
    "output": "一つには機械学習をコンピュータの画像に適用するやり方だ。そして二番目に、人工的なデータ合成のアイデアだ。"
  },
  {
    "index": "F18668",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, let's start by talking about what is the Photo OCR problem.",
    "output": "ではPhotoOCRの問題とは何なのかを話すところから始めよう。"
  },
  {
    "index": "F18669",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Photo OCR stands for Photo Optical Character Recognition.",
    "output": "PhotoOCRはPhotoOpticalCharacterRecognitionの略だ。"
  },
  {
    "index": "F18670",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "With the growth of digital photography and more recently the growth of camera in our cell phones we now have tons of visual pictures that we take all over the place.",
    "output": "デジタル写真の分野が成長するに連れて、さらに最近では携帯にカメラがついた事により、我らはそこら中で撮った大量の画像写真を所持している。"
  },
  {
    "index": "F18671",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And one of the things that has interested many developers is how to get our computers to understand the content of these pictures a little bit better.",
    "output": "そして多くのデベロッパが興味を持っている事の一つに、これらの写真をコンピュータにもうちょっと良く理解させるにはどうしたら良いか、というのがある。"
  },
  {
    "index": "F18672",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The photo OCR problem focuses on how to get computers to read the text to the purest in images that we take.",
    "output": "PhotoOCR問題は撮った写真の中のテキストをコンピュータにどうやって読ませるか、という事にフォーカスする。"
  },
  {
    "index": "F18673",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given an image like this it might be nice if a computer can read the text in this image so that if you're trying to look for this picture again you type in the words, lulu bees and and have it automatically pull up this picture, so that you're not spending lots of time digging through your photo collection Maybe hundreds of thousands of pictures in.",
    "output": "例えばもしこの写真を後でもう一度探したい時にLULAB'sとかANTIQUEMALLとタイプしたら自動的にこの写真を取ってこれるように出来るだろう。そうすれば大量の時間を費やして何百とか何千とかのあなたの写真のコレクションから頑張ってひっくり返して探さなくて済むように出来るだろう。"
  },
  {
    "index": "F18674",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The Photo OCR problem does exactly this, and it does so in several steps.",
    "output": "PhotoOCRの問題はまさにこれをやる問題だ。そしてそれは複数のステップで行う。"
  },
  {
    "index": "F18675",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First, given the picture it has to look through the image and detect where there is text in the picture.",
    "output": "まず、与えられた写真に対し、画像を見ていって、写真の中のどこにテキストがあるかを検出する。"
  },
  {
    "index": "F18676",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And after it has done that or if it successfully does that it then has to look at these text regions and actually read the text in those regions, and hopefully if it reads it correctly, it'll come up with these transcriptions of what is the text that appears in the image.",
    "output": "そしてそれを終えた後で、あるいはそれが成功裡に終えられたら、次にこれらのテキストの領域を見て、その領域のテキストを実際に読む。それが正しく読めたら、画像の中に現れたテキストが何なのか、という転写が得られる。"
  },
  {
    "index": "F18677",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas OCR, or optical character recognition of scanned documents is relatively easier problem, doing OCR from photographs today is still a very difficult machine learning problem, and you can do this.",
    "output": "スキャンしたドキュメントのOCR、つまりOpticalCharacterRecognitionはより簡単な問題だが、写真のOCRはこんにちでもまだ、とても難しい機械学習の問題である。"
  },
  {
    "index": "F18678",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Not only can this help our computers to understand the content of our though images better, there are also applications like helping blind people, for example, if you could provide to a blind person a camera that can look at what's in front of them, and just tell them the words that my be on the street sign in front of them.",
    "output": "もしこれが出来れば、コンピュータに自動で画像の内容をより良く理解させる助けになるだけでなく、例えば盲目の人を助ける、というような応用例もある。もし盲目の人にカメラを持たせて、その前にある物が何なのかを見る事が出来るようにすると、その前にある道路の標識とかの標識が何なのかを教えてくれたり出来る。"
  },
  {
    "index": "F18679",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For example, imagine if your car could read the street signs and help you navigate to your destination.",
    "output": "あなたの車が道路の標識を読む事が出来て、目的地までのナビゲーションを助けてくれる事を想像してみてくれ。"
  },
  {
    "index": "F18680",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to perform photo OCR, here's what we can do.",
    "output": "PhotoOCRを実現する為には、こんな手段が可能だ。"
  },
  {
    "index": "F18681",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "First we can go through the image and find the regions where there's text and image.",
    "output": "まず最初に、画像を見ていって、画像内のどこにテキストがあるかを探す。"
  },
  {
    "index": "F18682",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, shown here is one example of text and image that the photo OCR system may find.",
    "output": "ここに見せたのは、PhotoOCRのシステムが見つけたテキストと画像の一例だ。"
  },
  {
    "index": "F18683",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Second, given the rectangle around that text region, we can then do character segmentation, where we might take this text box that says \"Antique Mall\" and try to segment it out into the locations of the individual characters.",
    "output": "次に、テキストの範囲のあたりの矩形を元に、次に文字分割を行う事が出来る。例えばこのANTIQUEMALLと書いてあるテキストの箱を取って、これを個々の文字の場所に分割する事を試みる。"
  },
  {
    "index": "F18684",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, having segmented out into individual characters, we can then run a crossfire, which looks at the images of the visual characters, and tries to figure out the first character's an A, the second character's an N, the third character is a T, and so on, so that up by doing all this how that hopefully you can then figure out that this phrase is Rulegee's antique mall and similarly for some of the other words that appear in that image.",
    "output": "そして最後に、個々の文字に分割された物を元に、分類器を実行する、そこでは画像の文字を見ていってそして書かれている文字の認識を試みる:最初の文字がAで、二番目の文字がN、三番目の文字がT、という具合に。そうして、これらを全て終えたら、その時はこのフレーズがLULAB'sANTIQUEMALLだと判明する事が期待出来る。"
  },
  {
    "index": "F18685",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I should say that there are some photo OCR systems that do even more complex things, like a bit of spelling correction at the end.",
    "output": "いくつかのPhotoOCRのシステムはこれよりさらに複雑な事もやる、という事は言っておくべきだろう。例えば最後にちょっとしたスペル修正を行ったりとか。"
  },
  {
    "index": "F18686",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then, you know, a sort of spelling correction system might tell you that this is probably the word 'cleaning', and your character classification algorithm had just mistaken the l for a 1.",
    "output": "例えば、文字分割と文字分類システムが、c1eaningという単語を見た、と言ってきたら、その時は、お分かりの通りある種のスペリング修正システムがそれは多分cleaningだ、と言う事だろう。"
  },
  {
    "index": "F18687",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But for the purpose of what we want to do in this video, let's ignore this last step and just focus on the system that does these three steps of text detection, character segmentation, and character classification.",
    "output": "だが、このビデオでやりたい目的の為に、この最後のステップを無視して、これら三つの事をするシステム、つまりテキスト検出、文字分割、そして文字分類に集中しよう。"
  },
  {
    "index": "F18688",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "A system like this is what we call a machine learning pipeline.",
    "output": "このようなシステムを機械学習パイプラインと呼んでいる。"
  },
  {
    "index": "F18689",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular, here's a picture showing the photo OCR pipeline.",
    "output": "特に、ここでお見せしているのは、フォトOCRパイプラインだ。"
  },
  {
    "index": "F18690",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We have an image, which then fed to the text detection system text regions, we then segment out the characters--the individual characters in the text--and then finally we recognize the individual characters.",
    "output": "画像があって、それをテキスト検出システムに食わせて、テキストの領域から、次に文字を分割する--テキスト内の各文字に--そして最後に個々の文字を認識する。"
  },
  {
    "index": "F18691",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In many complex machine learning systems, these sorts of pipelines are common, where you can have multiple modules--in this example, the text detection, character segmentation, character recognition modules--each of which may be machine learning component, or sometimes it may not be a machine learning component but to have a set of modules that act one after another on some piece of data in order to produce the output you want, which in the photo OCR example is to find the transcription of the text that appeared in the image.",
    "output": "多くの複雑な機械学習のシステムではこの種のパイプラインは一般的で、そこでは複数のモジュールがありうる--この例では、テキスト検出、文字分割、文字認識モジュール--それらはおのおの、機械学習のコンポーネントかもしれないし、時には機械学習のコンポーネントでは無い物もあるかもしれない。何にせよ、あるデータ片に対して次々と機能する一連のモジュールを用いる事で、望みの出力を生成する。"
  },
  {
    "index": "F18692",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you're designing a machine learning system one of the most important decisions will often be what exactly is the pipeline that you want to put together.",
    "output": "もしあなたが機械学習のシステムをデザインする事になったら、もっとも重要な決定の一つは、しばしば組み合わせるパイプラインは何であるか?"
  },
  {
    "index": "F18693",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In other words, given the photo OCR problem, how do you break this problem down into a sequence of different modules.",
    "output": "言い換えると、PhotoOCRの問題が与えられた時に、この問題をどう別々のモジュールへと分割するか、という事。"
  },
  {
    "index": "F18694",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you design the pipeline and each the performance of each of the modules in your pipeline.",
    "output": "そしてパイプラインを設計する。"
  },
  {
    "index": "F18695",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "will often have a big impact on the final performance of your algorithm.",
    "output": "するとパイプライン内の各モジュールのパフォーマンスはしばしば最終的なアルゴリズムのパフォーマンスにとても大きな影響を与える。"
  },
  {
    "index": "F18696",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you have a team of engineers working on a problem like this is also very common to have different individuals work on different modules.",
    "output": "このような問題に取り組んでいるエンジニアチームがある場合、それぞれのモジュールに対する作業を別々の人がするのもまた、とても一般的な事だ。"
  },
  {
    "index": "F18697",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I could easily imagine tech easily being the of anywhere from 1 to 5 engineers, character segmentation maybe another 1-5 engineers, and character recognition being another 1-5 engineers, and so having a pipeline like often offers a natural way to divide up the workload amongst different members of an engineering team, as well.",
    "output": "こんな風に簡単に想像出来る:文字列検出を1〜5人のエンジニアが文字分割を別の一人から5人程度のエンジニアが、文字認識をまた別の1〜5人のエンジニアが受け持つ、というような事を。だからこのようなパイプラインを持つ事は、エンジニアのチームのそれぞれのメンバーに仕事を分割する自然な区分を提供する。"
  },
  {
    "index": "F18698",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Although, or course, all of this work could also be done by just one person if that's how you want to do it.",
    "output": "もちろん、これらの仕事全てを一人で全部やったって構わないが、それがお望みならね。"
  },
  {
    "index": "F18699",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In complex machine learning systems the idea of a pipeline, of a machine of a pipeline, is pretty pervasive.",
    "output": "複雑な機械学習のシステムでは、パイプラインのアイデアは、機械のパイプラインというアイデアは、極めて広く行き渡っている。"
  },
  {
    "index": "F18700",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you just saw is a specific example of how a Photo OCR pipeline might work.",
    "output": "そして今見て来たのは、PhotoOCRパイプラインがどう機能するかという具体的な例だ。"
  },
  {
    "index": "F18701",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos I'll tell you a little bit more about this pipeline, and we'll continue to use this as an example to illustrate--I think--a few more key concepts of machine learning.",
    "output": "次の一連のビデオで、このパイプラインについてもうちょっと議論をしていく。そこでもこの例を用いて、機械学習のーー私が思うに重要なーーキーコンセプトの幾つかを例示していく。"
  },
  {
    "index": "F18702",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the previous video, we talked about the photo OCR pipeline and how that worked.",
    "output": "前回のビデオではPhotoOCRのパイプラインについてと、それがどう機能するかについて議論してきた。"
  },
  {
    "index": "F18703",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In which we would take an image and pass the Through a sequence of machine learning components in order to try to read the text that appears in an image.",
    "output": "それは画像をとり、それを一連の機械学習のコンポーネントを通過させて、画像の中にあるテキストを読み取ることを試みる、という物だった。"
  },
  {
    "index": "F18704",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video I like to. A little bit more about how the individual components of the pipeline works.",
    "output": "このビデオでは、個々のパイプラインのコンポーネントがどう機能するかについてもう少し議論していきたい。"
  },
  {
    "index": "F18705",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In particular most of this video will center around the discussion. of whats called a sliding windows.",
    "output": "具体的には、このビデオの大半を、スライディングウィンドウ分類器(classifier)と呼ばれる物に関する議論に費やしたいと思う。"
  },
  {
    "index": "F18706",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first stage of the filter was the Text detection where we look at an image like this and try to find the regions of text that appear in this image.",
    "output": "PhotoOCRのパイプラインの最初のステージはテキスト検出だった。そこではこんな画像を見ていってこの画像の中でテキストがある位置を見つける事を試みる。"
  },
  {
    "index": "F18707",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Text detection is an unusual problem in computer vision.",
    "output": "テキスト検出はコンピュータビジョンにとっては普通でない問題だ。"
  },
  {
    "index": "F18708",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Because depending on the length of the text you're trying to find, these rectangles that you're trying to find can have different aspect.",
    "output": "何故なら見つけたいテキストの長さに応じて見つけようとするこれらの矩形も異なるアスペクト比となるからだ。"
  },
  {
    "index": "F18709",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in order to talk about detecting things in images let's start with a simpler example of pedestrian detection and we'll then later go back to.",
    "output": "だから画像の中の物事を検出する話をする為に、もっと簡単な例である、歩行者の検出から始めよう。"
  },
  {
    "index": "F18710",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Ideas that were developed in pedestrian detection and apply them to text detection.",
    "output": "その後に話を戻して歩行者の検出で構築したアイデアをテキスト検出に用いよう。"
  },
  {
    "index": "F18711",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in pedestrian detection you want to take an image that looks like this and the whole idea is the individual pedestrians that appear in the image.",
    "output": "さて、歩行者検出においては、こんな画像をとり、画像の中に居る個々の歩行者を見つける事を試みる物だ。"
  },
  {
    "index": "F18712",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there's one pedestrian that we found, there's a second one, a third one a fourth one, a fifth one.",
    "output": "つまり歩行者が一人見つかり、ここに二人目、三人目、四人目、五人目、そして六人目、と。"
  },
  {
    "index": "F18713",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This problem is maybe slightly simpler than text detection just for the reason that the aspect ratio of most pedestrians are pretty similar.",
    "output": "この問題はテキスト検出の問題よりもちょっとだけ簡単だろう。何故ならほとんどの歩行者のアスペクト比はきわめて似通っているからだ。"
  },
  {
    "index": "F18714",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So by aspect ratio I mean the ratio between the height and the width of these rectangles.",
    "output": "我らが見つけようと試みている矩形に対し固定されたアスペクト比を用いる事で、ここでアスペクト比という言葉はこれらの矩形の高さと幅の比の事を言う。"
  },
  {
    "index": "F18715",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "They're all the same.",
    "output": "それらは別々の歩行者に対しても同じだ。"
  },
  {
    "index": "F18716",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "for different pedestrians but for text detection the height and width ratio is different for different lines of text Although for pedestrian detection, the pedestrians can be different distances away from the camera and so the height of these rectangles can be different depending on how far away they are.",
    "output": "だが、テキスト検出の場合は、異なる行のテキストで縦と横の比は異なる。歩行者の検出の場合は歩行者の居る場所までのカメラからの距離が異なる事はあり得るので、これらの矩形の高さは、どれだけ離れているかに応じて違いうるけれど。"
  },
  {
    "index": "F18717",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "but the aspect ratio is the same.",
    "output": "でもアスペクト比は同一だ。"
  },
  {
    "index": "F18718",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to build a pedestrian detection system here's how you can go about it.",
    "output": "歩行者検出システムを構築する為にはこんな方法が考えられる。このアスペクト比を82x36に標準化する事にしよう。"
  },
  {
    "index": "F18719",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that we decide to standardize on this aspect ratio of 82 by 36 and we could have chosen some rounded number like 80 by 40 or something, but 82 by 36 seems alright.",
    "output": "別に丸めた数字、例えば80x40とかにしても良いんだけど、82x36で特に問題無さそうなのでこれで行く。"
  },
  {
    "index": "F18720",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we would do is then go out and collect large training sets of positive and negative examples.",
    "output": "そして次にやるべき事は外に出て大量の陽性と陰性のトレーニングセットを集めてくる事だ。"
  },
  {
    "index": "F18721",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "On this slide I show 12 positive examples of y1 and 12 examples of y0.",
    "output": "このスライドには、y=1となる12の陽性の手本とy=0となる12の手本をお見せしている。"
  },
  {
    "index": "F18722",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In a more typical pedestrian detection application, we may have anywhere from a 1,000 training examples up to maybe 10,000 training examples, or even more if you can get even larger training sets.",
    "output": "もっと典型的な歩行者検出のアプリケーションでは1000トレーニング手本から1万トレーニング手本くらいまで、またはもっとトレーニングセットが集められる時はそれ以上の場合すらあるのが一般的だ。"
  },
  {
    "index": "F18723",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you can do, is then train in your network or some other learning algorithm to take this input, an MS patch of dimension 82 by 36, and to classify 'y' and to classify that image patch as either containing a pedestrian or not.",
    "output": "そしてその時にとれる手段としては、ニューラルネットワークか、それ以外の何らかの学習アルゴリズムを訓練して、この画像のパッチ、82x36の次元のパッチを入力として、それをyかどうか、を分類出来るようにする。"
  },
  {
    "index": "F18724",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this gives you a way of applying supervised learning in order to take an image patch can determine whether or not a pedestrian appears in that image capture.",
    "output": "つまりこれらの画像パッチに歩行者が含まれているかどうかを分類出来るようにする。以上のように、画像のパッチを受け取りそこに歩行者が居るかどうかを区別する為に教師あり学習を用いる事が出来る。"
  },
  {
    "index": "F18725",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, lets say we get a new image, a test set image like this and we want to try to find a pedestrian's picture image.",
    "output": "今、新しい画像を与えられたとして、こんなテストセットの画像だとして、写真の画像から歩行者を見つけたいとする。"
  },
  {
    "index": "F18726",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we would do is start by taking a rectangular patch of this image.",
    "output": "そこで我らがやる事はこの画像から矩形のパッチをとっていき、ここにあげたような感じで、これは82x36の画像のパッチとかで、そしてその画像のパッチを分類器に通してその画像パッチに歩行者が居るかどうかを決定する。"
  },
  {
    "index": "F18727",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Like that shown up here, so that's maybe a 82 X 36 patch of this image, and run that image patch through our classifier to determine whether or not there is a pedestrian in that image patch, and hopefully our classifier will return y equals 0 for that patch, since there is no pedestrian.",
    "output": "そしてこのパッチについては分類器がy=0を返すのを期待する。"
  },
  {
    "index": "F18728",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next, we then take that green rectangle and we slide it over a bit and then run that new image patch through our classifier to decide if there's a pedestrian there.",
    "output": "何故なら歩行者は居ないから。次に、その緑の矩形を少しだけスライドさせて、その後にその新しい画像パッチに対して分類器を走らせて、歩行者がそこに居るかを判定する。"
  },
  {
    "index": "F18729",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And having done that, we then slide the window further to the right and run that patch through the classifier again.",
    "output": "それを終えたら、その後はさらにウィンドウを右にスライドさせてそのパッチを分類器にふたたび通す。"
  },
  {
    "index": "F18730",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The amount by which you shift the rectangle over each time is a parameter, that's sometimes called the step size of the parameter, sometimes also called the slide parameter, and if you step this one pixel at a time.",
    "output": "矩形を一度にどれだけシフトさせるかはパラメータだ。それはステップサイズのパラメータと呼ばれる事もあるし、また、ストライドパラメータと呼ばれる事もある。"
  },
  {
    "index": "F18731",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you can use the step size or stride of 1, that usually performs best, that is more cost effective, and so using a step size of maybe 4 pixels at a time, or eight pixels at a time or some large number of pixels might be more common, since you're then moving the rectangle a little bit more each time.",
    "output": "そしてもし一度に1ピクセルしか動かさなければ、つまりステップ幅、あるいは歩幅1を使う事が出来れば、それは普通一番良い実行結果が得られるが、計算量的にはより高価となる。だからステップサイズで4ピクセルとか8ピクセルとか、またはそれより大きな適当なピクセルを用いるのがより一般的だ。"
  },
  {
    "index": "F18732",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, using this process, you continue stepping the rectangle over to the right a bit at a time and running each of these patches through a classifier, until eventually, as you slide this window over the different locations in the image, first starting with the first row and then we go further rows in the image, you would then run all of these different image patches at some step size or some stride through your classifier.",
    "output": "このプロセスを用いる事で、毎回ちょっとずつ矩形を右に動かしていく事で、そしてこれらの各パッチを分類器にかけていく事で、最終的に、、、このウィンドウを画像の異なる場所へとスライドし続けて最初は最初の行から始めてその後に画像のさらなる先の行へと進めていき、なんらかのステップサイズであるいはあるストライドのサイズでこれら別々の画像のパッチに対し実行していく。"
  },
  {
    "index": "F18733",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, that was a pretty small rectangle, that would only detect pedestrians of one specific size.",
    "output": "ここまでは、とても小さな矩形だった。これは一つの特定のサイズの歩行者しか検出出来ない。"
  },
  {
    "index": "F18734",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we do next is start to look at larger image patches.",
    "output": "次にやる事は、より大きな画像のパッチを見て、つまりより大きな画像のパッチを取って、ここに示したような、そしてふたたび同様に分類器を走らせる。"
  },
  {
    "index": "F18735",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So now let's take larger images patches, like those shown here and run those through the crossfire as well.",
    "output": "ところで、より大きな画像のパッチを取る、と言った時に私が実際に意味している事は、こんな画像パッチをとった時、実際にやる事は、この画像のパッチを取り、これを82x36に縮小する、という事。"
  },
  {
    "index": "F18736",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way when I say take a larger image patch, what I really mean is when you take an image patch like this, what you're really doing is taking that image patch, and resizing it down to 82 X 36, say.",
    "output": "つまりこのより大きなパッチを取り、それをより小さい画像にリサイズして、そしてその小さくした画像こそが、分類器に渡す物で、そこで歩行者がパッチにいないか決定する事を試みる。"
  },
  {
    "index": "F18737",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you take this larger patch and re-size it to be smaller image and then it would be the smaller size image that is what you would pass through your classifier to try and decide if there is a pedestrian in that patch.",
    "output": "そして最後に、さらに大きなスケールでこれを行う事が出来て、そのスライディングウィンドウを最後まで実行する。"
  },
  {
    "index": "F18738",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally you can do this at an even larger scales and run that side of Windows to the end And after this whole process hopefully your algorithm will detect whether theres pedestrian appears in the image, so thats how you train a the classifier, and then use a sliding windows classifier, or use a sliding windows detector in order to find pedestrians in the image.",
    "output": "そしてこれらのプロセスが全て終わったら、あなたのアルゴリズムはこの画像の中にこれらの歩行者が居るかを検出する事が期待出来る訳だ。以上が分類器を訓練する方法と、そしてスライディングウィンドウの分類器、またはスライディングウィンドウの検出器を用いて、画像の中の歩行者を探す方法だ。"
  },
  {
    "index": "F18739",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's have a turn to the text detection example and talk about that stage in our photo OCR pipeline, where our goal is to find the text regions in unit.",
    "output": "テキスト検出の例に立ち戻って、PhotoOCRパイプラインでのテキスト検出のステージについて議論しよう、そこでは我らの目標は画像内のテキストの領域を見つける事だ。"
  },
  {
    "index": "F18740",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "similar to pedestrian detection you can come up with a label training set with positive examples and negative examples with examples corresponding to regions where text appears.",
    "output": "歩行者の検出と同様に、テキストが現れる場所に対応した陽性と陰性の手本を作り出す事ができる。"
  },
  {
    "index": "F18741",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So instead of trying to detect pedestrians, we're now trying to detect texts.",
    "output": "つまり歩行者を検出する代わりに、今度はテキストを検出したい。"
  },
  {
    "index": "F18742",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so positive examples are going to be patches of images where there is text.",
    "output": "これで分類器をトレーニングし終えたら、それを新規の画像、テストセットの画像に適用出来る。"
  },
  {
    "index": "F18743",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And negative examples is going to be patches of images where there isn't text.",
    "output": "これは例として使ってきた画像だ。"
  },
  {
    "index": "F18744",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having trained this we can now apply it to a new image, into a test set image.",
    "output": "ここでは、この例においては、スライディングウィンドウをたった一つの固定されたスケールの物で実行する事にしよう。これは例示の為だ。"
  },
  {
    "index": "F18745",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's the image that we've been using as example.",
    "output": "だが、小さな、スライディングウィンドウの分類器を、たくさんのちいさな画像のパッチに対して実行するとしよう、こんな感じに。"
  },
  {
    "index": "F18746",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, last time we run, for this example we are going to run a sliding windows at just one fixed scale just for purpose of illustration, meaning that I'm going to use just one rectangle size.",
    "output": "そうすると、結局はこんな結果が得らる。"
  },
  {
    "index": "F18747",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But lets say I run my little sliding windows classifier on lots of little image patches like this if I do that, what Ill end up with is a result like this where the white region show where my text detection system has found text and so the axis' of these two figures are the same.",
    "output": "つまりこの領域はこの領域に対応している。そしてここが黒であるという事実は、分類器がここにテキストは見つからない、と思っている事を表している。"
  },
  {
    "index": "F18748",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there is a region up here, of course also a region up here, so the fact that this black up here represents that the classifier does not think it's found any texts up there, whereas the fact that there's a lot of white stuff here, that reflects that classifier thinks that it's found a bunch of texts.",
    "output": "一方でここにはたくさんの白い物があるという事実は、分類器がたくさんのテキストがここにある、と思っている事を反映している。"
  },
  {
    "index": "F18749",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "over there on the image. What i have done on this image on the lower left is actually use white to show where the classifier thinks it has found text.",
    "output": "この左下の絵で私がやった事は分類器がテキストを見つけた、と実際に思った場所を見せる為に、白を使ったという事だ。"
  },
  {
    "index": "F18750",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And different shades of grey correspond to the probability that was output by the classifier, so like the shades of grey corresponds to where it thinks it might have found text but has lower confidence the bright white response to whether the classifier, up with a very high probability, estimated probability of there being pedestrians in that location.",
    "output": "つまりグレイの影は分類器がテキストを発見したように思ってはいるが、だがそんなに自信は無い、と思っている所。明るい白は分類器がとても高い確率でテキストがある場所だ、と推計している場所に対応している。"
  },
  {
    "index": "F18751",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We aren't quite done yet because what we actually want to do is draw rectangles around all the region where this text in the image, so were going to take one more step which is we take the output of the classifier and apply to it what is called an expansion operator.",
    "output": "何故なら我らが望んでいるのは、画像の中のこのテキストの全体の回りの領域に対して四角で囲む、という事だからだ。だからさらにもう一段階ステップを踏む、それは分類器の出力をとり、そこにexpansionoperatorと呼ばれる物を適用する、という事をする。"
  },
  {
    "index": "F18752",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what that does is, it take the image here, and it takes each of the white blobs, it takes each of the white regions and it expands that white region.",
    "output": "それがやる事は、この画像に対して、それぞれの白のシミに対して、それぞれの白い領域に対して、その白の領域を拡大する、という事をする。"
  },
  {
    "index": "F18753",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Mathematically, the way you implement that is, if you look at the image on the right, what we're doing to create the image on the right is, for every pixel we are going to ask, is it withing some distance of a white pixel in the left image.",
    "output": "数学的には、それを実装する方法は、右の画像を見てみると、右の画像を作る為にやれる事としては、各ピクセルに対し、以下のように尋ねてみる事だ:このピクセルは、左の画像の白いピクセルから一定の距離以内にあるだろうか?と。"
  },
  {
    "index": "F18754",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, if a specific pixel is within, say, five pixels or ten pixels of a white pixel in the leftmost image, then we'll also color that pixel white in the rightmost image.",
    "output": "白いピクセルの5ピクセルとか10ピクセル以内にあるのなら、その時は一番右のそのピクセルも白に塗る。"
  },
  {
    "index": "F18755",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, the effect of this is, we'll take each of the white blobs in the leftmost image and expand them a bit, grow them a little bit, by seeing whether the nearby pixels, the white pixels, and then coloring those nearby pixels in white as well.",
    "output": "このような操作が与える効果は、一番左の画像の白いしみを取り出して、ちょっとだけ拡張したような物となる。"
  },
  {
    "index": "F18756",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, we are just about done.",
    "output": "最後に、これで終わりになるが、この一番右の画像を見ていき、くっついている構成要素を見ていき、つながった白い領域のバウンディングボックスをその回りに描く。"
  },
  {
    "index": "F18757",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We can now look at this right most image and just look at the connecting components and look at the as white regions and draw bounding boxes around them.",
    "output": "具体的には、これらの白い領域を全部見ていくとすると、例えばこれとか、これとか、これとか。"
  },
  {
    "index": "F18758",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in particular, if we look at all the white regions, like this one, this one, this one, and so on, and if we use a simple heuristic to rule out rectangles whose aspect ratios look funny because we know that boxes around text should be much wider than they are tall.",
    "output": "そして簡単な経験則でアスペクト比がおかしい、と思うような物を除外していくと、、、何故なら我らはテキストの回りの箱は高さよりも幅の方が大きいべきだという事を知っているから。だから痩せてて高い箱を無視していくと、例えばこれとかこれとか。"
  },
  {
    "index": "F18759",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so if we ignore the thin, tall blobs like this one and this one, and we discard these ones because they are too tall and thin, and we then draw a the rectangles around the ones whose aspect ratio thats a height to what ratio looks like for text regions, then we can draw rectangles, the bounding boxes around this text region, this text region, and that text region, corresponding to the Lula B's antique mall logo, the Lula B's, and this little open sign.",
    "output": "そしてアスペクト比がテキストの領域っぽい物の回りを囲んだ矩形を描く、ここでアスペクト比とは高さと幅の比の事。"
  },
  {
    "index": "F18760",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This example by the actually misses one piece of text.",
    "output": "所で、この例は実の所、一片のテキストを見逃している。"
  },
  {
    "index": "F18761",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is very hard to read, but there is actually one piece of text there.",
    "output": "これはとても読みにくいが、だがここには実際は一片のテキストがある。"
  },
  {
    "index": "F18762",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That says are corresponding to this but the aspect ratio looks wrong so we discarded that one.",
    "output": "ここにもLULAB'sがあって、それはこれに対応しているのだが、だがこのアスペクト比は間違いっぽいので、それを捨てたのだった。"
  },
  {
    "index": "F18763",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you know it's ok on this image, but in this particular example the classifier actually missed one piece of text.",
    "output": "つまり、この画像に関しては問題無さそうだが、この具体例においては分類器は実は一片のテキストを見逃している。"
  },
  {
    "index": "F18764",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It's very hard to read because there's a piece of text written against a transparent window.",
    "output": "それは凄い読みにくい、何故なら透明の窓に対して書かれたテキストだから。"
  },
  {
    "index": "F18765",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's text detection using sliding windows.",
    "output": "以上がスライディングウィンドウを用いたテキスト検出だ。"
  },
  {
    "index": "F18766",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And having found these rectangles with the text in it, we can now just cut out these image regions and then use later stages of pipeline to try to meet the texts.",
    "output": "テキストを囲むこれらの矩形を見つけたのちには、我らはこれらの画像の領域を切り抜いて、それをあとに続くパイプラインのステージでテキストを読む為に用いる事が出来る。"
  },
  {
    "index": "F18767",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now, you recall that the second stage of pipeline was character segmentation, so given an image like that shown on top, how do we segment out the individual characters in this image?",
    "output": "さて、パイプラインの二番目のステージは文字分割だったのを覚えているだろうか?つまり上に見せたような画像を与えられた時に、この画像の各文字に、どうやって分割出来るだろうか?"
  },
  {
    "index": "F18768",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what we can do is again use a supervised learning algorithm with some set of positive and some set of negative examples, what were going to do is look in the image patch and try to decide if there is split between two characters right in the middle of that image match.",
    "output": "ある陽性の手本の集合と、ある陰性の手本の集合を共に用いた。それでやる事は画像のパッチを見て、画像のパッチのちょうど真ん中に二つの文字の区切りがあるかどうかを決めたい。"
  },
  {
    "index": "F18769",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So for initial positive examples.",
    "output": "つまり、これらは陽性の手本だ。"
  },
  {
    "index": "F18770",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This first cross example, this image patch looks like the middle of it is indeed the middle has splits between two characters and the second example again this looks like a positive example, because if I split two characters by putting a line right down the middle, that's the right thing to do.",
    "output": "その画像の真ん中は、隙間、または二つの文字を分ける区切りで、他方、陰性の手本は、真ん中で二つの文字を分割したい、と思わないような物。"
  },
  {
    "index": "F18771",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, these are positive examples, where the middle of the image represents a gap or a split between two distinct characters, whereas the negative examples, well, you know, you don't want to split two characters right in the middle, and so these are negative examples because they don't represent the midpoint between two characters.",
    "output": "つまりこれらは陰性の手本だ、何故ならこれらは二つの文字の真ん中を表していないからだ。"
  },
  {
    "index": "F18772",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what we will do is, we will train a classifier, maybe using new network, maybe using a different learning algorithm, to try to classify between the positive and negative examples.",
    "output": "そこで我らが行う事は、分類器をトレーニングする事で、ニューラルネットワークを使ってもいいし、別のアルゴリズムでもいいが、とにかく陽性と陰性の手本を分類しようとするアルゴリズムをトレーニングする。"
  },
  {
    "index": "F18773",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having trained such a classifier, we can then run this on this sort of text that our text detection system has pulled out.",
    "output": "そんな分類器をトレーニングし終えたら、その後我らはこれをテキスト検出器が取り出したこんな画像に対して走らせる。"
  },
  {
    "index": "F18774",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As we start by looking at that rectangle, and we ask, \"Gee, does it look like the middle of that green rectangle, does it look like the midpoint between two characters?\".",
    "output": "この矩形から始めて、こう問う:緑の矩形の真ん中は、、、2つの文字の間っぽく見えるか?と。"
  },
  {
    "index": "F18775",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And hopefully, the classifier will say no, then we slide the window over and this is a one dimensional sliding window classifier, because were going to slide the window only in one straight line from left to right, theres no different rows here.",
    "output": "そしてウィンドウをスライドさせて、、、所でこれは1次元のスライディングウィンドウの分類器だ。何故ならウィンドウを一直線に左から右へとスライドさせるだけでここでは別の行、というのが無いから。"
  },
  {
    "index": "F18776",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "There's only one row here.",
    "output": "ここには一行しか無い。"
  },
  {
    "index": "F18777",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But now, with the classifier in this position, we ask, well, should we split those two characters or should we put a split right down the middle of this rectangle.",
    "output": "さて、ここで、分類器がこの場所に来た時に、二つの文字をここで分割すべきか、または二つの文字の区切りをこの矩形の真ん中に置くべきか?と問う。"
  },
  {
    "index": "F18778",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And hopefully, the classifier will output y equals one, in which case we will decide to draw a line down there, to try to split two characters.",
    "output": "そして期待される事としては、分類器はy=1と出力する事。つまりその場合はそこに線を引くと決定し、二つの文字を分割しようとする訳だ。"
  },
  {
    "index": "F18779",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then we slide the window over again, optic process, don't close the gap, slide over again, optic says yes, do split there and so on, and we slowly slide the classifier over to the right and hopefully it will classify this as another positive example and so on.",
    "output": "そして次にウィンドウをまたスライドさせて、この場合は分類器はここでは分割しない、と出力する事が期待されて、さらにまたスライドさせて、yes、ここでスプリットせよ、と言う事が期待される。"
  },
  {
    "index": "F18780",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we will slide this window over to the right, running the classifier at every step, and hopefully it will tell us, you know, what are the right locations to split these characters up into, just split this image up into individual characters.",
    "output": "そしてこのウィンドウを右側へとスライドさせていき、各ステップで分類器を実行し、そしてそれをもって我らにこれらの文字列を分割する適切な場所を教えてくれる事を期待する訳だ、つまりこの画像を個々の文字へと分割する。"
  },
  {
    "index": "F18781",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so thats 1D sliding windows for character segmentation.",
    "output": "以上が1Dスライディングウィンドウによる文字分割だ。"
  },
  {
    "index": "F18782",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, here's the overall photo OCR pipe line again.",
    "output": "ここにPhotoOCRパイプラインの全体像を再掲した。"
  },
  {
    "index": "F18783",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video we've talked about the text detection step, where we use sliding windows to detect text.",
    "output": "このビデオでは、テキスト検出のステップを議論して来た。そこでは、テキストを検出する為に、スライディングウィンドウを使った。"
  },
  {
    "index": "F18784",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And we also use a one-dimensional sliding windows to do character segmentation to segment out, you know, this text image in division of characters.",
    "output": "そしてまた、文字分割でも1次元のスライディングウィンドウを分割する為に使った。このテキストの画像を、各文字に分割する。"
  },
  {
    "index": "F18785",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The final step through the pipeline is the character qualification step and that step you might already be much more familiar with the early videos on supervised learning where you can apply a standard supervised learning within maybe on your network or maybe something else in order to take it's input, an image like that and classify which alphabet or which 26 characters A to Z, or maybe we should have 36 characters if you have the numerical digits as well, the multi class classification problem where you take it's input and image contained a character and decide what is the character that appears in that image?",
    "output": "パイプラインの最後のステップは文字分類ステップだ。そしてそのステップに関しては、以前の教師あり学習のところのビデオでやったので、既になじみの物だろう、そこでは、通常の教師あり学習の、例えばニューラルネットワークとかそれ以外のなんでも良いが、その辺を使って画像を入力として、こんなような、そしてどのアルファベットか、言い換えると26文字のaからzまでのどの文字か、数字も入れるなら36文字にすべきかもしれない。"
  },
  {
    "index": "F18786",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that was the photo OCR pipeline and how you can use ideas like sliding windows classifiers in order to put these different components to develop a photo OCR system.",
    "output": "以上がPhotoOCRのパイプラインだ。そしてどうやってスライディングウィンドウの分類器などのアイデアを、、、これら別々のコンポーネントを組み合わせてPhotoOCRシステムを開発するか、という話だ。"
  },
  {
    "index": "F18787",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In the next few videos we keep on using the problem of photo OCR to explore somewhat interesting issues surrounding building an application like this.",
    "output": "次の一連のビデオでは、引き続きPhotoOCRの問題を用いてこのようなアプリケーションを開発する時にまつわるいくらか興味深い問題を探求していく。"
  },
  {
    "index": "F18788",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I've seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias learning algorithm and to train it on a massive training set.",
    "output": "何度も見てきたが高いパフォーマンスの機械学習システムを得るもっとも信頼出来る方法の一つに、低バイアスの学習アルゴリズムに大量のトレーニングセットで訓練する、というのがある。"
  },
  {
    "index": "F18789",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But where did you get so much training data from?",
    "output": "だが、そんな大量のトレーニングデータをどこから得たら良いだろうか?"
  },
  {
    "index": "F18790",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Turns out that the machine earnings there's a fascinating idea called artificial data synthesis, this doesn't apply to every single problem, and to apply to a specific problem, often takes some thought and innovation and insight.",
    "output": "機械学習においては、人工データ合成、と呼ばれる魅力的なアイデアが考え出されている。このアイデアはどの問題でも使えるという訳では無いし特定の問題に適用する時にもなんらかの思索、イノベーション、そして洞察が必要となる事が多い。"
  },
  {
    "index": "F18791",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if this idea applies to your machine, only problem, it can sometimes be a an easy way to get a huge training set to give to your learning algorithm.",
    "output": "だがもしこのアイデアがあなたの機械学習の問題に適用出来たら、それはあなたの学習アルゴリズムに膨大なトレーニングセットを与える簡単な方法となる事がある。"
  },
  {
    "index": "F18792",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The idea of artificial data synthesis comprises of two variations, main the first is if we are essentially creating data from , creating new data from scratch.",
    "output": "人工データ合成は2つのバリエーションから構成されている。最初の主要なバリエーションは本質的には無からデータを作り出す、つまり新しいデータをスクラッチから作り出す、という物。"
  },
  {
    "index": "F18793",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the second is if we already have it's small label training set and we somehow have amplify that training set or use a small training set to turn that into a larger training set and in this video we'll go over both those ideas.",
    "output": "そして二番目は、もし既に少量のラベルつきトレーニングセットを持っていたら、そのトレーニングセットをどうにか増幅する。"
  },
  {
    "index": "F18794",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To talk about the artificial data synthesis idea, let's use the character portion of the photo OCR pipeline, we want to take it's input image and recognize what character it is.",
    "output": "人口データ合成のアイデアを議論する為にPhotoOCRパイプラインの中の文字認識の部分を例にとろう。入力の画像を取り、その文字が何なのかを認識する。"
  },
  {
    "index": "F18795",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If we go out and collect a large label data set, here's what it is and what it look like.",
    "output": "外に出て多くのラベルづけされたデータセットを収集してくると、こんな感じとなる。"
  },
  {
    "index": "F18796",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For this particular example, I've chosen a square aspect ratio.",
    "output": "この具体例に関しては、正方形のアスペクト比を選んだ。"
  },
  {
    "index": "F18797",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we're taking square image patches.",
    "output": "つまり正方形の画像パッチをとる。"
  },
  {
    "index": "F18798",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And the goal is to take an image patch and recognize the character in the middle of that image patch.",
    "output": "そして目標は、画像のパッチを取り、その画像パッチの真ん中にある文字を認識する事だ。"
  },
  {
    "index": "F18799",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for the sake of simplicity, I'm going to treat these images as grey scale images, rather than color images.",
    "output": "シンプルにする為に、これらの画像はカラー画像では無くグレースケール画像として扱う。"
  },
  {
    "index": "F18800",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It turns out that using color doesn't seem to help that much for this particular problem.",
    "output": "色を使ってもこの問題に関しては大して変わらない事が分かっている。"
  },
  {
    "index": "F18801",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So given this image patch, we'd like to recognize that that's a T.",
    "output": "さて、この画像パッチが与えられて、これがTだ、と認識したい。"
  },
  {
    "index": "F18802",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given this image patch, we'd like to recognize that it's an 'S'.",
    "output": "この画像パッチが与えられたらこれはSだと認識したい。"
  },
  {
    "index": "F18803",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Given that image patch we would like to recognize that as an 'I' and so on.",
    "output": "この画像パッチを与えられたら、これはIだと認識したい、などなど。"
  },
  {
    "index": "F18804",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So all of these, our examples of row images, how can we come up with a much larger training set?",
    "output": "つまりこれら全て、我らの生画像の手本に対し、どうやったらもっと多くのトレーニングセットが得られるか?"
  },
  {
    "index": "F18805",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Modern computers often have a huge font library and if you use a word processing software, depending on what word processor you use, you might have all of these fonts and many, many more Already stored inside.",
    "output": "最近のコンピュータなら、普通膨大なフォントのライブラリを持ってる物だ。もしワープロのソフトを使ってるならどのワープロを使っているかに応じてこれらのフォントを全て持ってるかもしれないし、さらにもっと多くの物が既に内部に保存されているだろう。"
  },
  {
    "index": "F18806",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And, in fact, if you go different websites, there are, again, huge, free font libraries on the internet we can download many, many different types of fonts, hundreds or perhaps thousands of different fonts.",
    "output": "そして実の所、いろいろなwebサイトにはそこにもまた、膨大なフリーのフォントのライブラリがインターネット上にはある。我らは様々な種類のフォント、それこそ何百とか何千ものフォントをダウンロード出来る。"
  },
  {
    "index": "F18807",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you want more training examples, one thing you can do is just take characters from different fonts and paste these characters against different random backgrounds.",
    "output": "だからもっと多くのトレーニング手本が欲しければ考えられる手としては一つには別々のフォントから文字を取り出して、別々のランダムの背景にペーストしていく、というのが考えられる。"
  },
  {
    "index": "F18808",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So you might take this ---- and paste that c against a random background.",
    "output": "つまりこれを取って、このCをランダムの背景にペーストする。"
  },
  {
    "index": "F18809",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you do that you now have a training example of an image of the character C.",
    "output": "そうすれば、文字Cの画像のトレーニングセットを得る事が出来る。"
  },
  {
    "index": "F18810",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So after some amount of work, you know this, and it is a little bit of work to synthisize realistic looking data. But after some amount of work, you can get a synthetic training set like that.",
    "output": "いくらかの仕事を行えば、本当っぽく見せる為のこれらの行程はちょっとした仕事ではあるが、だがこの幾らかの仕事を行ったあとには、こんな感じの合成されたトレーニングセットが得られる。"
  },
  {
    "index": "F18811",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Every image shown on the right was actually a synthesized image.",
    "output": "右側に示した各画像は全て実際に合成された画像だ。"
  },
  {
    "index": "F18812",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Where you take a font, maybe a random font downloaded off the web and you paste an image of one character or a few characters from that font against this other random background image.",
    "output": "前面にはフォント、例えばwebからダウンロードしたランダムのフォントなどから一文字か数文字を背景画像の上にペーストした物でその背景画像はそれぞれ別々のランダムの背景画像。"
  },
  {
    "index": "F18813",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then apply maybe a little blurring operators -----of app finder, distortions that app finder, meaning just the sharing and scaling and little rotation operations and if you do that you get a synthetic training set, on what the one shown here.",
    "output": "そしてそこに、ちょっとしたブラーの操作をしても良い---適当なアフィン変換で歪めたり、アフィン変換というのはシアーしたり拡大縮小したり、ちょっとだけ回転させたりといった操作の事だ。"
  },
  {
    "index": "F18814",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this is work, grade, it is, it takes thought at work, in order to make the synthetic data look realistic, and if you do a sloppy job in terms of how you create the synthetic data then it actually won't work well.",
    "output": "それを行うと、合成されたトレーニングセットが出来上がり、それはここに示したような物となる。"
  },
  {
    "index": "F18815",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you look at the synthetic data looks remarkably similar to the real data.",
    "output": "そしてこのやり方は、合成されたデータが本物っぽく見えるように真面目に考えて頑張ればかなりうまく機能する。"
  },
  {
    "index": "F18816",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so by using synthetic data you have essentially an unlimited supply of training examples for artificial training synthesis And so, if you use this source synthetic data, you have essentially unlimited supply of label data to create a improvised learning algorithm for the character recognition problem.",
    "output": "だがもし実際のデータと驚くほど似たデータを合成する事が出来たら、その合成されたデータを使う事で、人工トレーニングセット合成による実質的には無制限の量のトレーニング手本を供給出来る。つまり、この合成データを使う事でラベルデータの供給を実質無制限に行う事が出来、それを用いて文字認識の教師あり学習のアルゴリズムをトレーニング出来る。"
  },
  {
    "index": "F18817",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is an example of artificial data synthesis where youre basically creating new data from scratch, you just generating brand new images from scratch.",
    "output": "以上が人工データ合成の例だ。そこでは、基本的にはデータをスクラッチから作る、スクラッチから全く新しい画像を生成する。"
  },
  {
    "index": "F18818",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The other main approach to artificial data synthesis is where you take a examples that you currently have, that we take a real example, maybe from real image, and you create additional data, so as to amplify your training set.",
    "output": "これとは別の、人工データ合成の良くあるアプローチとしては、既に持っている手本を持ってきて、つまり本当の画像などの本当の手本を持ってきて、追加のデータを作成して、トレーニングセットを増幅する。"
  },
  {
    "index": "F18819",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here is an image of a compared to a from a real image, not a synthesized image, and I have overlayed this with the grid lines just for the purpose of illustration.",
    "output": "さてここに文字Aの画像がある、これは実際の画像から取ってきた物だ、合成された画像では無い。そして格子上の線を例示の為に重ねて表示している。"
  },
  {
    "index": "F18820",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what you can do is then take this alphabet here, take this image and introduce artificial warpings or artificial distortions into the image so they can take the image a and turn that into 16 new examples.",
    "output": "さて、ここで取れる手段として、このここにあるアルファベットを取り、この画像を取り、人工的なたわみ、あるいは人工的な歪みを画像に導入する。つまり画像を取り、そこから16個の新しい手本を生成する。"
  },
  {
    "index": "F18821",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in this way you can take a small label training set and amplify your training set to suddenly get a lot more examples, all of it.",
    "output": "つまりこうやって、少量のラベル付きトレーニングセットを持ってきてそれを一気に増幅してもっとたくさんの手本を、元のトレーニングセットから得る事が出来る。"
  },
  {
    "index": "F18822",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Again, in order to do this for application, it does take thought and it does take insight to figure out what our reasonable sets of distortions, or whether these are ways that amplify and multiply your training set, and for the specific example of character recognition, introducing these warping seems like a natural choice, but for a different learning machine application, there may be different the distortions that might make more sense.",
    "output": "繰り返しになるが、具体的なアプリケーションの為にこれを行うには、我らにとっての合理的な歪みとはどんな物か、トレーニングセットを増幅する合理的な方法はどんな物か、について、良く考えて、洞察を元に探す必要がある。この文字認識の具体例に関しては、これらのたわみを導入するのは、自然な選択に思える。"
  },
  {
    "index": "F18823",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let me just show one example from the totally different domain of speech recognition.",
    "output": "全く異なる分野の例として、音声認識の例を見てみよう。"
  },
  {
    "index": "F18824",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the speech recognition, let's say you have audio clips and you want to learn from the audio clip to recognize what were the words spoken in that clip.",
    "output": "音声認識とは、オーディオクリップがあったとして、オーディオクリップから、その中になんという単語が喋られているかを認識するように学習したい。"
  },
  {
    "index": "F18825",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's see how one labeled training example.",
    "output": "ラベル付きトレーニング手本の一つがどんな物か、見てみよう。"
  },
  {
    "index": "F18826",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's say you have one labeled training example, of someone saying a few specific words.",
    "output": "あなたは一つのラベル付き手本を持ってるとして、それは誰かが幾つかの特定の単語を喋ってる物だとしよう。"
  },
  {
    "index": "F18827",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's play that audio clip here.",
    "output": "そのオーディオクリップを再生してみる。"
  },
  {
    "index": "F18828",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Alright, so someone counting from 0 to 5, and so you want to try to apply a learning algorithm to try to recognize the words said in that.",
    "output": "つまり誰かが0から5まで数えているとする、そしてその中で言われている単語が何なのかを認識する為に学習アルゴリズムを適用したいとする。"
  },
  {
    "index": "F18829",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, how can we amplify the data set?",
    "output": "その時、どうやってデータセットを増幅するか?"
  },
  {
    "index": "F18830",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well, one thing we do is introduce additional audio distortions into the data set.",
    "output": "うーん、一つ考えられるのは、データセットに追加のオーディオ的な歪みを加えるという事。"
  },
  {
    "index": "F18831",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When you hear beeping sounds, that's actually part of the audio track, that's nothing wrong with the speakers, I'm going to play this now.",
    "output": "ビープ音が聞こえても、それは実際にオーディオトラックの一部だからスピーカーの故障じゃありません。では再生しよう。"
  },
  {
    "index": "F18832",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "0-1-2-3-4-5.",
    "output": "0-1-2-3-4-5。"
  },
  {
    "index": "F18833",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Right, so you can listen to that sort of audio clip and recognize the sounds, that seems like another useful training example to have, here's another example, noisy background.",
    "output": "この種のオーディオクリップもあなたは聞き取れて、音を認識出来るのだから、これもまた追加するに値するトレーニング手本に思える。もう一つ、また別の例で、うるさい背後の音がある例を聞いてみよう。"
  },
  {
    "index": "F18834",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Zero, one, two, three four five you know of cars driving past, people walking in the background, here's another one, so taking the original clean audio clip so taking the clean audio of someone saying 0 1 2 3 4 5 we can then automatically synthesize these additional training examples and thus amplify one training example into maybe four different training examples.",
    "output": "これもまた先ほどとは別の物になっている。元のクリーンなオーディオクリップを取ってきて、つまり誰かがクリアに言っている、0,1,2,3,4,5というオーディオを取ってきて、そして自動的にこれらの追加的なトレーニング手本を合成する事が出来、かくして一つのトレーニング手本を4つの別々のトレーニング手本に増幅出来る。"
  },
  {
    "index": "F18835",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let me play this final example, as well.",
    "output": "ではこの最後の例を再生してみよう。"
  },
  {
    "index": "F18836",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "0-1 3-4-5 So by taking just one labelled example, we have to go through the effort to collect just one labelled example fall of the 01205, and by synthesizing additional distortions, by introducing different background sounds, we've now multiplied this one example into many more examples.",
    "output": "つまり、一つのラベル付き手本を持ってくるだけで、一つのラベルつき手本を収集する労力を払うだけで、0,1,2,3,4,5と言っている手本を得るだけで、歪みを追加して合成する事によって、異なる背後の音を導入するだけで、いまやこの一つの手本を何倍ものたくさんの手本に増やす事が出来た、そんなにたくさんの仕事をせずに。"
  },
  {
    "index": "F18837",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Much work by just automatically adding these different background sounds to the clean audio Just one word of warning about synthesizing data by introducing distortions: if you try to do this yourself, the distortions you introduce should be representative the source of noises, or distortions, that you might see in the test set.",
    "output": "歪みを導入する事でデータを合成する事に関して一言警告をしておく。これを自分でやる時には、あなたが導入する歪みは、テストセットで見られそうなノイズ音源や歪みを代表しているべきだ。"
  },
  {
    "index": "F18838",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, for the character recognition example, you know, the working things begin introduced are actually kind of reasonable, because an image A that looks like that, that's, could be an image that we could actually see in a test set.Reflect a fact And, you know, that image on the upper-right, that could be an image that we could imagine seeing.",
    "output": "文字認識の例では、先ほど導入したこの類の歪みは実際にアリだと思われる物だ、何故なら画像のAはAっぽく見えるから。つまり実際にテストセットでも見そうな物だから。"
  },
  {
    "index": "F18839",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And for audio, well, we do wanna recognize speech, even against a bad self internal connection, against different types of background noise, and so for the audio, we're again synthesizing examples are actually representative of the sorts of examples that we want to classify, that we want to recognize correctly.",
    "output": "そしてオーディオの例に関しては、以下のような悪条件でも会話を認識したい:携帯の接続が悪かったり、異なる種類の背後のノイズの中だったり。だからオーディオの場合も、我らが合成した手本は、実際に分類したい、実際に正しく認識したい種類の物を本当に代表している。"
  },
  {
    "index": "F18840",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In contrast, usually it does not help perhaps you actually a meaning as noise to your data.",
    "output": "これとは逆に、無意味でランダムなノイズをデータに付加するのは、だいたいの場合にはたぶん役に立たないだろう。"
  },
  {
    "index": "F18841",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm not sure you can see this, but what we've done here is taken the image, and for each pixel, in each of these 4 images, has just added some random Gaussian noise to each pixel.",
    "output": "これを見ても分からないかもしれないが、これにやった事は、画像を持ってきて、各ピクセルに対し、これら4つの画像のそれぞれに、ランダムのガウス分布のノイズを各ピクセルに付加したのだ。"
  },
  {
    "index": "F18842",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To each pixel, is the pixel brightness, it would just add some, you know, maybe Gaussian random noise to each pixel.",
    "output": "各ピクセルに対し、ピクセルの明るさとかに、各ピクセルにガウス分布のランダムのノイズを付加する。"
  },
  {
    "index": "F18843",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it's just a totally meaningless noise, right?",
    "output": "つまり、それは完全に無意味なノイズだ。でしょ?"
  },
  {
    "index": "F18844",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so, unless you're expecting to see these sorts of pixel wise noise in your test set, this sort of purely random meaningless noise is less likely to be useful.",
    "output": "だから、テストセットにこの種のピクセル全体に渡るノイズを観測する場合があると想定出来る場合を除いては、この種の純粋にランダムで無意味なノイズは、あまり役に立たない。"
  },
  {
    "index": "F18845",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the process of artificial data synthesis it is you know a little bit of an art as well and sometimes you just have to try it and see if it works.",
    "output": "しかし、人工データ合成のプロセスにおいては、ちょっとしたアートも必要となるし、時には試してみてうまく行くかを見てみるというのも必要だ。"
  },
  {
    "index": "F18846",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But if you're trying to decide what sorts of distortions to add, you know, do think about what other meaningful distortions you might add that will cause you to generate additional training examples that are at least somewhat representative of the sorts of images you expect to see in your test sets.",
    "output": "だが、もしどの種の歪みを加えるかを決めようとしているなら、それ以外にどんな意味がありそうな歪みがありそうかを真面目に考える必要がある。その歪みは少なくとも幾らかはテストセットを代表した画像を生成すると、つまりテストセットで実際に見そうなものが得られそうな範囲で。"
  },
  {
    "index": "F18847",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Finally, to wrap up this video, I just wanna say a couple of words, more about this idea of getting loss of data via artificial data synthesis.",
    "output": "最後に、このビデオをまとめる為に人工データ合成からたくさんのデータを得るというアイデアについて2,3言っておきたい事がある。"
  },
  {
    "index": "F18848",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As always, before expending a lot of effort, you know, figuring out how to create artificial training examples, it's often a good practice is to make sure that you really have a low biased crossfire, and having a lot more training data will be of help.",
    "output": "毎度の事だが、人工的にトレーニングデータを作り出す方法を編み出す為にたくさんの労力を払う前に、本当に手持ちの分類器が低バイアスか、そしてより多くのトレーニングデータが本当に役に立つのかを確認しておくのは多くの場合で良い習慣だ。"
  },
  {
    "index": "F18849",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And standard way to do this is to plot the learning curves, and make sure that you only have a low as well, high variance falsifier.",
    "output": "そしてこれをやる標準的な方法は、学習曲線をプロットして、確かに低バイアスの分類器を持っていて、高バリアンスの分類器では無い事を確認する。"
  },
  {
    "index": "F18850",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or if you don't have a low bias falsifier, you know, one other thing that's worth trying is to keep increasing the number of features that your classifier has, increasing the number of hidden units in your network, saying, until you actually have a low bias falsifier, and only then, should you put the effort into creating a large, artificial training set, so what you really want to avoid is to, you know, spend a whole week or spend a few months figuring out how to get a great artificially synthesized data set.",
    "output": "または、もし低バイアスの分類器を持っていなければ、もう一つ試す価値のある事としては、分類器の持つフィーチャーの数を増やしてみる、というのがある。"
  },
  {
    "index": "F18851",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Only to realize afterward, that, you know, your learning algorithm, performance doesn't improve that much, even when you're given a huge training set.",
    "output": "本当に避けなくてはいけない事態はまるまる一週間とか何ヶ月も費やしてとても良い人工合成されたデータセットを作る方法を発見した後で、結局あなたの学習アルゴリズムのパフォーマンスは大量のトレーニングセットがあってもあまり改善しない、と判明する事だ。"
  },
  {
    "index": "F18852",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's about my usual advice about of a testing that you really can make use of a large training set before spending a lot of effort going out to get that large training set.",
    "output": "以上がいつも通りのアドバイスである、大量のデータを実際にあなたが有効活用出来るかのテストを、大量のトレーニングセットを収集する努力を費やす前に行え、という事だ。二番目。"
  },
  {
    "index": "F18853",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Second is, when i'm working on machine learning problems, one question I often ask the team I'm working with, often ask my students, which is, how much work would it be to get 10 times as much date as we currently had.",
    "output": "私が機械学習の問題の仕事をしている時に、一緒に働いているチームにしょっちゅう尋ねる質問は、しょっちゅう生徒に尋ねる質問は、現在持ってるデータセットの10倍を得るのにどれだけの仕事が必要だろうか?という物がある。"
  },
  {
    "index": "F18854",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When I face a new machine learning application very often I will sit down with a team and ask exactly this question, I've asked this question over and over and over and I've been very surprised how often this answer has been that.",
    "output": "私が新しい機械学習の適用の場に直面した時には、とてもよくチームと共に座ってまさにこの質問を尋ねる。この問いを何度も何度もなーんども尋ねてきた。"
  },
  {
    "index": "F18855",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You know, it's really not that hard, maybe a few days of work at most, to get ten times as much data as we currently have for a machine running application and very often if you can get ten times as much data there will be a way to make your algorithm do much better.",
    "output": "そして私はこの問いの答えが何度も以下のようであるかを知る事になり、しばしば驚く:実際はそんなに大変じゃなくて、せいぜい2〜3日の仕事で現在持っているデータの10倍のデータを得る事が出来て機械学習のアプリケーションに使う事が出来、そしてしょっちゅう、もし10倍のデータが得られたら、あなたのアルゴリズムがずっと良い仕事をする、という事が起こる。"
  },
  {
    "index": "F18856",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, you know, if you ever join the product team working on some machine learning application product this is a very good questions ask yourself ask the team don't be too surprised if after a few minutes of brainstorming if your team comes up with a way to get literally ten times this much data, in which case, I think you would be a hero to that team, because with 10 times as much data, I think you'll really get much better performance, just from learning from so much data.",
    "output": "だからもし何らかの機械学習のアプリケーションの仕事をしているチームに参加する時には、これは自問してみるのにとても良い問いであり、チームに尋ねてみるのにとても良い問いだ。"
  },
  {
    "index": "F18857",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So there are several waysand that comprised both the ideas of generating data from scratch using random fonts and so on.",
    "output": "そして数分のブレーンストーミングの後に文字通り10倍のデータを得る方法を考え出したとしても、それほど驚くべき事では無い。"
  },
  {
    "index": "F18858",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As well as the second idea of taking an existing example and and introducing distortions that amplify to enlarge the training set A couple of other examples of ways to get a lot more data are to collect the data or to label them yourself.",
    "output": "それは二つのアイデアから構成されている:一つ目は適当なフォントなどを使ってデータをスクラッチから作る方法で、二つ目は、すでに存在する手本を取ってきてそれに歪ませて、トレーニングセットをさらにたくさんに増幅する。その他の手段としては、自分でデータを収集してラベルづけしていく、という物。"
  },
  {
    "index": "F18859",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So one useful calculation that I often do is, you know, how many minutes, how many hours does it take to get a certain number of examples, so actually sit down and figure out, you know, suppose it takes me ten seconds to label one example then and, suppose that, for our application, currently we have 1000 labeled examples examples so ten times as much of that would be if n were equal to ten thousand.",
    "output": "だから私が良くやる有用な計算として、ある数の手本を集めるのに何分かかるか何時間かかるか、を考えてみるというのがある。実際に座って、考えてみる、1つの手本をラベルづけするのに10秒かかるとして、我らのアプリケーションは現在1000個のラベルづけされた手本があるとしてみよう。"
  },
  {
    "index": "F18860",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "A second way to get a lot of data is to just collect the data and you label it yourself.",
    "output": "二番目のデータをたくさん集める方法は、単にデータを収集して自分でラベルづけする事だった。"
  },
  {
    "index": "F18861",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So what I mean by this is I will often set down and do a calculation to figure out how much time, you know just like how many hours will it take, how many hours or how many days will it take for me or for someone else to just sit down and collect ten times as much data, as we have currently, by collecting the data ourselves and labeling them ourselves.",
    "output": "どれだけの時間が、何時間かかるか、何日かかるか、椅子に座って考えてみるのだ。現在持っているデータの10倍のデータを集めてきて、自分たちの手で集めてきて自分たちの手でラベル付けして行ったらどれだけかかるのか、椅子に座って考えてみるのだ。"
  },
  {
    "index": "F18862",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, for example, that, for our machine learning application, currently we have 1,000 examples, so M 1,000.",
    "output": "例えば、我らの機械学習のアプリケーションは現在、1000個の手本があるとする、つまりm=1000。"
  },
  {
    "index": "F18863",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That what we do is sit down and ask, how long does it take me really to collect and label one example.",
    "output": "そこで我らがやるべき事は、椅子に座って、こう問うてみる事だ:一つのラベルつき手本を集めるのには、実際どれだけの時間がかかるだろう?"
  },
  {
    "index": "F18864",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And sometimes maybe it will take you, you know ten seconds to label one new example, and so if I want 10 X as many examples, I'd do a calculation.",
    "output": "と。例えばそれは、10秒かかるとする、新しいラベル付き手本1つを得るのに。"
  },
  {
    "index": "F18865",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If it takes me 10 seconds to get one training example.",
    "output": "つまり、10倍の数の手本を得たいと思えば、計算を行ってみると、もし一つの手本に10秒かかり、10倍の数の手本を得たいと思えば、その場合は1万個の手本が必要となる。"
  },
  {
    "index": "F18866",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If I wanted to get 10 times as much data, then I need 10,000 examples.",
    "output": "人力で1万個のラベルつき手本を集めたらどれだけの時間がかかるか?を。"
  },
  {
    "index": "F18867",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So I do the calculation, how long is it gonna take to label, to manually label 10,000 examples, if it takes me 10 seconds to label 1 example.",
    "output": "1手本につき10秒かかるとすると、その場合にこの計算を行うと、割とよく、あなたがたは驚くことになる、いかにちょっとの仕事で済むのか、時にはほんの2、3日の仕事で、またある時にはほんの数日で済む事を知る事で。"
  },
  {
    "index": "F18868",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So when you do this calculation, often I've seen many you would be surprised, you know, how little, or sometimes a few days at work, sometimes a small number of days of work, well I've seen many teams be very surprised that sometimes how little work it could be, to just get a lot more data, and let that be a way to give your learning app to give you a huge boost in performance, and necessarily, you know, sometimes when you've just managed to do this, you will be a hero and whatever product development, whatever team you're working on, because this can be a great way to get much better performance.",
    "output": "そしてそれをあなたが成し遂げたら、あなたはどんな製品を開発していようと、どんなチームで働いていようと、きっとヒーローになる。"
  },
  {
    "index": "F18869",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Third and finally, one sometimes good way to get a lot of data is to use what's now called crowd sourcing.",
    "output": "三番目は、これが最後だが、データをたくさん集めるのに時には良い方法たりえる物として、クラウドソーシングと呼ばれる物がある。"
  },
  {
    "index": "F18870",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So today, there are a few websites or a few services that allow you to hire people on the web to, you know, fairly inexpensively label large training sets for you.",
    "output": "こんにちでは、幾つかのサービスであなたがかなり安い価格であたなの為にラベル付きトレーニングセットをたくさん集めてくれる人を雇わせてくれるサービスが存在している。"
  },
  {
    "index": "F18871",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this idea of crowd sourcing, or crowd sourced data labeling, is something that has, is obviously, like an entire academic literature, has some of it's own complications and so on, pertaining to labeler reliability.",
    "output": "クラウドソーシング、あるいはデータのラベル付のクラウドソースは、それはそれでいろいろと問題がある事もあるが、たとえばラベルが信頼できるか、とか。"
  },
  {
    "index": "F18872",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe, you know, hundreds of thousands of labelers, around the world, working fairly inexpensively to help label data for you, and that I've just had mentioned, there's this one alternative as well.",
    "output": "世界中の何百、何千ものラベル付けを手伝ってくれる人が比較的安くあなたのデータのラベルづけを手伝ってくれる訳なのだから。私が既に言及したように、そういう選択肢もまたありうる。"
  },
  {
    "index": "F18873",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And probably Amazon Mechanical Turk systems is probably the most popular crowd sourcing option right now.",
    "output": "そしてたぶん、AmazonのMechanicalTurkシステムが現時点ではもっとも人気のあるクラウドソースの選択肢だ。"
  },
  {
    "index": "F18874",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is often quite a bit of work to get to work, if you want to get very high quality labels, but is sometimes an option worth considering as well.",
    "output": "これはしばしば、ちゃんと機能させる為にはかなりの作業を必要とする。もし高いクオリティのラベルを得たいと思えば。"
  },
  {
    "index": "F18875",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If you want to try to hire many people, fairly inexpensively on the web, our labels launch miles of data for you.",
    "output": "もしラベル付けをあなたの為に比較的安価に行ってくれるような大量の人を、web上で探したい時には。"
  },
  {
    "index": "F18876",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this video, we talked about the idea of artificial data synthesis of either creating new data from scratch, looking, using the ramming funds as an example, or by amplifying an existing training set, by taking existing label examples and introducing distortions to it, to sort of create extra label examples.",
    "output": "その中でもスクラッチからデータ全体を作る方法:この例としては適当にフォントを持ってくる例を見た。それと、既に存在しているトレーニングセットを増幅する、という方法:既存のラベル付きトレーニング手本を持ってきてそこに歪みを導入し、そこから新しい手本を生成する、という方法を見た。"
  },
  {
    "index": "F18877",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, one thing that I hope you remember from this video this idea of if you are facing a machine learning problem, it is often worth doing two things.",
    "output": "そして最後に、このビデオからあなたに覚えておいて欲しい事としては、もしあなたが機械学習の問題に直面していたら、二つの事はしばしば試してみる価値がある。"
  },
  {
    "index": "F18878",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "One just a sanity check, with learning curves, that having more data would help.",
    "output": "一つは単純なサニティチェックを学習曲線で行い、もっと多くのデータが役に立つかを確認する事。"
  },
  {
    "index": "F18879",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And second, assuming that that's the case, I will often seat down and ask yourself seriously: what would it take to get ten times as much creative data as you currently have, and not always, but sometimes, you may be surprised by how easy that turns out to be, maybe a few days, a few weeks at work, and that can be a great way to give your learning algorithm a huge boost in performance",
    "output": "二つ目は、もっと多くのデータが役に立つ場合には、椅子に座って自分自身に真剣にこう問うてみる:現在持っているデータの10倍のデータを得るには、どれだけ時間がかかるか、を。そしていつもでは無いにしても、ときには、それがいかに簡単かが判明して驚く事になる。"
  },
  {
    "index": "F18880",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In earlier videos, I've said over and over that, when you're developing a machine learning system, one of the most valuable resources is your time as the developer, in terms of picking what to work on next.",
    "output": "以前のビデオで、私は以下の事を繰り返し言ってきた:機械学習システムを開発する時にもっとも貴重なリソースの一つはデベロッパとしてのあなたの時間だ、とーー次に作業すべき事を選ぶ時には。"
  },
  {
    "index": "F18881",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Again, one of the most valuable resources is the time of the engineers or the developers working on the system.",
    "output": "あるいは、あなたはデベロッパのチームなりエンジニアのチーム一丸となって機械学習のシステムを開発する時もまた、もっとも貴重なリソースの一つはそのシステムを開発しているエンジニアとかデベロッパの時間だ。"
  },
  {
    "index": "F18882",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what you really want to avoid is that you or your colleagues your friends spend a lot of time working on some component. Only to realize after weeks or months of time spent, that all that worked just doesn't make a huge difference on the performance of the final system.",
    "output": "そして本当に避けたい事として、あなたなり、あなたの同僚なりあなたの友人なりが、あるコンポーネントに対してたくさんの作業をした後で何週間とか何ヶ月とか時間を費やした後ではじめてそれらの作業全てが最終的なシステムのパフォーマンスには大した違いを生まない、と気づく、という事だ。"
  },
  {
    "index": "F18883",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this video what I'd like to do is something called ceiling analysis.",
    "output": "このビデオでは、シーリング(天井)分析と呼ばれる物について議論したい。"
  },
  {
    "index": "F18884",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "When you're the team working on the pipeline machine on your system, this can sometimes give you a very strong signal, a very strong guidance on what parts of the pipeline might be the best use of your time to work on.",
    "output": "あなた、あるいはあなたのチームが機械学習パイプラインのシステムの仕事をしている時には、この手法はパイプラインのどの部分を改善するのがもっとも良いのかについての強力なシグナル、あるいはガイダンスを提供してくれる事がある。"
  },
  {
    "index": "F18885",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "To talk about ceiling analysis I'm going to keep on using the example of the photo OCR pipeline.",
    "output": "シーリング分析の議論を行う為にここでもPhotoOCRの例を引き続き採用していく。"
  },
  {
    "index": "F18886",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And see right here each of these boxes, text detection, character segmentation, character recognition, each of these boxes can have even a small engineering team working on it. Or maybe the entire system is just built by you, either way.",
    "output": "前にも言った通りこれらの箱、テキスト検出、文字分割、そして文字認識、、、これらの各箱はそれぞれ小規模かチームが担当する場合もあるし、システム全体をあなた一人だけで構築することもあるだろう。"
  },
  {
    "index": "F18887",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the question is where should you allocate resources?",
    "output": "どちらにせよ、問題は、どこにリソースを割くべきか、という事だ。"
  },
  {
    "index": "F18888",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Which of these boxes is most worth your effort of trying to improve the performance of.",
    "output": "これらの箱のうちどれがパフォーマンスを改善しよう、と労力を払うのにもっとも価値がある物だろうか?"
  },
  {
    "index": "F18889",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In order to explain the idea of ceiling analysis, I'm going to keep using the example of our photo OCR pipeline.",
    "output": "シーリング分析のアイデアを説明する為に、このPhotoOCRパイプラインの例を使い続けていく。"
  },
  {
    "index": "F18890",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As I mentioned earlier, each of these boxes here, each of these machines and components could be the work of a small team of engineers, or the whole system could be built by just one person.",
    "output": "以前述べたように、これらの各箱は、これらの各機械学習のコンポーネントはエンジニアの小さなチームでそれぞれ従事しても良いし、またはシステム全体を一人の人間が見ても良い。"
  },
  {
    "index": "F18891",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But the question is, where should you allocate scarce resources?",
    "output": "だが何にせよ問題は、いったいどこに貴重なリソースを割り振るべきだろうか?"
  },
  {
    "index": "F18892",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That is, which of these components, which one or two or maybe all three of these components is most worth your time, to try to improve the performance of.",
    "output": "この場合だと、これらのコンポーネントの一つ目か、二つ目か、または三つ目に時間を使うのがパフォーマンスを改善するのにもっとも有益か?"
  },
  {
    "index": "F18893",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So here's the idea of ceiling analysis.",
    "output": "シーリング分析とはこんな物だ。"
  },
  {
    "index": "F18894",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "As in the development process for other machine learning systems as well, in order to make decisions on what to do for developing the system is going to be very helpful to have a single rolled number evaluation metric for this learning system.",
    "output": "これ以外の機械学習の問題と同様、機械学習の開発の過程において、システムを開発する時の様々な決断を行う為には、学習システムに関する単一で実数の評価指標があるととても役に立つ。"
  },
  {
    "index": "F18895",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's say we pick character level accuracy.",
    "output": "例えば文字レベルでの正確さを選んだとしよう。"
  },
  {
    "index": "F18896",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So if you're given a test set image, what is the fraction of alphabets or characters in a test image that we recognize correctly?",
    "output": "つまり、、、あるテストセットの画像が与えられた時に、テストセットの画像内にある文字をどれだけの割合で正しく認識出来たのか、の割合。"
  },
  {
    "index": "F18897",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or you can pick some other single road number evaluation that you could, if you want.",
    "output": "別にこれ以外の単一の実数の評価指標を選んでも良い。お望みならね。"
  },
  {
    "index": "F18898",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But let's say for whatever evaluation measure we pick, we find that the overall system currently has 72% accuracy.",
    "output": "だがどんな評価指標を使うにせよ、とにかくシステム全体として、現在の所72%の正確さ(accuracy)だった、と分かったとしよう。"
  },
  {
    "index": "F18899",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And from each test set images, we run it through text detection, then character segmentation, then character recognition. And we find that on our test set the overall accuracy of the entire system was 72% on whatever metric you chose.",
    "output": "言い換えると、我らはあるテストセットの画像を持っていて、それらテストセットの各画像にテキスト検出、文字分割、文字認識を、順番に走らせてそして我らのテストセットに対してはシステム全体に対して、あなたの選んだ指標に関して72%の正確さだと分かった、とする。"
  },
  {
    "index": "F18900",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now here's the idea behind ceiling analysis, which is that we're going to go through, let's say the first module of our machinery pipeline, say text detection.",
    "output": "さて、以下にシーリング分析のアイデアの背景を述べる。それは、我らは最初のモジュールを見てこの機械学習パイプラインの最初のモジュールはテキスト検出だ。"
  },
  {
    "index": "F18901",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And what we're going to do, is we're going to monkey around with the test set.",
    "output": "そして我らがやる事は、テストセットに細工をする事だ。"
  },
  {
    "index": "F18902",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "For every test example, which is going to provide it the correct text detection outputs, so in other words, we're going to go to the test set and just manually tell the algorithm where the text is in each of the test examples.",
    "output": "テストセットに直接おもむき、各テスト手本に対し正解のテキスト検出の出力を直接提供する。"
  },
  {
    "index": "F18903",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So in other words gonna simulate what happens if you have a text detection system with a hundred percent accuracy, for the purpose of detecting text in an image.",
    "output": "言い換えると、テストセットにおもむき、単に手動でアルゴリズムに各テスト手本のどこにテキストがあるかを伝える。"
  },
  {
    "index": "F18904",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And really the way you do that's pretty simple, right? Instead of letting your learning algorhtim detect the text in the images.",
    "output": "さらに言い換えると、我らがもし100%正確なテキスト検出のシステムがあったら何が起こるのかをシミュレートする訳だ。"
  },
  {
    "index": "F18905",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You wouldn't say go to the images and just manually label what is the location of the text in my test set image.",
    "output": "画像のテキスト検出の目的の為に。そしてそれを実際に行う方法はとてもシンプルだ。"
  },
  {
    "index": "F18906",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And you would then let these correct or let these ground truth labels of where is the text be part of your test set.",
    "output": "学習アルゴリズムに画像のテキストを検出さえる代わりに、あなたが直接画像に赴き人力でテストセットの画像のどこにテキストがあるかをラベル付けする。"
  },
  {
    "index": "F18907",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And just use these ground truth labels as what you feed in to the next stage of the pipeline, so the character segmentation pipeline.",
    "output": "そしてこれらを正解にした上で、つまりテストセットの画像の中のどこにテキストがあるかの完璧に正しいラベルを付けた上で、これらの完璧に正しいラベルを用いて次のステージのパイプラインに食わせる、つまり文字分割のパイプラインに。"
  },
  {
    "index": "F18908",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Okay? So just to say that again.",
    "output": "もう一度言おう。"
  },
  {
    "index": "F18909",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By putting a checkmark over here, what I mean is I'm going to go to my test set and just give it the correct answers.",
    "output": "このチェックマークは私がテストセットに実際におもむき、単純に正解の答えを与える、正しいラベルを与える、という事を意味している。"
  },
  {
    "index": "F18910",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Give it the correct labels for the text detection part of the pipeline.",
    "output": "パイプラインのテキスト検出の部分を。"
  },
  {
    "index": "F18911",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that as if I have a perfect test detection system on my test set.",
    "output": "そうする事で、まるでテストセットに対して完璧なテキスト検出を持っているフリをする為に。"
  },
  {
    "index": "F18912",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What we need to do then is run this data through the rest of the pipeline. Through character segmentation and character recognition.",
    "output": "そして次にやることはこのデータを残りのパイプライン、つまり文字分割と文字認識に流す。"
  },
  {
    "index": "F18913",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then use the same evaluation metric as before, to measure what was the overall accuracy of the entire system.",
    "output": "そして次に、以前と同様の評価指標を用いて、システム全体の正確さを計測する。"
  },
  {
    "index": "F18914",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And with perfect text detection, hopefully the performance will go up.",
    "output": "そして完璧なテキスト検出を用いるのだから、たぶんパフォーマンスは向上する事が期待される。"
  },
  {
    "index": "F18915",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this example, it goes up by by 89%.",
    "output": "ここでは89%に向上したとしよう。"
  },
  {
    "index": "F18916",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then we're gonna keep going, let's got o the next stage of the pipeline, so character segmentation. So again, I'm gonna go to my test set, and now I'm going to give it the correct text detection output and give it the correct character segmentation output.",
    "output": "そして次に、そのまま続けて、次のパイプラインのセクションに進み、文字分割に対して、またテストセットの画像におもむき、いまや正確なテキスト検出の出力が与えられている所に今度は正確な文字分割の出力を与える。"
  },
  {
    "index": "F18917",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So go to the test set and manually label the correct segmentations of the text into individual characters, and see how much that helps.",
    "output": "つまり手動で正確な文字の分割のラベル付けを行い個々の文字に分割する。そしてそれがどれだけ改善するかを見てみるのだ。"
  },
  {
    "index": "F18918",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And let's say it goes up to 90% accuracy for the overall system.",
    "output": "そして例えば、全体のシステムが90%の正確さに改善したとしよう。"
  },
  {
    "index": "F18919",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So as always the accuracy of the overall system.",
    "output": "つまり正確さといったらいつでもシステム全体の正確さだ。"
  },
  {
    "index": "F18920",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So is whatever the final output of the character recognition system is.",
    "output": "つまり文字認識システムの最終的なアウトプットが何かだ。"
  },
  {
    "index": "F18921",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whatever the final output of the overall pipeline, is going to measure the accuracy of that.",
    "output": "つまりパイプライン全体のアウトプットが何かだ。それが正確さの指標となる。"
  },
  {
    "index": "F18922",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally I'm going to build a character recognition system and give that correct labels as well, and if I do that too then no surprise I should get 100% accuracy.",
    "output": "そして最後に、文字認識システムにも正しいラベルを与える。もしそれをやったら、当たり前だが、100%の正確さを得られる。"
  },
  {
    "index": "F18923",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Now the nice thing about having done this analysis is, we can now understand what is the upside potential of improving each of these components?",
    "output": "今、この分析を行う良い点としては、我らはこれらの各コンポーネントを改善する時のポテンシャルの上限が理解出来る、という事だ。"
  },
  {
    "index": "F18924",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we see that if we get perfect text detection, our performance went up from 72 to 89%.",
    "output": "つまりもし、我らが完璧なテキスト検出を得たら、我らのパフォーマンスは72%から89%へと上昇する事が分かる。"
  },
  {
    "index": "F18925",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's a 17% performance gain.",
    "output": "つまり17%のパフォーマンス向上が得られる。"
  },
  {
    "index": "F18926",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this means that if we take our current system we spend a lot of time improving text detection, that means that we could potentially improve our system's performance by 17%.",
    "output": "つまり、あなたが現在のシステムに対してテキスト検出の改善にたくさんの時間を費やしたら、その場合は我らのシステムのパフォーマンスを17%向上出来る可能性がある、という事を意味している。"
  },
  {
    "index": "F18927",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It seems like it's well worth our while.",
    "output": "これはやる価値がありそうに見える。"
  },
  {
    "index": "F18928",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Whereas in contrast, when going from text detection when we gave it perfect character segmentation, performance went up only by 1%, so that's a more sobering message.",
    "output": "一方、対照的に、テキスト検出から離れて、完璧な文字分割を与えても、パフォーマンスはたったの1%しか向上しない。これはより信頼出来るメッセージだ。"
  },
  {
    "index": "F18929",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It means that no matter how much time you spend on character segmentation.",
    "output": "これの意味する所は、どれだけ文字分割に時間を費やそうとも、潜在的な上限は、とても小さい、という事だ。"
  },
  {
    "index": "F18930",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe the upside potential is going to be pretty small, and maybe you do not want to have a large team of engineers working on character segmentation.",
    "output": "だからきっと、あなたは大きなエンジニアのチームをこの文字分割の仕事に従事させたい、とは思わないだろう。"
  },
  {
    "index": "F18931",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This sort of analysis shows that even when you give it the perfect character segmentation, you performance goes up by only one percent.",
    "output": "この種の分析は、仮に完璧な文字分割を与えられたとしても、パフォーマンスは1%しか向上しない、という事を示してくれる。"
  },
  {
    "index": "F18932",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "That really estimates what is the ceiling, or what is an upper bound on how much you can improve the performance of your system and working on one of these components.",
    "output": "何が天井(シーリング)なのか、何が上限なのか。これらのうちの一つのコンポーネントに対し作業を行った時に、どれだけシステムのパフォーマンスを改善出来るか?"
  },
  {
    "index": "F18933",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And finally, going from character, when we get better character recognition with the forms went up by ten percent.",
    "output": "そして最後に、文字まで行って、より良い文字認識を得たら、パフォーマンスはさらに10%上がる。"
  },
  {
    "index": "F18934",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So again you can decide is ten percent improvement, how much is worth your while?",
    "output": "つまり、ここでも10%の改善に、どれだけ時間を使うか、を決める事が出来る。"
  },
  {
    "index": "F18935",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This tells you that maybe with more effort spent on the last stage of the pipeline, you can improve the performance of the systems as well.",
    "output": "これはパイプラインの終着駅に、もっと労力を集中すべき、と言っているかもしれない。システム全体のパフォーマンスも改善出来る。"
  },
  {
    "index": "F18936",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Another way of thinking about this, is that by going through these sort of analysis you're trying to think about what is the upside potential of improving each of these components.",
    "output": "これについての別の考え方としては、この種の分析を行っていく事で、これらの各コンポーネントを改善した時の上限のポテンシャルを調べている、と考えても良い。"
  },
  {
    "index": "F18937",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Or how much could you possibly gain if one of these components became absolutely perfect?",
    "output": "あるいは、これらのコンポーネントが一つ究極的に完璧になったら、どれだけの物が得られるか、を調べていると考えても良い。"
  },
  {
    "index": "F18938",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And this really places an upper bound on the performance of that system.",
    "output": "これでシステムのパフォーマンスの上限が設定出来る。"
  },
  {
    "index": "F18939",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the idea of ceiling analysis is pretty important, let me just answer this idea again but with a different example but more complex one.",
    "output": "シーリング分析のアイデアはとても重要だ。このアイデアを再び、もっと複雑な別の例で例示しよう。"
  },
  {
    "index": "F18940",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say that you want to do face recognition from images.",
    "output": "あなたは画像の顔認識をしたいとしよう。"
  },
  {
    "index": "F18941",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "You want to look at the picture and recognize whether or not the person in this picture is a particular friend of yours, and try to recognize the person Shown in this image.",
    "output": "つまり写真を見て、この写真の人があなたの特定の友人かどうかを認識したい。この画像にいる人物を認識したいとする。"
  },
  {
    "index": "F18942",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "This is a slightly artificial example, this isn't actually how face recognition is done in practice.",
    "output": "これはちょっと人工的な例で、実際の現場で顔認識がどう行われているか、というのを正確に反映した物では無い。"
  },
  {
    "index": "F18943",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But we're going to set for an example, what a pipeline might look like to give you another example of how a ceiling analysis process might look.",
    "output": "だがシーリング分析のプロセスがどんな感じになるか、という例をもう一つ挙げる為にこのパイプラインがどうなっているのかを見ていきたい。"
  },
  {
    "index": "F18944",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So we have a camera image, and let's say that we design a pipeline as follows, the first thing you wanna do is pre-processing of the image.",
    "output": "さて、我らはカメラの画像を持っていて、以下のようなパイプラインをデザインするとしよう。最初にやるのは画像の前処理としよう。"
  },
  {
    "index": "F18945",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So let's take this image like we have shown on the upper right, and let's say we want to remove the background.",
    "output": "この右上に見せたような画像を取ってきて、そして背景を除去したい、としてみよう。"
  },
  {
    "index": "F18946",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So do pre-processing and the background disappears.",
    "output": "つまり前処理を通す事で背景が消える。"
  },
  {
    "index": "F18947",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Next we want to say detect the face of the person, that's usually done on the learning So we'll run a sliding Windows crossfire to draw a box around a person's face.",
    "output": "次に人の顔を認識したい、とする。これは通常学習アルゴリズムを用いて行われる。"
  },
  {
    "index": "F18948",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Having detected the face, it turns out that if you want to recognize people, it turns out that the eyes is a highly useful cue.",
    "output": "一旦顔を認識出来たら人を識別する為には、目というのはとても有力な手がかりだと分かってる。"
  },
  {
    "index": "F18949",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We actually are, in terms of recognizing your friends the appearance of their eyes is actually one of the most important cues that you use.",
    "output": "我らは実際の所、友人を認識する時に、目がどんな見た目かというのは、実際にもっとも重視している手がかりだ。"
  },
  {
    "index": "F18950",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So lets run another crossfire to detect the eyes of the person.",
    "output": "だから人間の目を検出する為の別の分類器を走らせよう。"
  },
  {
    "index": "F18951",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So the segment of the eyes and then since this will give us useful features to recognize the person.",
    "output": "目の部分を切りだして、というのは人物を認識するのにこれは有用なフィーチャー(特徴)だからだが、そして次に、顔の他の部分で使えそうな所、例えば鼻を切り出す。"
  },
  {
    "index": "F18952",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Maybe segment of the nose, segment of the mouth.",
    "output": "そして口を切り出す。"
  },
  {
    "index": "F18953",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then having found the eyes, the nose, and the mouth, all of these give us useful features to maybe feed into a logistic regression classifier.",
    "output": "そして次に、目、鼻、口を見つけた後に、これら全てが恐らく、ロジスティック回帰などの分類器に食わせるのに有用なフィーチャーとなる。"
  },
  {
    "index": "F18954",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And there's a job with a cost priority, they'd give us the overall label, to find the label for who we think is the identity of this person.",
    "output": "そしてこの分類器の仕事は、我らに全体として、この人物が誰だと思っているかのラベルを与える。"
  },
  {
    "index": "F18955",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is a kind of complicated pipeline, it's actually probably more complicated than you should be using if you actually want to recognize people, but there's an illustrative example that's useful to think about for ceiling analysis.",
    "output": "もし実際に人を認識したいなら。だがシーリング分析を考えてみる上では示唆に富む例だ。"
  },
  {
    "index": "F18956",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So how do you go through ceiling analysis for this pipeline.",
    "output": "ではどうやってこのパイプラインのシーリング分析を行えば良いだろう?"
  },
  {
    "index": "F18957",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Well se step through these pieces one at a time.",
    "output": "これらのピースを一度に一つづつ見ていこう。"
  },
  {
    "index": "F18958",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Let's say your overall system has 85% accuracy.",
    "output": "あなたのシステムが全体として85%の正確さを持つとしよう。"
  },
  {
    "index": "F18959",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The first thing I do is go to my test set and manually give it the full background segmentation.",
    "output": "最初にやる事は、テストセットにおもむき、そして手動で前景と背景の分割を行う、という事。"
  },
  {
    "index": "F18960",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So manually go to the test set.",
    "output": "つまりテストセットにおもむきPhotoshopなりなんなりを用いてどれが背景かを伝えて、そして手動で背景を除去する。"
  },
  {
    "index": "F18961",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And use Photoshop or something to just tell it where's the background and just manually remove the graph background, so this is a ground true background, and see how much the accuracy changes.",
    "output": "つまり完全に正しい背景で、どれだけ正確さが向上するかを見る。"
  },
  {
    "index": "F18962",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "In this example the accuracy goes up by 0.1%.",
    "output": "この例では、正確さは0.1%向上した。"
  },
  {
    "index": "F18963",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So this is a strong sign that even if you have perfect background segmentation, the form is, even with perfect background removal the performance or your system isn't going to go up that much.",
    "output": "つまりこれは、強烈なサインとなる。たとえ完璧な背景分離が、本当に完璧な背景除去であったとしても、あなたのシステムのパフォーマンスは大して向上しない。"
  },
  {
    "index": "F18964",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So it's maybe not worth a huge effort to work on pre-processing on background removal.",
    "output": "これはつまり、前処理の背景除去にはこれ以上莫大な労力を投入する価値は無い、という事だろう。"
  },
  {
    "index": "F18965",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Then quickly goes to test set give it the correct face detection images then again step though the eyes nose and mouth segmentation in some order just pick one order.",
    "output": "次に、またテストセットにおもむき、正確な顔検出の領域を与えて、また次に目、鼻、口の分割と順番に見ていく。順番を一つ選ぶ。"
  },
  {
    "index": "F18966",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Just give the correct location of the eyes.",
    "output": "目の正確な位置を与えて、鼻の正確な位置を与えて、口の正確な位置を与えて、そして最後に全体として正解のラベルを与えると、100%正確となる。"
  },
  {
    "index": "F18967",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Correct location in noses, correct location in mouth, and then finally if I just give it the correct overall label I can get 100% accuracy.",
    "output": "つまり、システムを順番に見ていってどんどん各コンポーネントにテストセットの正解のラベルを与えていくと、パフォーマンス、、、つまりシステム全体のパフォーマンスは向上する。"
  },
  {
    "index": "F18968",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so as I go through the system and just give more and more components, the correct labels in the test set, the performance of the overall system goes up and you can look at how much the performance went up on different steps.",
    "output": "そして各別々のステップでどれだけパフォーマンスが向上するかを見ていく事が出来る。"
  },
  {
    "index": "F18969",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So from giving it the perfect face detection, it looks like the overall performance of the system went up by 5.9%.",
    "output": "完璧な顔検出が提供されると、システム全体のパフォーマンスは5.9%向上する。"
  },
  {
    "index": "F18970",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So that's a pretty big jump.",
    "output": "これはかなり大きなジャンプだ。"
  },
  {
    "index": "F18971",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "It means that maybe it's worth quite a bit effort on better face detection.",
    "output": "つまり、顔検出を改善するのに、かなりの労力を費やす価値がありそうだ。"
  },
  {
    "index": "F18972",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "1% there, and 3% there. So it looks like the components that most work are while are, when I gave it perfect face detection system went up by 5.9 performance when given perfect eyes segmentation went to four percent.",
    "output": "つまり、どうやら最も我らが頑張るに値するコンポーネントは、完璧な顔検出を提供出来たらシステムは5.9%向上し、完璧な目の分割を行えたら4%向上する。"
  },
  {
    "index": "F18973",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And then my final which is cost for well there's another three percent, gap there maybe.",
    "output": "そして最後のロジスティック分類器は、またさらに3%のギャップがある。つまりこうして、我らが取り組む価値がもっともありそうなコンポーネントを教えてくれる。"
  },
  {
    "index": "F18974",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And so this tells maybe whether the components are most worthwhile working on.",
    "output": "ところで、これは本当にあった話なのだが、このプリプロセスの背景除去をここに含めた理由は、現実にこんな話があったのを知っているからだ。"
  },
  {
    "index": "F18975",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And by the way I want to tell you a true cautionary story.",
    "output": "分かるだろ?だがとにかく、コンピュータビジョンのアプリケーションがあってさ。"
  },
  {
    "index": "F18976",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The reason I put this is in this in preprocessing background removal is because I actually know of a true story where there was a research team that actually literally had to people spend about a year and a half, spend 18 months working on better background removal.",
    "output": "二人のエンジニアのチームが文字通り1.5年間、背景除去の仕事に従事したのだ。実際に彼らはとても複雑なアルゴリズムを用いて最終的には1本の研究論文まで出した。"
  },
  {
    "index": "F18977",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But actually I'm obscuring the details for obvious reasons, but there was a computer vision application where there's a team of two engineers that literally spent about a year and a half working on better background removal, actually worked out really complicated algorithms and ended up publishing one research paper.",
    "output": "だがそれらの仕事を終えた後になって彼らは気づいたのだ。彼らの実際のアプリケーションに関しては、その違いは全体のパフォーマンスについては大した違いを生まない、という事を。"
  },
  {
    "index": "F18978",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But after all that work they found that it just did not make huge difference to the overall performance of the actual application they were working on and if only someone were to do ceiling analysis before hand maybe they could have realized.",
    "output": "そしてお分かりの通り、もし誰かが、もし仮にシーリング分析を前もって行っていたら、この事が分かったと思われる。"
  },
  {
    "index": "F18979",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And one of them said to me afterward.",
    "output": "そして彼らの一人が後になってこう言った。"
  },
  {
    "index": "F18980",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "If only you've did this sort of analysis like this maybe they could have realized before their 18 months of work.",
    "output": "もしこの種の分析をやっていたら、たぶん彼らは18ヶ月の仕事をする前に、彼らが文字通り18ヶ月の仕事を背景除去に従事する前に彼らの労力を別のコンポーネントに注ぐべきだと気づけたんじゃないかな。"
  },
  {
    "index": "F18981",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So to summarize, pipelines are pretty pervasive in complex machine learning applications.",
    "output": "まとめよう。パイプラインは複雑な機械学習のアプリケーションにおいては広く使われている物だ。"
  },
  {
    "index": "F18982",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And when you're working on a big machine learning application, your time as developer is so valuable, so just don't waste your time working on something that ultimately isn't going to matter.",
    "output": "そして大きな機械学習のアプリケーションに従事している時は、あなたのデベロッパーとしての時間はあまりにも貴重だ。だからあなたは、あまり重要では無い事に多くの時間を費やしてはならない。"
  },
  {
    "index": "F18983",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And in this video we'll talk about this idea of ceiling analysis, which I've often found to be a very good tool for identifying the component of a video as you put focus on that component and make a big difference.",
    "output": "そしてこのビデオで、我らはシーリング分析のアイデアを議論してきた。この手法は作業すべきコンポーネントを見つけ出すのに、とても便利なツールだと、しばしば思う物だ。"
  },
  {
    "index": "F18984",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Will actually have a huge effect on the overall performance of your final system.",
    "output": "そしてあなたがそのコンポーネントに労力を集中して、大きく改善出来たら、最終的なシステム全体にも巨大な影響を与えるだろう。"
  },
  {
    "index": "F18985",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So over the years working machine learning, I've actually learned to not trust my own gut feeling about what components to work on.",
    "output": "何年も機械学習の仕事をしてきて、私は本当にどのコンポーネントに取り組むべきかの自分自身の直感を、あまり信じてはいけない、という事を学んできた。"
  },
  {
    "index": "F18986",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So very often, I've work on machine learning for a long time, but often I look at a machine learning problem, and I may have some gut feeling about oh, let's jump on that component and just spend all the time on that.",
    "output": "つまり、ほんとうに良く、長い期間機械学習の仕事をしていると、機械学習の問題を見た時に、直感がこう言う事がある:このコンポーネントにもっと時間を注ぎ込んだらいいんじゃないか、と。"
  },
  {
    "index": "F18987",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "But over the years, I've come to even trust my own gut feelings and learn not to trust gut feelings that much.",
    "output": "だが何年もかかって、自分自身の直感すら信じてはいけない、むしろ自分の直感をそれほどは信じない、という事を学習した。"
  },
  {
    "index": "F18988",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And instead, if you have a sort of machine learning problem where it's possible to structure things and do a ceiling analysis, often there's a much better and much more reliable way for deciding where to put a focused effort, to really improve the performance of some component.",
    "output": "シーリング分析などはしばしば、どこに労力を集中すべきかを決めるもっと良い、そしてもっと信頼出来る方法だ。それによってどこのコンポーネントのパフォーマンスを実際に改善すべきかが分かる。"
  },
  {
    "index": "F18989",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "And be kind of reassured that, when you do that, it won't actually have a huge effect on the final performance of the overall system.",
    "output": "そして実際にそれを行えば、全体的なシステムの最終的なパフォーマンスが実際に大きく改善する事を確認出来るだろう。"
  },
  {
    "index": "F18990",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Hello, and welcome back to the second module of advanced competitive strategy.",
    "output": "こんにちは、競争戦略上級クラスへようこそ。"
  },
  {
    "index": "F18991",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "We're on board the Ludwig Fessler a side-wheeler motor vessel on Lake Chiemsee, and we're going to talk about price discrimination.",
    "output": "私は、ミュンヘンのルドウィグ-マキシミリアン大学の戦略、テクノロジー、組織の教授をしているトビアス・クレチメルです。私たちはあたらしいコースという冒険を始めようとしています。"
  },
  {
    "index": "F18992",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "So, if you want to do a boat trip with them, you will have to pay a different price depending on whether you're an adult, whether you're a child from 16 to 15 years of age, part of a family consisting of two adults and two children, or whether you're travelling in a group of at least 20 people.",
    "output": "それぞれの異なる7つのモジュールは約10のビデオからなっており、それぞれに、一つか二つのクイズがついています。そこで皆さんが新しく学んだ知識をすぐにテストすることができます。"
  },
  {
    "index": "F18993",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "At the same time, you also pay different prices, that depend on the services that you require.",
    "output": "もちろん、これらは楽しくなくてはいけません。それぞれのモジュールの最後に長い小テストがあります。"
  },
  {
    "index": "F18994",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "By getting a combined ticket for both, the boat trip and the steam engine, you essentially pay less than if you purchase the two tickets individually. So that sounds pretty complicated right?",
    "output": "つまり、皆さんが7つのすべてのモジュールを終えると自動的に最終試験を受けているのです。"
  },
  {
    "index": "F18995",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Watch the following videos, if you want to find out more about these strategic advantages and benefits, that you can achieve by implementing a suitable price discrimination strategy.",
    "output": "そして、私たちのフォーラムを確認してください。なぜなら、クラスメイトとつながって、スタディグループなどを作る素晴らしい機会だからです。"
  },
  {
    "index": "F18996",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "Hello, I'm Keith Devlin. Welcome to this online course on mathematical thinking.",
    "output": "こんにちはキース・ダブリンですようこそ数学的思考に関するこのオンラインコースへ。"
  },
  {
    "index": "F18997",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "The goal of the course is to help you develop a valuable mental ability, a powerful way of thinking that people have developed over 3,000 years.",
    "output": "3,000年以上にわたって発展してきた強力な考え方、かつ貴重な精神能力を育成していきましょう。"
  },
  {
    "index": "F18998",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "What I want to do today is get you ready for the course and tell you a little bit about the way the course will work.",
    "output": "今日は、学習の準備についてと、授業の流れについての、2つのことをお話しします。"
  },
  {
    "index": "F18999",
    "instruction": "次の英語を日本語に翻訳してください。",
    "input": "I'm doing this because for most of you, this will be a very different perspective on what mathematics is.",
    "output": "なぜなら、このコースの数学に関する見解は、大抵の人のそれと、大きく異なるからです。"
  }
]